{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40b48e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel, utils\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification,Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "transformers.logging.set_verbosity_error()\n",
    "utils.logging.set_verbosity_error()  # Suppress standard warnings\n",
    "\n",
    "from bertviz import model_view, head_view\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "import eli5\n",
    "from eli5.lime import TextExplainer\n",
    "# from captum.attr import IntegratedGradients\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c860b88",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57f8c831",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train_sentence_data.csv')\n",
    "test_data = pd.read_csv('test_sentence_data.csv')\n",
    "val_data = pd.read_csv('val_sentence_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5b3249e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.drop(['File_id'],axis=1)\n",
    "test_data = test_data.drop(['File_id'],axis=1)\n",
    "val_data = val_data.drop(['File_id'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84a0a591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data=train_data.sample(frac = 1)\n",
    "# test_data=test_data.sample(frac = 1)\n",
    "# val_data=val_data.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47fe1544",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_20(data):\n",
    "    data_1 = data.loc[data['Status'] ==1].iloc[:10]\n",
    "    data_2 = data.loc[data['Status'] ==0].iloc[:10]\n",
    "    frames = [data_1, data_2]\n",
    "    return pd.concat(frames)\n",
    "\n",
    "train_data = get_20(train_data) \n",
    "test_data = get_20(test_data)\n",
    "val_data = get_20(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "276c5e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('Train_auto.csv',index=False)\n",
    "test_data.to_csv('Test_auto.csv',index=False)\n",
    "val_data.to_csv('val_auto.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0537e12d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    10\n",
       "0    10\n",
       "Name: Status, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.Status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "934f6dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /Users/admin/.cache/huggingface/datasets/csv/default-2f8aa1df66ec4f51/0.0.0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f348b9a29d04f7a89cb7a71d4157b91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97316ea84cc9456c91b82fc6116cec68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /Users/admin/.cache/huggingface/datasets/csv/default-2f8aa1df66ec4f51/0.0.0. Subsequent calls will reuse this data.\n",
      "Downloading and preparing dataset csv/default to /Users/admin/.cache/huggingface/datasets/csv/default-8386e1c1a2d603cd/0.0.0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4f23a19a478426f951d1eb62d4f2675",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad1b9427b05f4fdeac600ad27921a69d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /Users/admin/.cache/huggingface/datasets/csv/default-8386e1c1a2d603cd/0.0.0. Subsequent calls will reuse this data.\n",
      "Downloading and preparing dataset csv/default to /Users/admin/.cache/huggingface/datasets/csv/default-25a2f21b0e0aafd6/0.0.0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc06ecaf92e14ac986f980ed99379571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b6b1e12332047e1a397320c2baa1680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /Users/admin/.cache/huggingface/datasets/csv/default-25a2f21b0e0aafd6/0.0.0. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "# Load train and validation datasets from CSV files\n",
    "train_dataset = Dataset.from_csv('Train_auto.csv')\n",
    "val_dataset = Dataset.from_csv('val_auto.csv')\n",
    "test_dataset = Dataset.from_csv('Test_auto.csv')\n",
    "\n",
    "# Rename the columns to 'text' and 'label' to match the expected format for sequence classification\n",
    "train_dataset = train_dataset.rename_column('Sentence', 'text').rename_column('Status', 'label')\n",
    "val_dataset = val_dataset.rename_column('Sentence', 'text').rename_column('Status', 'label')\n",
    "test_dataset = test_dataset.rename_column('Sentence', 'text').rename_column('Status', 'label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28ade24e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/allenai-specter')\n",
    "\n",
    "# Define a function to tokenize the text and create input sequences\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding=True, truncation=True,max_length=512)\n",
    "\n",
    "# Apply the tokenization function to the train and validation datasets\n",
    "train_dataset = train_dataset.map(tokenize, batched=True, batch_size=len(train_dataset))\n",
    "val_dataset = val_dataset.map(tokenize, batched=True, batch_size=len(val_dataset))\n",
    "test_dataset = test_dataset.map(tokenize, batched=True, batch_size=len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08972c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained('sentence-transformers/allenai-specter', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3170f624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results/specter',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fbaf12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/anaconda3/envs/Newra/lib/python3.7/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 31.9613, 'train_samples_per_second': 1.877, 'train_steps_per_second': 0.188, 'train_loss': 0.7047828038533529, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6, training_loss=0.7047828038533529, metrics={'train_runtime': 31.9613, 'train_samples_per_second': 1.877, 'train_steps_per_second': 0.188, 'train_loss': 0.7047828038533529, 'epoch': 3.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                     # the instantiated Transformers model to be trained\n",
    "    args=training_args,              # training arguments, defined above\n",
    "    train_dataset=train_dataset,     # training dataset\n",
    "    eval_dataset=val_dataset,        # evaluation dataset\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "827aa56d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7017756104469299, 'eval_runtime': 2.9702, 'eval_samples_per_second': 6.734, 'eval_steps_per_second': 0.337, 'epoch': 3.0}\n",
      "{'eval_loss': 0.7017756104469299, 'eval_runtime': 2.9702, 'eval_samples_per_second': 6.734, 'eval_steps_per_second': 0.337, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate(test_dataset)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d03f6618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_adapter(texts: List[str]):\n",
    "    \n",
    "    all_scores = []\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), 64)):\n",
    "\n",
    "        batch = texts[i:i+64]\n",
    "        #print(batch)\n",
    "        \n",
    "        # use bert encoder to tokenize text \n",
    "        encoded_input = tokenizer(batch, \n",
    "          return_tensors='pt', \n",
    "          padding=True, \n",
    "          truncation=True, \n",
    "          max_length=model.config.max_position_embeddings-2)\n",
    "\n",
    "        # run the model\n",
    "        output = model(**encoded_input)\n",
    "        #print(output)\n",
    "        # by default this model gives raw logits rather \n",
    "        # than a nice smooth softmax so we apply it ourselves here\n",
    "        scores = output[0].softmax(1).detach().numpy()\n",
    "        #print(scores)\n",
    "\n",
    "        all_scores.extend(scores)\n",
    "\n",
    "    return np.array(all_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19876226",
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = test_data.Sentence\n",
    "lab = test_data.Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd968cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [03:01<00:00,  2.29s/it]\n",
      "/Users/admin/anaconda3/envs/Newra/lib/python3.7/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        \n",
       "\n",
       "    \n",
       "\n",
       "        \n",
       "\n",
       "        \n",
       "    \n",
       "        \n",
       "        \n",
       "    \n",
       "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
       "            <b>\n",
       "    \n",
       "        y=LABEL_1\n",
       "    \n",
       "</b>\n",
       "\n",
       "    \n",
       "    (probability <b>0.553</b>, score <b>0.213</b>)\n",
       "\n",
       "top features\n",
       "        </p>\n",
       "    \n",
       "    <table class=\"eli5-weights\"\n",
       "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
       "        <thead>\n",
       "        <tr style=\"border: none;\">\n",
       "            \n",
       "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature contribution already accounts for the feature value (for linear models, contribution = weight * feature value), and the sum of feature contributions is equal to the score or, for some classifiers, to the probability. Feature values are shown if &quot;show_feature_values&quot; is True.\">\n",
       "                    Contribution<sup>?</sup>\n",
       "                </th>\n",
       "            \n",
       "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "            \n",
       "        </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.122\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        &lt;BIAS&gt;\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 83.76%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.091\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        Highlighted in text (sum)\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "        \n",
       "\n",
       "        \n",
       "        \n",
       "\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "    <p style=\"margin-bottom: 2.5em; margin-top:-0.5em;\">\n",
       "        <span style=\"background-color: hsl(0, 100.00%, 89.89%); opacity: 0.83\" title=\"-0.023\">training</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 88.86%); opacity: 0.83\" title=\"0.026\">neural</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 88.61%); opacity: 0.83\" title=\"-0.027\">networks</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 87.17%); opacity: 0.84\" title=\"-0.032\">to</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 60.00%); opacity: 1.00\" title=\"-0.162\">synthesize</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 79.98%); opacity: 0.87\" title=\"0.060\">robust</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 86.89%); opacity: 0.84\" title=\"-0.033\">programs</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 86.92%); opacity: 0.84\" title=\"-0.033\">from</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 78.38%); opacity: 0.88\" title=\"-0.067\">a</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 88.97%); opacity: 0.83\" title=\"-0.026\">small</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.55%); opacity: 0.81\" title=\"0.012\">number</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 89.88%); opacity: 0.83\" title=\"-0.023\">of</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 81.17%); opacity: 0.87\" title=\"-0.055\">examples</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 86.07%); opacity: 0.84\" title=\"-0.036\">is</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 75.16%); opacity: 0.90\" title=\"-0.082\">a</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 68.15%); opacity: 0.94\" title=\"0.117\">challenging</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 83.31%); opacity: 0.86\" title=\"-0.047\">task</span>\n",
       "    </p>\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "Explanation(estimator=\"SGDClassifier(alpha=0.001, loss='log', penalty='elasticnet',\\n              random_state=RandomState(MT19937) at 0x7FB74CAD3EB8)\", description=None, error=None, method='linear model', is_regression=False, targets=[TargetExplanation(target='LABEL_1', feature_weights=FeatureWeights(pos=[FeatureWeight(feature='robust', weight=0.1919211653508533, std=None, value=1.0), FeatureWeight(feature='challenging', weight=0.16256223547061088, std=None, value=1.0), FeatureWeight(feature='a', weight=0.1223719069819177, std=None, value=2.0), FeatureWeight(feature='<BIAS>', weight=0.12237137467381633, std=None, value=1.0), FeatureWeight(feature='is', weight=0.11717365026855764, std=None, value=1.0), FeatureWeight(feature='small', weight=0.08394172617232439, std=None, value=1.0), FeatureWeight(feature='number', weight=0.07711881715977875, std=None, value=1.0), FeatureWeight(feature='to', weight=0.0725087879799271, std=None, value=1.0), FeatureWeight(feature='training neural', weight=0.060674885886275454, std=None, value=1.0), FeatureWeight(feature='neural networks', weight=0.04694544689000681, std=None, value=1.0), FeatureWeight(feature='of', weight=0.02840866425757574, std=None, value=1.0)], neg=[FeatureWeight(feature='synthesize robust', weight=-0.13160408084146333, std=None, value=1.0), FeatureWeight(feature='is a', weight=-0.09782084354586548, std=None, value=1.0), FeatureWeight(feature='a small', weight=-0.09568707029087749, std=None, value=1.0), FeatureWeight(feature='training', weight=-0.08342249025457367, std=None, value=1.0), FeatureWeight(feature='neural', weight=-0.08151509415631991, std=None, value=1.0), FeatureWeight(feature='networks to', weight=-0.07388587315934408, std=None, value=1.0), FeatureWeight(feature='examples is', weight=-0.05528529206661054, std=None, value=1.0), FeatureWeight(feature='number of', weight=-0.05117627624608565, std=None, value=1.0), FeatureWeight(feature='task', weight=-0.046543875915085124, std=None, value=1.0), FeatureWeight(feature='a challenging', weight=-0.045458923602536276, std=None, value=1.0), FeatureWeight(feature='programs', weight=-0.032953095517108566, std=None, value=1.0), FeatureWeight(feature='from a', weight=-0.0328334807120015, std=None, value=1.0), FeatureWeight(feature='to synthesize', weight=-0.030566130217806582, std=None, value=1.0), FeatureWeight(feature='small number', weight=-0.01398722714014193, std=None, value=1.0)], pos_remaining=0, neg_remaining=0), proba=0.5531135816242004, score=0.21325890742582393, weighted_spans=WeightedSpans(docs_weighted_spans=[DocWeightedSpans(document='training neural networks to synthesize robust programs from a small number of examples is a challenging task', spans=[('training', [(0, 8)], -0.08342249025457367), ('neural', [(9, 15)], -0.08151509415631991), ('to', [(25, 27)], 0.0725087879799271), ('robust', [(39, 45)], 0.1919211653508533), ('programs', [(46, 54)], -0.032953095517108566), ('a', [(60, 61)], 0.1223719069819177), ('small', [(62, 67)], 0.08394172617232439), ('number', [(68, 74)], 0.07711881715977875), ('of', [(75, 77)], 0.02840866425757574), ('is', [(87, 89)], 0.11717365026855764), ('a', [(90, 91)], 0.1223719069819177), ('challenging', [(92, 103)], 0.16256223547061088), ('task', [(104, 108)], -0.046543875915085124), ('training neural', [(0, 8), (9, 15)], 0.060674885886275454), ('neural networks', [(9, 15), (16, 24)], 0.04694544689000681), ('networks to', [(16, 24), (25, 27)], -0.07388587315934408), ('to synthesize', [(25, 27), (28, 38)], -0.030566130217806582), ('synthesize robust', [(28, 38), (39, 45)], -0.13160408084146333), ('from a', [(55, 59), (60, 61)], -0.0328334807120015), ('a small', [(60, 61), (62, 67)], -0.09568707029087749), ('small number', [(62, 67), (68, 74)], -0.01398722714014193), ('number of', [(68, 74), (75, 77)], -0.05117627624608565), ('examples is', [(78, 86), (87, 89)], -0.05528529206661054), ('is a', [(87, 89), (90, 91)], -0.09782084354586548), ('a challenging', [(90, 91), (92, 103)], -0.045458923602536276)], preserve_density=False, vec_name=None)], other=FeatureWeights(pos=[FeatureWeight(feature='<BIAS>', weight=0.12237137467381633, std=None, value=1.0), FeatureWeight(feature=<FormattedFeatureName 'Highlighted in text (sum)'>, weight=0.09088753275200753, std=None, value=None)], neg=[], pos_remaining=0, neg_remaining=0)), heatmap=None)], feature_importances=None, decision_tree=None, highlight_spaces=None, transition_features=None, image=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specter_exp = TextExplainer(n_samples=5000, random_state=42)\n",
    "specter_exp.fit(sen[0], model_adapter)\n",
    "specter_exp.explain_prediction(target_names=list(model.config.id2label.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71b9234d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/anaconda3/envs/Newra/lib/python3.7/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "weights = specter_exp.explain_weights()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96fdbc8",
   "metadata": {},
   "source": [
    "## very inportant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb033216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(weights.targets[0].feature_weights.pos[0].weight,weights.targets[0].feature_weights.pos[0].feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6db1c5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = specter_exp.explain_prediction()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcca8b51",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd7b8264",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./complete_sentence/train_processed_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76e95344",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_20(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3629f635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_id</th>\n",
       "      <th>Paper_text</th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>304.pdf.json</td>\n",
       "      <td>training neural networks to synthesize robust ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>305.pdf.json</td>\n",
       "      <td>data compression is a fundamental and well-stu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>306.pdf.json</td>\n",
       "      <td>deep learning has shown great success in a var...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>307.pdf.json</td>\n",
       "      <td>the most useful applications of dialog systems...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>308.pdf.json</td>\n",
       "      <td>generative adversarial networks (gans)(goodfel...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        File_id                                         Paper_text  Status\n",
       "0  304.pdf.json  training neural networks to synthesize robust ...       1\n",
       "1  305.pdf.json  data compression is a fundamental and well-stu...       1\n",
       "2  306.pdf.json  deep learning has shown great success in a var...       1\n",
       "3  307.pdf.json  the most useful applications of dialog systems...       1\n",
       "4  308.pdf.json  generative adversarial networks (gans)(goodfel...       1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e572c6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = data.Paper_text.values.tolist()\n",
    "status = data.Status.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d967849",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Padding(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self,model,tokenizer):\n",
    "        self.model = model # Configure model to return attention values\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mxlenght = 400\n",
    "        print('\\n>>>>>>>init() called.\\n')\n",
    "\n",
    "    def fit(self, X, y = None):\n",
    "        print('\\n>>>>>>>fit() called.\\n')\n",
    "        return self\n",
    "    \n",
    "\n",
    "    def transform(self, X, y = None):\n",
    "        embeddings = []\n",
    "        for i in tqdm(X):\n",
    "            temp = [] \n",
    "            sentence_list = i.split(\".\")\n",
    "            for i in sentence_list:\n",
    "                if len(i)==0:\n",
    "                    continue\n",
    "                encoded_input = tokenizer(i,return_tensors='pt', padding=True, truncation=True,max_length=model.config.max_position_embeddings-2)\n",
    "                output = model(**encoded_input)\n",
    "                pred = np.argmax(output[0].softmax(1).detach().numpy())\n",
    "                temp.append(pred)\n",
    "            size = self.mxlenght - len(temp)\n",
    "            if size > 0:\n",
    "                temp.extend([-1]*size)\n",
    "            elif size < 0:\n",
    "                temp = temp[0:self.mxlenght]\n",
    "            else:\n",
    "                pass\n",
    "            embeddings.append(temp)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "03828a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>>>>>>init() called.\n",
      "\n",
      "\n",
      ">>>>>>>fit() called.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [37:37<00:00, 112.86s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('Documnet Embeddings',\n",
       "                 Padding(model=BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(31116, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inpl...\n",
       "                         tokenizer=BertTokenizerFast(name_or_path='sentence-transformers/allenai-specter', vocab_size=31116, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True))),\n",
       "                ('Logistic Regression', LogisticRegression())])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe1 = Pipeline(steps=[\n",
    "                       ('Documnet Embeddings', Padding(model,tokenizer)), # this will trigger a call to __init__\n",
    "                       ('Logistic Regression', LogisticRegression(solver='lbfgs')),\n",
    "\n",
    "])\n",
    "\n",
    "pipe1.fit(text, status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e54d1c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "status[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0577d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██████████████████████▉                                                                     | 1249/5000 [15:10:14<32:31:38, 31.22s/it]"
     ]
    }
   ],
   "source": [
    "target =['Reject','Accept']\n",
    "doc = text[0]\n",
    "pipe1_exp = TextExplainer(n_samples=5000,random_state=42)\n",
    "pipe1_exp.fit(doc, pipe1.predict_proba)\n",
    "pipe1_exp.show_prediction(target_names= target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370cf7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = exp.explain_weights()\n",
    "positivie_words= {}\n",
    "\n",
    "for i in weights.targets[0].feature_weights.pos:\n",
    "    #print(i.feature)\n",
    "    g = positivie_words.get(i.feature,-1)\n",
    "    if g==-1:\n",
    "        positivie_words[i.feature]=1\n",
    "    else:\n",
    "        positivie_words[i.feature]+=1\n",
    "        \n",
    "\n",
    "negative_words= {}\n",
    "\n",
    "for i in weights.targets[0].feature_weights.neg:\n",
    "    #print(i.feature)\n",
    "    g = negative_words.get(i.feature,-1)\n",
    "    if g==-1:\n",
    "        negative_words[i.feature]=1\n",
    "    else:\n",
    "        negative_words[i.feature]+=1\n",
    "# print(weights.targets[0].feature_weights.pos[0].weight,weights.targets[0].feature_weights.pos[0].feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd46330",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(background_color=\"white\",width=1000,height=1000,relative_scaling=0.5,normalize_plurals=False).generate_from_frequencies(positivie_words)\n",
    "plt.imshow(wc)\n",
    "\n",
    "wc = WordCloud(background_color=\"white\",width=1000,height=1000,relative_scaling=0.5,normalize_plurals=False).generate_from_frequencies(negative_words)\n",
    "plt.imshow(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ff6a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_prediction(doc):\n",
    "    print(doc)\n",
    "    y_pred = pipe1.predict_proba([doc])[0]\n",
    "    tar =['Reject','Accept']\n",
    "    for target, prob in zip(tar, y_pred):\n",
    "        print(\"{:.3f} {}\".format(prob, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30941bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_prediction(text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944f6ddc",
   "metadata": {},
   "source": [
    "## SVM CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da722c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEmbedding(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model_name = \"sentence-transformers/bert-base-nli-mean-tokens\"\n",
    "        self.model = AutoModel.from_pretrained(self.model_name, output_attentions=True)  # Configure model to return attention values\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        print('\\n>>>>>>>init() called.\\n')\n",
    "\n",
    "    def fit(self, X, y = None):\n",
    "        print('\\n>>>>>>>fit() called.\\n')\n",
    "        return self\n",
    "    \n",
    "    def mean_pooling(self,model_output, attention_mask):\n",
    "        token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "    def bert(self,text):\n",
    "        encoded_input = self.tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "        inputs = self.tokenizer.encode(text, return_tensors='pt')  # Tokenize input text\n",
    "\n",
    "        outputs = self.model(inputs)  # Run model\n",
    "        attention = outputs[-1]  # Retrieve attention from model outputs\n",
    "\n",
    "        tokens = self.tokenizer.convert_ids_to_tokens(inputs[0]) \n",
    "        sentence_embeddings = self.mean_pooling(outputs, encoded_input['attention_mask'])\n",
    "        return sentence_embeddings.detach().numpy()[0].tolist()\n",
    "\n",
    "    def transform(self, X, y = None):\n",
    "        embeddings = []\n",
    "        for i in X:\n",
    "            emb = self.bert(i)\n",
    "            embeddings.append(emb)\n",
    "        #print(embeddings)\n",
    "        return embeddings\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c852dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe2 = Pipeline(steps=[\n",
    "                       ('Bert Embeddings', CustomEmbedding()),    # this will trigger a call to __init__\n",
    "                       ('Support Vector Classifier', SVC(kernel='rbf',probability=True))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa54a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = train_data['Sentence']\n",
    "label = train_data['Status']\n",
    "pipe2.fit(text, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3bcc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "te_se = test_data['Sentence']\n",
    "lab = test_data['Status']\n",
    "preds2 = pipe2.predict(te_se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccd52c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "target =['Reject','Accept']\n",
    "doc = 'however, models that make use of this strategy eventually fail after a certain level of complexity (e'\n",
    "pipe2_exp = TextExplainer(random_state=42)\n",
    "pipe2_exp.fit(doc, pipe2.predict_proba)\n",
    "pipe2_exp.show_prediction(target_names= target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb73ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_prediction_1(doc):\n",
    "    print(doc)\n",
    "    y_pred = pipe2.predict_proba([doc])[0]\n",
    "    tar =['Reject','Accept']\n",
    "    for target, prob in zip(tar, y_pred):\n",
    "        print(\"{:.3f} {}\".format(prob, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e123caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe2_exp.explain_weights(target_names=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e39116",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pipe2.classes_) \n",
    "print(preds2)\n",
    "print(pipe2_exp.metrics_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac46a31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Padding_2(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self,model):\n",
    "        self.model = model # Configure model to return attention values\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mxlenght = 400\n",
    "        print('\\n>>>>>>>init() called.\\n')\n",
    "\n",
    "    def fit(self, X, y = None):\n",
    "        print('\\n>>>>>>>fit() called.\\n')\n",
    "        return self\n",
    "    \n",
    "\n",
    "    def transform(self, X, y = None):\n",
    "        embeddings = []\n",
    "        for i in X:\n",
    "            temp = [] \n",
    "            sentence_list = i.split(\".\")\n",
    "            for i in sentence_list:\n",
    "                if len(i)==0:\n",
    "                    continue\n",
    "                pred = model.predict(i)\n",
    "                temp.append(pred)\n",
    "            size = self.mxlenght - len(temp)\n",
    "            if size > 0:\n",
    "                temp.extend([-1]*size)\n",
    "            elif size < 0:\n",
    "                temp = temp[0:self.mxlenght+1]\n",
    "            else:\n",
    "                pass\n",
    "            embeddings.append(temp)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d987f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe3 = Pipeline(steps=[\n",
    "                       ('Documnet Embeddings', Padding_2(pipe2)), # this will trigger a call to __init__\n",
    "                       ('Logistic Regression', LogisticRegression(solver='lbfgs')),\n",
    "\n",
    "])\n",
    "\n",
    "pipe3.fit(text,status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e634572c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_prediction_2(doc):\n",
    "    print(doc)\n",
    "    y_pred = pipe3.predict_proba([doc])[0]\n",
    "    tar =['Reject','Accept']\n",
    "    for target, prob in zip(tar, y_pred):\n",
    "        print(\"{:.3f} {}\".format(prob, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee754b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target =['Reject','Accept']\n",
    "doc = text[0]\n",
    "pipe3_exp = TextExplainer(random_state=42)\n",
    "pipe3_exp.fit(doc, pipe3.predict_proba)\n",
    "pipe3_exp.show_prediction(target_names= target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad6753a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba0c5d0e",
   "metadata": {},
   "source": [
    "#### Integrated Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6157745",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
