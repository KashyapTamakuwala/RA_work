{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db02fbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/admin/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /Users/admin/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/admin/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to /Users/admin/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# import openai\n",
    "import time\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# import tiktoken\n",
    "# from openai.embeddings_utils import get_embedding\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel, utils\n",
    "transformers.logging.set_verbosity_error()\n",
    "#from bertviz import model_view, head_view\n",
    "utils.logging.set_verbosity_error()  # Suppress standard warnings\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "# from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "700d2d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./iclr_2017/train/parsed_pdfs\" \n",
    "train_files = [join(path,f) for f in listdir(path) if isfile(join(path, f))]\n",
    "\n",
    "path_2 = \"./iclr_2017/train/reviews\" \n",
    "train_reviews = [join(path_2,f) for f in listdir(path_2) if isfile(join(path_2, f))]\n",
    "\n",
    "path = \"./iclr_2017/dev/parsed_pdfs\" \n",
    "val_files = [join(path,f) for f in listdir(path) if isfile(join(path, f))]\n",
    "\n",
    "path_2 = \"./iclr_2017/dev/reviews\" \n",
    "val_reviews = [join(path_2,f) for f in listdir(path_2) if isfile(join(path_2, f))]\n",
    "\n",
    "path = \"./iclr_2017/test/parsed_pdfs\" \n",
    "test_files = [join(path,f) for f in listdir(path) if isfile(join(path, f))]\n",
    "\n",
    "path_2 = \"./iclr_2017/test/reviews\" \n",
    "test_reviews = [join(path_2,f) for f in listdir(path_2) if isfile(join(path_2, f))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "598b7750",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files.sort()\n",
    "train_reviews.sort()\n",
    "val_files.sort()\n",
    "val_reviews.sort()\n",
    "test_files.sort()\n",
    "test_reviews.sort()\n",
    "\n",
    "\n",
    "train_file_list = list(zip(train_files,train_reviews))\n",
    "val_file_list = list(zip(val_files,val_reviews))\n",
    "test_file_list = list(zip(test_files,test_reviews))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47e4e32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 349/349 [00:01<00:00, 235.79it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = []\n",
    "i=0\n",
    "for file in tqdm(train_file_list):\n",
    "    i+=1\n",
    "    t = file[0].split('/')[-1]\n",
    "    f = open(file[0])\n",
    "    data = json.load(f)\n",
    "    textdata=\"\"\n",
    "    for section in data['metadata']['sections']:\n",
    "        if section[\"heading\"]!=None:\n",
    "            temp = re.sub(r\"\\n[0-9][0-9][0-9]\",\" \",section[\"text\"])\n",
    "            temp = re.sub(r\"\\n[0-9]\",\" \",temp)\n",
    "            temp = re.sub(r'(?<=\\w)-\\n|\\n-(?=\\w)', '',temp,flags=re.IGNORECASE) \n",
    "            temp = re.sub(r'et al.',\" \",temp,flags=re.IGNORECASE)\n",
    "            temp = re.sub(\"/n\",\".\",temp)\n",
    "            #temp = re.sub(r\"[!@#$%^&*∑]/g\",\"\",temp)\n",
    "            textdata = textdata+temp\n",
    "    label = open(file[1])\n",
    "    lab = json.load(label)\n",
    "    flag = 1 if lab['accepted'] else 0\n",
    "    train_dataset.append([t,textdata.lower(),flag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84a5ccf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "349"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "daa3a091",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [00:00<00:00, 223.26it/s]\n"
     ]
    }
   ],
   "source": [
    "val_dataset = []\n",
    "i=0\n",
    "for file in tqdm(val_file_list):\n",
    "    i+=1\n",
    "    t = file[0].split('/')[-1]\n",
    "    f = open(file[0])\n",
    "    data = json.load(f)\n",
    "    textdata=\"\"\n",
    "    if data['metadata']['sections'] == None:\n",
    "        continue\n",
    "    for section in data['metadata']['sections']:\n",
    "        if section[\"heading\"]!=None:\n",
    "            temp = re.sub(r\"\\n[0-9][0-9][0-9]\",\" \",section[\"text\"])\n",
    "            temp = re.sub(r\"\\n[0-9]\",\" \",temp)\n",
    "            temp = re.sub(r'(?<=\\w)-\\n|\\n-(?=\\w)', '',temp,flags=re.IGNORECASE) \n",
    "            temp = re.sub(r'et al.',\"\",temp,flags=re.IGNORECASE)\n",
    "            temp = re.sub(\"/n\",\".\",temp)\n",
    "            #temp = re.sub(r\"[!@#$%^&*∑]/g\",\"\",temp)\n",
    "            textdata = textdata+temp\n",
    "    label = open(file[1])\n",
    "    lab = json.load(label)\n",
    "    flag = 1 if lab['accepted'] else 0\n",
    "    val_dataset.append([t,textdata.lower(),flag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3aba9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 38/38 [00:00<00:00, 230.06it/s]\n"
     ]
    }
   ],
   "source": [
    "test_dataset = []\n",
    "i=0\n",
    "for file in tqdm(test_file_list):\n",
    "    i+=1\n",
    "    t = file[0].split('/')[-1]\n",
    "    f = open(file[0])\n",
    "    data = json.load(f)\n",
    "    textdata=\"\"\n",
    "    for section in data['metadata']['sections']:\n",
    "        if section[\"heading\"]!=None:\n",
    "            temp = re.sub(r\"\\n[0-9][0-9][0-9]\",\" \",section[\"text\"])\n",
    "            temp = re.sub(r\"\\n[0-9]\",\" \",temp)\n",
    "            temp = re.sub(r'(?<=\\w)-\\n|\\n-(?=\\w)', '',temp,flags=re.IGNORECASE) \n",
    "            temp = re.sub(\"/n\",\".\",temp)\n",
    "            temp = re.sub(r'et al.',\"\",temp,flags=re.IGNORECASE)\n",
    "            #temp = re.sub(r\"[!@#$%^&*∑]/g\",\"\",temp)\n",
    "            textdata = textdata+temp\n",
    "    label = open(file[1])\n",
    "    lab = json.load(label)\n",
    "    flag = 1 if lab['accepted'] else 0\n",
    "    test_dataset.append([t,textdata.lower(),flag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e219ee55",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataframe = pd.DataFrame(train_dataset)\n",
    "val_dataframe = pd.DataFrame(val_dataset)\n",
    "test_dataframe = pd.DataFrame(test_dataset)\n",
    "\n",
    "\n",
    "train_dataframe.columns = ['File_id','Paper_text', 'Status']\n",
    "val_dataframe.columns = ['File_id','Paper_text', 'Status']\n",
    "test_dataframe.columns = ['File_id','Paper_text', 'Status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4387076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_id</th>\n",
       "      <th>Paper_text</th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>304.pdf.json</td>\n",
       "      <td>training neural networks to synthesize robust ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>305.pdf.json</td>\n",
       "      <td>data compression is a fundamental and well-stu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>306.pdf.json</td>\n",
       "      <td>deep learning has shown great success in a var...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>307.pdf.json</td>\n",
       "      <td>the most useful applications of dialog systems...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>308.pdf.json</td>\n",
       "      <td>generative adversarial networks (gans)(goodfel...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        File_id                                         Paper_text  Status\n",
       "0  304.pdf.json  training neural networks to synthesize robust ...       1\n",
       "1  305.pdf.json  data compression is a fundamental and well-stu...       1\n",
       "2  306.pdf.json  deep learning has shown great success in a var...       1\n",
       "3  307.pdf.json  the most useful applications of dialog systems...       1\n",
       "4  308.pdf.json  generative adversarial networks (gans)(goodfel...       1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fac37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataframe.to_csv(\"train_processed_data.csv\",index=False)\n",
    "val_dataframe.to_csv(\"val_processed_data.csv\",index=False)\n",
    "test_dataframe.to_csv(\"test_processed_data.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b77ecbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataframe.Status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b808ea31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_dataframe['Paper_text'][0].split(\".\")))\n",
    "for i in train_dataframe['Paper_text'][0].split(\".\"):\n",
    "    try:\n",
    "        len(i)\n",
    "    except:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893a630f",
   "metadata": {},
   "outputs": [],
   "source": [
    "li=train_dataframe['Paper_text'][0].split(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7b6b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(li))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af921749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_split(dataframe):\n",
    "    train_lis=[] #[id, sentence , label]\n",
    "\n",
    "    for index,row in tqdm(dataframe.iterrows()):\n",
    "        li=train_dataframe['Paper_text'][0].split(\".\")\n",
    "        for sentence in li:\n",
    "                if len(sentence)==0:\n",
    "                    continue\n",
    "                train_lis.append([row['File_id'],sentence,row['Status']])\n",
    "\n",
    "    dataframe_list = pd.DataFrame(train_lis)\n",
    "    dataframe_list.columns = ['File_id','Sentence', 'Status']\n",
    "    return dataframe_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d041fa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataframe_li= sentence_split(train_dataframe)\n",
    "val_dataframe_li= sentence_split(val_dataframe)\n",
    "test_dataframe_li= sentence_split(test_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebae814",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataframe_li.to_csv('train_sentence_data.csv',index=False)\n",
    "val_dataframe_li.to_csv('val_sentence_data.csv',index=False)\n",
    "test_dataframe_li.to_csv('test_sentence_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e39f422",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataframe_li.Status.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac6fa49",
   "metadata": {},
   "source": [
    "#### sample code texting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "861191c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, utils\n",
    "from bertviz import model_view, head_view\n",
    "utils.logging.set_verbosity_error()  # Suppress standard warnings\n",
    "model_name = \"sentence-transformers/bert-base-nli-mean-tokens\"\n",
    "model = AutoModel.from_pretrained(model_name, output_attentions=True)  # Configure model to return attention values\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "def bert(text):\n",
    "    sentences = text.split(\".\")\n",
    "    embedding_attention_tokens = []\n",
    "\n",
    "    for i in sentences:\n",
    "        encoded_input = tokenizer(i, padding=True, truncation=True, return_tensors='pt')\n",
    "        #print(encoded_input)\n",
    "        inputs = tokenizer.encode(i, return_tensors='pt')  # Tokenize input text\n",
    "        #print(inputs)\n",
    "        outputs = model(inputs)  # Run model\n",
    "        attention = outputs[-1]  # Retrieve attention from model outputs\n",
    "\n",
    "        tokens = tokenizer.convert_ids_to_tokens(inputs[0]) \n",
    "        sentence_embeddings = mean_pooling(outputs, encoded_input['attention_mask'])\n",
    "        embedding_attention_tokens.append([sentence_embeddings.detach().numpy(),attention,tokens,encoded_input])\n",
    "    return embedding_attention_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a741f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = bert(\"I am kashyap. I am powerfull\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80031c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[0][0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ffe34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import LayerIntegratedGradients, TokenReferenceBase\n",
    "token_reference = TokenReferenceBase(reference_token_idx=tokenizer.pad_token_id)\n",
    "lig = LayerIntegratedGradients(model, model.embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a326588f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718cde8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
