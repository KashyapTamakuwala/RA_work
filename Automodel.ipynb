{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40b48e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel, utils\n",
    "from datasets import Dataset\n",
    "transformers.logging.set_verbosity_error()\n",
    "utils.logging.set_verbosity_error()  # Suppress standard warnings\n",
    "\n",
    "from bertviz import model_view, head_view\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from eli5.lime import TextExplainer\n",
    "from captum.attr import IntegratedGradients\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c860b88",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57f8c831",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train_sentence_data.csv')\n",
    "test_data = pd.read_csv('test_sentence_data.csv')\n",
    "val_data = pd.read_csv('val_sentence_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5b3249e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.drop(['File_id'],axis=1)\n",
    "test_data = test_data.drop(['File_id'],axis=1)\n",
    "val_data = val_data.drop(['File_id'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84a0a591",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=train_data.sample(frac = 1)\n",
    "test_data=test_data.sample(frac = 1)\n",
    "val_data=val_data.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "276c5e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('Train_auto.csv',index=False)\n",
    "test_data.to_csv('Test_auto.csv',index=False)\n",
    "val_data.to_csv('val_auto.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0537e12d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    9108\n",
       "1    5940\n",
       "Name: Status, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.Status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "934f6dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /Users/admin/.cache/huggingface/datasets/csv/default-18fd2725423c198a/0.0.0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fc18f029f774df9b330be33d56f48d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dae746f757734db4948d5e59d08291f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /Users/admin/.cache/huggingface/datasets/csv/default-18fd2725423c198a/0.0.0. Subsequent calls will reuse this data.\n",
      "Downloading and preparing dataset csv/default to /Users/admin/.cache/huggingface/datasets/csv/default-cb4b0c7dc1db0543/0.0.0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a84553adc73b4fdaa48fb3e008c508be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2bf04e6e0a442538b78f10ff3c1d92a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /Users/admin/.cache/huggingface/datasets/csv/default-cb4b0c7dc1db0543/0.0.0. Subsequent calls will reuse this data.\n",
      "Downloading and preparing dataset csv/default to /Users/admin/.cache/huggingface/datasets/csv/default-c3825700183e4555/0.0.0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c5fdff344704cebbc5043db90a988c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce7aedc9744a4ffa8afabb4772e67598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /Users/admin/.cache/huggingface/datasets/csv/default-c3825700183e4555/0.0.0. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "# Load train and validation datasets from CSV files\n",
    "train_dataset = Dataset.from_csv('Train_auto.csv')\n",
    "val_dataset = Dataset.from_csv('val_auto.csv')\n",
    "test_dataset = Dataset.from_csv('Test_auto.csv')\n",
    "\n",
    "# Rename the columns to 'text' and 'label' to match the expected format for sequence classification\n",
    "train_dataset = train_dataset.rename_column('Sentence', 'text').rename_column('Status', 'label')\n",
    "val_dataset = val_dataset.rename_column('Sentence', 'text').rename_column('Status', 'label')\n",
    "test_dataset = test_dataset.rename_column('Sentence', 'text').rename_column('Status', 'label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28ade24e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/138204 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15444 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15048 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification,Trainer, TrainingArguments\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens')\n",
    "\n",
    "# Define a function to tokenize the text and create input sequences\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding=True, truncation=True,max_length=512)\n",
    "\n",
    "# Apply the tokenization function to the train and validation datasets\n",
    "train_dataset = train_dataset.map(tokenize, batched=True, batch_size=len(train_dataset))\n",
    "val_dataset = val_dataset.map(tokenize, batched=True, batch_size=len(val_dataset))\n",
    "test_dataset = test_dataset.map(tokenize, batched=True, batch_size=len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08972c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3170f624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbaf12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6888, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.0}\n",
      "{'loss': 0.6965, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0}\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                     # the instantiated Transformers model to be trained\n",
    "    args=training_args,              # training arguments, defined above\n",
    "    train_dataset=train_dataset,     # training dataset\n",
    "    eval_dataset=val_dataset,        # evaluation dataset\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827aa56d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate(test_dataset)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03f6618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_adapter(texts: List[str]):\n",
    "    \n",
    "    all_scores = []\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), 64)):\n",
    "\n",
    "        batch = texts[i:i+64]\n",
    "        #print(batch)\n",
    "        \n",
    "        # use bert encoder to tokenize text \n",
    "        encoded_input = tokenizer(batch, \n",
    "          return_tensors='pt', \n",
    "          padding=True, \n",
    "          truncation=True, \n",
    "          max_length=model.config.max_position_embeddings-2)\n",
    "\n",
    "        # run the model\n",
    "        output = model(**encoded_input)\n",
    "        #print(output)\n",
    "        # by default this model gives raw logits rather \n",
    "        # than a nice smooth softmax so we apply it ourselves here\n",
    "        scores = output[0].softmax(1).detach().numpy()\n",
    "        #print(scores)\n",
    "\n",
    "        all_scores.extend(scores)\n",
    "\n",
    "    return np.array(all_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19876226",
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = test_data.Sentence\n",
    "lab = test_data.Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd968cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "te = TextExplainer(n_samples=5000, random_state=42)\n",
    "for i in sen:\n",
    "    te.fit(i, model_adapter)\n",
    "    te.explain_prediction(target_names=list(model.config.id2label.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b9234d",
   "metadata": {},
   "outputs": [],
   "source": [
    "te.explain_weights(target_names=list(model.config.id2label.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b5c2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "te.metrics_ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944f6ddc",
   "metadata": {},
   "source": [
    "## SVM CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da722c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEmbedding(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model_name = \"sentence-transformers/bert-base-nli-mean-tokens\"\n",
    "        self.model = AutoModel.from_pretrained(self.model_name, output_attentions=True)  # Configure model to return attention values\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        print('\\n>>>>>>>init() called.\\n')\n",
    "\n",
    "    def fit(self, X, y = None):\n",
    "        print('\\n>>>>>>>fit() called.\\n')\n",
    "        return self\n",
    "    \n",
    "    def mean_pooling(self,model_output, attention_mask):\n",
    "        token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "    def bert(self,text):\n",
    "        encoded_input = self.tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "        inputs = self.tokenizer.encode(text, return_tensors='pt')  # Tokenize input text\n",
    "\n",
    "        outputs = self.model(inputs)  # Run model\n",
    "        attention = outputs[-1]  # Retrieve attention from model outputs\n",
    "\n",
    "        tokens = self.tokenizer.convert_ids_to_tokens(inputs[0]) \n",
    "        sentence_embeddings = self.mean_pooling(outputs, encoded_input['attention_mask'])\n",
    "        return sentence_embeddings.detach().numpy()[0].tolist()\n",
    "\n",
    "    def transform(self, X, y = None):\n",
    "        embeddings = []\n",
    "        for i in X:\n",
    "            emb = self.bert(i)\n",
    "            embeddings.append(emb)\n",
    "        #print(embeddings)\n",
    "        return embeddings\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c852dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe2 = Pipeline(steps=[\n",
    "                       ('Bert Embeddings', CustomEmbedding()),    # this will trigger a call to __init__\n",
    "                       ('Support Vector Classifier', SVC(kernel='rbf',probability=True))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b6efef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa54a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = train_data['Sentence']\n",
    "label = train_data['Status']\n",
    "pipe2.fit(text, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3bcc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "te_se = test_data['Sentence']\n",
    "lab = test_data['Status']\n",
    "preds2 = pipe2.predict(te_se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e634572c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe2.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee754b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb73ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_prediction(doc):\n",
    "    print(doc)\n",
    "    y_pred = pipe2.predict_proba([doc])[0]\n",
    "    tar =['Reject','Accept']\n",
    "    for target, prob in zip(tar, y_pred):\n",
    "        print(\"{:.3f} {}\".format(prob, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccd52c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "target =['Reject','Accept']\n",
    "doc = 'however, models that make use of this strategy eventually fail after a certain level of complexity (e'\n",
    "te = TextExplainer(random_state=42)\n",
    "te.fit(doc, pipe2.predict_proba)\n",
    "te.show_prediction(target_names= target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e123caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "te.explain_weights(target_names=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad6753a",
   "metadata": {},
   "outputs": [],
   "source": [
    "te.metrics_ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0c5d0e",
   "metadata": {},
   "source": [
    "#### Integrated Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6157745",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
