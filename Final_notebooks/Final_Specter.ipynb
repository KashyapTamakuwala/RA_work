{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel, utils\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification,Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "transformers.logging.set_verbosity_error()\n",
    "utils.logging.set_verbosity_error()  # Suppress standard warnings\n",
    "\n",
    "from bertviz import model_view, head_view\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "import eli5\n",
    "from eli5.lime import TextExplainer\n",
    "# from captum.attr import IntegratedGradients\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_train_data = pd.read_csv('../sentence_split/train_sentence_data.csv')\n",
    "sen_test_data = pd.read_csv('../sentence_split/test_sentence_data.csv')\n",
    "sen_val_data = pd.read_csv('../sentence_split/val_sentence_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_train_data = pd.read_csv(\"../complete_sentence/train_processed_data.csv\")\n",
    "doc_test_data = pd.read_csv(\"../complete_sentence/train_processed_data.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_train_data = sen_train_data.drop(['File_id'],axis=1)\n",
    "sen_test_data = sen_test_data.drop(['File_id'],axis=1)\n",
    "sen_val_data = sen_val_data.drop(['File_id'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_20(data):\n",
    "#     data_1 = data.loc[data['Status'] ==1].iloc[:20]\n",
    "#     data_2 = data.loc[data['Status'] ==0].iloc[:20]\n",
    "#     frames = [data_1, data_2]\n",
    "#     return pd.concat(frames)\n",
    "\n",
    "# sen_train_data = get_20(sen_train_data) \n",
    "# sen_test_data = get_20(sen_test_data)\n",
    "# sen_val_data = get_20(sen_val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_train_text = doc_train_data.Paper_text.values.tolist()\n",
    "doc_train_status = doc_train_data.Status.values.tolist()\n",
    "\n",
    "doc_test_text = doc_test_data.Paper_text.values.tolist()\n",
    "doc_test_status = doc_test_data.Status.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_train_data=sen_train_data.sample(frac = 1)\n",
    "sen_test_data=sen_test_data.sample(frac = 1)\n",
    "sen_val_data=sen_val_data.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_train_data.to_csv('Train_auto.csv',index=False)\n",
    "sen_test_data.to_csv('Test_auto.csv',index=False)\n",
    "sen_val_data.to_csv('val_auto.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_train_data.Status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and validation datasets from CSV files\n",
    "train_dataset = Dataset.from_csv('Train_auto.csv')\n",
    "val_dataset = Dataset.from_csv('val_auto.csv')\n",
    "test_dataset = Dataset.from_csv('Test_auto.csv')\n",
    "\n",
    "# Rename the columns to 'text' and 'label' to match the expected format for sequence classification\n",
    "train_dataset = train_dataset.rename_column('Sentence', 'text').rename_column('Status', 'label')\n",
    "val_dataset = val_dataset.rename_column('Sentence', 'text').rename_column('Status', 'label')\n",
    "test_dataset = test_dataset.rename_column('Sentence', 'text').rename_column('Status', 'label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/allenai-specter')\n",
    "\n",
    "# Define a function to tokenize the text and create input sequences\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding=True, truncation=True,max_length=512)\n",
    "\n",
    "# Apply the tokenization function to the train and validation datasets\n",
    "train_dataset = train_dataset.map(tokenize, batched=True, batch_size=len(train_dataset))\n",
    "val_dataset = val_dataset.map(tokenize, batched=True, batch_size=len(val_dataset))\n",
    "test_dataset = test_dataset.map(tokenize, batched=True, batch_size=len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained('sentence-transformers/allenai-specter', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results/specter',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                     # the instantiated Transformers model to be trained\n",
    "    args=training_args,              # training arguments, defined above\n",
    "    train_dataset=train_dataset,     # training dataset\n",
    "    eval_dataset=val_dataset,        # evaluation dataset\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate(test_dataset)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_adapter(texts: List[str]):\n",
    "    \n",
    "    all_scores = []\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), 64)):\n",
    "\n",
    "        batch = texts[i:i+64]\n",
    "        #print(batch)\n",
    "        \n",
    "        # use bert encoder to tokenize text \n",
    "        encoded_input = tokenizer(batch, \n",
    "          return_tensors='pt', \n",
    "          padding=True, \n",
    "          truncation=True, \n",
    "          max_length=model.config.max_position_embeddings-2)\n",
    "\n",
    "        # run the model\n",
    "        output = model(**encoded_input)\n",
    "        #print(output)\n",
    "        # by default this model gives raw logits rather \n",
    "        # than a nice smooth softmax so we apply it ourselves here\n",
    "        scores = output[0].softmax(1).detach().numpy()\n",
    "        #print(scores)\n",
    "\n",
    "        all_scores.extend(scores)\n",
    "\n",
    "    return np.array(all_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = sen_test_data.Sentence\n",
    "lab = sen_test_data.Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specter_exp = TextExplainer(n_samples=5000, random_state=42)\n",
    "specter_exp.fit(sen[0], model_adapter)\n",
    "specter_exp.explain_prediction(target_names=list(model.config.id2label.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = specter_exp.explain_weights(top=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = specter_exp.explain_weights()\n",
    "\n",
    "sen_positivie_words= {}\n",
    "\n",
    "for i in weights.targets[0].feature_weights.pos:\n",
    "    #print(i.feature)\n",
    "    g = sen_positivie_words.get(i.feature,-1)\n",
    "    if g==-1:\n",
    "        sen_positivie_words[i.feature]=1\n",
    "    else:\n",
    "        sen_positivie_words[i.feature]+=1\n",
    "        \n",
    "\n",
    "sen_negative_words= {}\n",
    "\n",
    "for i in weights.targets[0].feature_weights.neg:\n",
    "    #print(i.feature)\n",
    "    g = sen_negative_words.get(i.feature,-1)\n",
    "    if g==-1:\n",
    "        sen_negative_words[i.feature]=1\n",
    "    else:\n",
    "        sen_negative_words[i.feature]+=1\n",
    "\n",
    "        \n",
    "wc = WordCloud(background_color=\"white\",width=1000,height=1000,relative_scaling=0.5,normalize_plurals=False).generate_from_frequencies(sen_positivie_words)\n",
    "plt.imshow(wc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(background_color=\"white\",width=1000,height=1000,relative_scaling=0.5,normalize_plurals=False).generate_from_frequencies(sen_negative_words)\n",
    "plt.imshow(wc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## very inportant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(weights.targets[0].feature_weights.pos[0].weight,weights.targets[0].feature_weights.pos[0].feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = specter_exp.explain_prediction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_train_data = pd.read_csv(\"./complete_sentence/train_processed_data.csv\")\n",
    "doc_test_data = pd.read_csv(\"./complete_sentence/train_processed_data.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_train_data = get_20(doc_train_data)\n",
    "doc_test_data = get_20(doc_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_train_text = doc_train_data.Paper_text.values.tolist()\n",
    "doc_train_status = doc_train_data.Status.values.tolist()\n",
    "\n",
    "doc_test_text = doc_test_data.Paper_text.values.tolist()\n",
    "doc_test_status = doc_test_data.Status.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Padding(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self,model,tokenizer):\n",
    "        self.model = model # Configure model to return attention values\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mxlenght = 400\n",
    "        print('\\n>>>>>>>init() called.\\n')\n",
    "\n",
    "    def fit(self, X, y = None):\n",
    "        print('\\n>>>>>>>fit() called.\\n')\n",
    "        return self\n",
    "    \n",
    "\n",
    "    def transform(self, X, y = None):\n",
    "        embeddings = []\n",
    "        for i in tqdm(X):\n",
    "            temp = [] \n",
    "            sentence_list = i.split(\".\")\n",
    "            for i in sentence_list:\n",
    "                if len(i)==0:\n",
    "                    continue\n",
    "                encoded_input = tokenizer(i,return_tensors='pt', padding=True, truncation=True,max_length=model.config.max_position_embeddings-2)\n",
    "                output = model(**encoded_input)\n",
    "                pred = np.argmax(output[0].softmax(1).detach().numpy())\n",
    "                temp.append(pred)\n",
    "            size = self.mxlenght - len(temp)\n",
    "            if size > 0:\n",
    "                temp.extend([-1]*size)\n",
    "            elif size < 0:\n",
    "                temp = temp[0:self.mxlenght]\n",
    "            else:\n",
    "                pass\n",
    "            embeddings.append(temp)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe1 = Pipeline(steps=[\n",
    "                       ('Documnet Embeddings', Padding(model,tokenizer)), # this will trigger a call to __init__\n",
    "                       ('Logistic Regression', LogisticRegression(solver='lbfgs')),\n",
    "\n",
    "])\n",
    "\n",
    "pipe1.fit(doc_train_text, doc_train_status)\n",
    "pipe1.score(doc_test_text,doc_test_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target =['Reject','Accept']\n",
    "doc = doc_test_text[0]\n",
    "pipe1_exp = TextExplainer(n_samples=10,random_state=42)\n",
    "pipe1_exp.fit(doc, pipe1.predict_proba)\n",
    "pipe1_exp.show_prediction(target_names= target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = pipe1_exp.explain_weights(top= None)\n",
    "positivie_words= {}\n",
    "\n",
    "for i in weights.targets[0].feature_weights.pos:\n",
    "    #print(i.feature)\n",
    "    g = positivie_words.get(i.feature,-1)\n",
    "    if g==-1:\n",
    "        positivie_words[i.feature]=1\n",
    "    else:\n",
    "        positivie_words[i.feature]+=1\n",
    "        \n",
    "\n",
    "negative_words= {}\n",
    "\n",
    "for i in weights.targets[0].feature_weights.neg:\n",
    "    #print(i.feature)\n",
    "    g = negative_words.get(i.feature,-1)\n",
    "    if g==-1:\n",
    "        negative_words[i.feature]=1\n",
    "    else:\n",
    "        negative_words[i.feature]+=1\n",
    "# print(weights.targets[0].feature_weights.pos[0].weight,weights.targets[0].feature_weights.pos[0].feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(background_color=\"white\",width=1000,height=1000,relative_scaling=0.5,normalize_plurals=False).generate_from_frequencies(positivie_words)\n",
    "plt.imshow(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(background_color=\"white\",width=1000,height=1000,relative_scaling=0.5,normalize_plurals=False).generate_from_frequencies(negative_words)\n",
    "plt.imshow(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_prediction(doc):\n",
    "    #print(doc)\n",
    "    y_pred = pipe1.predict_proba([doc])[0]\n",
    "    tar =['Reject','Accept']\n",
    "    for target, prob in zip(tar, y_pred):\n",
    "        print(\"{:.3f} {}\".format(prob, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_prediction(doc_test_text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEmbedding(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model_name = \"sentence-transformers/bert-base-nli-mean-tokens\"\n",
    "        self.model = AutoModel.from_pretrained(self.model_name, output_attentions=True)  # Configure model to return attention values\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        print('\\n>>>>>>>init() called.\\n')\n",
    "\n",
    "    def fit(self, X, y = None):\n",
    "        print('\\n>>>>>>>fit() called.\\n')\n",
    "        return self\n",
    "    \n",
    "    def mean_pooling(self,model_output, attention_mask):\n",
    "        token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "    def bert(self,text):\n",
    "        encoded_input = self.tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "        inputs = self.tokenizer.encode(text, return_tensors='pt')  # Tokenize input text\n",
    "\n",
    "        outputs = self.model(inputs)  # Run model\n",
    "        attention = outputs[-1]  # Retrieve attention from model outputs\n",
    "\n",
    "        tokens = self.tokenizer.convert_ids_to_tokens(inputs[0]) \n",
    "        sentence_embeddings = self.mean_pooling(outputs, encoded_input['attention_mask'])\n",
    "        return sentence_embeddings.detach().numpy()[0].tolist()\n",
    "\n",
    "    def transform(self, X, y = None):\n",
    "        embeddings = []\n",
    "        for i in X:\n",
    "            emb = self.bert(i)\n",
    "            embeddings.append(emb)\n",
    "        #print(embeddings)\n",
    "        return embeddings\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe2 = Pipeline(steps=[\n",
    "                       ('Bert Embeddings', CustomEmbedding()),    # this will trigger a call to __init__\n",
    "                       ('Support Vector Classifier', SVC(kernel='rbf',probability=True))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_text = sen_train_data['Sentence']\n",
    "sen_label = sen_train_data['Status']\n",
    "pipe2.fit(sen_text, sen_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_test_text = sen_test_data['Sentence']\n",
    "sen_test_lab = sen_test_data['Status']\n",
    "pipe2.score(sen_test_text,sen_test_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target =['Reject','Accept']\n",
    "doc = 'however, models that make use of this strategy eventually fail after a certain level of complexity (e'\n",
    "pipe2_exp = TextExplainer(random_state=42)\n",
    "pipe2_exp.fit(doc, pipe2.predict_proba)\n",
    "pipe2_exp.show_prediction(target_names= target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_prediction_1(doc):\n",
    "    print(doc)\n",
    "    y_pred = pipe2.predict_proba([doc])[0]\n",
    "    tar =['Reject','Accept']\n",
    "    for target, prob in zip(tar, y_pred):\n",
    "        print(\"{:.3f} {}\".format(prob, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe2_exp.explain_weights(target_names=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pipe2.classes_) \n",
    "print(pipe2_exp.metrics_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Padding_2(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self,model):\n",
    "        self.model = model # Configure model to return attention values\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mxlenght = 400\n",
    "        print('\\n>>>>>>>init() called.\\n')\n",
    "\n",
    "    def fit(self, X, y = None):\n",
    "        print('\\n>>>>>>>fit() called.\\n')\n",
    "        return self\n",
    "    \n",
    "\n",
    "    def transform(self, X, y = None):\n",
    "        embeddings = []\n",
    "        for i in X:\n",
    "            temp = [] \n",
    "            sentence_list = i.split(\".\")\n",
    "            for i in sentence_list:\n",
    "                if len(i)==0:\n",
    "                    continue\n",
    "                pred = model.predict(i)\n",
    "                temp.append(pred)\n",
    "            size = self.mxlenght - len(temp)\n",
    "            if size > 0:\n",
    "                temp.extend([-1]*size)\n",
    "            elif size < 0:\n",
    "                temp = temp[0:self.mxlenght+1]\n",
    "            else:\n",
    "                pass\n",
    "            embeddings.append(temp)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe3 = Pipeline(steps=[\n",
    "                       ('Documnet Embeddings', Padding_2(pipe2)), # this will trigger a call to __init__\n",
    "                       ('Logistic Regression', LogisticRegression(solver='lbfgs')),\n",
    "\n",
    "])\n",
    "\n",
    "pipe3.fit(doc_train_text, doc_train_status)\n",
    "pipe3.score(doc_test_text,doc_test_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_prediction_2(doc):\n",
    "    print(doc)\n",
    "    y_pred = pipe3.predict_proba([doc])[0]\n",
    "    tar =['Reject','Accept']\n",
    "    for target, prob in zip(tar, y_pred):\n",
    "        print(\"{:.3f} {}\".format(prob, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target =['Reject','Accept']\n",
    "doc = text[0]\n",
    "pipe3_exp = TextExplainer(random_state=42)\n",
    "pipe3_exp.fit(doc, pipe3.predict_proba)\n",
    "pipe3_exp.show_prediction(target_names= target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = pipe3_exp.explain_weights(top= None)\n",
    "positivie_words= {}\n",
    "\n",
    "for i in weights.targets[0].feature_weights.pos:\n",
    "    #print(i.feature)\n",
    "    g = positivie_words.get(i.feature,-1)\n",
    "    if g==-1:\n",
    "        positivie_words[i.feature]=1\n",
    "    else:\n",
    "        positivie_words[i.feature]+=1\n",
    "        \n",
    "\n",
    "negative_words= {}\n",
    "\n",
    "for i in weights.targets[0].feature_weights.neg:\n",
    "    #print(i.feature)\n",
    "    g = negative_words.get(i.feature,-1)\n",
    "    if g==-1:\n",
    "        negative_words[i.feature]=1\n",
    "    else:\n",
    "        negative_words[i.feature]+=1\n",
    "# print(weights.targets[0].feature_weights.pos[0].weight,weights.targets[0].feature_weights.pos[0].feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(background_color=\"white\",width=1000,height=1000,relative_scaling=0.5,normalize_plurals=False).generate_from_frequencies(positivie_words)\n",
    "plt.imshow(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(background_color=\"white\",width=1000,height=1000,relative_scaling=0.5,normalize_plurals=False).generate_from_frequencies(negative_words)\n",
    "plt.imshow(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ra_env",
   "language": "python",
   "name": "ra_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
