Sentence,Status
"
grade-school addition",0
" we prove every member of f is added properly, given that subproblems s = {anan−1 + bnbn−1, an−1an−2 + bn−1bn−2, ",1
"
bubble sort",1
" our implementation uses two stacks qstacklo and
qstackhi, each in ru , that store the arguments to the recursive quicksort calls in algorithm 3; before each recursive call, the appropriate arguments are popped off the stack and written to plo and phi",0
"as there is no public implementation of npi, we implemented a version of it in keras that is as faithful to the paper as possible",1
"we show that if we incorporate recursion, the learned npi programs can achieve provably perfect generalization for different tasks",1
" training on the full recursive trace leads to perfect generalization, as shown in section 4",0
"
arg 2 (object to copy): env stack lo peek, env stack hi peek, phi, plo, ppivot − 1, ppivot + 1, reset
env stack lo peek and env stack hi peek represent qstacklo(pstacklo − 1) and qstackhi(pstackhi − 1), respectively",0
g,0
" m , as described in algorithm 1, processes the sequence of step inputs by using an lstm",0
 this paper is the first (to our knowledge) to investigate the important problem of provable generalization properties of neural programs,1
"
formally, verification consists of proving the following theorem:
∀i ∈ v,m(i) ⇓ p (i) where i denotes a sequence of step inputs (within one function call), v denotes the set of valid sequences of step inputs, m denotes the neural network model, p denotes the correct program, and p (i) denotes the next step output from the correct program",0
" if yes, then this indicates that the learned neural program achieves provably perfect generalization",1
"
note that the recursion for quicksort is not purely tail recursive and therefore represents a more complex kind of recursion that is harder to learn than in the previous tasks",0
"
by nature, recursion reduces the complexity of a problem to simpler instances",1
" to make an analogy to mdps, this procedure is analogous to how value iteration obtains the correct value for each state starting from any initialization",0
" more specifically, when m is trained on simpler inputs, such as inputs of a small length, the generalization metric evaluates how well m will do on more complex inputs, such as inputs of much longer length",0
" we have three observations: the color of the start node, the color of the active node’s next child to be considered, and whether the stack is empty",0
" here as a concrete and general example, we consider a general neural programming architecture (npa), similar to neural programmer-interpreter (npi) in reed & de freitas (2016)",1
3 to construct v and then empirically create a verification set which covers v ,0
" 9: else 10: function run(e, p2, a2)
a description of the inference procedure is given in algorithm 1",1
"
through careful reasoning about the possible set of environment observations created by all valid inputs, and how each of the operations in the execution trace affects the environment, we can construct v using the procedure described in section 3",0
" also, compared to the bubble pointers in bubble sort, the pointers that perform the comparison for quicksort (the compswap function) are usually not adjacent to each other, making quicksort less local than bubble sort",0
" via the recursive call, we effectively forget that the column just added exists, since the recursive call to add starts with a new hidden state for the lstm controller",0
"
algorithm 1 neural programming inference 1: inputs: environment observation e, program p, arguments a, stop threshold α 2: function run(e, p, a) 3: h← 0, r ← 0 4: while r < α do 5: s← fenc(e, a), h← flstm(s, p, h) 6: r ← fend(h), p2 ← fprog(h), a2 ← farg(h) 7: if p is a primitive function then 8: e← fenv(e, p, a)",0
" note that recursion helps make this process tractable, because we only need to test a finite number of inputs to show that the model will work correctly on inputs of unbounded complexity",0
" we then construct problem configurations that span all possible valid environment states (for instance, in order to force the carry bit in a column to be 1, one can add the sum “1+9” in the column to the right)",0
" as described in the verification procedure, it is possible to prove our learned recursive program generalizes perfectly by testing on an appropriate set of problem inputs, i",0
 18: push lo and p− 1 to slo and shi,1
"
training setup",1
 pstacklo and pstackhi point to the top (empty) positions in qstacklo and qstackhi,1
" the recursive version of topological sort solves all graph instances we tried, from graphs of size 5 through 70",1
"
the dag is represented as an adjacency list where dag[i][j] refers to the j-th child of vertex i",0
" it is straightforward to deal with all other operations, which do not induce partial environment states",0
" the add reduction rule ensures that each call to add only covers two adjacent columns, and so the lstm only ever runs for a fixed number of steps necessary to process these two columns",1
 we created a program set that reflects the semantics of algorithm 2,1
" childlist ∈ nu is a vector of pointers, where childlist[i] points to the next child under consideration for vertex i",0
" although the model is exposed to more and more data, it might learn spurious and overly complex representations of the program, as suggested in zaremba   (2016)",1
 indentation indicates the stack is one level deeper than before,1
" the swap operation has the following arguments:
arg 1 (swap object 1): ppivot
arg 2 (swap object 2): phi, pj
write",0
" empirically, existing models suffer from a common limitation—generalization becomes poor beyond a threshold level of complexity",0
"
recursion drastically reduces the number of configurations we need to consider during the verification phase and makes the proof tractable, because it introduces structure that eliminates infinitely long sequences of step inputs that would otherwise need to be considered",0
this material is in part based upon work supported by the national science foundation under grant no,1
 ,0
" each step during an execution of the program does one of three things: (1) another subprogram along with associated arguments is called, as in line 10, (2) the program writes to the environment if it is primitive, as in line 8, or (3) the loop is terminated if the return probability exceeds a threshold α, after which the stack frame is popped and control is returned to the caller",1
"
in order to demonstrate that recursion can help learn and generalize better, for addition, we trained only on traces for 5 arbitrarily chosen 1-digit addition sum examples",0
" provably perfect generalization implies the model will behave correctly, given any valid input",0
" training on the full recursive trace leads to perfect generalization, as shown in section 4",0
" in future work, we would like to decrease supervision and construct models that are capable of coming up with recursive abstractions themselves",1
"
topological sort",0
"
arg 2 (increment or decrement): up, down1 // top level topological sort call 2 toposort() { 3 while (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 } 0 11 traverse() { 12 check_child() 13 explore() 14 } 15 16 check_child() { 17 while (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 18 move(childlist[vactive], up) 19 } 20 21 explore() { 22 do 23 if (qcolor(dag[vactive][childlist[vactive]]) is white): 24 write(color_next, color_grey) 25 stack(push) 26 write(save) 27 write(active_neighb) 28 move(childlist[vsave], up) 29 else: 30 write(color_curr, color_black) 31 write(result) 32 move(presult, up) 33 if(pstack == 1): 34 break 35 else: 36 stack(pop) 37 check_child() 38 while (true) 39 } 40 41 stack(op) { 42 if (op == push): 43 write(stack_push) 44 move(pstack, up) 45 46 if (op == pop): 47 write(active_stack) 48 write(stack_pop) 49 move(pstack, down) 50 } 51 52 next_start() { 53 while(qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 54 move(pstart, up) 55 }altered recursive functions  // top level topological sort call 2 toposort() { 3 if (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 toposort() // recursive call 0 } 11 12 check_child() { 13 if (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 14 move(childlist[vactive], up) 15 check_child() // recursive call 16 } 17 18 explore() { 19 if (qcolor(dag[vactive][childlist[vactive]]) is white): 20 write(color_next, color_grey) 21 stack(push) 22 write(save) 23 write(active_neighb) 24 move(childlist[vsave], up) 25 else: 26 write(color_curr, color_black) 27 write(result) 28 move(presult, up) 29 if(pstack == 1): 30 return 31 else: 32 stack(pop) 33 check_child() 34 explore() // recursive call 35 } 36 37 next_start() { 38 if (qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 39 move(pstart, up) 40 next_start() // recursive call 41 }algorithm 4 iterative quicksort 1: initialize an array a to sort and two empty stacks slo and shi",0
5,1
" as before, the non-recursive learned program lacks a provable guarantee of generalization, whereas we show later that the recursive learned program has one",0
"when constructing a neural network for the purpose of learning a program, there are two orthogonal aspects to consider",1
"
move",0
", an array for quicksort or a dag for topological sort)",0
 a1a0 + bnbn−1 ,0
", an array for quicksort or a dag for topological sort)",1
"
to evaluate a neural network that learns a neural program to accomplish a certain task, one common evaluation metric is how well the learned model m generalizes",0
"
as a concrete instantiation, we show in this paper that we can enable recursive neural programs in the npi model, and thus enable perfectly generalizable neural programs for tasks such as sorting where the original, non-recursive npi program fails",0
" however, models that make use of this strategy eventually fail after a certain level of complexity (e",0
" et+1 ∼ fenv(et, pt, at)",0
"
the non-recursive trace loops on cycles of add1 and lshift",0
" for example, a step input-step output pair in lines 2 and 3 of the left-hand side of figure 1 is (add1, write out 1)",1
" in this architecture, we consider a core controller, e",0
" we further have qresult ∈ nu , a scratch-pad which contains the sorted list of vertices at the end of execution, and qstack ∈ nu , which serves the role of the stack s in algorithm 2",0
" k is set to 11, to represent the range of 10 possible digits, along with a token representing the end of input",1
" lshift moves the four pointers to the left, to move to the next column",0
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,1
"
arg 2 (increment or decrement): up, down1 // top level topological sort call 2 toposort() { 3 while (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 } 0 11 traverse() { 12 check_child() 13 explore() 14 } 15 16 check_child() { 17 while (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 18 move(childlist[vactive], up) 19 } 20 21 explore() { 22 do 23 if (qcolor(dag[vactive][childlist[vactive]]) is white): 24 write(color_next, color_grey) 25 stack(push) 26 write(save) 27 write(active_neighb) 28 move(childlist[vsave], up) 29 else: 30 write(color_curr, color_black) 31 write(result) 32 move(presult, up) 33 if(pstack == 1): 34 break 35 else: 36 stack(pop) 37 check_child() 38 while (true) 39 } 40 41 stack(op) { 42 if (op == push): 43 write(stack_push) 44 move(pstack, up) 45 46 if (op == pop): 47 write(active_stack) 48 write(stack_pop) 49 move(pstack, down) 50 } 51 52 next_start() { 53 while(qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 54 move(pstart, up) 55 }altered recursive functions  // top level topological sort call 2 toposort() { 3 if (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 toposort() // recursive call 0 } 11 12 check_child() { 13 if (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 14 move(childlist[vactive], up) 15 check_child() // recursive call 16 } 17 18 explore() { 19 if (qcolor(dag[vactive][childlist[vactive]]) is white): 20 write(color_next, color_grey) 21 stack(push) 22 write(save) 23 write(active_neighb) 24 move(childlist[vsave], up) 25 else: 26 write(color_curr, color_black) 27 write(result) 28 move(presult, up) 29 if(pstack == 1): 30 return 31 else: 32 stack(pop) 33 check_child() 34 explore() // recursive call 35 } 36 37 next_start() { 38 if (qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 39 move(pstart, up) 40 next_start() // recursive call 41 }algorithm 4 iterative quicksort 1: initialize an array a to sort and two empty stacks slo and shi",0
5,0
" in computer science, recursion (as opposed to iteration) involves solving a larger problem by combining solutions to smaller instances of the same problem",0
"
the training set for bubble sort contains 100 traces, with maximum problem length of 2 (e",0
" qcolor(pstart) contains the color of the current start vertex, used in the evaluation of the condition in the while loop in line 5 of algorithm 2",1
"
algorithm 1 neural programming inference 1: inputs: environment observation e, program p, arguments a, stop threshold α 2: function run(e, p, a) 3: h← 0, r ← 0 4: while r < α do 5: s← fenc(e, a), h← flstm(s, p, h) 6: r ← fend(h), p2 ← fprog(h), a2 ← farg(h) 7: if p is a primitive function then 8: e← fenv(e, p, a)",1
g,0
" the general notion of recursion has been an important concept in many domains, including mathematics and computer science",0
"
we also report on generalization results for the non-recursive and recursive versions of this variant of bubble sort",1
 some functional programming languages go so far as not to define any looping constructs but rely solely on recursion to enable repeated execution of the same code,0
" the theorem states that for the same sequence of step inputs, the model produces the exact same step output as the target program it aims to learn",0
" however, the authors of npi did not consider explicitly the notion of recursion and as a consequence, did not learn recursive programs",0
" for these models, table 2 presents results on randomly generated dags of varying graph sizes (varying in the number of vertices)",1
" for tasks other than addition, it is difficult to analytically determine the verification set, so instead, we randomly generate input candidates until they completely cover the base cases and reduction rules",1
" we then construct problem configurations that span all possible valid environment states (for instance, in order to force the carry bit in a column to be 1, one can add the sum “1+9” in the column to the right)",0
" we can obtain this mapping by first considering the possible observations that can arise at the beginning of the entry function (add, bubblesort, toposort, and quicksort) for some valid program input, and iteratively applying the observation-to-observation mapping implied by the reference implementation’s step output at that point in the execution",0
"
it is important to emphasize that at inference time in the npi, the hidden state of the lstm controller is reset (to zero) at each subprogram call, as in line 3 of algorithm 1 (h ← 0)",0
"
move",0
" partial recursion only adds a tail recursive call to bubblesort after bubble and reset, similar to the tail recursive call described previously for addition",1
" an example of part of an addition task trace, written in shorthand, is given in figure 1",0
" for grade-school addition, the domain-specific encoder is
fenc(q, i1, i2, i3, i4, at) =mlp ([q(1, i1), q(2, i2), q(3, i3), q(4, i4), at(1), at(2), at(3)]),
where the environment q ∈ r4×n×k is a scratch-pad that contains four rows (the first input number, the second input number, the carry bits, and the output) and n columns",1
 the degree for any vertex in the dag is variable,0
 ,0
" the pointer at index i3 represents a counter internal to the environment that is incremented once after each pass of the algorithm (one cycle of bubble and reset); when incremented a number of times equal to the length of the array, the flag i3 == length becomes true and terminates the entire algorithm ",0
"we train using the adam optimizer and use a 2-layer lstm and task-specific state encoders for the external environments, as described in reed & de freitas (2016)",0
g,0
" in order to compensate for this, ppivot and pj require special functions (move pivot lo and move j lo) to properly set them to lo in lines 11 and 12 of the partition function in algorithm 3",0
" we further have qresult ∈ nu , a scratch-pad which contains the sorted list of vertices at the end of execution, and qstack ∈ nu , which serves the role of the stack s in algorithm 2",1
 we describe the recursive re-formulation of traces for two tasks from the original npi paper—grade-school addition and bubble sort,1
" note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values, and arg 2 can only take one of 7 values",0
 this verification phase only needs to be performed once after training,0
", to change the value of vactive) none described below move move a pointer (e",1
"
recursion drastically reduces the number of configurations we need to consider during the verification phase and makes the proof tractable, because it introduces structure that eliminates infinitely long sequences of step inputs that would otherwise need to be considered",1
" the partial recursion is not enough for perfect generalization, as will be presented later in section 4",0
" observe that the non-recursive program’s correctness degrades for length 11 and beyond, while the recursive program can sort any given array",0
"
inside bubble and reset, there are two operations that can be made recursive",1
" the outputs rt, pt+1, at+1 are used to determine the next action, as described in algorithm 1",0
 this algorithm is a variant of depth first search,0
"
to evaluate a neural network that learns a neural program to accomplish a certain task, one common evaluation metric is how well the learned model m generalizes",0
" the arrow in the theorem refers to evaluation, as in big-step semantics",0
" at each time step, the core controller can decide to select one of the programs to call with certain arguments",1
 the degree for any vertex in the dag is variable,0
"
the training set for quicksort contains 4 traces, synthesized from arrays of length 5",0
" partial recursion only adds a tail recursive call to bubblesort after bubble and reset, similar to the tail recursive call described previously for addition",1
" an example of part of an addition task trace, written in shorthand, is given in figure 1",1
 the degree for any vertex in the dag is variable,1
" for many problems, the base cases and reduction rules usually consist of a finite (often small) number of cases",1
" in future work, we seek to enable more tasks with recursive structure",0
"
an alternative method is to run p on many different program inputs and then observe step input sequences which occur, to create v ",1
"
in order to demonstrate that recursion can help learn and generalize better, for addition, we trained only on traces for 5 arbitrarily chosen 1-digit addition sum examples",1
" also the dag can have potentially more than one connected component, meaning it is necessary to transition between these components appropriately",0
" however, to be sure that the generated v is complete (covers all the cases needed), we need to check all pairs of observations seen in adjacent step inputs (in particular, those before and after a primitive function call), in a similar way as if we were constructing v from scratch",1
"
algorithm 2 shows the topological sort task of interest",0
" for each length, we test each program on 30 randomly generated problems",1
", to change the value of phi) none described belowstack",0
"
base cases and reduction rules for bubble sort",1
" childlist ∈ nu is a vector of pointers, where childlist[i] points to the next child under consideration for vertex i",0
"
after finding v , we construct a set of problem inputs which, when executed on p , create exactly the step input sequences which make up v ",0
" empirically, existing models suffer from a common limitation—generalization becomes poor beyond a threshold level of complexity",0
" our implementation uses two stacks qstacklo and
qstackhi, each in ru , that store the arguments to the recursive quicksort calls in algorithm 3; before each recursive call, the appropriate arguments are popped off the stack and written to plo and phi",0
" the move operation has the following arguments:
arg 1 (pointer): presult, pstack, pstart, childlist[vactive], childlist[vsave]
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 5 values",0
" the assumption of leading 0’s can be easily removed, at the cost of slightly increasing the size of the verification set",1
"we train using the adam optimizer and use a 2-layer lstm and task-specific state encoders for the external environments, as described in reed & de freitas (2016)",0
 we instantiated recursion for the neural programmerinterpreter by changing the training traces,0
" in this architecture, we consider a core controller, e",0
" via the recursive call, we effectively forget that the column just added exists, since the recursive call to add starts with a new hidden state for the lstm controller",0
training neural networks to synthesize robust programs from a small number of examples is a challenging task,0
 the maximum problem length in this training set is 3 (e,1
" we further have qresult ∈ nu , a scratch-pad which contains the sorted list of vertices at the end of execution, and qstack ∈ nu , which serves the role of the stack s in algorithm 2",0
" hence, if all subproblems in s are added correctly, we have proven that any member of f will be added correctly, thus eliminating an infinite family of inputs that need to be tested",1
 ,0
"
in order to demonstrate that recursion can help learn and generalize better, we trained a non-recursive and recursive model on just a single execution trace generated from a graph containing 5 nodes3 for the topological sort task",1
"as discussed in section 2, the neural programmer-interpreter (npi) is an instance of a neural programmer architecture and hence it naturally supports recursion",1
" given verification sets that cover all the base cases and reduction rules, we can provide proofs that these learned programs generalize perfectly",0
" the 4 environment observations aid with control flow; qstacklo(pstacklo− 1) < qstackhi(pstackhi− 1) implements the lo < hi comparison in line 5 of algorithm 3, pstacklo == 1 checks if the stacks are empty in line 18 of algorithm 4, and the other observations (all involving ppivot or pj) deal with logic in the partition function",1
 recursion enables provable guarantees on neural programs’ behavior without needing to exhaustively enumerate all possible inputs to the programs,1
" recall that this verification procedure cannot be performed for the non-recursive versions, since the propagation of the hidden state in the core lstm module makes reasoning difficult and so we would need to check an unbounded number of examples",0
 this makes it difficult to reason about what the model will do when given complex inputs,0
" , a1a0 + b1b0} are added properly",1
 recursion can be implemented differently for different neural programming models,0
" fa8750-15-2-0104, and berkeley deep drive",1
 we then construct a verification set of size 73 by ensuring that randomly generated graphs cover the analytically derived v ,0
3,0
"
algorithm 3 recursive quicksort 1: initialize an array a to sort",1
" , n , where the dag contains n vertices",1
g,1
g,0
" for the recursive formulation of addition, we analytically construct the set of input problems that cover all base cases and reduction rules",0
" without using a recursive program, such a proof is not possible, because the non-recursive program runs on an arbitrarily long addition problem that creates correspondingly long sequences of step inputs; in the non-recursive formulation of addition, add calls add1 a number of times that is dependent on the length of the input",1
" without using a recursive program, such a proof is not possible, because the non-recursive program runs on an arbitrarily long addition problem that creates correspondingly long sequences of step inputs; in the non-recursive formulation of addition, add calls add1 a number of times that is dependent on the length of the input",0
 we choose to implement a topological sort task for graphs,0
" in particular, this condition is satisfied by npi, and thus the npi model naturally supports recursion (even though the authors of npi did not consider this aspect explicitly)",0
"
the training set for topological sort contains 6 traces, with one synthesized from a graph of size 5 and the rest synthesized from graphs of size 7",1
 the logic for the non-recursive trace is shown in algorithm 4 in the appendix,1
" we hypothesize that the reason for this is that the neural network learns to spuriously depend on specific characteristics of the training examples that are irrelevant to the true program semantics, such as length of the training inputs, and thus fails to generalize to more complex inputs",1
" thus, we demonstrate that recursion enables provably perfect generalization for different tasks, including addition, topological sort, quicksort, and a variant of bubble sort",0
"1, we hypothesize the non-recursive programs do not generalize well because they have learned spurious dependencies specific to the training set, such as length of the input problems",0
"
as mentioned in section 1, all approaches to neural programming today fare poorly on this generalization issue",0
" et+1 ∼ fenv(et, pt, at)",0
 this is due to the properties of the training set – we trained on 2 traces synthesized from arrays of length 7 and 1 trace synthesized from an array of length 6,1
 ,0
 errors arise due to undesirable and uninterpretable dependencies and associations the architecture learns to store in some high-dimensional hidden state,0
" to make an analogy to mdps, this procedure is analogous to how value iteration obtains the correct value for each state starting from any initialization",1
"
by reasoning about the possible set of environment observations created by all valid inputs, we construct v using the procedure described in section 3",0
"
the non-recursive trace loops on cycles of bubble and reset, which logically represents one bubble sweep through the array and reset of the two bubble pointers to the very beginning of the array, respectively",0
", qstacklo or qstackhi) or to pointer (e",0
 ,0
" because the context of the caller is stored on a stack when it calls another program and the callee starts in a fresh context, this enables recursion simply by allowing a program to call itself",0
"
in this paper, we propose that recursion is an important concept for neural programs as well",1
" for each task, given the verification set, we check the traces and results of the learned, to-be-verified neural program (described in section 4",0
" to the best of our knowledge, this is the first time verification has been applied to a neural program, providing provable guarantees about its behavior",0
"
the training data for the neural programmer-interpreter consists of full execution traces for the program of interest",0
 table 4 also demonstrates that the (verified) recursive program generalizes perfectly,1
" more specifically, when m is trained on simpler inputs, such as inputs of a small length, the generalization metric evaluates how well m will do on more complex inputs, such as inputs of much longer length",1
", pstart or childlist[vactive]) up or down none described belowwrite",0
"
in this general neural programming architecture, we show it is easy to support recursion",0
" the only model, to our knowledge, that does not train on input-output pairs is the neural programmer-interpreter (reed & de freitas, 2016), which trains on synthetic execution traces",1
" we have three observations: the color of the start node, the color of the active node’s next child to be considered, and whether the stack is empty",1
" because the context of the caller is stored on a stack when it calls another program and the callee starts in a fresh context, this enables recursion simply by allowing a program to call itself",0
" an example of part of an addition task trace, written in shorthand, is given in figure 1",0
" in particular, this condition is satisfied by npi, and thus the npi model naturally supports recursion (even though the authors of npi did not consider this aspect explicitly)",1
"
for topological sort, the domain-specific encoder is
fenc(dag,qcolor, pstack, pstart, vactive, childlist, at)
=mlp ([qcolor(pstart), qcolor(dag[vactive][childlist[vactive]]), pstack == 1, at(1), at(2), at(3)]),
where qcolor ∈ ru×4 is a scratch-pad that contains u rows, each containing one of four colors (white, gray, black, invalid) with one-hot encoding",1
" although the model is exposed to more and more data, it might learn spurious and overly complex representations of the program, as suggested in zaremba   (2016)",0
" in future work, we would like to decrease supervision and construct models that are capable of coming up with recursive abstractions themselves",0
 we leave this as future work to devise a verification procedure more appropriate to this setting,1
" for brevity, we refer the reader to the appendix for information about the program set and non-recursive and recursive trace-generating functions for quicksort",1
" in contrast, each call to the recursive add always runs for a fixed number of steps, even on arbitrarily long problems in f , so we can test that it performs correctly on a small, fixed number of step input sequences",0
" in particular, in the explore function, adding a tail recursive call resets and stores the hidden states associated with vertices in a stack-like fashion",1
 ,0
" partial recursion only adds a tail recursive call to bubblesort after bubble and reset, similar to the tail recursive call described previously for addition",0
" we also hope to decrease supervision, for example by training with only partial or non-recursive traces, and to develop novel neural programming architectures integrated directly with a notion of recursion",0
" on the other hand, the recursive programs have learned the true program semantics",0
"
the dag is represented as an adjacency list where dag[i][j] refers to the j-th child of vertex i",1
" in future work, we would like to decrease supervision and construct models that are capable of coming up with recursive abstractions themselves",0
" observe that the non-recursive program’s correctness degrades for length 11 and beyond, while the recursive program can sort any given array",0
"
the dag is represented as an adjacency list where dag[i][j] refers to the j-th child of vertex i",0
" without using a recursive program, such a proof is not possible, because the non-recursive program runs on an arbitrarily long addition problem that creates correspondingly long sequences of step inputs; in the non-recursive formulation of addition, add calls add1 a number of times that is dependent on the length of the input",0
"
base cases and reduction rules for topological sort",0
" even though the earlier version also generalized just as well in practice, relatively small differences in the formulation of the traces and the environment observations can drastically change the difficulty of verification",0
"
the non-recursive trace loops on cycles of add1 and lshift",0
" formally, the npi is represented by the following set of equations:
st = fenc(et, at)
ht = flstm(st, pt, ht−1)
rt = fend(ht), pt+1 = fprog(ht), at+1 = farg(ht)
t is a subscript denoting the time-step; fenc is a domain-specific encoder (to be described later) that takes in the environment slice et and arguments at; flstm represents the core module, which takes in the state st generated by fenc, a program embedding pt ∈ rp , and hidden lstm state ht; fend decodes the return probability rt; fprog decodes a program key embedding pt+1;1 and farg decodes arguments at+1",1
" for instance, for recursive addition, consider the family f of addition problems anan−1 ",1
"
to evaluate a neural network that learns a neural program to accomplish a certain task, one common evaluation metric is how well the learned model m generalizes",0
" if the step output specifies a primitive function call, we need to reason about how it can affect the environment so as to change the observation in the next step input",1
" as before, the non-recursive learned program lacks a provable guarantee of generalization, whereas we show later that the recursive learned program has one",1
g,1
" this guarantees that the step input sequences considered during verification contain all step input sequences which arise during execution of an unseen problem in f , leading to generalization to any problem in f ",0
" in future work, we would like to decrease supervision and construct models that are capable of coming up with recursive abstractions themselves",0
" we show that by modifying the training procedure, we enable the npi model to learn recursive neural programs",1
"
quicksort",1
" in this architecture, we consider a core controller, e",0
" while this matches the description from reed & de freitas (2016), we found that this causes an unnecessary blowup in the size of v and makes it much more difficult to construct the verification set",1
" k is set to 11, to denote the range of possible numbers (0 through 9), along with the start/end token (represented with the same encoding) which is observed when a pointer reaches beyond the bounds of the input",0
" there are 3 pointers (presult, pstack, pstart), presult points to the next empty location in qresult,
pstack points to the top of the stack in qstack, and pstart points to the candidate starting node for a connected component",0
" we prove every member of f is added properly, given that subproblems s = {anan−1 + bnbn−1, an−1an−2 + bn−1bn−2, ",1
" an example of part of an addition task trace, written in shorthand, is given in figure 1",0
 we find that recursion makes it easier for the network to learn the right program and generalize to unknown situations,0
 this makes it difficult to reason about what the model will do when given complex inputs,0
"
arg 2 (increment or decrement): up, down1 // top level topological sort call 2 toposort() { 3 while (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 } 0 11 traverse() { 12 check_child() 13 explore() 14 } 15 16 check_child() { 17 while (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 18 move(childlist[vactive], up) 19 } 20 21 explore() { 22 do 23 if (qcolor(dag[vactive][childlist[vactive]]) is white): 24 write(color_next, color_grey) 25 stack(push) 26 write(save) 27 write(active_neighb) 28 move(childlist[vsave], up) 29 else: 30 write(color_curr, color_black) 31 write(result) 32 move(presult, up) 33 if(pstack == 1): 34 break 35 else: 36 stack(pop) 37 check_child() 38 while (true) 39 } 40 41 stack(op) { 42 if (op == push): 43 write(stack_push) 44 move(pstack, up) 45 46 if (op == pop): 47 write(active_stack) 48 write(stack_pop) 49 move(pstack, down) 50 } 51 52 next_start() { 53 while(qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 54 move(pstart, up) 55 }altered recursive functions  // top level topological sort call 2 toposort() { 3 if (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 toposort() // recursive call 0 } 11 12 check_child() { 13 if (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 14 move(childlist[vactive], up) 15 check_child() // recursive call 16 } 17 18 explore() { 19 if (qcolor(dag[vactive][childlist[vactive]]) is white): 20 write(color_next, color_grey) 21 stack(push) 22 write(save) 23 write(active_neighb) 24 move(childlist[vsave], up) 25 else: 26 write(color_curr, color_black) 27 write(result) 28 move(presult, up) 29 if(pstack == 1): 30 return 31 else: 32 stack(pop) 33 check_child() 34 explore() // recursive call 35 } 36 37 next_start() { 38 if (qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 39 move(pstart, up) 40 next_start() // recursive call 41 }algorithm 4 iterative quicksort 1: initialize an array a to sort and two empty stacks slo and shi",0
"
algorithm 2 shows the topological sort task of interest",1
 ,1
"we show that if we incorporate recursion, the learned npi programs can achieve provably perfect generalization for different tasks",0
"
in order to demonstrate that recursion can help learn and generalize better, we trained a non-recursive and recursive model on just a single execution trace generated from a graph containing 5 nodes3 for the topological sort task",1
 ,0
 stack pop pushes −∞ values to qstacklo and qstackhi,0
 we found that changing the npi training traces is a simple way to enable this,0
" in this pair, the step input runs a subprogram add1 that has no arguments, and the step output contains a program write that has arguments of out and 1",1
" empirically, we observe that the learned recursive programs solve all valid inputs with 100% accuracy after training on a very small number of examples, out-performing previous generalization results",1
 this verification phase only needs to be performed once after training,1
" formally, the npi is represented by the following set of equations:
st = fenc(et, at)
ht = flstm(st, pt, ht−1)
rt = fend(ht), pt+1 = fprog(ht), at+1 = farg(ht)
t is a subscript denoting the time-step; fenc is a domain-specific encoder (to be described later) that takes in the environment slice et and arguments at; flstm represents the core module, which takes in the state st generated by fenc, a program embedding pt ∈ rp , and hidden lstm state ht; fend decodes the return probability rt; fprog decodes a program key embedding pt+1;1 and farg decodes arguments at+1",0
" for problems where the base cases may be extremely large or infinite, such as certain forms of motor control, recursion can still help reduce the problem of generalization to these two aspects and make the generalization problem significantly simpler to handle and reason about",0
" the second is the training procedure, which consists of the form of the training data and the optimization process",0
"
topological sort",1
"
the same set of problems was used to generate the training traces for all formulations of the task, for non-recursive and recursive versions",0
"1, we hypothesize the non-recursive programs do not generalize well because they have learned spurious dependencies specific to the training set, such as length of the input problems",1
"
the training data for the neural programmer-interpreter consists of full execution traces for the program of interest",1
" note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values, and arg 2 can only take one of 7 values",1
" we prove every member of f is added properly, given that subproblems s = {anan−1 + bnbn−1, an−1an−2 + bn−1bn−2, ",0
" the theorem states that for the same sequence of step inputs, the model produces the exact same step output as the target program it aims to learn",0
 the environment and return probability are omitted for readability,0
" we make the trace recursive by adding a tail recursive call into the trace for the add program after calling add1 and lshift, the original paper uses k = 10, but we found it necessary to augment the range with an end token, in order to terminate properly",0
 ,1
" we performed experiments on the partially recursive version in order to examine what happens when only one recursive call is implemented, when in reality three are required for perfect generalization",1
 the memory updates of these neural programs are so complex and interdependent that it is difficult to reason about the behaviors of the learned neural program under previously unseen situations (such as problems with longer inputs),0
" the swap operation has the following arguments:
arg 1 (swap object 1): ppivot
arg 2 (swap object 2): phi, pj
write",0
" nevertheless, we would like the model to quickly learn to represent the right semantics of the underlying program from a small number of training examples, not an exhaustive number of them",0
"
the three environment observations aid with control flow in algorithm 2",1
" training on the full recursive trace leads to perfect generalization, as shown in section 4",0
"
we describe the base cases, reduction rules, and the verification set for each task in appendix a",0
"as discussed in section 2, the neural programmer-interpreter (npi) is an instance of a neural programmer architecture and hence it naturally supports recursion",0
" naı̈vely, we might expect to synthesize and test an input for any sequence created by combining the four possible colors in two variables and another boolean variable for whether the stack is empty (so 32 possible observations at any point), but for various reasons, most of these combinations are impossible to occur at any given point in the execution trace",0
" the only model, to our knowledge, that does not train on input-output pairs is the neural programmer-interpreter (reed & de freitas, 2016), which trains on synthetic execution traces",0
" in particular, given that a recursion is defined by two properties as mentioned before, the base cases and the set of reduction rules, we can prove a recursive neural program generalizes perfectly if we can prove that (1) it performs correctly on the base cases; (2) it learns the reduction rules correctly",0
we emphasize that the notion of a neural recursive program has not been presented in the literature before: this is our main contribution,0
" the core lstm module’s hidden state is preserved over all these add1 calls, and it is difficult to interpret with certainty what happens over longer timesteps without concretely evaluating the lstm with an input of that length",0
"1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 while (pstacklo 6= 1): 6 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 7 stack(stack_pop) 8 else: 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 stack(stack_pop) 12 partition() 13 stack(stack_push_call2) 14 stack(stack_push_call1) 15 } 16 17 partition() { 18 set_pivot_lo() 19 set_j_lo() 20 compswap_loop() 21 swap(ppivot, phi) 22 set_j_null() 23 } 24 25 compswap_loop() { 26 while (pj 6= phi): 27 compswap() 28 move(pj, up) 29 } 30 31 compswap() { 32 if (a[pj ] ≤ a[phi]): 33 swap(ppivot, pj) 34 move(ppivot, up) 35 } 36 37 stack(op) { 38 if (op == stack_push_call1): 39 write(env_stack_lo, plo) 40 write(env_stack_hi, ppivot − 1) 41 move(pstacklo, up) 42 move(pstackhi, up) 43 44 if (op == stack_push_call2): 45 write(env_stack_lo, ppivot + 1) 46 write(env_stack_hi, phi) 47 move(pstacklo, up) 48 move(pstackhi, up) 49 50 if (op == stack_pop): 51 write(env_stack_lo, reset) 52 write(env_stack_hi, reset) 53 move(pstacklo, down) 54 move(pstackhi, down) 55 }altered recursive functions 1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 6 partition() 7 stack(stack_push_call2) 8 stack(stack_push_call1) 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 quicksort() // recursive call 12 stack(stack_pop) 13 write(phi, env_stack_hi_peek) 14 write(plo, env_stack_lo_peek) 15 quicksort() // recursive call 16 stack(stack_pop) 17 } 18 19 compswap_loop() { 20 if (pj 6= phi): 21 compswap() 22 move(pj, up) 23 compswap_loop() // recursive call 24 }in this section, we describe the space of base cases and reduction rules that must be covered for each of the four sample tasks, in order to create the verification set",1
 this algorithm is a variant of depth first search,0
"
figure 1 shows examples of non-recursive and recursive addition traces",0
" numerous models have been proposed for learning programs; to name a few, this includes the differentiable neural computer (graves  , 2016), neural turing machine (graves  , 2014), neural gpu (kaiser & sutskever, 2015), neural programmer (neelakantan  , 2015), pointer network (vinyals  , 2015), hierarchical attentive memory (andrychowicz & kurach, 2016), and neural random access machine (kurach  , 2016)",0
" any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of national science foundation and darpa",0
 a1a0 + bnbn−1 ,0
" the assumption of leading 0’s can be easily removed, at the cost of slightly increasing the size of the verification set",1
e,0
" 3: 4: function quicksort(a, lo, hi) 5: if lo < hi: then 6: p = partition(a, lo, hi) 7: quicksort(a, lo, p− 1) 8: quicksort(a, p+ 1, hi) 9: 10: function partition(a, lo, hi) 11: pivot = lo 12: for j ∈ [lo, hi− 1] : do 13: if a[j] ≤ a[hi] then 14: swap a[pivot] with a[j] 15: pivot = pivot+ 1 16: swap a[pivot] with a[hi] 17: return pivot
for quicksort, the domain-specific encoder is
fenc(qarray, qstacklo, qstackhi, plo, phi, pstacklo, pstackhi, ppivot, pj , at) =
mlp ([qarray(pj) ≤ qarray(phi), pj == phi, qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1), pstacklo == 1, at(1), at(2), at(3)]),
whereqarray ∈ ru×11 is a scratch-pad that contains u rows, each containing one of 11 values (one of the numbers 0 through 9 or an invalid state)",0
 the maximum problem length in this training set is 3 (e,0
 this algorithm is a variant of depth first search,0
"
to perform the verification as described here, it is critical to construct v correctly",0
" note that recursion helps make this process tractable, because we only need to test a finite number of inputs to show that the model will work correctly on inputs of unbounded complexity",1
" in particular, given that a recursion is defined by two properties as mentioned before, the base cases and the set of reduction rules, we can prove a recursive neural program generalizes perfectly if we can prove that (1) it performs correctly on the base cases; (2) it learns the reduction rules correctly",0
" as this complexity interfered with determining the base cases and reductions, we changed the algorithm to its current form",0
", the verification set",0
" in this pair, the step input runs a subprogram add1 that has no arguments, and the step output contains a program write that has arguments of out and 1",1
 this modification also enables us to sort arrays containing arbitrary comparable elements,1
", qstacklo or qstackhi) or to pointer (e",0
 table 4 also demonstrates that the (verified) recursive program generalizes perfectly,1
", to color a vertex) or variable (e",0
" lshift, used in reset, shifts the pointers left until reaching the start token",0
" however, to be sure that the generated v is complete (covers all the cases needed), we need to check all pairs of observations seen in adjacent step inputs (in particular, those before and after a primitive function call), in a similar way as if we were constructing v from scratch",0
" the general notion of recursion has been an important concept in many domains, including mathematics and computer science",1
"in this paper, we propose that the key abstraction of recursion is necessary for neural programs to generalize",1
" recursion is an important concept in programming languages and a critical tool to
reduce the complexity of programs",1
"
arg 2 (object to copy): env stack lo peek, env stack hi peek, phi, plo, ppivot − 1, ppivot + 1, reset
env stack lo peek and env stack hi peek represent qstacklo(pstacklo − 1) and qstackhi(pstackhi − 1), respectively",0
"
algorithm 3 recursive quicksort 1: initialize an array a to sort",1
" for instance, for recursive addition, consider the family f of addition problems anan−1 ",0
 almost all architectures train on program input/output pairs,0
g,0
" in this architecture, we consider a core controller, e",0
" for bubble sort, the domain-specific encoder is
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1), q(1, i2), i3 == length, at(1), at(2), at(3)]),
where the environment q ∈ r1×n×k is a scratch-pad that contains 1 row, to represent the state of the array as sorting proceeds in-place, andn columns",1
 figure 2 shows examples of traces for the different versions of bubble sort,0
" in particular, recursion can be implemented as a program calling itself",0
 we choose to implement a topological sort task for graphs,0
 the first is the actual model architecture,1
"1; and for bubble sort, appendix a",1
 almost all architectures train on program input/output pairs,0
" , a1a0 + b1b0} are added properly",0
"
quicksort",1
" provably perfect generalization implies the model will behave correctly, given any valid input",0
"
to construct this set, by using the reference implementation of each subprogram, we construct a mapping between two sets of environment observations: the first set consists of all observations that can occur at the beginning of a particular subprogram’s invocation, and the second set contains the observations at the end of that subprogram",1
 stack push call2 pushes pivot + 1 and hi to qstacklo and qstackhi,0
" the pointer at index i3 represents a counter internal to the environment that is incremented once after each pass of the algorithm (one cycle of bubble and reset); when incremented a number of times equal to the length of the array, the flag i3 == length becomes true and terminates the entire algorithm ",1
 we created a program set that reflects the semantics of algorithm 2,0
" in particular, we construct new training traces which explicitly contain recursive elements and show that with this type of trace, npi easily learns recursive programs",0
" this is possible if and only if the graph has no directed cycles; that is to say, it must be a directed acyclic graph (dag)",1
"
as mentioned in section 1, all approaches to neural programming today fare poorly on this generalization issue",1
" this guarantees that the step input sequences considered during verification contain all step input sequences which arise during execution of an unseen problem in f , leading to generalization to any problem in f ",1
3,0
" for these models, table 2 presents results on randomly generated dags of varying graph sizes (varying in the number of vertices)",0
" this functionality the original npi paper decodes to a program key embedding kt ∈ rk and then computes a program embedding pt+1, which we also did in our implementation, but we omit this for brevity",0
e,1
 indentation indicates the stack is one level deeper than before,1
" however, note that there is no provable guarantee that the non-recursive learned program will generalize to all inputs, whereas we show later that the recursive learned program has a provable guarantee of perfect generalization",1
" each step during an execution of the program does one of three things: (1) another subprogram along with associated arguments is called, as in line 10, (2) the program writes to the environment if it is primitive, as in line 8, or (3) the loop is terminated if the return probability exceeds a threshold α, after which the stack frame is popped and control is returned to the caller",0
"
under the assumption that there are no leading 0’s (except in the case of single digits) and the two numbers to be added have the same number of digits, the verification set for addition contains 20,181 input problems",0
", pstart or childlist[vactive]) up or down none described belowwrite",1
 ,1
", pstart or childlist[vactive]) up or down none described belowwrite",0
" 3: 4: function quicksort(a, lo, hi) 5: if lo < hi: then 6: p = partition(a, lo, hi) 7: quicksort(a, lo, p− 1) 8: quicksort(a, p+ 1, hi) 9: 10: function partition(a, lo, hi) 11: pivot = lo 12: for j ∈ [lo, hi− 1] : do 13: if a[j] ≤ a[hi] then 14: swap a[pivot] with a[j] 15: pivot = pivot+ 1 16: swap a[pivot] with a[hi] 17: return pivot
for quicksort, the domain-specific encoder is
fenc(qarray, qstacklo, qstackhi, plo, phi, pstacklo, pstackhi, ppivot, pj , at) =
mlp ([qarray(pj) ≤ qarray(phi), pj == phi, qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1), pstacklo == 1, at(1), at(2), at(3)]),
whereqarray ∈ ru×11 is a scratch-pad that contains u rows, each containing one of 11 values (one of the numbers 0 through 9 or an invalid state)",0
"
for topological sort, the domain-specific encoder is
fenc(dag,qcolor, pstack, pstart, vactive, childlist, at)
=mlp ([qcolor(pstart), qcolor(dag[vactive][childlist[vactive]]), pstack == 1, at(1), at(2), at(3)]),
where qcolor ∈ ru×4 is a scratch-pad that contains u rows, each containing one of four colors (white, gray, black, invalid) with one-hot encoding",0
we emphasize that the notion of a neural recursive program has not been presented in the literature before: this is our main contribution,1
 ,0
 this verification phase only needs to be performed once after training,1
" this makes it so that we only need to consider the vertices in the subgraph that are currently relevant for computing the sort, allowing simpler reasoning about behavior for large graphs",1
" thus, any general neural programming architecture supporting such a call structure can be made to support recursion",0
" for example, a step input-step output pair in lines 2 and 3 of the left-hand side of figure 1 is (add1, write out 1)",0
" as with the original npi architecture, the experiments for this paper always used a 3-tuple of integers at = (at(1), at(2), at(3))",0
" for bubble sort, the domain-specific encoder is
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1), q(1, i2), i3 == length, at(1), at(2), at(3)]),
where the environment q ∈ r1×n×k is a scratch-pad that contains 1 row, to represent the state of the array as sorting proceeds in-place, andn columns",0
"
arg 2 (auxiliary variable): color grey, color black
color grey and color black color the given vertex grey and black, respectively",1
g,0
"
training setup",0
 table 4 demonstrates that the accuracy of the non-recursive program degrades sharply when moving from arrays of length 7 to arrays of length 8,0
" an example of part of an addition task trace, written in shorthand, is given in figure 1",0
 table 3 presents results on randomly generated arrays of varying length for the learned non-recursive and recursive programs,1
 this paper is the first (to our knowledge) to investigate the important problem of provable generalization properties of neural programs,0
 this paper is the first (to our knowledge) to investigate the important problem of provable generalization properties of neural programs,0
" observe that partially recursive does slightly better than non-recursive for the setting in which the length of the array is 3, and that the fully recursive version is able to sort every array given to it",0
"
quicksort",1
 we found that changing the npi training traces is a simple way to enable this,1
 almost all architectures train on program input/output pairs,0
 u varies with the number of vertices in the graph,0
" an example of part of an addition task trace, written in shorthand, is given in figure 1",1
"
as a concrete instantiation, we show in this paper that we can enable recursive neural programs in the npi model, and thus enable perfectly generalizable neural programs for tasks such as sorting where the original, non-recursive npi program fails",1
"
algorithm 2 depth first search topological sort 1: color all vertices white",0
" the corresponding edge list is [(1, 2), (1, 5), (2, 4), (2, 5), (3, 5)]",0
" this is highly undesirable, since being able to provide the correct answer in all possible settings is one of the most important aspects of any learned neural program",1
" this agrees with the generalization of non-recursive addition in reed & de freitas (2016), where they reported generalization up to 3000 digits",0
" for these programs, we re-use the appropriate program sets (the associated subprograms), and we refer the reader to the appendix of reed & de freitas (2016) for further details on the subprograms used in addition and bubble sort",1
" this makes it so that we only need to consider the vertices in the subgraph that are currently relevant for computing the sort, allowing simpler reasoning about behavior for large graphs",1
we emphasize that the notion of a neural recursive program has not been presented in the literature before: this is our main contribution,0
 this modification also enables us to sort arrays containing arbitrary comparable elements,0
" training on the full recursive trace leads to perfect generalization, as shown in section 4",0
 18: push lo and p− 1 to slo and shi,1
", qstacklo or qstackhi) or to pointer (e",0
"
is critical for implementing recursion, since it permits us to restrict our attention to the currently relevant recursive call, ignoring irrelevant details about other contexts",0
" as described in the verification procedure, it is possible to prove our learned recursive program generalizes perfectly by testing on an appropriate set of problem inputs, i",1
"
we describe the base cases, reduction rules, and the verification set for each task in appendix a",1
" using this modification, we constructed a verification set consisting of one array of size 10",1
"
an alternative method is to run p on many different program inputs and then observe step input sequences which occur, to create v ",1
 pstacklo and pstackhi point to the top (empty) positions in qstacklo and qstackhi,1
 the environment and return probability are omitted for readability,1
" for example, for the addition task, the model is trained on short inputs and then tested on its ability to sum inputs with much longer numbers of digits",1
"1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 while (pstacklo 6= 1): 6 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 7 stack(stack_pop) 8 else: 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 stack(stack_pop) 12 partition() 13 stack(stack_push_call2) 14 stack(stack_push_call1) 15 } 16 17 partition() { 18 set_pivot_lo() 19 set_j_lo() 20 compswap_loop() 21 swap(ppivot, phi) 22 set_j_null() 23 } 24 25 compswap_loop() { 26 while (pj 6= phi): 27 compswap() 28 move(pj, up) 29 } 30 31 compswap() { 32 if (a[pj ] ≤ a[phi]): 33 swap(ppivot, pj) 34 move(ppivot, up) 35 } 36 37 stack(op) { 38 if (op == stack_push_call1): 39 write(env_stack_lo, plo) 40 write(env_stack_hi, ppivot − 1) 41 move(pstacklo, up) 42 move(pstackhi, up) 43 44 if (op == stack_push_call2): 45 write(env_stack_lo, ppivot + 1) 46 write(env_stack_hi, phi) 47 move(pstacklo, up) 48 move(pstackhi, up) 49 50 if (op == stack_pop): 51 write(env_stack_lo, reset) 52 write(env_stack_hi, reset) 53 move(pstacklo, down) 54 move(pstackhi, down) 55 }altered recursive functions 1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 6 partition() 7 stack(stack_push_call2) 8 stack(stack_push_call1) 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 quicksort() // recursive call 12 stack(stack_pop) 13 write(phi, env_stack_hi_peek) 14 write(plo, env_stack_lo_peek) 15 quicksort() // recursive call 16 stack(stack_pop) 17 } 18 19 compswap_loop() { 20 if (pj 6= phi): 21 compswap() 22 move(pj, up) 23 compswap_loop() // recursive call 24 }in this section, we describe the space of base cases and reduction rules that must be covered for each of the four sample tasks, in order to create the verification set",0
" at end, pointer ppivot is moved to the pivot compswap loop, move pivot lo, move j lo, swap
none
compswap loop runs the for loop inside the partition function compswap, move none compswap compares a[pivot] ≤ a[j]; if so, perform a swap and increment ppivot swap, move none set pivot lo sets ppivot to lo index none none set j lo sets pj to lo index none none set j null sets pj to −∞ none none stack pushes lo/hi states
onto stacks slo and shi according to argument (described below)
write, move described below
move moves pointer one unit up or down none described below swap swaps elements at given array indices none described below write write a value either to stack (e",0
"
to evaluate a neural network that learns a neural program to accomplish a certain task, one common evaluation metric is how well the learned model m generalizes",0
"
in order to demonstrate that recursion can help learn and generalize better, for addition, we trained only on traces for 5 arbitrarily chosen 1-digit addition sum examples",0
 b1b0 where no carry operations occur,1
" this is highly undesirable, since being able to provide the correct answer in all possible settings is one of the most important aspects of any learned neural program",0
"
we describe the details of the npi model relevant to our contributions",1
" thus, recursion helps decompose a problem and makes it easier to reason about a program’s behavior for previously unseen situations such as longer inputs",0
" the arrow in the theorem refers to evaluation, as in big-step semantics",0
g,1
" we have three observations: the color of the start node, the color of the active node’s next child to be considered, and whether the stack is empty",0
"
grade-school addition",0
"we train using the adam optimizer and use a 2-layer lstm and task-specific state encoders for the external environments, as described in reed & de freitas (2016)",1
"
algorithm 2 shows the topological sort task of interest",1
" twc-1409915, darpa under grant no",0
 the training set for addition contains 200 traces,0
" the single-digit multiplication task in zaremba   (2016), the bubble sort task in reed & de freitas (2016), and the graph tasks in graves   (2016))",0
" on the other hand, the non-recursive version has low accuracy, beginning from size 5, and fails completely for graphs of size 8 and beyond",1
"as there is no public implementation of npi, we implemented a version of it in keras that is as faithful to the paper as possible",0
"
bubble sort",0
 these pointers are referred to as bubble pointers,1
g,0
"
base cases and reduction rules for bubble sort",1
" if the program is primitive, the next environmental state et+1 will be affected by pt and at, i",0
 we found that changing the npi training traces is a simple way to enable this,1
"
the training set for bubble sort contains 100 traces, with maximum problem length of 2 (e",0
 recursion can be implemented differently for different neural programming models,0
" recall that this verification procedure cannot be performed for the non-recursive versions, since the propagation of the hidden state in the core lstm module makes reasoning difficult and so we would need to check an unbounded number of examples",0
" numerous models have been proposed for learning programs; to name a few, this includes the differentiable neural computer (graves  , 2016), neural turing machine (graves  , 2014), neural gpu (kaiser & sutskever, 2015), neural programmer (neelakantan  , 2015), pointer network (vinyals  , 2015), hierarchical attentive memory (andrychowicz & kurach, 2016), and neural random access machine (kurach  , 2016)",1
", to change the value of vactive) none described below move move a pointer (e",1
", to change the value of vactive) none described below move move a pointer (e",1
 we adapt machinery from the original paper slightly to fit our needs,1
" for many problems, the base cases and reduction rules usually consist of a finite (often small) number of cases",0
" this functionality the original npi paper decodes to a program key embedding kt ∈ rk and then computes a program embedding pt+1, which we also did in our implementation, but we omit this for brevity",0
" however, the authors of npi did not consider explicitly the notion of recursion and as a consequence, did not learn recursive programs",1
"
the operations we need to be concerned most about are carry and lshift, which induce partial environment states spanning two columns",0
 table 4 demonstrates that the accuracy of the non-recursive program degrades sharply when moving from arrays of length 7 to arrays of length 8,0
"we train using the adam optimizer and use a 2-layer lstm and task-specific state encoders for the external environments, as described in reed & de freitas (2016)",1
g,0
"
for topological sort, the domain-specific encoder is
fenc(dag,qcolor, pstack, pstart, vactive, childlist, at)
=mlp ([qcolor(pstart), qcolor(dag[vactive][childlist[vactive]]), pstack == 1, at(1), at(2), at(3)]),
where qcolor ∈ ru×4 is a scratch-pad that contains u rows, each containing one of four colors (white, gray, black, invalid) with one-hot encoding",0
"
is critical for implementing recursion, since it permits us to restrict our attention to the currently relevant recursive call, ignoring irrelevant details about other contexts",1
" at any given time, the npi has access to the values referred to by two pointers, represented by q(1, i1) and q(1, i2),",0
"
the training data for the neural programmer-interpreter consists of full execution traces for the program of interest",1
" we made the assumption of equivalent lengths in order to parametrize the input format with respect to length, but this assumption can be removed as well",0
 table 3 presents results on randomly generated arrays of varying length for the learned non-recursive and recursive programs,1
" we made the assumption of equivalent lengths in order to parametrize the input format with respect to length, but this assumption can be removed as well",0
"
we describe the base cases, reduction rules, and the verification set for each task in appendix a",1
"
we refer the reader to the appendix for the non-recursive trace generating functions",1
" the learned recursive program is different from neural programs learned in all previous work in an important aspect: previous approaches do not explicitly incorporate this abstraction, and hence generalize poorly, whereas our learned neural programs incorporate recursion and achieve perfect generalization",1
"
algorithm 3 recursive quicksort 1: initialize an array a to sort",1
" naı̈vely, we might expect to synthesize and test an input for any sequence created by combining the four possible colors in two variables and another boolean variable for whether the stack is empty (so 32 possible observations at any point), but for various reasons, most of these combinations are impossible to occur at any given point in the execution trace",1
" the core module of the npi is an lstm controller that takes as input a slice of the current external environment, via a set of pointers, and a program and arguments to execute",0
" if the step output specifies a primitive function call, we need to reason about how it can affect the environment so as to change the observation in the next step input",0
" by iterating with this procedure, and then running p on the input observation set that we obtain for the entry point function, we can obtain v precisely",1
" twc-1409915, darpa under grant no",0
" note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values, and arg 2 can only take one of 7 values",1
", an lstm in npi’s case, but possibly other networks in different cases",0
 the maximum problem length in this training set is 3 (e,0
as mentioned in section 2,0
"
the operations we need to be concerned most about are carry and lshift, which induce partial environment states spanning two columns",1
" for each function we use to implement the recursive version of topological sort, we need to consider the set of possible environment observation sequences we can create from all valid inputs and test that the learned program produces the correct behavior on each of these inputs",0
" at any given time, the npi has access to the values referred to by two pointers, represented by q(1, i1) and q(1, i2),",1
"
through careful reasoning about the possible set of environment observations created by all valid inputs, and how each of the operations in the execution trace affects the environment, we can construct v using the procedure described in section 3",1
" the npi model has three learnable components: a task-agnostic core, a program-key embedding, and domain-specific encoders that allow the npi to operate in diverse environments",0
" et+1 ∼ fenv(et, pt, at)",1
2,1
we emphasize that the notion of a neural recursive program has not been presented in the literature before: this is our main contribution,1
", to change the value of phi) none described belowstack",0
"
in addition, none of the current approaches to neural programming provide a method or even aim to enable provable guarantees about generalization",0
"when constructing a neural network for the purpose of learning a program, there are two orthogonal aspects to consider",1
" this is possible if and only if the graph has no directed cycles; that is to say, it must be a directed acyclic graph (dag)",0
" also, compared to the bubble pointers in bubble sort, the pointers that perform the comparison for quicksort (the compswap function) are usually not adjacent to each other, making quicksort less local than bubble sort",0
" the only model, to our knowledge, that does not train on input-output pairs is the neural programmer-interpreter (reed & de freitas, 2016), which trains on synthetic execution traces",0
 this algorithm is a variant of depth first search,0
" the recursive version can generalize perfectly to long problems constructed from these components (such as the sum “822+233”, where “8+2” and “2+3” are in the training set), but the non-recursive version fails to sum these long problems properly",1
" as described in the verification procedure, it is possible to prove our learned recursive program generalizes perfectly by testing on an appropriate set of problem inputs, i",0
"
the three environment observations aid with control flow in algorithm 2",1
"
an alternative way of representing the environment slice is to expose the values of the absolute vertices to the model; however, this makes it difficult to scale the model to larger graphs, since large vertex values are not seen during training time",0
" we show that by modifying the training procedure, we enable the npi model to learn recursive neural programs",0
"1; and for bubble sort, appendix a",0
" 4: function toposort(dag) 5: while there is still a white vertex u: do 6: color[u] = grey 7: vactive = u 8: do 9: if vactive has a white child v then 10: color[v] = grey 11: push vactive onto s 12: vactive = v 13: else 14: color[vactive] = black 15: write vactive to result 16: if s is empty then pass 17: else pop the top vertex off s and set it to vactive 18: while s is not empty
topological sort",0
", an array for quicksort or a dag for topological sort)",0
" for each length, we test each program on 30 randomly generated problems",0
" in fact, we argue that recursion is an essential element for neural programs to generalize, and makes it tractable to prove the generalization of neural programs",0
" as this complexity interfered with determining the base cases and reductions, we changed the algorithm to its current form",0
"
grade school addition",0
 stack push call2 pushes pivot + 1 and hi to qstacklo and qstackhi,0
"
move",1
" in fact, it is one of our future directions to explore new ways to train a recursive neural program without providing explicit training execution traces or with only partial or non-recursive traces",0
"
is critical for implementing recursion, since it permits us to restrict our attention to the currently relevant recursive call, ignoring irrelevant details about other contexts",0
"
base cases and reduction rules for quicksort",1
" more specifically, when m is trained on simpler inputs, such as inputs of a small length, the generalization metric evaluates how well m will do on more complex inputs, such as inputs of much longer length",0
g,1
" in particular, in the explore function, adding a tail recursive call resets and stores the hidden states associated with vertices in a stack-like fashion",1
"
inside bubble and reset, there are two operations that can be made recursive",0
"
as a concrete instantiation, we show in this paper that we can enable recursive neural programs in the npi model, and thus enable perfectly generalizable neural programs for tasks such as sorting where the original, non-recursive npi program fails",0
 we outline how to construct this set,0
"
in order to demonstrate that recursion can help learn and generalize better, for addition, we trained only on traces for 5 arbitrarily chosen 1-digit addition sum examples",0
"
in this paper, we propose that recursion is an important concept for neural programs as well",0
 we find that recursion makes it easier for the network to learn the right program and generalize to unknown situations,1
" both the non-recursive and recursive learned programs generalize on all input lengths we tried, up to 5000 digits",0
 our experiments use a small number of training examples,0
"
by nature, recursion reduces the complexity of a problem to simpler instances",0
"
grade school addition",0
"
in order to demonstrate that recursion can help learn and generalize better, we trained a non-recursive and recursive model on just a single execution trace generated from a graph containing 5 nodes3 for the topological sort task",1
" this agrees with the generalization of non-recursive addition in reed & de freitas (2016), where they reported generalization up to 3000 digits",1
" this agrees with the generalization of non-recursive addition in reed & de freitas (2016), where they reported generalization up to 3000 digits",0
" in fact, we argue that recursion is an essential element for neural programs to generalize, and makes it tractable to prove the generalization of neural programs",0
" to make an analogy to mdps, this procedure is analogous to how value iteration obtains the correct value for each state starting from any initialization",1
" for grade-school addition, the domain-specific encoder is
fenc(q, i1, i2, i3, i4, at) =mlp ([q(1, i1), q(2, i2), q(3, i3), q(4, i4), at(1), at(2), at(3)]),
where the environment q ∈ r4×n×k is a scratch-pad that contains four rows (the first input number, the second input number, the carry bits, and the output) and n columns",1
" for each task, given the verification set, we check the traces and results of the learned, to-be-verified neural program (described in section 4",0
"
bubble sort",0
"
the training set for quicksort contains 4 traces, synthesized from arrays of length 5",0
" the pointer at index i3 represents a counter internal to the environment that is incremented once after each pass of the algorithm (one cycle of bubble and reset); when incremented a number of times equal to the length of the array, the flag i3 == length becomes true and terminates the entire algorithm ",1
" at any given time, the npi has access to the values referred to by two pointers, represented by q(1, i1) and q(1, i2),",0
", an lstm in npi’s case, but possibly other networks in different cases",1
"as there is no public implementation of npi, we implemented a version of it in keras that is as faithful to the paper as possible",1
 the logic for the non-recursive trace is shown in algorithm 4 in the appendix,0
" a topological sort is a linear ordering of vertices such that for every directed edge (u, v) from u to v, u comes before v in the ordering",0
"
training setup",0
"
for topological sort, the domain-specific encoder is
fenc(dag,qcolor, pstack, pstart, vactive, childlist, at)
=mlp ([qcolor(pstart), qcolor(dag[vactive][childlist[vactive]]), pstack == 1, at(1), at(2), at(3)]),
where qcolor ∈ ru×4 is a scratch-pad that contains u rows, each containing one of four colors (white, gray, black, invalid) with one-hot encoding",0
 stack push call2 pushes pivot + 1 and hi to qstacklo and qstackhi,0
"
we describe the base cases, reduction rules, and the verification set for each task in appendix a",0
"
algorithm 3 recursive quicksort 1: initialize an array a to sort",0
" in particular, we construct new training traces which explicitly contain recursive elements and show that with this type of trace, npi easily learns recursive programs",0
" for each length, we test each program on 30 randomly generated problems",1
" the non-recursive and partially recursive versions are unable to sort long arrays, beyond length 8",0
" however, models that make use of this strategy eventually fail after a certain level of complexity (e",0
" also, compared to the bubble pointers in bubble sort, the pointers that perform the comparison for quicksort (the compswap function) are usually not adjacent to each other, making quicksort less local than bubble sort",1
" formally, the npi is represented by the following set of equations:
st = fenc(et, at)
ht = flstm(st, pt, ht−1)
rt = fend(ht), pt+1 = fprog(ht), at+1 = farg(ht)
t is a subscript denoting the time-step; fenc is a domain-specific encoder (to be described later) that takes in the environment slice et and arguments at; flstm represents the core module, which takes in the state st generated by fenc, a program embedding pt ∈ rp , and hidden lstm state ht; fend decodes the return probability rt; fprog decodes a program key embedding pt+1;1 and farg decodes arguments at+1",1
" both the non-recursive and recursive learned programs generalize on all graphs we tried, up to 120 vertices",0
 18: push lo and p− 1 to slo and shi,0
 we adapt machinery from the original paper slightly to fit our needs,1
 the sequence of primitive operations (move and write operations) for the non-recursive and recursive versions are exactly the same,0
 there is a (changing) list of neural programs used to accomplish a given task,0
" formally, a function exhibits recursive behavior when it possesses two properties: (1) base cases— terminating scenarios that do not use recursion to produce answers; (2) a set of rules that reduces all other problems toward the base cases",0
" fa8750-15-2-0104, and berkeley deep drive",0
" in fact, we argue that recursion is an essential element for neural programs to generalize, and makes it tractable to prove the generalization of neural programs",0
" however, the authors of npi did not consider explicitly the notion of recursion and as a consequence, did not learn recursive programs",1
"
the non-recursive trace loops on cycles of add1 and lshift",0
" twc-1409915, darpa under grant no",1
" for tasks other than addition, it is difficult to analytically determine the verification set, so instead, we randomly generate input candidates until they completely cover the base cases and reduction rules",0
" in this version of curriculum learning, even though the inputs are gradually becoming more complex, the semantics of the program is succinct and does not change",0
" when the program is called, the current context including the caller’s memory state is stored on a stack; when the program returns, the stored context is popped off the stack to resume execution in the previous caller’s context",0
" for each function we use to implement the recursive version of topological sort, we need to consider the set of possible environment observation sequences we can create from all valid inputs and test that the learned program produces the correct behavior on each of these inputs",1
"
we describe the base cases, reduction rules, and the verification set for each task in appendix a",0
" the outputs rt, pt+1, at+1 are used to determine the next action, as described in algorithm 1",1
", the trace corresponding to the array [3,2])",1
 these pointers are referred to as bubble pointers,1
" given a precise definition of p , it may be possible to automate the generation of v from p in future work",0
", to color a vertex) or variable (e",0
" this guarantees that the step input sequences considered during verification contain all step input sequences which arise during execution of an unseen problem in f , leading to generalization to any problem in f ",0
"
algorithm 2 depth first search topological sort 1: color all vertices white",0
3,1
" recall that this verification procedure cannot be performed for the non-recursive versions, since the propagation of the hidden state in the core lstm module makes reasoning difficult and so we would need to check an unbounded number of examples",0
" as with the original npi architecture, the experiments for this paper always used a 3-tuple of integers at = (at(1), at(2), at(3))",1
"
in addition, none of the current approaches to neural programming provide a method or even aim to enable provable guarantees about generalization",0
"
the non-recursive trace loops on cycles of add1 and lshift",1
2,0
"
algorithm 3 recursive quicksort 1: initialize an array a to sort",0
"
the training set for topological sort contains 6 traces, with one synthesized from a graph of size 5 and the rest synthesized from graphs of size 7",1
"
the training data for the neural programmer-interpreter consists of full execution traces for the program of interest",0
 the program terminates when seeing no numbers in the current column,1
 the training set for addition contains 200 traces,0
" given a precise definition of p , it may be possible to automate the generation of v from p in future work",1
"
in this paper, we propose that recursion is an important concept for neural programs as well",1
"
we note that for tasks with very large input domains, such as ones involving mnist digits or speech samples, the state space of base cases and reduction rules could be prohibitively large, possibly infinite",1
" each time a subprogram is called, the stack depth increases",1
" consequently, there is no concept of length relevant to the problem, which has traditionally been an important focus of length-based curriculum learning",0
3,0
"
thus far, to evaluate the efficacy of neural models on programming tasks, the only metric that has been used is generalization of expected behavior to inputs of greater complexity (vinyals   (2015), kaiser & sutskever (2015), reed & de freitas (2016), graves   (2016), zaremba   (2016))",0
" given a precise definition of p , it may be possible to automate the generation of v from p in future work",0
 we found that changing the npi training traces is a simple way to enable this,1
" 3: 4: function quicksort(a, lo, hi) 5: if lo < hi: then 6: p = partition(a, lo, hi) 7: quicksort(a, lo, p− 1) 8: quicksort(a, p+ 1, hi) 9: 10: function partition(a, lo, hi) 11: pivot = lo 12: for j ∈ [lo, hi− 1] : do 13: if a[j] ≤ a[hi] then 14: swap a[pivot] with a[j] 15: pivot = pivot+ 1 16: swap a[pivot] with a[hi] 17: return pivot
for quicksort, the domain-specific encoder is
fenc(qarray, qstacklo, qstackhi, plo, phi, pstacklo, pstackhi, ppivot, pj , at) =
mlp ([qarray(pj) ≤ qarray(phi), pj == phi, qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1), pstacklo == 1, at(1), at(2), at(3)]),
whereqarray ∈ ru×11 is a scratch-pad that contains u rows, each containing one of 11 values (one of the numbers 0 through 9 or an invalid state)",1
"
we would also like to point out that in this paper, we provide as an example one way to train a recursive neural program, by providing a certain training execution trace to the npi model",1
" for example, a step input-step output pair in lines 2 and 3 of the left-hand side of figure 1 is (add1, write out 1)",0
" the learned recursive program is different from neural programs learned in all previous work in an important aspect: previous approaches do not explicitly incorporate this abstraction, and hence generalize poorly, whereas our learned neural programs incorporate recursion and achieve perfect generalization",0
 ,1
"as discussed in section 2, the neural programmer-interpreter (npi) is an instance of a neural programmer architecture and hence it naturally supports recursion",0
g,1
"
bubble sort",0
" 16: p = partition(a, lo, hi) 17: push p+ 1 and hi to slo and shi",1
" 2: initialize lo and hi to be 1 and n, where n is the length of a",1
"in this paper, we propose that the key abstraction of recursion is necessary for neural programs to generalize",0
"1, we hypothesize the non-recursive programs do not generalize well because they have learned spurious dependencies specific to the training set, such as length of the input problems",0
 table 4 also demonstrates that the (verified) recursive program generalizes perfectly,0
"
to perform the verification as described here, it is critical to construct v correctly",0
 our experiments use a small number of training examples,0
" here as a concrete and general example, we consider a general neural programming architecture (npa), similar to neural programmer-interpreter (npi) in reed & de freitas (2016)",1
 18: push lo and p− 1 to slo and shi,0
"
the non-recursive trace loops on cycles of bubble and reset, which logically represents one bubble sweep through the array and reset of the two bubble pointers to the very beginning of the array, respectively",1
" this guarantees that the step input sequences considered during verification contain all step input sequences which arise during execution of an unseen problem in f , leading to generalization to any problem in f ",0
" as an application, we incorporate recursion into the neural programmer-interpreter architecture and consider four sample tasks: grade-school addition, bubble sort, topological sort, and quicksort",0
" thus, recursion helps decompose a problem and makes it easier to reason about a program’s behavior for previously unseen situations such as longer inputs",0
" however, the authors of npi did not consider explicitly the notion of recursion and as a consequence, did not learn recursive programs",0
"
the training set for topological sort contains 6 traces, with one synthesized from a graph of size 5 and the rest synthesized from graphs of size 7",0
" we further have qresult ∈ nu , a scratch-pad which contains the sorted list of vertices at the end of execution, and qstack ∈ nu , which serves the role of the stack s in algorithm 2",0
"
the training set for quicksort contains 4 traces, synthesized from arrays of length 5",0
 we then construct a verification set of size 73 by ensuring that randomly generated graphs cover the analytically derived v ,0
" we can obtain this mapping by first considering the possible observations that can arise at the beginning of the entry function (add, bubblesort, toposort, and quicksort) for some valid program input, and iteratively applying the observation-to-observation mapping implied by the reference implementation’s step output at that point in the execution",0
" the stack operation has the following arguments:
arg 1 (operation): stack push call1, stack push call2, stack pop
stack push call1 pushes lo and pivot−1 toqstacklo andqstackhi",1
"1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 while (pstacklo 6= 1): 6 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 7 stack(stack_pop) 8 else: 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 stack(stack_pop) 12 partition() 13 stack(stack_push_call2) 14 stack(stack_push_call1) 15 } 16 17 partition() { 18 set_pivot_lo() 19 set_j_lo() 20 compswap_loop() 21 swap(ppivot, phi) 22 set_j_null() 23 } 24 25 compswap_loop() { 26 while (pj 6= phi): 27 compswap() 28 move(pj, up) 29 } 30 31 compswap() { 32 if (a[pj ] ≤ a[phi]): 33 swap(ppivot, pj) 34 move(ppivot, up) 35 } 36 37 stack(op) { 38 if (op == stack_push_call1): 39 write(env_stack_lo, plo) 40 write(env_stack_hi, ppivot − 1) 41 move(pstacklo, up) 42 move(pstackhi, up) 43 44 if (op == stack_push_call2): 45 write(env_stack_lo, ppivot + 1) 46 write(env_stack_hi, phi) 47 move(pstacklo, up) 48 move(pstackhi, up) 49 50 if (op == stack_pop): 51 write(env_stack_lo, reset) 52 write(env_stack_hi, reset) 53 move(pstacklo, down) 54 move(pstackhi, down) 55 }altered recursive functions 1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 6 partition() 7 stack(stack_push_call2) 8 stack(stack_push_call1) 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 quicksort() // recursive call 12 stack(stack_pop) 13 write(phi, env_stack_hi_peek) 14 write(plo, env_stack_lo_peek) 15 quicksort() // recursive call 16 stack(stack_pop) 17 } 18 19 compswap_loop() { 20 if (pj 6= phi): 21 compswap() 22 move(pj, up) 23 compswap_loop() // recursive call 24 }in this section, we describe the space of base cases and reduction rules that must be covered for each of the four sample tasks, in order to create the verification set",0
" for each length, we test each program on 30 randomly generated problems",1
" while this matches the description from reed & de freitas (2016), we found that this causes an unnecessary blowup in the size of v and makes it much more difficult to construct the verification set",0
" nevertheless, we would like the model to quickly learn to represent the right semantics of the underlying program from a small number of training examples, not an exhaustive number of them",0
 reset represents a −∞ value,1
" recall that this verification procedure cannot be performed for the non-recursive versions, since the propagation of the hidden state in the core lstm module makes reasoning difficult and so we would need to check an unbounded number of examples",1
" the 4 environment observations aid with control flow; qstacklo(pstacklo− 1) < qstackhi(pstackhi− 1) implements the lo < hi comparison in line 5 of algorithm 3, pstacklo == 1 checks if the stacks are empty in line 18 of algorithm 4, and the other observations (all involving ppivot or pj) deal with logic in the partition function",0
"
the training set for topological sort contains 6 traces, with one synthesized from a graph of size 5 and the rest synthesized from graphs of size 7",0
" qcolor(pstart) contains the color of the current start vertex, used in the evaluation of the condition in the while loop in line 5 of algorithm 2",0
 we adapt machinery from the original paper slightly to fit our needs,0
 this paper is the first (to our knowledge) to investigate the important problem of provable generalization properties of neural programs,1
" we show that by modifying the training procedure, we enable the npi model to learn recursive neural programs",1
g,0
 the maximum problem length in this training set is 3 (e,0
"
directed acyclic graphs are structurally more diverse than inputs in the two tasks of grade-school addition and bubble sort",0
"1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 while (pstacklo 6= 1): 6 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 7 stack(stack_pop) 8 else: 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 stack(stack_pop) 12 partition() 13 stack(stack_push_call2) 14 stack(stack_push_call1) 15 } 16 17 partition() { 18 set_pivot_lo() 19 set_j_lo() 20 compswap_loop() 21 swap(ppivot, phi) 22 set_j_null() 23 } 24 25 compswap_loop() { 26 while (pj 6= phi): 27 compswap() 28 move(pj, up) 29 } 30 31 compswap() { 32 if (a[pj ] ≤ a[phi]): 33 swap(ppivot, pj) 34 move(ppivot, up) 35 } 36 37 stack(op) { 38 if (op == stack_push_call1): 39 write(env_stack_lo, plo) 40 write(env_stack_hi, ppivot − 1) 41 move(pstacklo, up) 42 move(pstackhi, up) 43 44 if (op == stack_push_call2): 45 write(env_stack_lo, ppivot + 1) 46 write(env_stack_hi, phi) 47 move(pstacklo, up) 48 move(pstackhi, up) 49 50 if (op == stack_pop): 51 write(env_stack_lo, reset) 52 write(env_stack_hi, reset) 53 move(pstacklo, down) 54 move(pstackhi, down) 55 }altered recursive functions 1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 6 partition() 7 stack(stack_push_call2) 8 stack(stack_push_call1) 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 quicksort() // recursive call 12 stack(stack_pop) 13 write(phi, env_stack_hi_peek) 14 write(plo, env_stack_lo_peek) 15 quicksort() // recursive call 16 stack(stack_pop) 17 } 18 19 compswap_loop() { 20 if (pj 6= phi): 21 compswap() 22 move(pj, up) 23 compswap_loop() // recursive call 24 }in this section, we describe the space of base cases and reduction rules that must be covered for each of the four sample tasks, in order to create the verification set",0
3 to construct v and then empirically create a verification set which covers v ,0
"
arg 2 (object to copy): env stack lo peek, env stack hi peek, phi, plo, ppivot − 1, ppivot + 1, reset
env stack lo peek and env stack hi peek represent qstacklo(pstacklo − 1) and qstackhi(pstackhi − 1), respectively",0
"program descriptions calls arguments toposort perform topological
sort on graph traverse, next start, write, move
none
traverse traverse graph until stack is empty check child, explore none check child check if a white child exists; if so, set childlist[vactive] to point to it move none explore repeatedly traverse subgraphs until stack is empty stack, check child, write, move none stack interact with stack, either pushing or popping write, move push, pop next start move pstart until reaching a white vertex",0
" for bubble sort, the domain-specific encoder is
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1), q(1, i2), i3 == length, at(1), at(2), at(3)]),
where the environment q ∈ r1×n×k is a scratch-pad that contains 1 row, to represent the state of the array as sorting proceeds in-place, andn columns",0
" as this complexity interfered with determining the base cases and reductions, we changed the algorithm to its current form",0
"
base cases and reduction rules for bubble sort",0
" for grade-school addition, the domain-specific encoder is
fenc(q, i1, i2, i3, i4, at) =mlp ([q(1, i1), q(2, i2), q(3, i3), q(4, i4), at(1), at(2), at(3)]),
where the environment q ∈ r4×n×k is a scratch-pad that contains four rows (the first input number, the second input number, the carry bits, and the output) and n columns",0
 we outline how to construct this set,1
" a single element of an execution trace consists of a step input-step output pair, which can be synthesized from algorithm 1: this corresponds to, for a given time-step, the step input tuple (e, p, a) and step output tuple (r, p2, a2)",0
" we prove every member of f is added properly, given that subproblems s = {anan−1 + bnbn−1, an−1an−2 + bn−1bn−2, ",0
 we outline how to construct this set,0
" in this section, we give a brief review of the npi architecture from reed & de freitas (2016) as background",1
" for brevity, we refer the reader to the appendix for further details on the program set and non-recursive and recursive trace-generating functions used for topological sort",1
" 4: function toposort(dag) 5: while there is still a white vertex u: do 6: color[u] = grey 7: vactive = u 8: do 9: if vactive has a white child v then 10: color[v] = grey 11: push vactive onto s 12: vactive = v 13: else 14: color[vactive] = black 15: write vactive to result 16: if s is empty then pass 17: else pop the top vertex off s and set it to vactive 18: while s is not empty
topological sort",1
"
by nature, recursion reduces the complexity of a problem to simpler instances",1
" m , as described in algorithm 1, processes the sequence of step inputs by using an lstm",1
" more specifically, when m is trained on simpler inputs, such as inputs of a small length, the generalization metric evaluates how well m will do on more complex inputs, such as inputs of much longer length",1
" the corresponding edge list is [(1, 2), (1, 5), (2, 4), (2, 5), (3, 5)]",0
" naı̈vely, we might expect to synthesize and test an input for any sequence created by combining the four possible colors in two variables and another boolean variable for whether the stack is empty (so 32 possible observations at any point), but for various reasons, most of these combinations are impossible to occur at any given point in the execution trace",1
" note that recursion helps make this process tractable, because we only need to test a finite number of inputs to show that the model will work correctly on inputs of unbounded complexity",1
"
since npi naturally supports the notion of recursion, a key question is how to enable npi to learn recursive programs",0
"
quicksort",1
" finally, we implement recursive traces for our own topological sort and quicksort tasks",0
"
we describe the details of the npi model relevant to our contributions",0
 ,0
" we then construct problem configurations that span all possible valid environment states (for instance, in order to force the carry bit in a column to be 1, one can add the sum “1+9” in the column to the right)",1
" by iterating with this procedure, and then running p on the input observation set that we obtain for the entry point function, we can obtain v precisely",0
 a1a0 + bnbn−1 ,0
 table 3 presents results on randomly generated arrays of varying length for the learned non-recursive and recursive programs,0
" when the program is called, the current context including the caller’s memory state is stored on a stack; when the program returns, the stored context is popped off the stack to resume execution in the previous caller’s context",0
" both the non-recursive and recursive learned programs generalize on all graphs we tried, up to 120 vertices",0
" also, compared to the bubble pointers in bubble sort, the pointers that perform the comparison for quicksort (the compswap function) are usually not adjacent to each other, making quicksort less local than bubble sort",0
" on the other hand, the recursive programs have learned the true program semantics",1
"
an alternative way of representing the environment slice is to expose the values of the absolute vertices to the model; however, this makes it difficult to scale the model to larger graphs, since large vertex values are not seen during training time",1
" on the other hand, the recursive programs have learned the true program semantics",0
" as an application, we incorporate recursion into the neural programmer-interpreter architecture and consider four sample tasks: grade-school addition, bubble sort, topological sort, and quicksort",0
" the recursive version can generalize perfectly to long problems constructed from these components (such as the sum “822+233”, where “8+2” and “2+3” are in the training set), but the non-recursive version fails to sum these long problems properly",0
"
it is important to emphasize that at inference time in the npi, the hidden state of the lstm controller is reset (to zero) at each subprogram call, as in line 3 of algorithm 1 (h ← 0)",1
"
directed acyclic graphs are structurally more diverse than inputs in the two tasks of grade-school addition and bubble sort",0
"
an alternative method is to run p on many different program inputs and then observe step input sequences which occur, to create v ",0
" finally, the boolean pstack == 1 is used to check whether the stack is empty in line 18 of algorithm 2",0
" for grade-school addition, the domain-specific encoder is
fenc(q, i1, i2, i3, i4, at) =mlp ([q(1, i1), q(2, i2), q(3, i3), q(4, i4), at(1), at(2), at(3)]),
where the environment q ∈ r4×n×k is a scratch-pad that contains four rows (the first input number, the second input number, the carry bits, and the output) and n columns",1
g,0
 this procedure verifies that all base cases and reductions are handled properly by the model via explicit tests,1
" training on the full recursive trace leads to perfect generalization, as shown in section 4",1
"
algorithm 2 shows the topological sort task of interest",1
" as aforementioned, the npi model naturally supports recursion",1
" at end, pointer ppivot is moved to the pivot compswap loop, move pivot lo, move j lo, swap
none
compswap loop runs the for loop inside the partition function compswap, move none compswap compares a[pivot] ≤ a[j]; if so, perform a swap and increment ppivot swap, move none set pivot lo sets ppivot to lo index none none set j lo sets pj to lo index none none set j null sets pj to −∞ none none stack pushes lo/hi states
onto stacks slo and shi according to argument (described below)
write, move described below
move moves pointer one unit up or down none described below swap swaps elements at given array indices none described below write write a value either to stack (e",0
"
move",1
" we performed experiments on the partially recursive version in order to examine what happens when only one recursive call is implemented, when in reality three are required for perfect generalization",0
2,0
"
there are 6 pointers (plo, phi, pstacklo, pstackhi, ppivot, pj)",0
" in this version of curriculum learning, even though the inputs are gradually becoming more complex, the semantics of the program is succinct and does not change",0
" in all experiments, α is set to 0",0
"
by reasoning about the possible set of environment observations created by all valid inputs, we construct v using the procedure described in section 3",1
" in fact, we argue that recursion is an essential element for neural programs to generalize, and makes it tractable to prove the generalization of neural programs",1
" this is highly undesirable, since being able to provide the correct answer in all possible settings is one of the most important aspects of any learned neural program",0
" we prove every member of f is added properly, given that subproblems s = {anan−1 + bnbn−1, an−1an−2 + bn−1bn−2, ",1
" the verification set can be very small, as we found a 10-element array ([8,2,1,2,0,8,5,8,3,7]) is sufficient to cover all of v ",0
"
in this paper, we propose that recursion is an important concept for neural programs as well",1
 ,0
" here as a concrete and general example, we consider a general neural programming architecture (npa), similar to neural programmer-interpreter (npi) in reed & de freitas (2016)",1
" each time a subprogram is called, the stack depth increases",1
 the core controller acts as a dispatcher for the programs,1
"
figure 1 shows examples of non-recursive and recursive addition traces",1
"
to construct this set, by using the reference implementation of each subprogram, we construct a mapping between two sets of environment observations: the first set consists of all observations that can occur at the beginning of a particular subprogram’s invocation, and the second set contains the observations at the end of that subprogram",1
 our result shows that the training procedure and the npi architecture is capable of generalizing from the step input-output pairs seen in the training data to the unseen ones present in the verification set,0
" each time a subprogram is called, the stack depth increases",0
" here as a concrete and general example, we consider a general neural programming architecture (npa), similar to neural programmer-interpreter (npi) in reed & de freitas (2016)",0
 the logic for the non-recursive trace is shown in algorithm 4 in the appendix,0
" without using a recursive program, such a proof is not possible, because the non-recursive program runs on an arbitrarily long addition problem that creates correspondingly long sequences of step inputs; in the non-recursive formulation of addition, add calls add1 a number of times that is dependent on the length of the input",0
" the theorem states that for the same sequence of step inputs, the model produces the exact same step output as the target program it aims to learn",1
 the maximum problem length in this training set is 3 (e,0
this material is in part based upon work supported by the national science foundation under grant no,1
" the swap operation has the following arguments:
arg 1 (swap object 1): ppivot
arg 2 (swap object 2): phi, pj
write",0
3,0
" lshift, used in reset, shifts the pointers left until reaching the start token",1
" we implement a quicksort task, in order to demonstrate that recursion helps with learning divide-and-conquer algorithms",0
", to color a vertex) or variable (e",1
"
since npi naturally supports the notion of recursion, a key question is how to enable npi to learn recursive programs",1
2,0
 the memory updates of these neural programs are so complex and interdependent that it is difficult to reason about the behaviors of the learned neural program under previously unseen situations (such as problems with longer inputs),0
 we found that changing the npi training traces is a simple way to enable this,0
" as aforementioned, the npi model naturally supports recursion",0
", to color a vertex) or variable (e",1
"we describe how models trained with recursive traces can be proven to generalize, by using the verification procedure described in section 3",1
 3: begin traversing from vertex 1 in the dag,0
"1, we hypothesize the non-recursive programs do not generalize well because they have learned spurious dependencies specific to the training set, such as length of the input problems",0
"
base cases and reduction rules for bubble sort",1
" the partial recursion is not enough for perfect generalization, as will be presented later in section 4",0
 this paper is the first (to our knowledge) to investigate the important problem of provable generalization properties of neural programs,0
"
note that v should also contain the necessary reductions, which corresponds to making the recursive calls at the correct time, as indicated by p ",0
" in particular, in the explore function, adding a tail recursive call resets and stores the hidden states associated with vertices in a stack-like fashion",1
"
in what follows, we describe the way in which we constructed npi training traces so as to make them contain recursive elements and thus enable npi to learn recursive programs",0
" our results show that for all learned, to-be-verified neural programs, they all produced the same traces as those produced by p on the verification set",0
" we can obtain this mapping by first considering the possible observations that can arise at the beginning of the entry function (add, bubblesort, toposort, and quicksort) for some valid program input, and iteratively applying the observation-to-observation mapping implied by the reference implementation’s step output at that point in the execution",1
 the environment and return probability are omitted for readability,1
"
move",0
" this functionality the original npi paper decodes to a program key embedding kt ∈ rk and then computes a program embedding pt+1, which we also did in our implementation, but we omit this for brevity",0
6,0
" qcolor(dag[vactive][childlist[vactive]]) refers to the color of the next child of vactive, used in the evaluation of the condition in the if branch in line 9 of algorithm 2",0
"
one common strategy to improve generalization is to use curriculum learning, where the model is trained on inputs of gradually increasing complexity",1
"
in this paper, we propose that recursion is an important concept for neural programs as well",0
 recursion enables provable guarantees on neural programs’ behavior without needing to exhaustively enumerate all possible inputs to the programs,0
" observe that the non-recursive program’s correctness degrades for length 11 and beyond, while the recursive program can sort any given array",0
" the arrow in the theorem refers to evaluation, as in big-step semantics",1
" 3: 4: function quicksort(a, lo, hi) 5: if lo < hi: then 6: p = partition(a, lo, hi) 7: quicksort(a, lo, p− 1) 8: quicksort(a, p+ 1, hi) 9: 10: function partition(a, lo, hi) 11: pivot = lo 12: for j ∈ [lo, hi− 1] : do 13: if a[j] ≤ a[hi] then 14: swap a[pivot] with a[j] 15: pivot = pivot+ 1 16: swap a[pivot] with a[hi] 17: return pivot
for quicksort, the domain-specific encoder is
fenc(qarray, qstacklo, qstackhi, plo, phi, pstacklo, pstackhi, ppivot, pj , at) =
mlp ([qarray(pj) ≤ qarray(phi), pj == phi, qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1), pstacklo == 1, at(1), at(2), at(3)]),
whereqarray ∈ ru×11 is a scratch-pad that contains u rows, each containing one of 11 values (one of the numbers 0 through 9 or an invalid state)",0
"
for addition, we analytically determine the verification set",0
"
arg 2 (increment or decrement): up, down1 // top level topological sort call 2 toposort() { 3 while (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 } 0 11 traverse() { 12 check_child() 13 explore() 14 } 15 16 check_child() { 17 while (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 18 move(childlist[vactive], up) 19 } 20 21 explore() { 22 do 23 if (qcolor(dag[vactive][childlist[vactive]]) is white): 24 write(color_next, color_grey) 25 stack(push) 26 write(save) 27 write(active_neighb) 28 move(childlist[vsave], up) 29 else: 30 write(color_curr, color_black) 31 write(result) 32 move(presult, up) 33 if(pstack == 1): 34 break 35 else: 36 stack(pop) 37 check_child() 38 while (true) 39 } 40 41 stack(op) { 42 if (op == push): 43 write(stack_push) 44 move(pstack, up) 45 46 if (op == pop): 47 write(active_stack) 48 write(stack_pop) 49 move(pstack, down) 50 } 51 52 next_start() { 53 while(qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 54 move(pstart, up) 55 }altered recursive functions  // top level topological sort call 2 toposort() { 3 if (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 toposort() // recursive call 0 } 11 12 check_child() { 13 if (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 14 move(childlist[vactive], up) 15 check_child() // recursive call 16 } 17 18 explore() { 19 if (qcolor(dag[vactive][childlist[vactive]]) is white): 20 write(color_next, color_grey) 21 stack(push) 22 write(save) 23 write(active_neighb) 24 move(childlist[vsave], up) 25 else: 26 write(color_curr, color_black) 27 write(result) 28 move(presult, up) 29 if(pstack == 1): 30 return 31 else: 32 stack(pop) 33 check_child() 34 explore() // recursive call 35 } 36 37 next_start() { 38 if (qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 39 move(pstart, up) 40 next_start() // recursive call 41 }algorithm 4 iterative quicksort 1: initialize an array a to sort and two empty stacks slo and shi",1
" when the program is called, the current context including the caller’s memory state is stored on a stack; when the program returns, the stored context is popped off the stack to resume execution in the previous caller’s context",1
" recall that this verification procedure cannot be performed for the non-recursive versions, since the propagation of the hidden state in the core lstm module makes reasoning difficult and so we would need to check an unbounded number of examples",0
" for grade-school addition, the domain-specific encoder is
fenc(q, i1, i2, i3, i4, at) =mlp ([q(1, i1), q(2, i2), q(3, i3), q(4, i4), at(1), at(2), at(3)]),
where the environment q ∈ r4×n×k is a scratch-pad that contains four rows (the first input number, the second input number, the carry bits, and the output) and n columns",0
" finally, the boolean pstack == 1 is used to check whether the stack is empty in line 18 of algorithm 2",0
" more specifically, when m is trained on simpler inputs, such as inputs of a small length, the generalization metric evaluates how well m will do on more complex inputs, such as inputs of much longer length",1
 ,0
 table 4 also demonstrates that the (verified) recursive program generalizes perfectly,0
" in particular, recursion can be implemented as a program calling itself",0
" observe that partially recursive does slightly better than non-recursive for the setting in which the length of the array is 3, and that the fully recursive version is able to sort every array given to it",1
3,0
"
inside bubble and reset, there are two operations that can be made recursive",0
g,0
" for each length, we test each program on 30 randomly generated problems",1
 we outline how to construct this set,0
 we then construct a verification set of size 73 by ensuring that randomly generated graphs cover the analytically derived v ,0
3 to construct v and then empirically create a verification set which covers v ,1
"
the three environment observations aid with control flow in algorithm 2",0
"
it is important to emphasize that at inference time in the npi, the hidden state of the lstm controller is reset (to zero) at each subprogram call, as in line 3 of algorithm 1 (h ← 0)",1
" lshift, used in reset, shifts the pointers left until reaching the start token",1
" 4: function toposort(dag) 5: while there is still a white vertex u: do 6: color[u] = grey 7: vactive = u 8: do 9: if vactive has a white child v then 10: color[v] = grey 11: push vactive onto s 12: vactive = v 13: else 14: color[vactive] = black 15: write vactive to result 16: if s is empty then pass 17: else pop the top vertex off s and set it to vactive 18: while s is not empty
topological sort",0
 the first is the actual model architecture,0
"
figure 1 shows examples of non-recursive and recursive addition traces",0
 these pointers are referred to as bubble pointers,0
" thus, recursion helps decompose a problem and makes it easier to reason about a program’s behavior for previously unseen situations such as longer inputs",0
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,1
" in this version, there is a dependence on length: bstep and lshift are called a number of times equivalent to one less than the length of the input array, in bubble and reset respectively",1
" finally, the boolean pstack == 1 is used to check whether the stack is empty in line 18 of algorithm 2",1
" the add reduction rule ensures that each call to add only covers two adjacent columns, and so the lstm only ever runs for a fixed number of steps necessary to process these two columns",1
"
we describe the base cases, reduction rules, and the verification set for each task in appendix a",0
"
by nature, recursion reduces the complexity of a problem to simpler instances",0
" we hypothesize that the reason for this is that the neural network learns to spuriously depend on specific characteristics of the training examples that are irrelevant to the true program semantics, such as length of the training inputs, and thus fails to generalize to more complex inputs",1
" both the non-recursive and recursive learned programs generalize on all input lengths we tried, up to 5000 digits",0
" the pointers at index i1 and i2 are used to compare the pair of numbers considered during the bubble sweep, swapping them if the number at i1 is greater than that in i2",1
"
note that v should also contain the necessary reductions, which corresponds to making the recursive calls at the correct time, as indicated by p ",0
" thus, recursion helps decompose a problem and makes it easier to reason about a program’s behavior for previously unseen situations such as longer inputs",1
"
the training set for topological sort contains 6 traces, with one synthesized from a graph of size 5 and the rest synthesized from graphs of size 7",1
"
we also report on generalization results for the non-recursive and recursive versions of this variant of bubble sort",0
" empirically, existing models suffer from a common limitation—generalization becomes poor beyond a threshold level of complexity",0
" for each function we use to implement the recursive version of topological sort, we need to consider the set of possible environment observation sequences we can create from all valid inputs and test that the learned program produces the correct behavior on each of these inputs",1
 the program terminates when seeing no numbers in the current column,0
"
in addition, none of the current approaches to neural programming provide a method or even aim to enable provable guarantees about generalization",1
 ,1
" the pointers at index i1 and i2 are used to compare the pair of numbers considered during the bubble sweep, swapping them if the number at i1 is greater than that in i2",0
"
we note that for tasks with very large input domains, such as ones involving mnist digits or speech samples, the state space of base cases and reduction rules could be prohibitively large, possibly infinite",1
"
is critical for implementing recursion, since it permits us to restrict our attention to the currently relevant recursive call, ignoring irrelevant details about other contexts",0
" if no white vertex is found, the entire execution is terminated move none write write a value either to environment (e",0
"
there are 6 pointers (plo, phi, pstacklo, pstackhi, ppivot, pj)",0
 this algorithm is a variant of depth first search,1
" partial recursion only adds a tail recursive call to bubblesort after bubble and reset, similar to the tail recursive call described previously for addition",1
" bstep, used in bubble, compares pairs of numbers, continuously moving the bubble pointers once to the right each time until reaching the end of the array",0
" 2: initialize lo and hi to be 1 and n, where n is the length of a",1
g,0
g,0
" 16: p = partition(a, lo, hi) 17: push p+ 1 and hi to slo and shi",0
 we describe the recursive re-formulation of traces for two tasks from the original npi paper—grade-school addition and bubble sort,1
" at any given time, the npi has access to the values referred to by two pointers, represented by q(1, i1) and q(1, i2),",1
" as this complexity interfered with determining the base cases and reductions, we changed the algorithm to its current form",0
g,0
" the move operation has the following arguments:
arg 1 (pointer): presult, pstack, pstart, childlist[vactive], childlist[vsave]
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 5 values",1
" note that recursion helps make this process tractable, because we only need to test a finite number of inputs to show that the model will work correctly on inputs of unbounded complexity",0
"
thus far, to evaluate the efficacy of neural models on programming tasks, the only metric that has been used is generalization of expected behavior to inputs of greater complexity (vinyals   (2015), kaiser & sutskever (2015), reed & de freitas (2016), graves   (2016), zaremba   (2016))",0
" lshift, used in reset, shifts the pointers left until reaching the start token",1
" the learned recursive program is different from neural programs learned in all previous work in an important aspect: previous approaches do not explicitly incorporate this abstraction, and hence generalize poorly, whereas our learned neural programs incorporate recursion and achieve perfect generalization",0
" in the non-recursive trace, there are four functions that can be made recursive—toposort, check child, explore, and next start, and we add a tail recursive call to each of these functions in order to make the recursive trace",0
" this is highly undesirable, since being able to provide the correct answer in all possible settings is one of the most important aspects of any learned neural program",0
 this procedure verifies that all base cases and reductions are handled properly by the model via explicit tests,0
" the contents ofqresult andqstack are not exposed directly through the domain-specific encoder; rather, we define primitive functions which manipulate these scratch-pads",0
 the original version of the bubblesort implementation exposes the values within the array,0
 the maximum problem length in this training set is 3 (e,1
" the recursive version can generalize perfectly to long problems constructed from these components (such as the sum “822+233”, where “8+2” and “2+3” are in the training set), but the non-recursive version fails to sum these long problems properly",0
" while this matches the description from reed & de freitas (2016), we found that this causes an unnecessary blowup in the size of v and makes it much more difficult to construct the verification set",1
 the memory updates of these neural programs are so complex and interdependent that it is difficult to reason about the behaviors of the learned neural program under previously unseen situations (such as problems with longer inputs),1
"we show that if we incorporate recursion, the learned npi programs can achieve provably perfect generalization for different tasks",0
 our result shows that the training procedure and the npi architecture is capable of generalizing from the step input-output pairs seen in the training data to the unseen ones present in the verification set,1
" as with the others, we apply the procedure described in section 3",1
" as before, the non-recursive learned program lacks a provable guarantee of generalization, whereas we show later that the recursive learned program has one",0
"
arg 2 (increment or decrement): up, down1 // top level topological sort call 2 toposort() { 3 while (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 } 0 11 traverse() { 12 check_child() 13 explore() 14 } 15 16 check_child() { 17 while (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 18 move(childlist[vactive], up) 19 } 20 21 explore() { 22 do 23 if (qcolor(dag[vactive][childlist[vactive]]) is white): 24 write(color_next, color_grey) 25 stack(push) 26 write(save) 27 write(active_neighb) 28 move(childlist[vsave], up) 29 else: 30 write(color_curr, color_black) 31 write(result) 32 move(presult, up) 33 if(pstack == 1): 34 break 35 else: 36 stack(pop) 37 check_child() 38 while (true) 39 } 40 41 stack(op) { 42 if (op == push): 43 write(stack_push) 44 move(pstack, up) 45 46 if (op == pop): 47 write(active_stack) 48 write(stack_pop) 49 move(pstack, down) 50 } 51 52 next_start() { 53 while(qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 54 move(pstart, up) 55 }altered recursive functions  // top level topological sort call 2 toposort() { 3 if (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 toposort() // recursive call 0 } 11 12 check_child() { 13 if (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 14 move(childlist[vactive], up) 15 check_child() // recursive call 16 } 17 18 explore() { 19 if (qcolor(dag[vactive][childlist[vactive]]) is white): 20 write(color_next, color_grey) 21 stack(push) 22 write(save) 23 write(active_neighb) 24 move(childlist[vsave], up) 25 else: 26 write(color_curr, color_black) 27 write(result) 28 move(presult, up) 29 if(pstack == 1): 30 return 31 else: 32 stack(pop) 33 check_child() 34 explore() // recursive call 35 } 36 37 next_start() { 38 if (qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 39 move(pstart, up) 40 next_start() // recursive call 41 }algorithm 4 iterative quicksort 1: initialize an array a to sort and two empty stacks slo and shi",1
" if no white vertex is found, the entire execution is terminated move none write write a value either to environment (e",1
"
base cases and reduction rules for quicksort",1
" for these models, table 2 presents results on randomly generated dags of varying graph sizes (varying in the number of vertices)",0
" the arrow in the theorem refers to evaluation, as in big-step semantics",1
" in order to claim a proof, we must verify the model produces correct behavior over all base cases and reductions, as described in section 2",1
"
we would also like to point out that in this paper, we provide as an example one way to train a recursive neural program, by providing a certain training execution trace to the npi model",0
" in future work, we would like to decrease supervision and construct models that are capable of coming up with recursive abstractions themselves",0
g,0
" at each time step, the core controller can decide to select one of the programs to call with certain arguments",0
"
an alternative way of representing the environment slice is to expose the values of the absolute vertices to the model; however, this makes it difficult to scale the model to larger graphs, since large vertex values are not seen during training time",0
"
bubble sort",0
"
the training set for topological sort contains 6 traces, with one synthesized from a graph of size 5 and the rest synthesized from graphs of size 7",1
" the write operation has the following arguments:
arg 1 (object to write): env stack lo, env stack hi, phi, plo
env stack lo and env stack hi represent qstacklo(pstacklo) and qstackhi(pstackhi), respectively",0
" fa8750-15-2-0104, and berkeley deep drive",1
" naı̈vely, we might expect to synthesize and test an input for any sequence created by combining the four possible colors in two variables and another boolean variable for whether the stack is empty (so 32 possible observations at any point), but for various reasons, most of these combinations are impossible to occur at any given point in the execution trace",1
" the write operation has the following arguments:
arg 1 (object to write): env stack lo, env stack hi, phi, plo
env stack lo and env stack hi represent qstacklo(pstacklo) and qstackhi(pstackhi), respectively",0
"
algorithm 3 recursive quicksort 1: initialize an array a to sort",0
"
it is sufficient to construct problems where every transition between two adjacent columns is covered",1
" our implementation uses two stacks qstacklo and
qstackhi, each in ru , that store the arguments to the recursive quicksort calls in algorithm 3; before each recursive call, the appropriate arguments are popped off the stack and written to plo and phi",0
"
we also report on generalization results for the non-recursive and recursive versions of this variant of bubble sort",0
" the corresponding edge list is [(1, 2), (1, 5), (2, 4), (2, 5), (3, 5)]",0
 recursion enables provable guarantees on neural programs’ behavior without needing to exhaustively enumerate all possible inputs to the programs,1
" this guarantees that the step input sequences considered during verification contain all step input sequences which arise during execution of an unseen problem in f , leading to generalization to any problem in f ",0
" k is set to 11, to represent the range of 10 possible digits, along with a token representing the end of input",1
" the outputs rt, pt+1, at+1 are used to determine the next action, as described in algorithm 1",1
"
base cases and reduction rules for addition",0
" while this matches the description from reed & de freitas (2016), we found that this causes an unnecessary blowup in the size of v and makes it much more difficult to construct the verification set",1
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,0
 the original version of the bubblesort implementation exposes the values within the array,1
" more specifically, when m is trained on simpler inputs, such as inputs of a small length, the generalization metric evaluates how well m will do on more complex inputs, such as inputs of much longer length",0
" the recursive version of topological sort solves all graph instances we tried, from graphs of size 5 through 70",1
"when constructing a neural network for the purpose of learning a program, there are two orthogonal aspects to consider",0
 figure 2 shows examples of traces for the different versions of bubble sort,1
"
thus far, to evaluate the efficacy of neural models on programming tasks, the only metric that has been used is generalization of expected behavior to inputs of greater complexity (vinyals   (2015), kaiser & sutskever (2015), reed & de freitas (2016), graves   (2016), zaremba   (2016))",1
" this agrees with the generalization of non-recursive addition in reed & de freitas (2016), where they reported generalization up to 3000 digits",0
 some functional programming languages go so far as not to define any looping constructs but rely solely on recursion to enable repeated execution of the same code,0
" we note that an earlier version of quicksort we tried lacked primitive operations to directly move a pointer to another, and therefore needed more functions and observations",1
"
arg 2 (increment or decrement): up, down
swap",0
"
by reasoning about the possible set of environment observations created by all valid inputs, we construct v using the procedure described in section 3",0
"
after finding v , we construct a set of problem inputs which, when executed on p , create exactly the step input sequences which make up v ",0
"
note that the training set can often be considerably smaller than the verification set, and despite this, the learned model can still pass the entire verification set",1
" although the model is exposed to more and more data, it might learn spurious and overly complex representations of the program, as suggested in zaremba   (2016)",1
"
in this paper, we propose to resolve these issues by explicitly incorporating recursion into neural architectures",0
"as discussed in section 2, the neural programmer-interpreter (npi) is an instance of a neural programmer architecture and hence it naturally supports recursion",1
" for example, for the addition task, the model is trained on short inputs and then tested on its ability to sum inputs with much longer numbers of digits",0
" empirically, existing models suffer from a common limitation—generalization becomes poor beyond a threshold level of complexity",0
" also, compared to the bubble pointers in bubble sort, the pointers that perform the comparison for quicksort (the compswap function) are usually not adjacent to each other, making quicksort less local than bubble sort",0
" ppivot and pj point to the pivot and j indices of the array, used in the partition function in algorithm 3",0
 there is a (changing) list of neural programs used to accomplish a given task,0
 the maximum problem length in this training set is 3 (e,0
" on the other hand, the non-recursive version has low accuracy, beginning from size 5, and fails completely for graphs of size 8 and beyond",0
" our results show that for all learned, to-be-verified neural programs, they all produced the same traces as those produced by p on the verification set",0
" , n , where the dag contains n vertices",0
", to change the value of vactive) none described below move move a pointer (e",1
" for purposes of verification, we replace the domain-specific encoder with the following:
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1) ≤ q(1, i2), 1 ≤ i1 ≤ length, 1 ≤ i2 ≤ length, i3 == length, at(1), at(2), at(3)]),which directly exposes which of the two values pointed to is larger",0
" the general notion of recursion has been an important concept in many domains, including mathematics and computer science",0
" in particular, recursion can be implemented as a program calling itself",0
" however, to be sure that the generated v is complete (covers all the cases needed), we need to check all pairs of observations seen in adjacent step inputs (in particular, those before and after a primitive function call), in a similar way as if we were constructing v from scratch",0
 2: initialize an empty stack s and a directed acyclic graph dag to traverse,0
"
formally, verification consists of proving the following theorem:
∀i ∈ v,m(i) ⇓ p (i) where i denotes a sequence of step inputs (within one function call), v denotes the set of valid sequences of step inputs, m denotes the neural network model, p denotes the correct program, and p (i) denotes the next step output from the correct program",1
", pstart or childlist[vactive]) up or down none described belowwrite",1
" the verification set can be very small, as we found a 10-element array ([8,2,1,2,0,8,5,8,3,7]) is sufficient to cover all of v ",0
 we outline how to construct this set,1
" formally, the npi is represented by the following set of equations:
st = fenc(et, at)
ht = flstm(st, pt, ht−1)
rt = fend(ht), pt+1 = fprog(ht), at+1 = farg(ht)
t is a subscript denoting the time-step; fenc is a domain-specific encoder (to be described later) that takes in the environment slice et and arguments at; flstm represents the core module, which takes in the state st generated by fenc, a program embedding pt ∈ rp , and hidden lstm state ht; fend decodes the return probability rt; fprog decodes a program key embedding pt+1;1 and farg decodes arguments at+1",0
 we describe the recursive re-formulation of traces for two tasks from the original npi paper—grade-school addition and bubble sort,0
" recall that this verification procedure cannot be performed for the non-recursive versions, since the propagation of the hidden state in the core lstm module makes reasoning difficult and so we would need to check an unbounded number of examples",0
" in order to claim a proof, we must verify the model produces correct behavior over all base cases and reductions, as described in section 2",1
" 2: initialize lo and hi to be 1 and n, where n is the length of a",0
 a1a0 + bnbn−1 ,1
3 to construct v and then empirically create a verification set which covers v ,1
"
quicksort",1
" using this modification, we constructed a verification set consisting of one array of size 10",0
"we show that if we incorporate recursion, the learned npi programs can achieve provably perfect generalization for different tasks",1
" in particular, we construct new training traces which explicitly contain recursive elements and show that with this type of trace, npi easily learns recursive programs",1
" in all experiments, α is set to 0",1
" we also hope to decrease supervision, for example by training with only partial or non-recursive traces, and to develop novel neural programming architectures integrated directly with a notion of recursion",1
" the corresponding edge list is [(1, 2), (1, 5), (2, 4), (2, 5), (3, 5)]",0
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,0
"
it is important to emphasize that at inference time in the npi, the hidden state of the lstm controller is reset (to zero) at each subprogram call, as in line 3 of algorithm 1 (h ← 0)",0
 ,0
 these pointers are referred to as bubble pointers,1
" this guarantees that the step input sequences considered during verification contain all step input sequences which arise during execution of an unseen problem in f , leading to generalization to any problem in f ",0
 recursion enables provable guarantees on neural programs’ behavior without needing to exhaustively enumerate all possible inputs to the programs,0
g,1
 a1a0 + bnbn−1 ,1
g,0
"when constructing a neural network for the purpose of learning a program, there are two orthogonal aspects to consider",1
"
recursion drastically reduces the number of configurations we need to consider during the verification phase and makes the proof tractable, because it introduces structure that eliminates infinitely long sequences of step inputs that would otherwise need to be considered",0
" for these programs, we re-use the appropriate program sets (the associated subprograms), and we refer the reader to the appendix of reed & de freitas (2016) for further details on the subprograms used in addition and bubble sort",0
" , a1a0 + b1b0} are added properly",0
"
to construct this set, by using the reference implementation of each subprogram, we construct a mapping between two sets of environment observations: the first set consists of all observations that can occur at the beginning of a particular subprogram’s invocation, and the second set contains the observations at the end of that subprogram",0
"
grade-school addition",0
 recursion enables provable guarantees on neural programs’ behavior without needing to exhaustively enumerate all possible inputs to the programs,0
" we call this set of inputs the verification set, sv ",0
"we show that if we incorporate recursion, the learned npi programs can achieve provably perfect generalization for different tasks",1
" in future work, we would like to decrease supervision and construct models that are capable of coming up with recursive abstractions themselves",0
" however, to be sure that the generated v is complete (covers all the cases needed), we need to check all pairs of observations seen in adjacent step inputs (in particular, those before and after a primitive function call), in a similar way as if we were constructing v from scratch",0
" the partial recursion is not enough for perfect generalization, as will be presented later in section 4",0
 the maximum problem length in this training set is 3 (e,0
 ,1
 our experiments use a small number of training examples,0
" for these programs, we re-use the appropriate program sets (the associated subprograms), and we refer the reader to the appendix of reed & de freitas (2016) for further details on the subprograms used in addition and bubble sort",1
" this guarantees that the step input sequences considered during verification contain all step input sequences which arise during execution of an unseen problem in f , leading to generalization to any problem in f ",1
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,0
" the swap operation has the following arguments:
arg 1 (swap object 1): ppivot
arg 2 (swap object 2): phi, pj
write",0
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,0
" 2: initialize lo and hi to be 1 and n, where n is the length of a",0
"
we propose and describe our verification procedure",1
"
arg 2 (increment or decrement): up, down1 // top level topological sort call 2 toposort() { 3 while (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 } 0 11 traverse() { 12 check_child() 13 explore() 14 } 15 16 check_child() { 17 while (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 18 move(childlist[vactive], up) 19 } 20 21 explore() { 22 do 23 if (qcolor(dag[vactive][childlist[vactive]]) is white): 24 write(color_next, color_grey) 25 stack(push) 26 write(save) 27 write(active_neighb) 28 move(childlist[vsave], up) 29 else: 30 write(color_curr, color_black) 31 write(result) 32 move(presult, up) 33 if(pstack == 1): 34 break 35 else: 36 stack(pop) 37 check_child() 38 while (true) 39 } 40 41 stack(op) { 42 if (op == push): 43 write(stack_push) 44 move(pstack, up) 45 46 if (op == pop): 47 write(active_stack) 48 write(stack_pop) 49 move(pstack, down) 50 } 51 52 next_start() { 53 while(qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 54 move(pstart, up) 55 }altered recursive functions  // top level topological sort call 2 toposort() { 3 if (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 toposort() // recursive call 0 } 11 12 check_child() { 13 if (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 14 move(childlist[vactive], up) 15 check_child() // recursive call 16 } 17 18 explore() { 19 if (qcolor(dag[vactive][childlist[vactive]]) is white): 20 write(color_next, color_grey) 21 stack(push) 22 write(save) 23 write(active_neighb) 24 move(childlist[vsave], up) 25 else: 26 write(color_curr, color_black) 27 write(result) 28 move(presult, up) 29 if(pstack == 1): 30 return 31 else: 32 stack(pop) 33 check_child() 34 explore() // recursive call 35 } 36 37 next_start() { 38 if (qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 39 move(pstart, up) 40 next_start() // recursive call 41 }algorithm 4 iterative quicksort 1: initialize an array a to sort and two empty stacks slo and shi",1
" naı̈vely, we might expect to synthesize and test an input for any sequence created by combining the four possible colors in two variables and another boolean variable for whether the stack is empty (so 32 possible observations at any point), but for various reasons, most of these combinations are impossible to occur at any given point in the execution trace",1
 there is a (changing) list of neural programs used to accomplish a given task,0
 this algorithm is a variant of depth first search,0
" thus, we demonstrate that recursion enables provably perfect generalization for different tasks, including addition, topological sort, quicksort, and a variant of bubble sort",0
 reset represents a −∞ value,0
"
through careful reasoning about the possible set of environment observations created by all valid inputs, and how each of the operations in the execution trace affects the environment, we can construct v using the procedure described in section 3",1
" consequently, there is no concept of length relevant to the problem, which has traditionally been an important focus of length-based curriculum learning",0
 the core controller acts as a dispatcher for the programs,0
" provably perfect generalization implies the model will behave correctly, given any valid input",0
" we note that an earlier version of quicksort we tried lacked primitive operations to directly move a pointer to another, and therefore needed more functions and observations",1
"
figure 1 shows examples of non-recursive and recursive addition traces",1
 figure 2 shows examples of traces for the different versions of bubble sort,0
 u varies with the number of vertices in the graph,1
" in all experiments, α is set to 0",0
" if it is too small, then execution of the program on some input might require evaluation of m(i) on some i /∈ v , and so the behavior ofm(i) might deviate from p (i)",0
" as a consequence, our learned neural programs empirically achieve perfect generalization from a very small number of training examples",0
"
one common strategy to improve generalization is to use curriculum learning, where the model is trained on inputs of gradually increasing complexity",0
" table 1 presents results on randomly generated arrays of varying length for the learned non-recursive, partially recursive, and full recursive programs",0
" on the other hand, the recursive programs have learned the true program semantics",1
" as a consequence, our learned neural programs empirically achieve perfect generalization from a very small number of training examples",0
"
in this general neural programming architecture, we show it is easy to support recursion",0
" observe that partially recursive does slightly better than non-recursive for the setting in which the length of the array is 3, and that the fully recursive version is able to sort every array given to it",1
"
after finding v , we construct a set of problem inputs which, when executed on p , create exactly the step input sequences which make up v ",1
"
we would also like to point out that in this paper, we provide as an example one way to train a recursive neural program, by providing a certain training execution trace to the npi model",0
 b1b0 where no carry operations occur,1
"
arg 2 (auxiliary variable): color grey, color black
color grey and color black color the given vertex grey and black, respectively",1
"
note that v should also contain the necessary reductions, which corresponds to making the recursive calls at the correct time, as indicated by p ",1
" full recursion, in addition to making the aforementioned tail recursive call, adds two additional recursive calls; bstep and lshift are made tail recursive",1
" on the other hand, the recursive programs have learned the true program semantics",0
" naı̈vely, we might expect to synthesize and test an input for any sequence created by combining the four possible colors in two variables and another boolean variable for whether the stack is empty (so 32 possible observations at any point), but for various reasons, most of these combinations are impossible to occur at any given point in the execution trace",0
 this paper is the first (to our knowledge) to investigate the important problem of provable generalization properties of neural programs,0
" in particular, this condition is satisfied by npi, and thus the npi model naturally supports recursion (even though the authors of npi did not consider this aspect explicitly)",0
 3: begin traversing from vertex 1 in the dag,0
", to change the value of phi) none described belowstack",1
" this is possible if and only if the graph has no directed cycles; that is to say, it must be a directed acyclic graph (dag)",0
" table 1 presents results on randomly generated arrays of varying length for the learned non-recursive, partially recursive, and full recursive programs",0
"
the non-recursive trace loops on cycles of add1 and lshift",0
g,0
", the verification set",0
" observe that partially recursive does slightly better than non-recursive for the setting in which the length of the array is 3, and that the fully recursive version is able to sort every array given to it",0
" hence, if all subproblems in s are added correctly, we have proven that any member of f will be added correctly, thus eliminating an infinite family of inputs that need to be tested",1
"
to construct this set, by using the reference implementation of each subprogram, we construct a mapping between two sets of environment observations: the first set consists of all observations that can occur at the beginning of a particular subprogram’s invocation, and the second set contains the observations at the end of that subprogram",1
" however, models that make use of this strategy eventually fail after a certain level of complexity (e",1
"
arg 2 (increment or decrement): up, down
swap",0
 this paper is the first (to our knowledge) to investigate the important problem of provable generalization properties of neural programs,0
 we describe the recursive re-formulation of traces for two tasks from the original npi paper—grade-school addition and bubble sort,0
"
quicksort",1
" the 4 environment observations aid with control flow; qstacklo(pstacklo− 1) < qstackhi(pstackhi− 1) implements the lo < hi comparison in line 5 of algorithm 3, pstacklo == 1 checks if the stacks are empty in line 18 of algorithm 4, and the other observations (all involving ppivot or pj) deal with logic in the partition function",1
"
note that v should also contain the necessary reductions, which corresponds to making the recursive calls at the correct time, as indicated by p ",1
" the learned recursive program is different from neural programs learned in all previous work in an important aspect: previous approaches do not explicitly incorporate this abstraction, and hence generalize poorly, whereas our learned neural programs incorporate recursion and achieve perfect generalization",0
"we train using the adam optimizer and use a 2-layer lstm and task-specific state encoders for the external environments, as described in reed & de freitas (2016)",1
"
grade-school addition",0
"
there are 6 pointers (plo, phi, pstacklo, pstackhi, ppivot, pj)",0
 the degree for any vertex in the dag is variable,0
" for non-primitive subprograms, we can update the observation-to-observation mapping currently associated with the subprogram and then apply that mapping to the current set",0
" if it is too small, then execution of the program on some input might require evaluation of m(i) on some i /∈ v , and so the behavior ofm(i) might deviate from p (i)",0
 2: initialize an empty stack s and a directed acyclic graph dag to traverse,0
" in particular, this condition is satisfied by npi, and thus the npi model naturally supports recursion (even though the authors of npi did not consider this aspect explicitly)",1
 some functional programming languages go so far as not to define any looping constructs but rely solely on recursion to enable repeated execution of the same code,1
g,0
 we choose to implement a topological sort task for graphs,0
 ,0
"
as in line 13 of the right-hand side of figure 1",0
 this modification also enables us to sort arrays containing arbitrary comparable elements,0
 the logic for the non-recursive trace is shown in algorithm 4 in the appendix,0
 a1a0 + bnbn−1 ,0
" as before, the non-recursive learned program lacks a provable guarantee of generalization, whereas we show later that the recursive learned program has one",0
g,0
 ,0
"
arg 2 (object to copy): env stack lo peek, env stack hi peek, phi, plo, ppivot − 1, ppivot + 1, reset
env stack lo peek and env stack hi peek represent qstacklo(pstacklo − 1) and qstackhi(pstackhi − 1), respectively",0
"
algorithm 2 shows the topological sort task of interest",0
 pstacklo and pstackhi point to the top (empty) positions in qstacklo and qstackhi,1
" each time a subprogram is called, the stack depth increases",1
" the core module of the npi is an lstm controller that takes as input a slice of the current external environment, via a set of pointers, and a program and arguments to execute",0
" if yes, then this indicates that the learned neural program achieves provably perfect generalization",0
"
in this paper, we propose that recursion is an important concept for neural programs as well",1
" the learned recursive program is different from neural programs learned in all previous work in an important aspect: previous approaches do not explicitly incorporate this abstraction, and hence generalize poorly, whereas our learned neural programs incorporate recursion and achieve perfect generalization",1
" via the recursive call, we effectively forget that the column just added exists, since the recursive call to add starts with a new hidden state for the lstm controller",1
" for each task, given the verification set, we check the traces and results of the learned, to-be-verified neural program (described in section 4",0
", to change the value of phi) none described belowstack",0
" by iterating with this procedure, and then running p on the input observation set that we obtain for the entry point function, we can obtain v precisely",0
" using this modification, we constructed a verification set consisting of one array of size 10",0
 we describe the recursive re-formulation of traces for two tasks from the original npi paper—grade-school addition and bubble sort,0
" full recursion, in addition to making the aforementioned tail recursive call, adds two additional recursive calls; bstep and lshift are made tail recursive",0
" empirically, we observe that the learned recursive programs solve all valid inputs with 100% accuracy after training on a very small number of examples, out-performing previous generalization results",0
" the core module of the npi is an lstm controller that takes as input a slice of the current external environment, via a set of pointers, and a program and arguments to execute",0
" for tasks other than addition, it is difficult to analytically determine the verification set, so instead, we randomly generate input candidates until they completely cover the base cases and reduction rules",0
"
base cases and reduction rules for topological sort",0
 stack pop pushes −∞ values to qstacklo and qstackhi,1
" the architecture usually possesses some form of memory, which could be internal (such as the hidden state of a recurrent neural network) or external (such as a discrete “scratch pad” or a memory block with differentiable access)",1
 we created a program set that reflects the semantics of algorithm 2,0
 we then construct a verification set of size 73 by ensuring that randomly generated graphs cover the analytically derived v ,1
" as with the others, we apply the procedure described in section 3",0
 we choose to implement a topological sort task for graphs,1
" both the non-recursive and recursive learned programs generalize on all graphs we tried, up to 120 vertices",1
3 to construct v and then empirically create a verification set which covers v ,1
"
arg 2 (auxiliary variable): color grey, color black
color grey and color black color the given vertex grey and black, respectively",0
" each time a subprogram is called, the stack depth increases",1
 errors arise due to undesirable and uninterpretable dependencies and associations the architecture learns to store in some high-dimensional hidden state,0
"
grade-school addition",0
"
the three environment observations aid with control flow in algorithm 2",0
" at each time step, the core controller can decide to select one of the programs to call with certain arguments",1
 our experiments use a small number of training examples,0
"
algorithm 1 neural programming inference 1: inputs: environment observation e, program p, arguments a, stop threshold α 2: function run(e, p, a) 3: h← 0, r ← 0 4: while r < α do 5: s← fenc(e, a), h← flstm(s, p, h) 6: r ← fend(h), p2 ← fprog(h), a2 ← farg(h) 7: if p is a primitive function then 8: e← fenv(e, p, a)",0
" via the recursive call, we effectively forget that the column just added exists, since the recursive call to add starts with a new hidden state for the lstm controller",0
"when constructing a neural network for the purpose of learning a program, there are two orthogonal aspects to consider",0
 add1 is a subprogram that adds the current column (writing the appropriate digit to the output row and carrying a bit to the next column if needed),0
" the stack operation has the following arguments:
arg 1 (operation): stack push call1, stack push call2, stack pop
stack push call1 pushes lo and pivot−1 toqstacklo andqstackhi",0
 table 3 presents results on randomly generated arrays of varying length for the learned non-recursive and recursive programs,0
"
directed acyclic graphs are structurally more diverse than inputs in the two tasks of grade-school addition and bubble sort",0
" observe that partially recursive does slightly better than non-recursive for the setting in which the length of the array is 3, and that the fully recursive version is able to sort every array given to it",1
"
we describe the base cases, reduction rules, and the verification set for each task in appendix a",0
", the trace corresponding to the problem “109 + 101”)",0
"
we construct input problems by splitting into two cases: one case in which the left column contains a null value and another in which the left column does not contain any null values",1
" empirically, we observe that the learned recursive programs solve all valid inputs with 100% accuracy after training on a very small number of examples, out-performing previous generalization results",1
"
training setup",0
"
we propose and describe our verification procedure",0
 stack push call2 pushes pivot + 1 and hi to qstacklo and qstackhi,0
"
bubble sort",0
 the degree for any vertex in the dag is variable,1
" while this matches the description from reed & de freitas (2016), we found that this causes an unnecessary blowup in the size of v and makes it much more difficult to construct the verification set",1
" for problems where the base cases may be extremely large or infinite, such as certain forms of motor control, recursion can still help reduce the problem of generalization to these two aspects and make the generalization problem significantly simpler to handle and reason about",1
" 16: p = partition(a, lo, hi) 17: push p+ 1 and hi to slo and shi",0
"
we also report on generalization results for the non-recursive and recursive versions of this variant of bubble sort",1
" each time a subprogram is called, the stack depth increases",0
" if it is too large, then the semantics of p might not be well-defined on some elements in v , or the spurious step input sequences may not be reachable from any valid problem input (e",0
"we describe how models trained with recursive traces can be proven to generalize, by using the verification procedure described in section 3",1
" plo and phi point to the lo and hi indices of the array, as in algorithm 3",0
" lshift moves the four pointers to the left, to move to the next column",1
"
there are 6 pointers (plo, phi, pstacklo, pstackhi, ppivot, pj)",0
 ,0
" the outputs rt, pt+1, at+1 are used to determine the next action, as described in algorithm 1",1
" if a white vertex is found, set pstart to point to it; this signifies the start of a traversal of a new connected component",0
" formally, a function exhibits recursive behavior when it possesses two properties: (1) base cases— terminating scenarios that do not use recursion to produce answers; (2) a set of rules that reduces all other problems toward the base cases",0
" also the dag can have potentially more than one connected component, meaning it is necessary to transition between these components appropriately",0
"program descriptions calls arguments toposort perform topological
sort on graph traverse, next start, write, move
none
traverse traverse graph until stack is empty check child, explore none check child check if a white child exists; if so, set childlist[vactive] to point to it move none explore repeatedly traverse subgraphs until stack is empty stack, check child, write, move none stack interact with stack, either pushing or popping write, move push, pop next start move pstart until reaching a white vertex",0
" for brevity, we refer the reader to the appendix for further details on the program set and non-recursive and recursive trace-generating functions used for topological sort",1
" note that recursion helps make this process tractable, because we only need to test a finite number of inputs to show that the model will work correctly on inputs of unbounded complexity",1
"
the dag is represented as an adjacency list where dag[i][j] refers to the j-th child of vertex i",1
g,0
" the verification set can be very small, as we found a 10-element array ([8,2,1,2,0,8,5,8,3,7]) is sufficient to cover all of v ",0
"
as mentioned in section 1, all approaches to neural programming today fare poorly on this generalization issue",0
" the arrow in the theorem refers to evaluation, as in big-step semantics",0
" a single element of an execution trace consists of a step input-step output pair, which can be synthesized from algorithm 1: this corresponds to, for a given time-step, the step input tuple (e, p, a) and step output tuple (r, p2, a2)",1
" in particular, recursion can be implemented as a program calling itself",0
" there are 3 pointers (presult, pstack, pstart), presult points to the next empty location in qresult,
pstack points to the top of the stack in qstack, and pstart points to the candidate starting node for a connected component",0
"
an alternative way of representing the environment slice is to expose the values of the absolute vertices to the model; however, this makes it difficult to scale the model to larger graphs, since large vertex values are not seen during training time",1
 errors arise due to undesirable and uninterpretable dependencies and associations the architecture learns to store in some high-dimensional hidden state,0
" in the non-recursive trace, there are four functions that can be made recursive—toposort, check child, explore, and next start, and we add a tail recursive call to each of these functions in order to make the recursive trace",0
" because the context of the caller is stored on a stack when it calls another program and the callee starts in a fresh context, this enables recursion simply by allowing a program to call itself",1
" if no white vertex is found, the entire execution is terminated move none write write a value either to environment (e",0
" for many problems, the base cases and reduction rules usually consist of a finite (often small) number of cases",0
" at each time step, the core controller can decide to select one of the programs to call with certain arguments",1
g,0
" the assumption of leading 0’s can be easily removed, at the cost of slightly increasing the size of the verification set",1
"
training setup",1
"
quicksort",0
" in contrast, each call to the recursive add always runs for a fixed number of steps, even on arbitrarily long problems in f , so we can test that it performs correctly on a small, fixed number of step input sequences",1
" 3: 4: function partition(a, lo, hi) 5: pivot = lo 6: for j ∈ [lo, hi− 1] : do 7: if a[j] ≤ a[hi] then 8: swap a[pivot] with a[j] 9: pivot = pivot+ 1 0: swap a[pivot] with a[hi] 11: return pivot 12: 13: function quicksort(a, lo, hi) 14: while slo and shi are not empty: do 15: pop states off slo and shi, writing them to lo and hi",0
 the logic for the non-recursive trace is shown in algorithm 4 in the appendix,0
"
move",1
 this verification phase only needs to be performed once after training,1
" the move operation has the following arguments:
arg 1 (pointer): pstacklo, pstackhi, pj , ppivot
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values",0
" note that recursion helps make this process tractable, because we only need to test a finite number of inputs to show that the model will work correctly on inputs of unbounded complexity",0
" 3: 4: function quicksort(a, lo, hi) 5: if lo < hi: then 6: p = partition(a, lo, hi) 7: quicksort(a, lo, p− 1) 8: quicksort(a, p+ 1, hi) 9: 10: function partition(a, lo, hi) 11: pivot = lo 12: for j ∈ [lo, hi− 1] : do 13: if a[j] ≤ a[hi] then 14: swap a[pivot] with a[j] 15: pivot = pivot+ 1 16: swap a[pivot] with a[hi] 17: return pivot
for quicksort, the domain-specific encoder is
fenc(qarray, qstacklo, qstackhi, plo, phi, pstacklo, pstackhi, ppivot, pj , at) =
mlp ([qarray(pj) ≤ qarray(phi), pj == phi, qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1), pstacklo == 1, at(1), at(2), at(3)]),
whereqarray ∈ ru×11 is a scratch-pad that contains u rows, each containing one of 11 values (one of the numbers 0 through 9 or an invalid state)",0
3,0
" the move operation has the following arguments:
arg 1 (pointer): pstacklo, pstackhi, pj , ppivot
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values",0
" k is set to 11, to represent the range of 10 possible digits, along with a token representing the end of input",0
"
after finding v , we construct a set of problem inputs which, when executed on p , create exactly the step input sequences which make up v ",1
 recursion enables provable guarantees on neural programs’ behavior without needing to exhaustively enumerate all possible inputs to the programs,0
"
by reasoning about the possible set of environment observations created by all valid inputs, we construct v using the procedure described in section 3",0
" thus, we demonstrate that recursion enables provably perfect generalization for different tasks, including addition, topological sort, quicksort, and a variant of bubble sort",0
"
after finding v , we construct a set of problem inputs which, when executed on p , create exactly the step input sequences which make up v ",0
"1; and for bubble sort, appendix a",0
"
bubble sort",0
" k is set to 11, to represent the range of 10 possible digits, along with a token representing the end of input",1
" m , as described in algorithm 1, processes the sequence of step inputs by using an lstm",1
 indentation indicates the stack is one level deeper than before,0
"when constructing a neural network for the purpose of learning a program, there are two orthogonal aspects to consider",1
"
arg 2 (auxiliary variable): color grey, color black
color grey and color black color the given vertex grey and black, respectively",1
 npi then outputs the return probability and next program and arguments to execute,0
"
the training set for quicksort contains 4 traces, synthesized from arrays of length 5",1
 the original version of the bubblesort implementation exposes the values within the array,0
"
we note that for tasks with very large input domains, such as ones involving mnist digits or speech samples, the state space of base cases and reduction rules could be prohibitively large, possibly infinite",1
" lshift moves the four pointers to the left, to move to the next column",1
"
there are 6 pointers (plo, phi, pstacklo, pstackhi, ppivot, pj)",0
" both the non-recursive and recursive learned programs generalize on all input lengths we tried, up to 5000 digits",0
 this procedure verifies that all base cases and reductions are handled properly by the model via explicit tests,1
"
as mentioned in section 1, all approaches to neural programming today fare poorly on this generalization issue",1
" as before, the non-recursive learned program lacks a provable guarantee of generalization, whereas we show later that the recursive learned program has one",1
" table 1 presents results on randomly generated arrays of varying length for the learned non-recursive, partially recursive, and full recursive programs",1
", to color a vertex) or variable (e",1
" the stack operation has the following arguments:
arg 1 (operation): stack push call1, stack push call2, stack pop
stack push call1 pushes lo and pivot−1 toqstacklo andqstackhi",0
"
move",0
" for brevity, we refer the reader to the appendix for further details on the program set and non-recursive and recursive trace-generating functions used for topological sort",0
"we train using the adam optimizer and use a 2-layer lstm and task-specific state encoders for the external environments, as described in reed & de freitas (2016)",0
" , n , where the dag contains n vertices",0
"we train using the adam optimizer and use a 2-layer lstm and task-specific state encoders for the external environments, as described in reed & de freitas (2016)",0
" on the other hand, the non-recursive version has low accuracy, beginning from size 5, and fails completely for graphs of size 8 and beyond",1
" the single-digit multiplication task in zaremba   (2016), the bubble sort task in reed & de freitas (2016), and the graph tasks in graves   (2016))",1
 recursion enables provably perfect generalization,0
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,0
"
to construct this set, by using the reference implementation of each subprogram, we construct a mapping between two sets of environment observations: the first set consists of all observations that can occur at the beginning of a particular subprogram’s invocation, and the second set contains the observations at the end of that subprogram",1
"
we describe the details of the npi model relevant to our contributions",0
" in order to compensate for this, ppivot and pj require special functions (move pivot lo and move j lo) to properly set them to lo in lines 11 and 12 of the partition function in algorithm 3",1
 there are 2 variables (vactive and vsave); vactive holds the active vertex (as in algorithm 2) and vsave holds the value of vactive before executing line 12 of algorithm 2,0
" as described in the verification procedure, it is possible to prove our learned recursive program generalizes perfectly by testing on an appropriate set of problem inputs, i",1
as mentioned in section 2,1
" the 4 environment observations aid with control flow; qstacklo(pstacklo− 1) < qstackhi(pstackhi− 1) implements the lo < hi comparison in line 5 of algorithm 3, pstacklo == 1 checks if the stacks are empty in line 18 of algorithm 4, and the other observations (all involving ppivot or pj) deal with logic in the partition function",0
" we can obtain this mapping by first considering the possible observations that can arise at the beginning of the entry function (add, bubblesort, toposort, and quicksort) for some valid program input, and iteratively applying the observation-to-observation mapping implied by the reference implementation’s step output at that point in the execution",1
 our experiments use a small number of training examples,1
" 4: function toposort(dag) 5: while there is still a white vertex u: do 6: color[u] = grey 7: vactive = u 8: do 9: if vactive has a white child v then 10: color[v] = grey 11: push vactive onto s 12: vactive = v 13: else 14: color[vactive] = black 15: write vactive to result 16: if s is empty then pass 17: else pop the top vertex off s and set it to vactive 18: while s is not empty
topological sort",0
"
under the assumption that there are no leading 0’s (except in the case of single digits) and the two numbers to be added have the same number of digits, the verification set for addition contains 20,181 input problems",1
5,1
" the verification set can be very small, as we found a 10-element array ([8,2,1,2,0,8,5,8,3,7]) is sufficient to cover all of v ",0
" in contrast, each call to the recursive add always runs for a fixed number of steps, even on arbitrarily long problems in f , so we can test that it performs correctly on a small, fixed number of step input sequences",0
" if no white vertex is found, the entire execution is terminated move none write write a value either to environment (e",0
" in this version, there is a dependence on length: bstep and lshift are called a number of times equivalent to one less than the length of the input array, in bubble and reset respectively",0
" 4: function toposort(dag) 5: while there is still a white vertex u: do 6: color[u] = grey 7: vactive = u 8: do 9: if vactive has a white child v then 10: color[v] = grey 11: push vactive onto s 12: vactive = v 13: else 14: color[vactive] = black 15: write vactive to result 16: if s is empty then pass 17: else pop the top vertex off s and set it to vactive 18: while s is not empty
topological sort",0
" as this complexity interfered with determining the base cases and reductions, we changed the algorithm to its current form",1
", an lstm in npi’s case, but possibly other networks in different cases",1
" consequently, it is infeasible to construct a verification set that covers all cases, and the verification procedure we have described is inadequate",1
" however, our concept of recursion for neural programs is general",0
3,0
" childlist ∈ nu is a vector of pointers, where childlist[i] points to the next child under consideration for vertex i",1
" for the recursive formulation of addition, we analytically construct the set of input problems that cover all base cases and reduction rules",1
" in fact, it is one of our future directions to explore new ways to train a recursive neural program without providing explicit training execution traces or with only partial or non-recursive traces",1
 we leave this as future work to devise a verification procedure more appropriate to this setting,0
" for example, a step input-step output pair in lines 2 and 3 of the left-hand side of figure 1 is (add1, write out 1)",0
" if no white vertex is found, the entire execution is terminated move none write write a value either to environment (e",1
" for instance, for recursive addition, consider the family f of addition problems anan−1 ",1
" the verification set can be very small, as we found a 10-element array ([8,2,1,2,0,8,5,8,3,7]) is sufficient to cover all of v ",0
" in contrast, each call to the recursive add always runs for a fixed number of steps, even on arbitrarily long problems in f , so we can test that it performs correctly on a small, fixed number of step input sequences",1
" the move operation has the following arguments:
arg 1 (pointer): pstacklo, pstackhi, pj , ppivot
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values",0
we now report on generalization for the varying tasks,0
 ,0
"
quicksort",1
 recursion can be implemented differently for different neural programming models,1
" more specifically, when m is trained on simpler inputs, such as inputs of a small length, the generalization metric evaluates how well m will do on more complex inputs, such as inputs of much longer length",1
" the second is the training procedure, which consists of the form of the training data and the optimization process",0
 this verification phase only needs to be performed once after training,0
" in order to claim a proof, we must verify the model produces correct behavior over all base cases and reductions, as described in section 2",0
 3: begin traversing from vertex 1 in the dag,0
" the non-recursive and partially recursive versions are unable to sort long arrays, beyond length 8",0
" 4: function toposort(dag) 5: while there is still a white vertex u: do 6: color[u] = grey 7: vactive = u 8: do 9: if vactive has a white child v then 10: color[v] = grey 11: push vactive onto s 12: vactive = v 13: else 14: color[vactive] = black 15: write vactive to result 16: if s is empty then pass 17: else pop the top vertex off s and set it to vactive 18: while s is not empty
topological sort",1
" the partial recursion is not enough for perfect generalization, as will be presented later in section 4",1
"
algorithm 2 depth first search topological sort 1: color all vertices white",1
we emphasize the overall goal of this work is to enable the learning of a recursive program,0
" in our experiments, we only present dag’s as inputs and represent the vertices as values ranging from 1, ",1
"
base cases and reduction rules for topological sort",0
" the pointer at index i3 represents a counter internal to the environment that is incremented once after each pass of the algorithm (one cycle of bubble and reset); when incremented a number of times equal to the length of the array, the flag i3 == length becomes true and terminates the entire algorithm ",1
" both the non-recursive and recursive learned programs generalize on all graphs we tried, up to 120 vertices",0
" for non-primitive subprograms, we can update the observation-to-observation mapping currently associated with the subprogram and then apply that mapping to the current set",0
 our result shows that the training procedure and the npi architecture is capable of generalizing from the step input-output pairs seen in the training data to the unseen ones present in the verification set,1
" for the recursive formulation of addition, we analytically construct the set of input problems that cover all base cases and reduction rules",0
"
it is sufficient to construct problems where every transition between two adjacent columns is covered",0
"
we construct input problems by splitting into two cases: one case in which the left column contains a null value and another in which the left column does not contain any null values",0
" in this version of curriculum learning, even though the inputs are gradually becoming more complex, the semantics of the program is succinct and does not change",0
"
the same set of problems was used to generate the training traces for all formulations of the task, for non-recursive and recursive versions",0
"
in order to demonstrate that recursion can help learn and generalize better, we trained a non-recursive and recursive model on just a single execution trace generated from a graph containing 5 nodes3 for the topological sort task",1
" as before, the non-recursive learned program lacks a provable guarantee of generalization, whereas we show later that the recursive learned program has one",0
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,1
" for grade-school addition, the domain-specific encoder is
fenc(q, i1, i2, i3, i4, at) =mlp ([q(1, i1), q(2, i2), q(3, i3), q(4, i4), at(1), at(2), at(3)]),
where the environment q ∈ r4×n×k is a scratch-pad that contains four rows (the first input number, the second input number, the carry bits, and the output) and n columns",1
 the environment and return probability are omitted for readability,1
"
in addition, none of the current approaches to neural programming provide a method or even aim to enable provable guarantees about generalization",1
"
under the assumption that there are no leading 0’s (except in the case of single digits) and the two numbers to be added have the same number of digits, the verification set for addition contains 20,181 input problems",0
" in practice, we can additionally use tail recursion optimization to avoid problems with the call stack growing too deep",1
", qstacklo or qstackhi) or to pointer (e",1
" each time a subprogram is called, the stack depth increases",0
"
bubble sort",1
" for each length, we test each program on 30 randomly generated problems",0
" as aforementioned, the npi model naturally supports recursion",0
" however, the authors of npi did not consider explicitly the notion of recursion and as a consequence, did not learn recursive programs",0
" we then construct problem configurations that span all possible valid environment states (for instance, in order to force the carry bit in a column to be 1, one can add the sum “1+9” in the column to the right)",0
" given verification sets that cover all the base cases and reduction rules, we can provide proofs that these learned programs generalize perfectly",1
 we use the lomuto partition scheme; the logic for the recursive trace is shown in algorithm 3,0
" naı̈vely, we might expect to synthesize and test an input for any sequence created by combining the four possible colors in two variables and another boolean variable for whether the stack is empty (so 32 possible observations at any point), but for various reasons, most of these combinations are impossible to occur at any given point in the execution trace",0
"1; and for bubble sort, appendix a",1
" the add reduction rule ensures that each call to add only covers two adjacent columns, and so the lstm only ever runs for a fixed number of steps necessary to process these two columns",1
" we made the assumption of equivalent lengths in order to parametrize the input format with respect to length, but this assumption can be removed as well",0
" formally, a function exhibits recursive behavior when it possesses two properties: (1) base cases— terminating scenarios that do not use recursion to produce answers; (2) a set of rules that reduces all other problems toward the base cases",1
", the verification set",0
" however, models that make use of this strategy eventually fail after a certain level of complexity (e",0
" however, note that there is no provable guarantee that the non-recursive learned program will generalize to all inputs, whereas we show later that the recursive learned program has a provable guarantee of perfect generalization",0
" we show that by modifying the training procedure, we enable the npi model to learn recursive neural programs",1
" for the recursive formulation of addition, we analytically construct the set of input problems that cover all base cases and reduction rules",1
 there are 2 variables (vactive and vsave); vactive holds the active vertex (as in algorithm 2) and vsave holds the value of vactive before executing line 12 of algorithm 2,0
" by iterating with this procedure, and then running p on the input observation set that we obtain for the entry point function, we can obtain v precisely",1
" the swap operation has the following arguments:
arg 1 (swap object 1): ppivot
arg 2 (swap object 2): phi, pj
write",0
" qcolor(pstart) contains the color of the current start vertex, used in the evaluation of the condition in the while loop in line 5 of algorithm 2",0
 the program terminates when seeing no numbers in the current column,1
g,0
" in particular, we construct new training traces which explicitly contain recursive elements and show that with this type of trace, npi easily learns recursive programs",1
" for these programs, we re-use the appropriate program sets (the associated subprograms), and we refer the reader to the appendix of reed & de freitas (2016) for further details on the subprograms used in addition and bubble sort",1
" this makes it so that we only need to consider the vertices in the subgraph that are currently relevant for computing the sort, allowing simpler reasoning about behavior for large graphs",0
"
it is important to emphasize that at inference time in the npi, the hidden state of the lstm controller is reset (to zero) at each subprogram call, as in line 3 of algorithm 1 (h ← 0)",0
"
for addition, we analytically determine the verification set",0
" provably perfect generalization implies the model will behave correctly, given any valid input",1
"
the training data for the neural programmer-interpreter consists of full execution traces for the program of interest",0
"
algorithm 1 neural programming inference 1: inputs: environment observation e, program p, arguments a, stop threshold α 2: function run(e, p, a) 3: h← 0, r ← 0 4: while r < α do 5: s← fenc(e, a), h← flstm(s, p, h) 6: r ← fend(h), p2 ← fprog(h), a2 ← farg(h) 7: if p is a primitive function then 8: e← fenv(e, p, a)",1
" the recursive version can generalize perfectly to long problems constructed from these components (such as the sum “822+233”, where “8+2” and “2+3” are in the training set), but the non-recursive version fails to sum these long problems properly",1
" formally, the npi is represented by the following set of equations:
st = fenc(et, at)
ht = flstm(st, pt, ht−1)
rt = fend(ht), pt+1 = fprog(ht), at+1 = farg(ht)
t is a subscript denoting the time-step; fenc is a domain-specific encoder (to be described later) that takes in the environment slice et and arguments at; flstm represents the core module, which takes in the state st generated by fenc, a program embedding pt ∈ rp , and hidden lstm state ht; fend decodes the return probability rt; fprog decodes a program key embedding pt+1;1 and farg decodes arguments at+1",0
" the contents ofqresult andqstack are not exposed directly through the domain-specific encoder; rather, we define primitive functions which manipulate these scratch-pads",1
"
quicksort",0
"
by nature, recursion reduces the complexity of a problem to simpler instances",1
" we note that an earlier version of quicksort we tried lacked primitive operations to directly move a pointer to another, and therefore needed more functions and observations",0
" in particular, this condition is satisfied by npi, and thus the npi model naturally supports recursion (even though the authors of npi did not consider this aspect explicitly)",0
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,1
" even though the earlier version also generalized just as well in practice, relatively small differences in the formulation of the traces and the environment observations can drastically change the difficulty of verification",0
 this verification phase only needs to be performed once after training,1
 recursion can be implemented differently for different neural programming models,1
" on the other hand, the recursive programs have learned the true program semantics",0
 ,0
"
base cases and reduction rules for bubble sort",0
" lshift moves the four pointers to the left, to move to the next column",0
"
move",0
" this is highly undesirable, since being able to provide the correct answer in all possible settings is one of the most important aspects of any learned neural program",0
" in our experiments, we only present dag’s as inputs and represent the vertices as values ranging from 1, ",0
" we also hope to decrease supervision, for example by training with only partial or non-recursive traces, and to develop novel neural programming architectures integrated directly with a notion of recursion",0
" this agrees with the generalization of non-recursive addition in reed & de freitas (2016), where they reported generalization up to 3000 digits",0
" in practice, we can additionally use tail recursion optimization to avoid problems with the call stack growing too deep",0
" as a consequence, our learned neural programs empirically achieve perfect generalization from a very small number of training examples",0
" in this architecture, we consider a core controller, e",0
" we made the assumption of equivalent lengths in order to parametrize the input format with respect to length, but this assumption can be removed as well",0
" for the recursive formulation of addition, we analytically construct the set of input problems that cover all base cases and reduction rules",1
"
given a verification set, we can then run the model on the verification set to check if the produced traces and results are correct",1
" recall that this verification procedure cannot be performed for the non-recursive versions, since the propagation of the hidden state in the core lstm module makes reasoning difficult and so we would need to check an unbounded number of examples",0
" it is straightforward to deal with all other operations, which do not induce partial environment states",1
"
recursion drastically reduces the number of configurations we need to consider during the verification phase and makes the proof tractable, because it introduces structure that eliminates infinitely long sequences of step inputs that would otherwise need to be considered",0
" for these programs, we re-use the appropriate program sets (the associated subprograms), and we refer the reader to the appendix of reed & de freitas (2016) for further details on the subprograms used in addition and bubble sort",1
" 3: 4: function partition(a, lo, hi) 5: pivot = lo 6: for j ∈ [lo, hi− 1] : do 7: if a[j] ≤ a[hi] then 8: swap a[pivot] with a[j] 9: pivot = pivot+ 1 0: swap a[pivot] with a[hi] 11: return pivot 12: 13: function quicksort(a, lo, hi) 14: while slo and shi are not empty: do 15: pop states off slo and shi, writing them to lo and hi",1
" the non-recursive and partially recursive versions are unable to sort long arrays, beyond length 8",0
" in practice, we can additionally use tail recursion optimization to avoid problems with the call stack growing too deep",0
"
figure 1 shows examples of non-recursive and recursive addition traces",1
" in practice, we can additionally use tail recursion optimization to avoid problems with the call stack growing too deep",1
" the recursive version of topological sort solves all graph instances we tried, from graphs of size 5 through 70",0
" as with the original npi architecture, the experiments for this paper always used a 3-tuple of integers at = (at(1), at(2), at(3))",0
" given verification sets that cover all the base cases and reduction rules, we can provide proofs that these learned programs generalize perfectly",0
 table 3 presents results on randomly generated arrays of varying length for the learned non-recursive and recursive programs,1
" we further have qresult ∈ nu , a scratch-pad which contains the sorted list of vertices at the end of execution, and qstack ∈ nu , which serves the role of the stack s in algorithm 2",0
" 2: initialize lo and hi to be 1 and n, where n is the length of a",0
" qcolor(pstart) contains the color of the current start vertex, used in the evaluation of the condition in the while loop in line 5 of algorithm 2",0
"
an alternative way of representing the environment slice is to expose the values of the absolute vertices to the model; however, this makes it difficult to scale the model to larger graphs, since large vertex values are not seen during training time",0
 the memory updates of these neural programs are so complex and interdependent that it is difficult to reason about the behaviors of the learned neural program under previously unseen situations (such as problems with longer inputs),0
g,1
"
we describe the base cases, reduction rules, and the verification set for each task in appendix a",1
" however, models that make use of this strategy eventually fail after a certain level of complexity (e",0
" given a precise definition of p , it may be possible to automate the generation of v from p in future work",0
"
we note that for tasks with very large input domains, such as ones involving mnist digits or speech samples, the state space of base cases and reduction rules could be prohibitively large, possibly infinite",1
" childlist ∈ nu is a vector of pointers, where childlist[i] points to the next child under consideration for vertex i",0
" the pointers at index i1 and i2 are used to compare the pair of numbers considered during the bubble sweep, swapping them if the number at i1 is greater than that in i2",0
" the second is the training procedure, which consists of the form of the training data and the optimization process",1
" m , as described in algorithm 1, processes the sequence of step inputs by using an lstm",1
"
to perform the verification as described here, it is critical to construct v correctly",1
" the recursive version can generalize perfectly to long problems constructed from these components (such as the sum “822+233”, where “8+2” and “2+3” are in the training set), but the non-recursive version fails to sum these long problems properly",0
" each time a subprogram is called, the stack depth increases",0
" 2: initialize lo and hi to be 1 and n, where n is the length of a",0
" for these models, table 2 presents results on randomly generated dags of varying graph sizes (varying in the number of vertices)",1
" the npi model has three learnable components: a task-agnostic core, a program-key embedding, and domain-specific encoders that allow the npi to operate in diverse environments",0
" this makes it so that we only need to consider the vertices in the subgraph that are currently relevant for computing the sort, allowing simpler reasoning about behavior for large graphs",0
" on the other hand, the recursive programs have learned the true program semantics",1
"1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 while (pstacklo 6= 1): 6 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 7 stack(stack_pop) 8 else: 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 stack(stack_pop) 12 partition() 13 stack(stack_push_call2) 14 stack(stack_push_call1) 15 } 16 17 partition() { 18 set_pivot_lo() 19 set_j_lo() 20 compswap_loop() 21 swap(ppivot, phi) 22 set_j_null() 23 } 24 25 compswap_loop() { 26 while (pj 6= phi): 27 compswap() 28 move(pj, up) 29 } 30 31 compswap() { 32 if (a[pj ] ≤ a[phi]): 33 swap(ppivot, pj) 34 move(ppivot, up) 35 } 36 37 stack(op) { 38 if (op == stack_push_call1): 39 write(env_stack_lo, plo) 40 write(env_stack_hi, ppivot − 1) 41 move(pstacklo, up) 42 move(pstackhi, up) 43 44 if (op == stack_push_call2): 45 write(env_stack_lo, ppivot + 1) 46 write(env_stack_hi, phi) 47 move(pstacklo, up) 48 move(pstackhi, up) 49 50 if (op == stack_pop): 51 write(env_stack_lo, reset) 52 write(env_stack_hi, reset) 53 move(pstacklo, down) 54 move(pstackhi, down) 55 }altered recursive functions 1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 6 partition() 7 stack(stack_push_call2) 8 stack(stack_push_call1) 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 quicksort() // recursive call 12 stack(stack_pop) 13 write(phi, env_stack_hi_peek) 14 write(plo, env_stack_lo_peek) 15 quicksort() // recursive call 16 stack(stack_pop) 17 } 18 19 compswap_loop() { 20 if (pj 6= phi): 21 compswap() 22 move(pj, up) 23 compswap_loop() // recursive call 24 }in this section, we describe the space of base cases and reduction rules that must be covered for each of the four sample tasks, in order to create the verification set",1
 ,0
" the pointers at index i1 and i2 are used to compare the pair of numbers considered during the bubble sweep, swapping them if the number at i1 is greater than that in i2",1
"as there is no public implementation of npi, we implemented a version of it in keras that is as faithful to the paper as possible",0
" our implementation uses two stacks qstacklo and
qstackhi, each in ru , that store the arguments to the recursive quicksort calls in algorithm 3; before each recursive call, the appropriate arguments are popped off the stack and written to plo and phi",0
"
as in line 13 of the right-hand side of figure 1",1
" the 4 environment observations aid with control flow; qstacklo(pstacklo− 1) < qstackhi(pstackhi− 1) implements the lo < hi comparison in line 5 of algorithm 3, pstacklo == 1 checks if the stacks are empty in line 18 of algorithm 4, and the other observations (all involving ppivot or pj) deal with logic in the partition function",0
 recursion enables provable guarantees on neural programs’ behavior without needing to exhaustively enumerate all possible inputs to the programs,1
 3: begin traversing from vertex 1 in the dag,1
" although the model is exposed to more and more data, it might learn spurious and overly complex representations of the program, as suggested in zaremba   (2016)",0
" this functionality the original npi paper decodes to a program key embedding kt ∈ rk and then computes a program embedding pt+1, which we also did in our implementation, but we omit this for brevity",1
this material is in part based upon work supported by the national science foundation under grant no,0
" for bubble sort, the domain-specific encoder is
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1), q(1, i2), i3 == length, at(1), at(2), at(3)]),
where the environment q ∈ r1×n×k is a scratch-pad that contains 1 row, to represent the state of the array as sorting proceeds in-place, andn columns",1
" by iterating with this procedure, and then running p on the input observation set that we obtain for the entry point function, we can obtain v precisely",0
" 2: initialize lo and hi to be 1 and n, where n is the length of a",0
" the partial recursion is not enough for perfect generalization, as will be presented later in section 4",1
"
algorithm 2 depth first search topological sort 1: color all vertices white",0
" any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of national science foundation and darpa",0
" the corresponding edge list is [(1, 2), (1, 5), (2, 4), (2, 5), (3, 5)]",1
"
by nature, recursion reduces the complexity of a problem to simpler instances",0
"
arg 2 (increment or decrement): up, down1 // top level topological sort call 2 toposort() { 3 while (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 } 0 11 traverse() { 12 check_child() 13 explore() 14 } 15 16 check_child() { 17 while (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 18 move(childlist[vactive], up) 19 } 20 21 explore() { 22 do 23 if (qcolor(dag[vactive][childlist[vactive]]) is white): 24 write(color_next, color_grey) 25 stack(push) 26 write(save) 27 write(active_neighb) 28 move(childlist[vsave], up) 29 else: 30 write(color_curr, color_black) 31 write(result) 32 move(presult, up) 33 if(pstack == 1): 34 break 35 else: 36 stack(pop) 37 check_child() 38 while (true) 39 } 40 41 stack(op) { 42 if (op == push): 43 write(stack_push) 44 move(pstack, up) 45 46 if (op == pop): 47 write(active_stack) 48 write(stack_pop) 49 move(pstack, down) 50 } 51 52 next_start() { 53 while(qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 54 move(pstart, up) 55 }altered recursive functions  // top level topological sort call 2 toposort() { 3 if (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 toposort() // recursive call 0 } 11 12 check_child() { 13 if (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 14 move(childlist[vactive], up) 15 check_child() // recursive call 16 } 17 18 explore() { 19 if (qcolor(dag[vactive][childlist[vactive]]) is white): 20 write(color_next, color_grey) 21 stack(push) 22 write(save) 23 write(active_neighb) 24 move(childlist[vsave], up) 25 else: 26 write(color_curr, color_black) 27 write(result) 28 move(presult, up) 29 if(pstack == 1): 30 return 31 else: 32 stack(pop) 33 check_child() 34 explore() // recursive call 35 } 36 37 next_start() { 38 if (qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 39 move(pstart, up) 40 next_start() // recursive call 41 }algorithm 4 iterative quicksort 1: initialize an array a to sort and two empty stacks slo and shi",0
" for brevity, we refer the reader to the appendix for further details on the program set and non-recursive and recursive trace-generating functions used for topological sort",1
" the corresponding edge list is [(1, 2), (1, 5), (2, 4), (2, 5), (3, 5)]",0
" ppivot and pj point to the pivot and j indices of the array, used in the partition function in algorithm 3",1
" in this version of curriculum learning, even though the inputs are gradually becoming more complex, the semantics of the program is succinct and does not change",1
" k is set to 11, to represent the range of 10 possible digits, along with a token representing the end of input",0
" here as a concrete and general example, we consider a general neural programming architecture (npa), similar to neural programmer-interpreter (npi) in reed & de freitas (2016)",0
" for the recursive formulation of addition, we analytically construct the set of input problems that cover all base cases and reduction rules",0
 the memory updates of these neural programs are so complex and interdependent that it is difficult to reason about the behaviors of the learned neural program under previously unseen situations (such as problems with longer inputs),0
 2: initialize an empty stack s and a directed acyclic graph dag to traverse,0
" at end, pointer ppivot is moved to the pivot compswap loop, move pivot lo, move j lo, swap
none
compswap loop runs the for loop inside the partition function compswap, move none compswap compares a[pivot] ≤ a[j]; if so, perform a swap and increment ppivot swap, move none set pivot lo sets ppivot to lo index none none set j lo sets pj to lo index none none set j null sets pj to −∞ none none stack pushes lo/hi states
onto stacks slo and shi according to argument (described below)
write, move described below
move moves pointer one unit up or down none described below swap swaps elements at given array indices none described below write write a value either to stack (e",0
"
quicksort",0
" the write operation has the following arguments:
arg 1 (object to write): env stack lo, env stack hi, phi, plo
env stack lo and env stack hi represent qstacklo(pstacklo) and qstackhi(pstackhi), respectively",0
3,1
" 9: else 10: function run(e, p2, a2)
a description of the inference procedure is given in algorithm 1",0
" as this complexity interfered with determining the base cases and reductions, we changed the algorithm to its current form",1
 18: push lo and p− 1 to slo and shi,0
"
it is important to emphasize that at inference time in the npi, the hidden state of the lstm controller is reset (to zero) at each subprogram call, as in line 3 of algorithm 1 (h ← 0)",1
" in this section, we give a brief review of the npi architecture from reed & de freitas (2016) as background",0
" each time a subprogram is called, the stack depth increases",0
", to change the value of vactive) none described below move move a pointer (e",0
g,1
" we call this set of inputs the verification set, sv ",1
"
we would also like to point out that in this paper, we provide as an example one way to train a recursive neural program, by providing a certain training execution trace to the npi model",0
 we found that changing the npi training traces is a simple way to enable this,0
"
there are 6 pointers (plo, phi, pstacklo, pstackhi, ppivot, pj)",1
"
grade school addition",0
"
by reasoning about the possible set of environment observations created by all valid inputs, we construct v using the procedure described in section 3",0
"
figure 1 shows examples of non-recursive and recursive addition traces",1
 ,0
"
algorithm 3 recursive quicksort 1: initialize an array a to sort",1
 the core controller acts as a dispatcher for the programs,0
" thus, recursion helps decompose a problem and makes it easier to reason about a program’s behavior for previously unseen situations such as longer inputs",0
" at any given time, the npi has access to the values referred to by two pointers, represented by q(1, i1) and q(1, i2),",0
"
base cases and reduction rules for addition",1
"
note that v should also contain the necessary reductions, which corresponds to making the recursive calls at the correct time, as indicated by p ",1
2,0
" in contrast, each call to the recursive add always runs for a fixed number of steps, even on arbitrarily long problems in f , so we can test that it performs correctly on a small, fixed number of step input sequences",0
" in particular, in the explore function, adding a tail recursive call resets and stores the hidden states associated with vertices in a stack-like fashion",0
"we describe how models trained with recursive traces can be proven to generalize, by using the verification procedure described in section 3",1
" numerous models have been proposed for learning programs; to name a few, this includes the differentiable neural computer (graves  , 2016), neural turing machine (graves  , 2014), neural gpu (kaiser & sutskever, 2015), neural programmer (neelakantan  , 2015), pointer network (vinyals  , 2015), hierarchical attentive memory (andrychowicz & kurach, 2016), and neural random access machine (kurach  , 2016)",0
6,0
"
in order to demonstrate that recursion can help learn and generalize better, for addition, we trained only on traces for 5 arbitrarily chosen 1-digit addition sum examples",0
" each time a subprogram is called, the stack depth increases",0
" however, note that there is no provable guarantee that the non-recursive learned program will generalize to all inputs, whereas we show later that the recursive learned program has a provable guarantee of perfect generalization",0
 there are 2 variables (vactive and vsave); vactive holds the active vertex (as in algorithm 2) and vsave holds the value of vactive before executing line 12 of algorithm 2,0
we emphasize that the notion of a neural recursive program has not been presented in the literature before: this is our main contribution,0
" 2: initialize lo and hi to be 1 and n, where n is the length of a",1
" formally, a function exhibits recursive behavior when it possesses two properties: (1) base cases— terminating scenarios that do not use recursion to produce answers; (2) a set of rules that reduces all other problems toward the base cases",0
" 16: p = partition(a, lo, hi) 17: push p+ 1 and hi to slo and shi",0
" the pointers at index i1 and i2 are used to compare the pair of numbers considered during the bubble sweep, swapping them if the number at i1 is greater than that in i2",0
", the verification set",0
"
we experiment with two levels of recursion—partial and full",1
 ,0
" note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values, and arg 2 can only take one of 7 values",0
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,1
" for brevity, we refer the reader to the appendix for further details on the program set and non-recursive and recursive trace-generating functions used for topological sort",0
" as an application, we incorporate recursion into the neural programmer-interpreter architecture and consider four sample tasks: grade-school addition, bubble sort, topological sort, and quicksort",0
" if it is too large, then the semantics of p might not be well-defined on some elements in v , or the spurious step input sequences may not be reachable from any valid problem input (e",1
 we use the lomuto partition scheme; the logic for the recursive trace is shown in algorithm 3,0
"
to construct this set, by using the reference implementation of each subprogram, we construct a mapping between two sets of environment observations: the first set consists of all observations that can occur at the beginning of a particular subprogram’s invocation, and the second set contains the observations at the end of that subprogram",1
" the stack operation has the following arguments:
arg 1 (operation): stack push call1, stack push call2, stack pop
stack push call1 pushes lo and pivot−1 toqstacklo andqstackhi",1
3,0
"
we propose and describe our verification procedure",0
we emphasize the overall goal of this work is to enable the learning of a recursive program,0
" on the other hand, the recursive programs have learned the true program semantics",1
" for purposes of verification, we replace the domain-specific encoder with the following:
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1) ≤ q(1, i2), 1 ≤ i1 ≤ length, 1 ≤ i2 ≤ length, i3 == length, at(1), at(2), at(3)]),which directly exposes which of the two values pointed to is larger",0
 the logic for the non-recursive trace is shown in algorithm 4 in the appendix,1
" numerous models have been proposed for learning programs; to name a few, this includes the differentiable neural computer (graves  , 2016), neural turing machine (graves  , 2014), neural gpu (kaiser & sutskever, 2015), neural programmer (neelakantan  , 2015), pointer network (vinyals  , 2015), hierarchical attentive memory (andrychowicz & kurach, 2016), and neural random access machine (kurach  , 2016)",0
" thus, any general neural programming architecture supporting such a call structure can be made to support recursion",0
g,1
 reset represents a −∞ value,1
" the swap operation has the following arguments:
arg 1 (swap object 1): ppivot
arg 2 (swap object 2): phi, pj
write",0
"
move",0
 errors arise due to undesirable and uninterpretable dependencies and associations the architecture learns to store in some high-dimensional hidden state,0
" the verification set can be very small, as we found a 10-element array ([8,2,1,2,0,8,5,8,3,7]) is sufficient to cover all of v ",0
" also the dag can have potentially more than one connected component, meaning it is necessary to transition between these components appropriately",1
" if a white vertex is found, set pstart to point to it; this signifies the start of a traversal of a new connected component",1
" nevertheless, we would like the model to quickly learn to represent the right semantics of the underlying program from a small number of training examples, not an exhaustive number of them",0
", pstart or childlist[vactive]) up or down none described belowwrite",1
g,0
", pstart or childlist[vactive]) up or down none described belowwrite",1
" however, note that there is no provable guarantee that the non-recursive learned program will generalize to all inputs, whereas we show later that the recursive learned program has a provable guarantee of perfect generalization",1
we now report on generalization for the varying tasks,1
we emphasize the overall goal of this work is to enable the learning of a recursive program,1
"
as in line 13 of the right-hand side of figure 1",0
" qcolor(dag[vactive][childlist[vactive]]) refers to the color of the next child of vactive, used in the evaluation of the condition in the if branch in line 9 of algorithm 2",1
" 2: initialize lo and hi to be 1 and n, where n is the length of a",0
" nevertheless, we would like the model to quickly learn to represent the right semantics of the underlying program from a small number of training examples, not an exhaustive number of them",0
"
base cases and reduction rules for addition",0
 we adapt machinery from the original paper slightly to fit our needs,0
3,0
 pstacklo and pstackhi point to the top (empty) positions in qstacklo and qstackhi,0
" 3: 4: function partition(a, lo, hi) 5: pivot = lo 6: for j ∈ [lo, hi− 1] : do 7: if a[j] ≤ a[hi] then 8: swap a[pivot] with a[j] 9: pivot = pivot+ 1 0: swap a[pivot] with a[hi] 11: return pivot 12: 13: function quicksort(a, lo, hi) 14: while slo and shi are not empty: do 15: pop states off slo and shi, writing them to lo and hi",0
 this makes it difficult to reason about what the model will do when given complex inputs,0
g,0
"
under the assumption that there are no leading 0’s (except in the case of single digits) and the two numbers to be added have the same number of digits, the verification set for addition contains 20,181 input problems",0
" as a consequence, our learned neural programs empirically achieve perfect generalization from a very small number of training examples",1
" the theorem states that for the same sequence of step inputs, the model produces the exact same step output as the target program it aims to learn",0
 we leave this as future work to devise a verification procedure more appropriate to this setting,1
"
training setup",1
6,0
" furthermore, given a verification input set that covers all base cases and reduction rules, we can formally prove that the learned neural programs achieve perfect generalization after verifying its behavior on the verification input set",0
"
the training data for the neural programmer-interpreter consists of full execution traces for the program of interest",0
" we can obtain this mapping by first considering the possible observations that can arise at the beginning of the entry function (add, bubblesort, toposort, and quicksort) for some valid program input, and iteratively applying the observation-to-observation mapping implied by the reference implementation’s step output at that point in the execution",0
 this procedure verifies that all base cases and reductions are handled properly by the model via explicit tests,1
 stack push call2 pushes pivot + 1 and hi to qstacklo and qstackhi,0
" the arrow in the theorem refers to evaluation, as in big-step semantics",0
" table 1 presents results on randomly generated arrays of varying length for the learned non-recursive, partially recursive, and full recursive programs",0
 18: push lo and p− 1 to slo and shi,0
", pstart or childlist[vactive]) up or down none described belowwrite",0
 our experiments use a small number of training examples,0
 we choose to implement a topological sort task for graphs,0
" 16: p = partition(a, lo, hi) 17: push p+ 1 and hi to slo and shi",0
 table 4 demonstrates that the accuracy of the non-recursive program degrades sharply when moving from arrays of length 7 to arrays of length 8,1
" for each function we use to implement the recursive version of topological sort, we need to consider the set of possible environment observation sequences we can create from all valid inputs and test that the learned program produces the correct behavior on each of these inputs",0
" k is set to 11, to represent the range of 10 possible digits, along with a token representing the end of input",1
" qcolor(pstart) contains the color of the current start vertex, used in the evaluation of the condition in the while loop in line 5 of algorithm 2",1
" while this matches the description from reed & de freitas (2016), we found that this causes an unnecessary blowup in the size of v and makes it much more difficult to construct the verification set",1
" for these programs, we re-use the appropriate program sets (the associated subprograms), and we refer the reader to the appendix of reed & de freitas (2016) for further details on the subprograms used in addition and bubble sort",0
 table 4 also demonstrates that the (verified) recursive program generalizes perfectly,1
" nevertheless, we would like the model to quickly learn to represent the right semantics of the underlying program from a small number of training examples, not an exhaustive number of them",0
" we made the assumption of equivalent lengths in order to parametrize the input format with respect to length, but this assumption can be removed as well",0
" because the context of the caller is stored on a stack when it calls another program and the callee starts in a fresh context, this enables recursion simply by allowing a program to call itself",1
" for grade-school addition, the domain-specific encoder is
fenc(q, i1, i2, i3, i4, at) =mlp ([q(1, i1), q(2, i2), q(3, i3), q(4, i4), at(1), at(2), at(3)]),
where the environment q ∈ r4×n×k is a scratch-pad that contains four rows (the first input number, the second input number, the carry bits, and the output) and n columns",1
 ,0
" the write operation has the following arguments:
arg 1 (object to write): env stack lo, env stack hi, phi, plo
env stack lo and env stack hi represent qstacklo(pstacklo) and qstackhi(pstackhi), respectively",0
 2: initialize an empty stack s and a directed acyclic graph dag to traverse,0
 errors arise due to undesirable and uninterpretable dependencies and associations the architecture learns to store in some high-dimensional hidden state,1
" we then construct problem configurations that span all possible valid environment states (for instance, in order to force the carry bit in a column to be 1, one can add the sum “1+9” in the column to the right)",0
 the memory updates of these neural programs are so complex and interdependent that it is difficult to reason about the behaviors of the learned neural program under previously unseen situations (such as problems with longer inputs),0
" in all experiments, α is set to 0",0
"
the dag is represented as an adjacency list where dag[i][j] refers to the j-th child of vertex i",1
" the move operation has the following arguments:
arg 1 (pointer): pstacklo, pstackhi, pj , ppivot
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values",1
" 3: 4: function quicksort(a, lo, hi) 5: if lo < hi: then 6: p = partition(a, lo, hi) 7: quicksort(a, lo, p− 1) 8: quicksort(a, p+ 1, hi) 9: 10: function partition(a, lo, hi) 11: pivot = lo 12: for j ∈ [lo, hi− 1] : do 13: if a[j] ≤ a[hi] then 14: swap a[pivot] with a[j] 15: pivot = pivot+ 1 16: swap a[pivot] with a[hi] 17: return pivot
for quicksort, the domain-specific encoder is
fenc(qarray, qstacklo, qstackhi, plo, phi, pstacklo, pstackhi, ppivot, pj , at) =
mlp ([qarray(pj) ≤ qarray(phi), pj == phi, qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1), pstacklo == 1, at(1), at(2), at(3)]),
whereqarray ∈ ru×11 is a scratch-pad that contains u rows, each containing one of 11 values (one of the numbers 0 through 9 or an invalid state)",1
" 9: else 10: function run(e, p2, a2)
a description of the inference procedure is given in algorithm 1",1
" the core lstm module’s hidden state is preserved over all these add1 calls, and it is difficult to interpret with certainty what happens over longer timesteps without concretely evaluating the lstm with an input of that length",1
" the contents ofqresult andqstack are not exposed directly through the domain-specific encoder; rather, we define primitive functions which manipulate these scratch-pads",1
we emphasize that the notion of a neural recursive program has not been presented in the literature before: this is our main contribution,1
6,0
" observe that the non-recursive program’s correctness degrades for length 11 and beyond, while the recursive program can sort any given array",1
 the core controller acts as a dispatcher for the programs,1
" this is possible if and only if the graph has no directed cycles; that is to say, it must be a directed acyclic graph (dag)",0
" as with the others, we apply the procedure described in section 3",1
"
we would also like to point out that in this paper, we provide as an example one way to train a recursive neural program, by providing a certain training execution trace to the npi model",1
"
we refer the reader to the appendix for the non-recursive trace generating functions",0
"
we also report on generalization results for the non-recursive and recursive versions of this variant of bubble sort",1
" in fact, we argue that recursion is an essential element for neural programs to generalize, and makes it tractable to prove the generalization of neural programs",0
"
is critical for implementing recursion, since it permits us to restrict our attention to the currently relevant recursive call, ignoring irrelevant details about other contexts",0
" as before, the non-recursive learned program lacks a provable guarantee of generalization, whereas we show later that the recursive learned program has one",0
" if the program is primitive, the next environmental state et+1 will be affected by pt and at, i",0
"we describe how models trained with recursive traces can be proven to generalize, by using the verification procedure described in section 3",1
" for bubble sort, the domain-specific encoder is
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1), q(1, i2), i3 == length, at(1), at(2), at(3)]),
where the environment q ∈ r1×n×k is a scratch-pad that contains 1 row, to represent the state of the array as sorting proceeds in-place, andn columns",0
"
base cases and reduction rules for quicksort",0
"
we propose and describe our verification procedure",1
" the write operation has the following arguments:
arg 1 (main action): color curr, color next, active start, active neighb, active stack, save, stack push, stack pop, result
color curr colors vactive, color next colors vertex dag[vactive][childlist[vactive]], active start writes pstart to vactive, active neighb writes dag[vactive][childlist[vactive]] to vactive, active stack writes qstack(pstack) to vactive, save writes vactive to vsave, stack push pushes vactive to the top of the stack, stack pop writes a null value to the top of the stack, and result writes vactive to qresult(presult)",0
"
we propose and describe our verification procedure",0
"
the non-recursive trace loops on cycles of bubble and reset, which logically represents one bubble sweep through the array and reset of the two bubble pointers to the very beginning of the array, respectively",1
"
in order to demonstrate that recursion can help learn and generalize better, we trained a non-recursive and recursive model on just a single execution trace generated from a graph containing 5 nodes3 for the topological sort task",0
" for non-primitive subprograms, we can update the observation-to-observation mapping currently associated with the subprogram and then apply that mapping to the current set",0
" an example of part of an addition task trace, written in shorthand, is given in figure 1",0
we now report on generalization for the varying tasks,0
"
base cases and reduction rules for addition",0
" qcolor(dag[vactive][childlist[vactive]]) refers to the color of the next child of vactive, used in the evaluation of the condition in the if branch in line 9 of algorithm 2",1
" partial recursion only adds a tail recursive call to bubblesort after bubble and reset, similar to the tail recursive call described previously for addition",1
g,0
" as with the others, we apply the procedure described in section 3",1
" for the recursive formulation of addition, we analytically construct the set of input problems that cover all base cases and reduction rules",0
" the recursive version can generalize perfectly to long problems constructed from these components (such as the sum “822+233”, where “8+2” and “2+3” are in the training set), but the non-recursive version fails to sum these long problems properly",1
g,1
 the training set for addition contains 200 traces,1
" however, the authors of npi did not consider explicitly the notion of recursion and as a consequence, did not learn recursive programs",0
"
to perform the verification as described here, it is critical to construct v correctly",1
 this algorithm is a variant of depth first search,0
" the learned recursive program is different from neural programs learned in all previous work in an important aspect: previous approaches do not explicitly incorporate this abstraction, and hence generalize poorly, whereas our learned neural programs incorporate recursion and achieve perfect generalization",0
" empirically, we observe that the learned recursive programs solve all valid inputs with 100% accuracy after training on a very small number of examples, out-performing previous generalization results",1
" the non-recursive and partially recursive versions are unable to sort long arrays, beyond length 8",1
"
it is important to emphasize that at inference time in the npi, the hidden state of the lstm controller is reset (to zero) at each subprogram call, as in line 3 of algorithm 1 (h ← 0)",0
3,1
 this modification also enables us to sort arrays containing arbitrary comparable elements,1
we emphasize the overall goal of this work is to enable the learning of a recursive program,1
" qcolor(pstart) contains the color of the current start vertex, used in the evaluation of the condition in the while loop in line 5 of algorithm 2",0
" we then construct problem configurations that span all possible valid environment states (for instance, in order to force the carry bit in a column to be 1, one can add the sum “1+9” in the column to the right)",0
" this is possible if and only if the graph has no directed cycles; that is to say, it must be a directed acyclic graph (dag)",0
"
formally, verification consists of proving the following theorem:
∀i ∈ v,m(i) ⇓ p (i) where i denotes a sequence of step inputs (within one function call), v denotes the set of valid sequences of step inputs, m denotes the neural network model, p denotes the correct program, and p (i) denotes the next step output from the correct program",1
" our implementation uses two stacks qstacklo and
qstackhi, each in ru , that store the arguments to the recursive quicksort calls in algorithm 3; before each recursive call, the appropriate arguments are popped off the stack and written to plo and phi",0
" this makes it so that we only need to consider the vertices in the subgraph that are currently relevant for computing the sort, allowing simpler reasoning about behavior for large graphs",0
"
quicksort",1
", qstacklo or qstackhi) or to pointer (e",1
" this functionality the original npi paper decodes to a program key embedding kt ∈ rk and then computes a program embedding pt+1, which we also did in our implementation, but we omit this for brevity",1
" for these models, table 2 presents results on randomly generated dags of varying graph sizes (varying in the number of vertices)",0
"
the training set for bubble sort contains 100 traces, with maximum problem length of 2 (e",0
 the maximum problem length in this training set is 3 (e,1
" formally, the npi is represented by the following set of equations:
st = fenc(et, at)
ht = flstm(st, pt, ht−1)
rt = fend(ht), pt+1 = fprog(ht), at+1 = farg(ht)
t is a subscript denoting the time-step; fenc is a domain-specific encoder (to be described later) that takes in the environment slice et and arguments at; flstm represents the core module, which takes in the state st generated by fenc, a program embedding pt ∈ rp , and hidden lstm state ht; fend decodes the return probability rt; fprog decodes a program key embedding pt+1;1 and farg decodes arguments at+1",0
 ,1
", an lstm in npi’s case, but possibly other networks in different cases",1
" if the program is primitive, the next environmental state et+1 will be affected by pt and at, i",1
" however, our concept of recursion for neural programs is general",0
 npi then outputs the return probability and next program and arguments to execute,1
" note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values, and arg 2 can only take one of 7 values",0
we emphasize the overall goal of this work is to enable the learning of a recursive program,0
", qstacklo or qstackhi) or to pointer (e",0
"1, we hypothesize the non-recursive programs do not generalize well because they have learned spurious dependencies specific to the training set, such as length of the input problems",0
"program descriptions calls arguments quicksort run the quicksort
routine in place for the array a, for indices from lo to hi non-recursive: partition, stack, write recursive: same as non-recursive version, along with quicksort implicitly: array a to sort, lo, hi
partition runs the partition function",1
"as there is no public implementation of npi, we implemented a version of it in keras that is as faithful to the paper as possible",1
" training on the full recursive trace leads to perfect generalization, as shown in section 4",1
 our experiments use a small number of training examples,0
" the 4 environment observations aid with control flow; qstacklo(pstacklo− 1) < qstackhi(pstackhi− 1) implements the lo < hi comparison in line 5 of algorithm 3, pstacklo == 1 checks if the stacks are empty in line 18 of algorithm 4, and the other observations (all involving ppivot or pj) deal with logic in the partition function",0
" in order to claim a proof, we must verify the model produces correct behavior over all base cases and reductions, as described in section 2",0
 the degree for any vertex in the dag is variable,0
 a1a0 + bnbn−1 ,1
" this functionality the original npi paper decodes to a program key embedding kt ∈ rk and then computes a program embedding pt+1, which we also did in our implementation, but we omit this for brevity",1
" furthermore, given a verification input set that covers all base cases and reduction rules, we can formally prove that the learned neural programs achieve perfect generalization after verifying its behavior on the verification input set",0
", to change the value of phi) none described belowstack",0
" in fact, it is one of our future directions to explore new ways to train a recursive neural program without providing explicit training execution traces or with only partial or non-recursive traces",0
"
the training set for bubble sort contains 100 traces, with maximum problem length of 2 (e",1
" each step during an execution of the program does one of three things: (1) another subprogram along with associated arguments is called, as in line 10, (2) the program writes to the environment if it is primitive, as in line 8, or (3) the loop is terminated if the return probability exceeds a threshold α, after which the stack frame is popped and control is returned to the caller",0
" this is possible if and only if the graph has no directed cycles; that is to say, it must be a directed acyclic graph (dag)",0
"
we describe the details of the npi model relevant to our contributions",0
" if a white vertex is found, set pstart to point to it; this signifies the start of a traversal of a new connected component",1
"
the non-recursive trace loops on cycles of bubble and reset, which logically represents one bubble sweep through the array and reset of the two bubble pointers to the very beginning of the array, respectively",0
 add1 is a subprogram that adds the current column (writing the appropriate digit to the output row and carrying a bit to the next column if needed),1
" fa8750-15-2-0104, and berkeley deep drive",0
"
training setup",1
" partial recursion only adds a tail recursive call to bubblesort after bubble and reset, similar to the tail recursive call described previously for addition",1
"
in addition, none of the current approaches to neural programming provide a method or even aim to enable provable guarantees about generalization",0
" that is to say, the network does not learn the true program semantics",0
" we implement a quicksort task, in order to demonstrate that recursion helps with learning divide-and-conquer algorithms",0
"
in order to demonstrate that recursion can help learn and generalize better, we trained a non-recursive and recursive model on just a single execution trace generated from a graph containing 5 nodes3 for the topological sort task",1
"
by reasoning about the possible set of environment observations created by all valid inputs, we construct v using the procedure described in section 3",0
"
in order to demonstrate that recursion can help learn and generalize better, we trained a non-recursive and recursive model on just a single execution trace generated from a graph containing 5 nodes3 for the topological sort task",1
"program descriptions calls arguments toposort perform topological
sort on graph traverse, next start, write, move
none
traverse traverse graph until stack is empty check child, explore none check child check if a white child exists; if so, set childlist[vactive] to point to it move none explore repeatedly traverse subgraphs until stack is empty stack, check child, write, move none stack interact with stack, either pushing or popping write, move push, pop next start move pstart until reaching a white vertex",0
 figure 2 shows examples of traces for the different versions of bubble sort,0
"
as in line 13 of the right-hand side of figure 1",1
" hence, if all subproblems in s are added correctly, we have proven that any member of f will be added correctly, thus eliminating an infinite family of inputs that need to be tested",0
" the architecture usually possesses some form of memory, which could be internal (such as the hidden state of a recurrent neural network) or external (such as a discrete “scratch pad” or a memory block with differentiable access)",0
we emphasize the overall goal of this work is to enable the learning of a recursive program,0
"
it is important to emphasize that at inference time in the npi, the hidden state of the lstm controller is reset (to zero) at each subprogram call, as in line 3 of algorithm 1 (h ← 0)",1
"
the npi accesses an external environment, q, which varies according to the task",1
"
directed acyclic graphs are structurally more diverse than inputs in the two tasks of grade-school addition and bubble sort",0
"
in this paper, we propose to resolve these issues by explicitly incorporating recursion into neural architectures",1
" this agrees with the generalization of non-recursive addition in reed & de freitas (2016), where they reported generalization up to 3000 digits",0
" in fact, we argue that recursion is an essential element for neural programs to generalize, and makes it tractable to prove the generalization of neural programs",0
" in contrast, each call to the recursive add always runs for a fixed number of steps, even on arbitrarily long problems in f , so we can test that it performs correctly on a small, fixed number of step input sequences",0
" even though the earlier version also generalized just as well in practice, relatively small differences in the formulation of the traces and the environment observations can drastically change the difficulty of verification",0
" , a1a0 + b1b0} are added properly",1
 ,0
" nevertheless, we would like the model to quickly learn to represent the right semantics of the underlying program from a small number of training examples, not an exhaustive number of them",0
" thus, we demonstrate that recursion enables provably perfect generalization for different tasks, including addition, topological sort, quicksort, and a variant of bubble sort",0
 recursion enables provably perfect generalization,1
" our implementation uses two stacks qstacklo and
qstackhi, each in ru , that store the arguments to the recursive quicksort calls in algorithm 3; before each recursive call, the appropriate arguments are popped off the stack and written to plo and phi",1
"
we experiment with two levels of recursion—partial and full",0
" the core module of the npi is an lstm controller that takes as input a slice of the current external environment, via a set of pointers, and a program and arguments to execute",1
 recursion enables provably perfect generalization,0
"
grade-school addition",0
"we describe how models trained with recursive traces can be proven to generalize, by using the verification procedure described in section 3",0
" the theorem states that for the same sequence of step inputs, the model produces the exact same step output as the target program it aims to learn",1
"
to construct this set, by using the reference implementation of each subprogram, we construct a mapping between two sets of environment observations: the first set consists of all observations that can occur at the beginning of a particular subprogram’s invocation, and the second set contains the observations at the end of that subprogram",0
" numerous models have been proposed for learning programs; to name a few, this includes the differentiable neural computer (graves  , 2016), neural turing machine (graves  , 2014), neural gpu (kaiser & sutskever, 2015), neural programmer (neelakantan  , 2015), pointer network (vinyals  , 2015), hierarchical attentive memory (andrychowicz & kurach, 2016), and neural random access machine (kurach  , 2016)",0
 the first is the actual model architecture,0
"
the non-recursive trace loops on cycles of bubble and reset, which logically represents one bubble sweep through the array and reset of the two bubble pointers to the very beginning of the array, respectively",0
 this algorithm is a variant of depth first search,1
"
bubble sort",0
" the npi model has three learnable components: a task-agnostic core, a program-key embedding, and domain-specific encoders that allow the npi to operate in diverse environments",1
" the core module of the npi is an lstm controller that takes as input a slice of the current external environment, via a set of pointers, and a program and arguments to execute",1
"
the npi accesses an external environment, q, which varies according to the task",0
" when the program is called, the current context including the caller’s memory state is stored on a stack; when the program returns, the stored context is popped off the stack to resume execution in the previous caller’s context",1
g,0
 u varies with the number of vertices in the graph,1
" thus, any general neural programming architecture supporting such a call structure can be made to support recursion",1
"
we refer the reader to the appendix for the non-recursive trace generating functions",0
" we prove every member of f is added properly, given that subproblems s = {anan−1 + bnbn−1, an−1an−2 + bn−1bn−2, ",1
"
by nature, recursion reduces the complexity of a problem to simpler instances",1
" both the non-recursive and recursive learned programs generalize on all input lengths we tried, up to 5000 digits",0
" the write operation has the following arguments:
arg 1 (main action): color curr, color next, active start, active neighb, active stack, save, stack push, stack pop, result
color curr colors vactive, color next colors vertex dag[vactive][childlist[vactive]], active start writes pstart to vactive, active neighb writes dag[vactive][childlist[vactive]] to vactive, active stack writes qstack(pstack) to vactive, save writes vactive to vsave, stack push pushes vactive to the top of the stack, stack pop writes a null value to the top of the stack, and result writes vactive to qresult(presult)",1
" the assumption of leading 0’s can be easily removed, at the cost of slightly increasing the size of the verification set",0
2,1
" the write operation has the following arguments:
arg 1 (object to write): env stack lo, env stack hi, phi, plo
env stack lo and env stack hi represent qstacklo(pstacklo) and qstackhi(pstackhi), respectively",1
" hence, if all subproblems in s are added correctly, we have proven that any member of f will be added correctly, thus eliminating an infinite family of inputs that need to be tested",1
"in this paper, we propose that the key abstraction of recursion is necessary for neural programs to generalize",0
" this guarantees that the step input sequences considered during verification contain all step input sequences which arise during execution of an unseen problem in f , leading to generalization to any problem in f ",1
" the write operation has the following arguments:
arg 1 (main action): color curr, color next, active start, active neighb, active stack, save, stack push, stack pop, result
color curr colors vactive, color next colors vertex dag[vactive][childlist[vactive]], active start writes pstart to vactive, active neighb writes dag[vactive][childlist[vactive]] to vactive, active stack writes qstack(pstack) to vactive, save writes vactive to vsave, stack push pushes vactive to the top of the stack, stack pop writes a null value to the top of the stack, and result writes vactive to qresult(presult)",1
 we created a program set that reflects the semantics of algorithm 2,1
" if it is too small, then execution of the program on some input might require evaluation of m(i) on some i /∈ v , and so the behavior ofm(i) might deviate from p (i)",0
3,0
", to change the value of vactive) none described below move move a pointer (e",0
" we made the assumption of equivalent lengths in order to parametrize the input format with respect to length, but this assumption can be removed as well",0
" qcolor(dag[vactive][childlist[vactive]]) refers to the color of the next child of vactive, used in the evaluation of the condition in the if branch in line 9 of algorithm 2",0
"
we would also like to point out that in this paper, we provide as an example one way to train a recursive neural program, by providing a certain training execution trace to the npi model",1
 figure 2 shows examples of traces for the different versions of bubble sort,1
g,1
" note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values, and arg 2 can only take one of 7 values",0
" finally, the boolean pstack == 1 is used to check whether the stack is empty in line 18 of algorithm 2",0
"
algorithm 2 depth first search topological sort 1: color all vertices white",0
" observe that partially recursive does slightly better than non-recursive for the setting in which the length of the array is 3, and that the fully recursive version is able to sort every array given to it",0
" thus, recursion helps decompose a problem and makes it easier to reason about a program’s behavior for previously unseen situations such as longer inputs",0
" when the program is called, the current context including the caller’s memory state is stored on a stack; when the program returns, the stored context is popped off the stack to resume execution in the previous caller’s context",1
this material is in part based upon work supported by the national science foundation under grant no,1
 ,0
" there are 3 pointers (presult, pstack, pstart), presult points to the next empty location in qresult,
pstack points to the top of the stack in qstack, and pstart points to the candidate starting node for a connected component",0
 table 3 presents results on randomly generated arrays of varying length for the learned non-recursive and recursive programs,0
" note that recursion helps make this process tractable, because we only need to test a finite number of inputs to show that the model will work correctly on inputs of unbounded complexity",1
" the add reduction rule ensures that each call to add only covers two adjacent columns, and so the lstm only ever runs for a fixed number of steps necessary to process these two columns",1
 ,0
" we hypothesize that the reason for this is that the neural network learns to spuriously depend on specific characteristics of the training examples that are irrelevant to the true program semantics, such as length of the training inputs, and thus fails to generalize to more complex inputs",0
" empirically, existing models suffer from a common limitation—generalization becomes poor beyond a threshold level of complexity",1
" the move operation has the following arguments:
arg 1 (pointer): presult, pstack, pstart, childlist[vactive], childlist[vsave]
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 5 values",1
 figure 2 shows examples of traces for the different versions of bubble sort,1
"when constructing a neural network for the purpose of learning a program, there are two orthogonal aspects to consider",1
" in all experiments, α is set to 0",1
 some functional programming languages go so far as not to define any looping constructs but rely solely on recursion to enable repeated execution of the same code,1
" the learned recursive program is different from neural programs learned in all previous work in an important aspect: previous approaches do not explicitly incorporate this abstraction, and hence generalize poorly, whereas our learned neural programs incorporate recursion and achieve perfect generalization",0
" consequently, it is infeasible to construct a verification set that covers all cases, and the verification procedure we have described is inadequate",0
" that is to say, the network does not learn the true program semantics",0
" while this matches the description from reed & de freitas (2016), we found that this causes an unnecessary blowup in the size of v and makes it much more difficult to construct the verification set",0
"
since npi naturally supports the notion of recursion, a key question is how to enable npi to learn recursive programs",0
" k is set to 11, to denote the range of possible numbers (0 through 9), along with the start/end token (represented with the same encoding) which is observed when a pointer reaches beyond the bounds of the input",0
" in this version, there is a dependence on length: bstep and lshift are called a number of times equivalent to one less than the length of the input array, in bubble and reset respectively",0
" the architecture usually possesses some form of memory, which could be internal (such as the hidden state of a recurrent neural network) or external (such as a discrete “scratch pad” or a memory block with differentiable access)",1
"
grade school addition",0
" both the non-recursive and recursive learned programs generalize on all graphs we tried, up to 120 vertices",0
" in this version, there is a dependence on length: bstep and lshift are called a number of times equivalent to one less than the length of the input array, in bubble and reset respectively",1
" using this modification, we constructed a verification set consisting of one array of size 10",0
" as a consequence, our learned neural programs empirically achieve perfect generalization from a very small number of training examples",0
" if yes, then this indicates that the learned neural program achieves provably perfect generalization",1
" provably perfect generalization implies the model will behave correctly, given any valid input",1
g,0
" observe that the non-recursive program’s correctness degrades for length 11 and beyond, while the recursive program can sort any given array",1
"
the operations we need to be concerned most about are carry and lshift, which induce partial environment states spanning two columns",1
 3: begin traversing from vertex 1 in the dag,0
"
after finding v , we construct a set of problem inputs which, when executed on p , create exactly the step input sequences which make up v ",1
g,1
" to the best of our knowledge, this is the first time verification has been applied to a neural program, providing provable guarantees about its behavior",1
 table 3 presents results on randomly generated arrays of varying length for the learned non-recursive and recursive programs,0
" twc-1409915, darpa under grant no",0
" the move operation has the following arguments:
arg 1 (pointer): presult, pstack, pstart, childlist[vactive], childlist[vsave]
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 5 values",0
we now report on generalization for the varying tasks,1
"
base cases and reduction rules for addition",0
g,0
"
the training data for the neural programmer-interpreter consists of full execution traces for the program of interest",0
"
the non-recursive trace loops on cycles of bubble and reset, which logically represents one bubble sweep through the array and reset of the two bubble pointers to the very beginning of the array, respectively",0
 the model described in the training setup of section 4 (trained on 6 traces) was verified to be correct via the matching procedure described in section 4,0
" 4: function toposort(dag) 5: while there is still a white vertex u: do 6: color[u] = grey 7: vactive = u 8: do 9: if vactive has a white child v then 10: color[v] = grey 11: push vactive onto s 12: vactive = v 13: else 14: color[vactive] = black 15: write vactive to result 16: if s is empty then pass 17: else pop the top vertex off s and set it to vactive 18: while s is not empty
topological sort",0
" we also hope to decrease supervision, for example by training with only partial or non-recursive traces, and to develop novel neural programming architectures integrated directly with a notion of recursion",1
" in particular, recursion can be implemented as a program calling itself",0
" lshift, used in reset, shifts the pointers left until reaching the start token",0
" naı̈vely, we might expect to synthesize and test an input for any sequence created by combining the four possible colors in two variables and another boolean variable for whether the stack is empty (so 32 possible observations at any point), but for various reasons, most of these combinations are impossible to occur at any given point in the execution trace",0
"
in this paper, we propose that recursion is an important concept for neural programs as well",0
2,0
" childlist ∈ nu is a vector of pointers, where childlist[i] points to the next child under consideration for vertex i",0
 we instantiated recursion for the neural programmerinterpreter by changing the training traces,0
" given verification sets that cover all the base cases and reduction rules, we can provide proofs that these learned programs generalize perfectly",0
"
note that the recursion for quicksort is not purely tail recursive and therefore represents a more complex kind of recursion that is harder to learn than in the previous tasks",0
g,0
" in particular, given that a recursion is defined by two properties as mentioned before, the base cases and the set of reduction rules, we can prove a recursive neural program generalizes perfectly if we can prove that (1) it performs correctly on the base cases; (2) it learns the reduction rules correctly",0
g,0
" partial recursion only adds a tail recursive call to bubblesort after bubble and reset, similar to the tail recursive call described previously for addition",1
 ,1
" the second is the training procedure, which consists of the form of the training data and the optimization process",0
" for many problems, the base cases and reduction rules usually consist of a finite (often small) number of cases",0
" the recursive version can generalize perfectly to long problems constructed from these components (such as the sum “822+233”, where “8+2” and “2+3” are in the training set), but the non-recursive version fails to sum these long problems properly",0
" lshift, used in reset, shifts the pointers left until reaching the start token",1
" 3: 4: function partition(a, lo, hi) 5: pivot = lo 6: for j ∈ [lo, hi− 1] : do 7: if a[j] ≤ a[hi] then 8: swap a[pivot] with a[j] 9: pivot = pivot+ 1 0: swap a[pivot] with a[hi] 11: return pivot 12: 13: function quicksort(a, lo, hi) 14: while slo and shi are not empty: do 15: pop states off slo and shi, writing them to lo and hi",1
"
thus far, to evaluate the efficacy of neural models on programming tasks, the only metric that has been used is generalization of expected behavior to inputs of greater complexity (vinyals   (2015), kaiser & sutskever (2015), reed & de freitas (2016), graves   (2016), zaremba   (2016))",0
 the sequence of primitive operations (move and write operations) for the non-recursive and recursive versions are exactly the same,1
"
we construct input problems by splitting into two cases: one case in which the left column contains a null value and another in which the left column does not contain any null values",0
" k is set to 11, to represent the range of 10 possible digits, along with a token representing the end of input",1
"
arg 2 (increment or decrement): up, down1 // top level topological sort call 2 toposort() { 3 while (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 } 0 11 traverse() { 12 check_child() 13 explore() 14 } 15 16 check_child() { 17 while (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 18 move(childlist[vactive], up) 19 } 20 21 explore() { 22 do 23 if (qcolor(dag[vactive][childlist[vactive]]) is white): 24 write(color_next, color_grey) 25 stack(push) 26 write(save) 27 write(active_neighb) 28 move(childlist[vsave], up) 29 else: 30 write(color_curr, color_black) 31 write(result) 32 move(presult, up) 33 if(pstack == 1): 34 break 35 else: 36 stack(pop) 37 check_child() 38 while (true) 39 } 40 41 stack(op) { 42 if (op == push): 43 write(stack_push) 44 move(pstack, up) 45 46 if (op == pop): 47 write(active_stack) 48 write(stack_pop) 49 move(pstack, down) 50 } 51 52 next_start() { 53 while(qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 54 move(pstart, up) 55 }altered recursive functions  // top level topological sort call 2 toposort() { 3 if (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 toposort() // recursive call 0 } 11 12 check_child() { 13 if (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 14 move(childlist[vactive], up) 15 check_child() // recursive call 16 } 17 18 explore() { 19 if (qcolor(dag[vactive][childlist[vactive]]) is white): 20 write(color_next, color_grey) 21 stack(push) 22 write(save) 23 write(active_neighb) 24 move(childlist[vsave], up) 25 else: 26 write(color_curr, color_black) 27 write(result) 28 move(presult, up) 29 if(pstack == 1): 30 return 31 else: 32 stack(pop) 33 check_child() 34 explore() // recursive call 35 } 36 37 next_start() { 38 if (qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 39 move(pstart, up) 40 next_start() // recursive call 41 }algorithm 4 iterative quicksort 1: initialize an array a to sort and two empty stacks slo and shi",1
"
formally, verification consists of proving the following theorem:
∀i ∈ v,m(i) ⇓ p (i) where i denotes a sequence of step inputs (within one function call), v denotes the set of valid sequences of step inputs, m denotes the neural network model, p denotes the correct program, and p (i) denotes the next step output from the correct program",1
"
an alternative method is to run p on many different program inputs and then observe step input sequences which occur, to create v ",0
"1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 while (pstacklo 6= 1): 6 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 7 stack(stack_pop) 8 else: 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 stack(stack_pop) 12 partition() 13 stack(stack_push_call2) 14 stack(stack_push_call1) 15 } 16 17 partition() { 18 set_pivot_lo() 19 set_j_lo() 20 compswap_loop() 21 swap(ppivot, phi) 22 set_j_null() 23 } 24 25 compswap_loop() { 26 while (pj 6= phi): 27 compswap() 28 move(pj, up) 29 } 30 31 compswap() { 32 if (a[pj ] ≤ a[phi]): 33 swap(ppivot, pj) 34 move(ppivot, up) 35 } 36 37 stack(op) { 38 if (op == stack_push_call1): 39 write(env_stack_lo, plo) 40 write(env_stack_hi, ppivot − 1) 41 move(pstacklo, up) 42 move(pstackhi, up) 43 44 if (op == stack_push_call2): 45 write(env_stack_lo, ppivot + 1) 46 write(env_stack_hi, phi) 47 move(pstacklo, up) 48 move(pstackhi, up) 49 50 if (op == stack_pop): 51 write(env_stack_lo, reset) 52 write(env_stack_hi, reset) 53 move(pstacklo, down) 54 move(pstackhi, down) 55 }altered recursive functions 1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 6 partition() 7 stack(stack_push_call2) 8 stack(stack_push_call1) 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 quicksort() // recursive call 12 stack(stack_pop) 13 write(phi, env_stack_hi_peek) 14 write(plo, env_stack_lo_peek) 15 quicksort() // recursive call 16 stack(stack_pop) 17 } 18 19 compswap_loop() { 20 if (pj 6= phi): 21 compswap() 22 move(pj, up) 23 compswap_loop() // recursive call 24 }in this section, we describe the space of base cases and reduction rules that must be covered for each of the four sample tasks, in order to create the verification set",0
"in this paper, we propose that the key abstraction of recursion is necessary for neural programs to generalize",1
" at each time step, the core controller can decide to select one of the programs to call with certain arguments",0
 b1b0 where no carry operations occur,1
"
we describe the details of the npi model relevant to our contributions",0
" that is to say, the network does not learn the true program semantics",0
"
it is sufficient to construct problems where every transition between two adjacent columns is covered",0
" although the model is exposed to more and more data, it might learn spurious and overly complex representations of the program, as suggested in zaremba   (2016)",1
" , a1a0 + b1b0} are added properly",0
"
since npi naturally supports the notion of recursion, a key question is how to enable npi to learn recursive programs",0
 the core controller acts as a dispatcher for the programs,0
 table 4 also demonstrates that the (verified) recursive program generalizes perfectly,1
" this makes it so that we only need to consider the vertices in the subgraph that are currently relevant for computing the sort, allowing simpler reasoning about behavior for large graphs",0
"
note that v should also contain the necessary reductions, which corresponds to making the recursive calls at the correct time, as indicated by p ",1
"
we describe the details of the npi model relevant to our contributions",1
" qcolor(pstart) contains the color of the current start vertex, used in the evaluation of the condition in the while loop in line 5 of algorithm 2",1
"
it is important to emphasize that at inference time in the npi, the hidden state of the lstm controller is reset (to zero) at each subprogram call, as in line 3 of algorithm 1 (h ← 0)",0
 recursion enables provably perfect generalization,0
" the architecture usually possesses some form of memory, which could be internal (such as the hidden state of a recurrent neural network) or external (such as a discrete “scratch pad” or a memory block with differentiable access)",1
" nevertheless, we would like the model to quickly learn to represent the right semantics of the underlying program from a small number of training examples, not an exhaustive number of them",0
"
thus far, to evaluate the efficacy of neural models on programming tasks, the only metric that has been used is generalization of expected behavior to inputs of greater complexity (vinyals   (2015), kaiser & sutskever (2015), reed & de freitas (2016), graves   (2016), zaremba   (2016))",1
" we prove every member of f is added properly, given that subproblems s = {anan−1 + bnbn−1, an−1an−2 + bn−1bn−2, ",0
" at any given time, the npi has access to the values referred to by two pointers, represented by q(1, i1) and q(1, i2),",1
 we instantiated recursion for the neural programmerinterpreter by changing the training traces,0
"
note that the recursion for quicksort is not purely tail recursive and therefore represents a more complex kind of recursion that is harder to learn than in the previous tasks",1
" naı̈vely, we might expect to synthesize and test an input for any sequence created by combining the four possible colors in two variables and another boolean variable for whether the stack is empty (so 32 possible observations at any point), but for various reasons, most of these combinations are impossible to occur at any given point in the execution trace",0
 errors arise due to undesirable and uninterpretable dependencies and associations the architecture learns to store in some high-dimensional hidden state,0
"
algorithm 3 recursive quicksort 1: initialize an array a to sort",1
we emphasize that the notion of a neural recursive program has not been presented in the literature before: this is our main contribution,0
" lshift moves the four pointers to the left, to move to the next column",1
" without using a recursive program, such a proof is not possible, because the non-recursive program runs on an arbitrarily long addition problem that creates correspondingly long sequences of step inputs; in the non-recursive formulation of addition, add calls add1 a number of times that is dependent on the length of the input",0
g,0
 this paper is the first (to our knowledge) to investigate the important problem of provable generalization properties of neural programs,1
" for each length, we test each program on 30 randomly generated problems",0
" at end, pointer ppivot is moved to the pivot compswap loop, move pivot lo, move j lo, swap
none
compswap loop runs the for loop inside the partition function compswap, move none compswap compares a[pivot] ≤ a[j]; if so, perform a swap and increment ppivot swap, move none set pivot lo sets ppivot to lo index none none set j lo sets pj to lo index none none set j null sets pj to −∞ none none stack pushes lo/hi states
onto stacks slo and shi according to argument (described below)
write, move described below
move moves pointer one unit up or down none described below swap swaps elements at given array indices none described below write write a value either to stack (e",0
"we show that if we incorporate recursion, the learned npi programs can achieve provably perfect generalization for different tasks",1
" provably perfect generalization implies the model will behave correctly, given any valid input",0
" each time a subprogram is called, the stack depth increases",1
" , n , where the dag contains n vertices",1
"
arg 2 (increment or decrement): up, down
swap",1
" the move operation has the following arguments:
arg 1 (pointer): presult, pstack, pstart, childlist[vactive], childlist[vsave]
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 5 values",0
" , n , where the dag contains n vertices",1
"
in addition, none of the current approaches to neural programming provide a method or even aim to enable provable guarantees about generalization",0
" for these programs, we re-use the appropriate program sets (the associated subprograms), and we refer the reader to the appendix of reed & de freitas (2016) for further details on the subprograms used in addition and bubble sort",0
 our result shows that the training procedure and the npi architecture is capable of generalizing from the step input-output pairs seen in the training data to the unseen ones present in the verification set,0
g,1
"
the training set for topological sort contains 6 traces, with one synthesized from a graph of size 5 and the rest synthesized from graphs of size 7",0
" we also hope to decrease supervision, for example by training with only partial or non-recursive traces, and to develop novel neural programming architectures integrated directly with a notion of recursion",0
" as before, the non-recursive learned program lacks a provable guarantee of generalization, whereas we show later that the recursive learned program has one",1
" our results show that for all learned, to-be-verified neural programs, they all produced the same traces as those produced by p on the verification set",0
" both the non-recursive and recursive learned programs generalize on all graphs we tried, up to 120 vertices",0
 pstacklo and pstackhi point to the top (empty) positions in qstacklo and qstackhi,1
" the contents ofqresult andqstack are not exposed directly through the domain-specific encoder; rather, we define primitive functions which manipulate these scratch-pads",0
", an lstm in npi’s case, but possibly other networks in different cases",0
" this makes it so that we only need to consider the vertices in the subgraph that are currently relevant for computing the sort, allowing simpler reasoning about behavior for large graphs",0
" at each time step, the core controller can decide to select one of the programs to call with certain arguments",1
"
in what follows, we describe the way in which we constructed npi training traces so as to make them contain recursive elements and thus enable npi to learn recursive programs",0
" furthermore, given a verification input set that covers all base cases and reduction rules, we can formally prove that the learned neural programs achieve perfect generalization after verifying its behavior on the verification input set",1
 ,1
"
training setup",1
"
we would also like to point out that in this paper, we provide as an example one way to train a recursive neural program, by providing a certain training execution trace to the npi model",1
 this is due to the properties of the training set – we trained on 2 traces synthesized from arrays of length 7 and 1 trace synthesized from an array of length 6,0
"
base cases and reduction rules for bubble sort",0
 ,1
 this is due to the properties of the training set – we trained on 2 traces synthesized from arrays of length 7 and 1 trace synthesized from an array of length 6,0
" on the other hand, the recursive programs have learned the true program semantics",1
" this functionality the original npi paper decodes to a program key embedding kt ∈ rk and then computes a program embedding pt+1, which we also did in our implementation, but we omit this for brevity",0
" in future work, we would like to decrease supervision and construct models that are capable of coming up with recursive abstractions themselves",1
 this verification phase only needs to be performed once after training,1
"
we construct input problems by splitting into two cases: one case in which the left column contains a null value and another in which the left column does not contain any null values",1
" for example, for the addition task, the model is trained on short inputs and then tested on its ability to sum inputs with much longer numbers of digits",1
"
the same set of problems was used to generate the training traces for all formulations of the task, for non-recursive and recursive versions",0
"
we describe the details of the npi model relevant to our contributions",0
"
arg 2 (object to copy): env stack lo peek, env stack hi peek, phi, plo, ppivot − 1, ppivot + 1, reset
env stack lo peek and env stack hi peek represent qstacklo(pstacklo − 1) and qstackhi(pstackhi − 1), respectively",0
", qstacklo or qstackhi) or to pointer (e",0
" here as a concrete and general example, we consider a general neural programming architecture (npa), similar to neural programmer-interpreter (npi) in reed & de freitas (2016)",0
" in this version, there is a dependence on length: bstep and lshift are called a number of times equivalent to one less than the length of the input array, in bubble and reset respectively",1
" for many problems, the base cases and reduction rules usually consist of a finite (often small) number of cases",0
"1, we hypothesize the non-recursive programs do not generalize well because they have learned spurious dependencies specific to the training set, such as length of the input problems",1
" , n , where the dag contains n vertices",1
" the second is the training procedure, which consists of the form of the training data and the optimization process",0
" note that recursion helps make this process tractable, because we only need to test a finite number of inputs to show that the model will work correctly on inputs of unbounded complexity",0
" the assumption of leading 0’s can be easily removed, at the cost of slightly increasing the size of the verification set",1
" qcolor(dag[vactive][childlist[vactive]]) refers to the color of the next child of vactive, used in the evaluation of the condition in the if branch in line 9 of algorithm 2",1
", the trace corresponding to the problem “109 + 101”)",1
"
an alternative way of representing the environment slice is to expose the values of the absolute vertices to the model; however, this makes it difficult to scale the model to larger graphs, since large vertex values are not seen during training time",1
 this makes it difficult to reason about what the model will do when given complex inputs,1
 2: initialize an empty stack s and a directed acyclic graph dag to traverse,1
 ,0
" while this matches the description from reed & de freitas (2016), we found that this causes an unnecessary blowup in the size of v and makes it much more difficult to construct the verification set",0
"
one common strategy to improve generalization is to use curriculum learning, where the model is trained on inputs of gradually increasing complexity",0
"
arg 2 (auxiliary variable): color grey, color black
color grey and color black color the given vertex grey and black, respectively",0
" in future work, we seek to enable more tasks with recursive structure",0
" for these models, table 2 presents results on randomly generated dags of varying graph sizes (varying in the number of vertices)",0
 ,0
"
through careful reasoning about the possible set of environment observations created by all valid inputs, and how each of the operations in the execution trace affects the environment, we can construct v using the procedure described in section 3",0
" qcolor(pstart) contains the color of the current start vertex, used in the evaluation of the condition in the while loop in line 5 of algorithm 2",1
", the trace corresponding to the array [3,2])",1
3,1
" however, to be sure that the generated v is complete (covers all the cases needed), we need to check all pairs of observations seen in adjacent step inputs (in particular, those before and after a primitive function call), in a similar way as if we were constructing v from scratch",1
" each step during an execution of the program does one of three things: (1) another subprogram along with associated arguments is called, as in line 10, (2) the program writes to the environment if it is primitive, as in line 8, or (3) the loop is terminated if the return probability exceeds a threshold α, after which the stack frame is popped and control is returned to the caller",0
" 3: 4: function quicksort(a, lo, hi) 5: if lo < hi: then 6: p = partition(a, lo, hi) 7: quicksort(a, lo, p− 1) 8: quicksort(a, p+ 1, hi) 9: 10: function partition(a, lo, hi) 11: pivot = lo 12: for j ∈ [lo, hi− 1] : do 13: if a[j] ≤ a[hi] then 14: swap a[pivot] with a[j] 15: pivot = pivot+ 1 16: swap a[pivot] with a[hi] 17: return pivot
for quicksort, the domain-specific encoder is
fenc(qarray, qstacklo, qstackhi, plo, phi, pstacklo, pstackhi, ppivot, pj , at) =
mlp ([qarray(pj) ≤ qarray(phi), pj == phi, qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1), pstacklo == 1, at(1), at(2), at(3)]),
whereqarray ∈ ru×11 is a scratch-pad that contains u rows, each containing one of 11 values (one of the numbers 0 through 9 or an invalid state)",1
" finally, the boolean pstack == 1 is used to check whether the stack is empty in line 18 of algorithm 2",0
" the theorem states that for the same sequence of step inputs, the model produces the exact same step output as the target program it aims to learn",1
" we made the assumption of equivalent lengths in order to parametrize the input format with respect to length, but this assumption can be removed as well",0
"
base cases and reduction rules for addition",0
" the only model, to our knowledge, that does not train on input-output pairs is the neural programmer-interpreter (reed & de freitas, 2016), which trains on synthetic execution traces",1
"
the non-recursive trace loops on cycles of add1 and lshift",1
"
an alternative way of representing the environment slice is to expose the values of the absolute vertices to the model; however, this makes it difficult to scale the model to larger graphs, since large vertex values are not seen during training time",0
"
algorithm 2 shows the topological sort task of interest",0
"
bubble sort",1
g,0
" the single-digit multiplication task in zaremba   (2016), the bubble sort task in reed & de freitas (2016), and the graph tasks in graves   (2016))",0
" in practice, we can additionally use tail recursion optimization to avoid problems with the call stack growing too deep",1
" childlist ∈ nu is a vector of pointers, where childlist[i] points to the next child under consideration for vertex i",0
"
in this general neural programming architecture, we show it is easy to support recursion",0
" childlist ∈ nu is a vector of pointers, where childlist[i] points to the next child under consideration for vertex i",0
"
note that the recursion for quicksort is not purely tail recursive and therefore represents a more complex kind of recursion that is harder to learn than in the previous tasks",0
 u varies with the number of vertices in the graph,1
" at each time step, the core controller can decide to select one of the programs to call with certain arguments",1
" on the other hand, the non-recursive version has low accuracy, beginning from size 5, and fails completely for graphs of size 8 and beyond",1
"
the non-recursive trace loops on cycles of add1 and lshift",0
" we note that an earlier version of quicksort we tried lacked primitive operations to directly move a pointer to another, and therefore needed more functions and observations",1
" for each length, we test each program on 30 randomly generated problems",0
" naı̈vely, we might expect to synthesize and test an input for any sequence created by combining the four possible colors in two variables and another boolean variable for whether the stack is empty (so 32 possible observations at any point), but for various reasons, most of these combinations are impossible to occur at any given point in the execution trace",0
" 3: 4: function partition(a, lo, hi) 5: pivot = lo 6: for j ∈ [lo, hi− 1] : do 7: if a[j] ≤ a[hi] then 8: swap a[pivot] with a[j] 9: pivot = pivot+ 1 0: swap a[pivot] with a[hi] 11: return pivot 12: 13: function quicksort(a, lo, hi) 14: while slo and shi are not empty: do 15: pop states off slo and shi, writing them to lo and hi",0
" in future work, we seek to enable more tasks with recursive structure",0
" we then construct problem configurations that span all possible valid environment states (for instance, in order to force the carry bit in a column to be 1, one can add the sum “1+9” in the column to the right)",0
" the contents ofqresult andqstack are not exposed directly through the domain-specific encoder; rather, we define primitive functions which manipulate these scratch-pads",0
"
we experiment with two levels of recursion—partial and full",1
"
base cases and reduction rules for quicksort",1
"
given a verification set, we can then run the model on the verification set to check if the produced traces and results are correct",0
" even though the earlier version also generalized just as well in practice, relatively small differences in the formulation of the traces and the environment observations can drastically change the difficulty of verification",1
"
the dag is represented as an adjacency list where dag[i][j] refers to the j-th child of vertex i",0
" in particular, we construct new training traces which explicitly contain recursive elements and show that with this type of trace, npi easily learns recursive programs",0
" the non-recursive and partially recursive versions are unable to sort long arrays, beyond length 8",0
"
as in line 13 of the right-hand side of figure 1",0
" this is highly undesirable, since being able to provide the correct answer in all possible settings is one of the most important aspects of any learned neural program",0
" in the non-recursive trace, there are four functions that can be made recursive—toposort, check child, explore, and next start, and we add a tail recursive call to each of these functions in order to make the recursive trace",1
" observe that partially recursive does slightly better than non-recursive for the setting in which the length of the array is 3, and that the fully recursive version is able to sort every array given to it",0
"
to evaluate a neural network that learns a neural program to accomplish a certain task, one common evaluation metric is how well the learned model m generalizes",0
"
is critical for implementing recursion, since it permits us to restrict our attention to the currently relevant recursive call, ignoring irrelevant details about other contexts",0
 we leave this as future work to devise a verification procedure more appropriate to this setting,1
 we leave this as future work to devise a verification procedure more appropriate to this setting,0
"
note that the training set can often be considerably smaller than the verification set, and despite this, the learned model can still pass the entire verification set",0
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,0
2,1
" finally, the boolean pstack == 1 is used to check whether the stack is empty in line 18 of algorithm 2",0
" the second is the training procedure, which consists of the form of the training data and the optimization process",1
 ,1
 the environment and return probability are omitted for readability,0
" we implement a quicksort task, in order to demonstrate that recursion helps with learning divide-and-conquer algorithms",0
"
we describe the base cases, reduction rules, and the verification set for each task in appendix a",0
"
since npi naturally supports the notion of recursion, a key question is how to enable npi to learn recursive programs",0
" as with the original npi architecture, the experiments for this paper always used a 3-tuple of integers at = (at(1), at(2), at(3))",0
"
grade school addition",1
" for each graph size, we test the learned programs on 30 randomly generated dags",1
 there is a (changing) list of neural programs used to accomplish a given task,1
6,1
"
inside bubble and reset, there are two operations that can be made recursive",0
"
the non-recursive trace loops on cycles of add1 and lshift",1
" our results show that for all learned, to-be-verified neural programs, they all produced the same traces as those produced by p on the verification set",0
"as there is no public implementation of npi, we implemented a version of it in keras that is as faithful to the paper as possible",0
"
move",1
" k is set to 11, to denote the range of possible numbers (0 through 9), along with the start/end token (represented with the same encoding) which is observed when a pointer reaches beyond the bounds of the input",1
 this paper is the first (to our knowledge) to investigate the important problem of provable generalization properties of neural programs,0
"
we note that for tasks with very large input domains, such as ones involving mnist digits or speech samples, the state space of base cases and reduction rules could be prohibitively large, possibly infinite",1
" that is to say, the network does not learn the true program semantics",1
"
thus far, to evaluate the efficacy of neural models on programming tasks, the only metric that has been used is generalization of expected behavior to inputs of greater complexity (vinyals   (2015), kaiser & sutskever (2015), reed & de freitas (2016), graves   (2016), zaremba   (2016))",0
" the space of possible programs is extremely large, and composing a program that performs robustly on the infinite space of possible inputs is difficult—in part because it is impractical to obtain enough training examples to easily disambiguate amongst all possible programs",0
" for each length, we test each program on 30 randomly generated problems",0
"we describe how models trained with recursive traces can be proven to generalize, by using the verification procedure described in section 3",0
", to change the value of vactive) none described below move move a pointer (e",0
" consequently, it is infeasible to construct a verification set that covers all cases, and the verification procedure we have described is inadequate",0
" as an application, we incorporate recursion into the neural programmer-interpreter architecture and consider four sample tasks: grade-school addition, bubble sort, topological sort, and quicksort",0
we emphasize the overall goal of this work is to enable the learning of a recursive program,0
" finally, the boolean pstack == 1 is used to check whether the stack is empty in line 18 of algorithm 2",0
"
after finding v , we construct a set of problem inputs which, when executed on p , create exactly the step input sequences which make up v ",0
 we use the lomuto partition scheme; the logic for the recursive trace is shown in algorithm 3,0
"
arg 2 (auxiliary variable): color grey, color black
color grey and color black color the given vertex grey and black, respectively",1
"
under the assumption that there are no leading 0’s (except in the case of single digits) and the two numbers to be added have the same number of digits, the verification set for addition contains 20,181 input problems",1
 ,1
"
an alternative way of representing the environment slice is to expose the values of the absolute vertices to the model; however, this makes it difficult to scale the model to larger graphs, since large vertex values are not seen during training time",1
 errors arise due to undesirable and uninterpretable dependencies and associations the architecture learns to store in some high-dimensional hidden state,1
", to color a vertex) or variable (e",0
" if a white vertex is found, set pstart to point to it; this signifies the start of a traversal of a new connected component",0
 the memory updates of these neural programs are so complex and interdependent that it is difficult to reason about the behaviors of the learned neural program under previously unseen situations (such as problems with longer inputs),1
" the theorem states that for the same sequence of step inputs, the model produces the exact same step output as the target program it aims to learn",1
" 2: initialize lo and hi to be 1 and n, where n is the length of a",0
"
arg 2 (increment or decrement): up, down1 // top level topological sort call 2 toposort() { 3 while (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 } 0 11 traverse() { 12 check_child() 13 explore() 14 } 15 16 check_child() { 17 while (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 18 move(childlist[vactive], up) 19 } 20 21 explore() { 22 do 23 if (qcolor(dag[vactive][childlist[vactive]]) is white): 24 write(color_next, color_grey) 25 stack(push) 26 write(save) 27 write(active_neighb) 28 move(childlist[vsave], up) 29 else: 30 write(color_curr, color_black) 31 write(result) 32 move(presult, up) 33 if(pstack == 1): 34 break 35 else: 36 stack(pop) 37 check_child() 38 while (true) 39 } 40 41 stack(op) { 42 if (op == push): 43 write(stack_push) 44 move(pstack, up) 45 46 if (op == pop): 47 write(active_stack) 48 write(stack_pop) 49 move(pstack, down) 50 } 51 52 next_start() { 53 while(qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 54 move(pstart, up) 55 }altered recursive functions  // top level topological sort call 2 toposort() { 3 if (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 toposort() // recursive call 0 } 11 12 check_child() { 13 if (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 14 move(childlist[vactive], up) 15 check_child() // recursive call 16 } 17 18 explore() { 19 if (qcolor(dag[vactive][childlist[vactive]]) is white): 20 write(color_next, color_grey) 21 stack(push) 22 write(save) 23 write(active_neighb) 24 move(childlist[vsave], up) 25 else: 26 write(color_curr, color_black) 27 write(result) 28 move(presult, up) 29 if(pstack == 1): 30 return 31 else: 32 stack(pop) 33 check_child() 34 explore() // recursive call 35 } 36 37 next_start() { 38 if (qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 39 move(pstart, up) 40 next_start() // recursive call 41 }algorithm 4 iterative quicksort 1: initialize an array a to sort and two empty stacks slo and shi",0
g,0
" for purposes of verification, we replace the domain-specific encoder with the following:
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1) ≤ q(1, i2), 1 ≤ i1 ≤ length, 1 ≤ i2 ≤ length, i3 == length, at(1), at(2), at(3)]),which directly exposes which of the two values pointed to is larger",1
"
training setup",0
"
the training set for quicksort contains 4 traces, synthesized from arrays of length 5",1
" given verification sets that cover all the base cases and reduction rules, we can provide proofs that these learned programs generalize perfectly",0
" for non-primitive subprograms, we can update the observation-to-observation mapping currently associated with the subprogram and then apply that mapping to the current set",0
 this is due to the properties of the training set – we trained on 2 traces synthesized from arrays of length 7 and 1 trace synthesized from an array of length 6,1
" qcolor(pstart) contains the color of the current start vertex, used in the evaluation of the condition in the while loop in line 5 of algorithm 2",0
"we show that if we incorporate recursion, the learned npi programs can achieve provably perfect generalization for different tasks",0
 table 4 also demonstrates that the (verified) recursive program generalizes perfectly,1
 we describe the recursive re-formulation of traces for two tasks from the original npi paper—grade-school addition and bubble sort,0
" if it is too large, then the semantics of p might not be well-defined on some elements in v , or the spurious step input sequences may not be reachable from any valid problem input (e",0
" at end, pointer ppivot is moved to the pivot compswap loop, move pivot lo, move j lo, swap
none
compswap loop runs the for loop inside the partition function compswap, move none compswap compares a[pivot] ≤ a[j]; if so, perform a swap and increment ppivot swap, move none set pivot lo sets ppivot to lo index none none set j lo sets pj to lo index none none set j null sets pj to −∞ none none stack pushes lo/hi states
onto stacks slo and shi according to argument (described below)
write, move described below
move moves pointer one unit up or down none described below swap swaps elements at given array indices none described below write write a value either to stack (e",1
"
the non-recursive trace loops on cycles of add1 and lshift",1
" the space of possible programs is extremely large, and composing a program that performs robustly on the infinite space of possible inputs is difficult—in part because it is impractical to obtain enough training examples to easily disambiguate amongst all possible programs",0
"
arg 2 (object to copy): env stack lo peek, env stack hi peek, phi, plo, ppivot − 1, ppivot + 1, reset
env stack lo peek and env stack hi peek represent qstacklo(pstacklo − 1) and qstackhi(pstackhi − 1), respectively",1
" the space of possible programs is extremely large, and composing a program that performs robustly on the infinite space of possible inputs is difficult—in part because it is impractical to obtain enough training examples to easily disambiguate amongst all possible programs",0
", pstart or childlist[vactive]) up or down none described belowwrite",0
" if it is too large, then the semantics of p might not be well-defined on some elements in v , or the spurious step input sequences may not be reachable from any valid problem input (e",1
"
by reasoning about the possible set of environment observations created by all valid inputs, we construct v using the procedure described in section 3",0
" because the context of the caller is stored on a stack when it calls another program and the callee starts in a fresh context, this enables recursion simply by allowing a program to call itself",1
" note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values, and arg 2 can only take one of 7 values",1
" in order to compensate for this, ppivot and pj require special functions (move pivot lo and move j lo) to properly set them to lo in lines 11 and 12 of the partition function in algorithm 3",0
" et+1 ∼ fenv(et, pt, at)",1
"
to construct this set, by using the reference implementation of each subprogram, we construct a mapping between two sets of environment observations: the first set consists of all observations that can occur at the beginning of a particular subprogram’s invocation, and the second set contains the observations at the end of that subprogram",0
 recursion enables provable guarantees on neural programs’ behavior without needing to exhaustively enumerate all possible inputs to the programs,0
" on the other hand, the recursive programs have learned the true program semantics",0
"
grade school addition",1
 this makes it difficult to reason about what the model will do when given complex inputs,0
" this is highly undesirable, since being able to provide the correct answer in all possible settings is one of the most important aspects of any learned neural program",0
 ,1
" however, our concept of recursion for neural programs is general",0
"
in order to demonstrate that recursion can help learn and generalize better, for addition, we trained only on traces for 5 arbitrarily chosen 1-digit addition sum examples",1
"
for addition, we analytically determine the verification set",1
" however, to be sure that the generated v is complete (covers all the cases needed), we need to check all pairs of observations seen in adjacent step inputs (in particular, those before and after a primitive function call), in a similar way as if we were constructing v from scratch",0
 the first is the actual model architecture,0
" twc-1409915, darpa under grant no",1
" the learned recursive program is different from neural programs learned in all previous work in an important aspect: previous approaches do not explicitly incorporate this abstraction, and hence generalize poorly, whereas our learned neural programs incorporate recursion and achieve perfect generalization",0
" in practice, we can additionally use tail recursion optimization to avoid problems with the call stack growing too deep",1
 our result shows that the training procedure and the npi architecture is capable of generalizing from the step input-output pairs seen in the training data to the unseen ones present in the verification set,0
" the write operation has the following arguments:
arg 1 (main action): color curr, color next, active start, active neighb, active stack, save, stack push, stack pop, result
color curr colors vactive, color next colors vertex dag[vactive][childlist[vactive]], active start writes pstart to vactive, active neighb writes dag[vactive][childlist[vactive]] to vactive, active stack writes qstack(pstack) to vactive, save writes vactive to vsave, stack push pushes vactive to the top of the stack, stack pop writes a null value to the top of the stack, and result writes vactive to qresult(presult)",0
3,0
" the general notion of recursion has been an important concept in many domains, including mathematics and computer science",1
"
grade-school addition",1
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,0
"
the training set for bubble sort contains 100 traces, with maximum problem length of 2 (e",1
"
figure 1 shows examples of non-recursive and recursive addition traces",0
" k is set to 11, to denote the range of possible numbers (0 through 9), along with the start/end token (represented with the same encoding) which is observed when a pointer reaches beyond the bounds of the input",1
"we train using the adam optimizer and use a 2-layer lstm and task-specific state encoders for the external environments, as described in reed & de freitas (2016)",0
" if it is too large, then the semantics of p might not be well-defined on some elements in v , or the spurious step input sequences may not be reachable from any valid problem input (e",0
"
base cases and reduction rules for bubble sort",0
 we found that changing the npi training traces is a simple way to enable this,0
" we show that by modifying the training procedure, we enable the npi model to learn recursive neural programs",1
 we use the lomuto partition scheme; the logic for the recursive trace is shown in algorithm 3,1
"
base cases and reduction rules for addition",0
"program descriptions calls arguments toposort perform topological
sort on graph traverse, next start, write, move
none
traverse traverse graph until stack is empty check child, explore none check child check if a white child exists; if so, set childlist[vactive] to point to it move none explore repeatedly traverse subgraphs until stack is empty stack, check child, write, move none stack interact with stack, either pushing or popping write, move push, pop next start move pstart until reaching a white vertex",0
 2: initialize an empty stack s and a directed acyclic graph dag to traverse,1
", to color a vertex) or variable (e",0
" the only model, to our knowledge, that does not train on input-output pairs is the neural programmer-interpreter (reed & de freitas, 2016), which trains on synthetic execution traces",1
" for instance, for recursive addition, consider the family f of addition problems anan−1 ",0
"
we would also like to point out that in this paper, we provide as an example one way to train a recursive neural program, by providing a certain training execution trace to the npi model",0
 reset represents a −∞ value,0
" the write operation has the following arguments:
arg 1 (object to write): env stack lo, env stack hi, phi, plo
env stack lo and env stack hi represent qstacklo(pstacklo) and qstackhi(pstackhi), respectively",0
" because the context of the caller is stored on a stack when it calls another program and the callee starts in a fresh context, this enables recursion simply by allowing a program to call itself",0
" consequently, there is no concept of length relevant to the problem, which has traditionally been an important focus of length-based curriculum learning",1
", the trace corresponding to the array [3,2])",1
"
training setup",1
"
for addition, we analytically determine the verification set",0
" lshift moves the four pointers to the left, to move to the next column",1
" finally, the boolean pstack == 1 is used to check whether the stack is empty in line 18 of algorithm 2",0
" we further have qresult ∈ nu , a scratch-pad which contains the sorted list of vertices at the end of execution, and qstack ∈ nu , which serves the role of the stack s in algorithm 2",0
"
quicksort",1
" we show that by modifying the training procedure, we enable the npi model to learn recursive neural programs",1
"
in this general neural programming architecture, we show it is easy to support recursion",1
"
grade-school addition",1
"we describe how models trained with recursive traces can be proven to generalize, by using the verification procedure described in section 3",1
" lshift, used in reset, shifts the pointers left until reaching the start token",0
" for these models, table 2 presents results on randomly generated dags of varying graph sizes (varying in the number of vertices)",0
" partial recursion only adds a tail recursive call to bubblesort after bubble and reset, similar to the tail recursive call described previously for addition",0
 this is due to the properties of the training set – we trained on 2 traces synthesized from arrays of length 7 and 1 trace synthesized from an array of length 6,0
 the first is the actual model architecture,1
" each step during an execution of the program does one of three things: (1) another subprogram along with associated arguments is called, as in line 10, (2) the program writes to the environment if it is primitive, as in line 8, or (3) the loop is terminated if the return probability exceeds a threshold α, after which the stack frame is popped and control is returned to the caller",0
 the memory updates of these neural programs are so complex and interdependent that it is difficult to reason about the behaviors of the learned neural program under previously unseen situations (such as problems with longer inputs),0
"
an alternative way of representing the environment slice is to expose the values of the absolute vertices to the model; however, this makes it difficult to scale the model to larger graphs, since large vertex values are not seen during training time",1
" for these models, table 2 presents results on randomly generated dags of varying graph sizes (varying in the number of vertices)",0
" also the dag can have potentially more than one connected component, meaning it is necessary to transition between these components appropriately",0
" on the other hand, the non-recursive version has low accuracy, beginning from size 5, and fails completely for graphs of size 8 and beyond",0
" for brevity, we refer the reader to the appendix for further details on the program set and non-recursive and recursive trace-generating functions used for topological sort",0
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,1
" for bubble sort, the domain-specific encoder is
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1), q(1, i2), i3 == length, at(1), at(2), at(3)]),
where the environment q ∈ r1×n×k is a scratch-pad that contains 1 row, to represent the state of the array as sorting proceeds in-place, andn columns",1
" however, note that there is no provable guarantee that the non-recursive learned program will generalize to all inputs, whereas we show later that the recursive learned program has a provable guarantee of perfect generalization",0
"
after finding v , we construct a set of problem inputs which, when executed on p , create exactly the step input sequences which make up v ",0
" in contrast, each call to the recursive add always runs for a fixed number of steps, even on arbitrarily long problems in f , so we can test that it performs correctly on a small, fixed number of step input sequences",0
" note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values, and arg 2 can only take one of 7 values",1
"
in this general neural programming architecture, we show it is easy to support recursion",1
"
the training set for bubble sort contains 100 traces, with maximum problem length of 2 (e",1
" for each task, given the verification set, we check the traces and results of the learned, to-be-verified neural program (described in section 4",1
 we instantiated recursion for the neural programmerinterpreter by changing the training traces,1
" the architecture usually possesses some form of memory, which could be internal (such as the hidden state of a recurrent neural network) or external (such as a discrete “scratch pad” or a memory block with differentiable access)",0
" qcolor(dag[vactive][childlist[vactive]]) refers to the color of the next child of vactive, used in the evaluation of the condition in the if branch in line 9 of algorithm 2",0
" on the other hand, the recursive programs have learned the true program semantics",1
" in this architecture, we consider a core controller, e",1
 18: push lo and p− 1 to slo and shi,0
" each time a subprogram is called, the stack depth increases",0
"
directed acyclic graphs are structurally more diverse than inputs in the two tasks of grade-school addition and bubble sort",1
"
algorithm 3 recursive quicksort 1: initialize an array a to sort",1
3,0
" in our experiments, we only present dag’s as inputs and represent the vertices as values ranging from 1, ",0
"
we would also like to point out that in this paper, we provide as an example one way to train a recursive neural program, by providing a certain training execution trace to the npi model",0
", qstacklo or qstackhi) or to pointer (e",1
" as described in the verification procedure, it is possible to prove our learned recursive program generalizes perfectly by testing on an appropriate set of problem inputs, i",0
", the trace corresponding to the array [3,2])",1
" for tasks other than addition, it is difficult to analytically determine the verification set, so instead, we randomly generate input candidates until they completely cover the base cases and reduction rules",0
"
arg 2 (object to copy): env stack lo peek, env stack hi peek, phi, plo, ppivot − 1, ppivot + 1, reset
env stack lo peek and env stack hi peek represent qstacklo(pstacklo − 1) and qstackhi(pstackhi − 1), respectively",0
5,1
 figure 2 shows examples of traces for the different versions of bubble sort,0
" by iterating with this procedure, and then running p on the input observation set that we obtain for the entry point function, we can obtain v precisely",0
" bstep, used in bubble, compares pairs of numbers, continuously moving the bubble pointers once to the right each time until reaching the end of the array",0
"
quicksort",1
"
in this paper, we propose to resolve these issues by explicitly incorporating recursion into neural architectures",1
 ,0
g,1
" in particular, in the explore function, adding a tail recursive call resets and stores the hidden states associated with vertices in a stack-like fashion",0
 the degree for any vertex in the dag is variable,1
" given a precise definition of p , it may be possible to automate the generation of v from p in future work",0
"
in order to demonstrate that recursion can help learn and generalize better, for addition, we trained only on traces for 5 arbitrarily chosen 1-digit addition sum examples",0
" on the other hand, the recursive programs have learned the true program semantics",0
 this makes it difficult to reason about what the model will do when given complex inputs,0
 this algorithm is a variant of depth first search,1
 a1a0 + bnbn−1 ,1
" if it is too large, then the semantics of p might not be well-defined on some elements in v , or the spurious step input sequences may not be reachable from any valid problem input (e",0
"program descriptions calls arguments quicksort run the quicksort
routine in place for the array a, for indices from lo to hi non-recursive: partition, stack, write recursive: same as non-recursive version, along with quicksort implicitly: array a to sort, lo, hi
partition runs the partition function",1
 the original version of the bubblesort implementation exposes the values within the array,0
" the assumption of leading 0’s can be easily removed, at the cost of slightly increasing the size of the verification set",1
" in this version of curriculum learning, even though the inputs are gradually becoming more complex, the semantics of the program is succinct and does not change",1
" in particular, recursion can be implemented as a program calling itself",0
" empirically, we observe that the learned recursive programs solve all valid inputs with 100% accuracy after training on a very small number of examples, out-performing previous generalization results",1
" we make the trace recursive by adding a tail recursive call into the trace for the add program after calling add1 and lshift, the original paper uses k = 10, but we found it necessary to augment the range with an end token, in order to terminate properly",1
2,0
" however, to be sure that the generated v is complete (covers all the cases needed), we need to check all pairs of observations seen in adjacent step inputs (in particular, those before and after a primitive function call), in a similar way as if we were constructing v from scratch",1
g,1
we emphasize that the notion of a neural recursive program has not been presented in the literature before: this is our main contribution,0
"
algorithm 2 depth first search topological sort 1: color all vertices white",0
" the second is the training procedure, which consists of the form of the training data and the optimization process",1
" in future work, we seek to enable more tasks with recursive structure",0
" in this section, we give a brief review of the npi architecture from reed & de freitas (2016) as background",1
" in our experiments, we only present dag’s as inputs and represent the vertices as values ranging from 1, ",1
"
by nature, recursion reduces the complexity of a problem to simpler instances",1
" for brevity, we refer the reader to the appendix for information about the program set and non-recursive and recursive trace-generating functions for quicksort",0
" for each function we use to implement the recursive version of topological sort, we need to consider the set of possible environment observation sequences we can create from all valid inputs and test that the learned program produces the correct behavior on each of these inputs",0
"
base cases and reduction rules for quicksort",0
" note that recursion helps make this process tractable, because we only need to test a finite number of inputs to show that the model will work correctly on inputs of unbounded complexity",1
 add1 is a subprogram that adds the current column (writing the appropriate digit to the output row and carrying a bit to the next column if needed),0
"
we also report on generalization results for the non-recursive and recursive versions of this variant of bubble sort",0
" the space of possible programs is extremely large, and composing a program that performs robustly on the infinite space of possible inputs is difficult—in part because it is impractical to obtain enough training examples to easily disambiguate amongst all possible programs",0
"
quicksort",0
" for bubble sort, the domain-specific encoder is
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1), q(1, i2), i3 == length, at(1), at(2), at(3)]),
where the environment q ∈ r1×n×k is a scratch-pad that contains 1 row, to represent the state of the array as sorting proceeds in-place, andn columns",0
"
we would also like to point out that in this paper, we provide as an example one way to train a recursive neural program, by providing a certain training execution trace to the npi model",0
" for the recursive formulation of addition, we analytically construct the set of input problems that cover all base cases and reduction rules",0
" it is straightforward to deal with all other operations, which do not induce partial environment states",0
" hence, if all subproblems in s are added correctly, we have proven that any member of f will be added correctly, thus eliminating an infinite family of inputs that need to be tested",1
"
figure 1 shows examples of non-recursive and recursive addition traces",0
" in fact, we argue that recursion is an essential element for neural programs to generalize, and makes it tractable to prove the generalization of neural programs",1
"1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 while (pstacklo 6= 1): 6 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 7 stack(stack_pop) 8 else: 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 stack(stack_pop) 12 partition() 13 stack(stack_push_call2) 14 stack(stack_push_call1) 15 } 16 17 partition() { 18 set_pivot_lo() 19 set_j_lo() 20 compswap_loop() 21 swap(ppivot, phi) 22 set_j_null() 23 } 24 25 compswap_loop() { 26 while (pj 6= phi): 27 compswap() 28 move(pj, up) 29 } 30 31 compswap() { 32 if (a[pj ] ≤ a[phi]): 33 swap(ppivot, pj) 34 move(ppivot, up) 35 } 36 37 stack(op) { 38 if (op == stack_push_call1): 39 write(env_stack_lo, plo) 40 write(env_stack_hi, ppivot − 1) 41 move(pstacklo, up) 42 move(pstackhi, up) 43 44 if (op == stack_push_call2): 45 write(env_stack_lo, ppivot + 1) 46 write(env_stack_hi, phi) 47 move(pstacklo, up) 48 move(pstackhi, up) 49 50 if (op == stack_pop): 51 write(env_stack_lo, reset) 52 write(env_stack_hi, reset) 53 move(pstacklo, down) 54 move(pstackhi, down) 55 }altered recursive functions 1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 6 partition() 7 stack(stack_push_call2) 8 stack(stack_push_call1) 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 quicksort() // recursive call 12 stack(stack_pop) 13 write(phi, env_stack_hi_peek) 14 write(plo, env_stack_lo_peek) 15 quicksort() // recursive call 16 stack(stack_pop) 17 } 18 19 compswap_loop() { 20 if (pj 6= phi): 21 compswap() 22 move(pj, up) 23 compswap_loop() // recursive call 24 }in this section, we describe the space of base cases and reduction rules that must be covered for each of the four sample tasks, in order to create the verification set",1
" 3: 4: function quicksort(a, lo, hi) 5: if lo < hi: then 6: p = partition(a, lo, hi) 7: quicksort(a, lo, p− 1) 8: quicksort(a, p+ 1, hi) 9: 10: function partition(a, lo, hi) 11: pivot = lo 12: for j ∈ [lo, hi− 1] : do 13: if a[j] ≤ a[hi] then 14: swap a[pivot] with a[j] 15: pivot = pivot+ 1 16: swap a[pivot] with a[hi] 17: return pivot
for quicksort, the domain-specific encoder is
fenc(qarray, qstacklo, qstackhi, plo, phi, pstacklo, pstackhi, ppivot, pj , at) =
mlp ([qarray(pj) ≤ qarray(phi), pj == phi, qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1), pstacklo == 1, at(1), at(2), at(3)]),
whereqarray ∈ ru×11 is a scratch-pad that contains u rows, each containing one of 11 values (one of the numbers 0 through 9 or an invalid state)",0
" in fact, we argue that recursion is an essential element for neural programs to generalize, and makes it tractable to prove the generalization of neural programs",0
" as with the others, we apply the procedure described in section 3",0
" as this complexity interfered with determining the base cases and reductions, we changed the algorithm to its current form",0
" even though the earlier version also generalized just as well in practice, relatively small differences in the formulation of the traces and the environment observations can drastically change the difficulty of verification",0
 we choose to implement a topological sort task for graphs,0
" if it is too small, then execution of the program on some input might require evaluation of m(i) on some i /∈ v , and so the behavior ofm(i) might deviate from p (i)",1
" 16: p = partition(a, lo, hi) 17: push p+ 1 and hi to slo and shi",0
"
in this paper, we propose that recursion is an important concept for neural programs as well",0
"
in this paper, we propose that recursion is an important concept for neural programs as well",0
" it is straightforward to deal with all other operations, which do not induce partial environment states",0
", the trace corresponding to the array [3,2])",0
training neural networks to synthesize robust programs from a small number of examples is a challenging task,0
", an lstm in npi’s case, but possibly other networks in different cases",0
" in particular, this condition is satisfied by npi, and thus the npi model naturally supports recursion (even though the authors of npi did not consider this aspect explicitly)",1
" to the best of our knowledge, this is the first time verification has been applied to a neural program, providing provable guarantees about its behavior",0
"
it is important to emphasize that at inference time in the npi, the hidden state of the lstm controller is reset (to zero) at each subprogram call, as in line 3 of algorithm 1 (h ← 0)",0
6,0
" each step during an execution of the program does one of three things: (1) another subprogram along with associated arguments is called, as in line 10, (2) the program writes to the environment if it is primitive, as in line 8, or (3) the loop is terminated if the return probability exceeds a threshold α, after which the stack frame is popped and control is returned to the caller",0
 recursion enables provably perfect generalization,0
" et+1 ∼ fenv(et, pt, at)",0
" the only model, to our knowledge, that does not train on input-output pairs is the neural programmer-interpreter (reed & de freitas, 2016), which trains on synthetic execution traces",0
 there is a (changing) list of neural programs used to accomplish a given task,0
"
under the assumption that there are no leading 0’s (except in the case of single digits) and the two numbers to be added have the same number of digits, the verification set for addition contains 20,181 input problems",1
g,0
"
we describe the base cases, reduction rules, and the verification set for each task in appendix a",0
 add1 is a subprogram that adds the current column (writing the appropriate digit to the output row and carrying a bit to the next column if needed),1
" however, models that make use of this strategy eventually fail after a certain level of complexity (e",0
"
we describe the details of the npi model relevant to our contributions",1
 the maximum problem length in this training set is 3 (e,0
" for these programs, we re-use the appropriate program sets (the associated subprograms), and we refer the reader to the appendix of reed & de freitas (2016) for further details on the subprograms used in addition and bubble sort",1
" in this version, there is a dependence on length: bstep and lshift are called a number of times equivalent to one less than the length of the input array, in bubble and reset respectively",0
" the verification set can be very small, as we found a 10-element array ([8,2,1,2,0,8,5,8,3,7]) is sufficient to cover all of v ",0
"
base cases and reduction rules for bubble sort",0
"
by nature, recursion reduces the complexity of a problem to simpler instances",0
 the model described in the training setup of section 4 (trained on 6 traces) was verified to be correct via the matching procedure described in section 4,0
 recursion can be implemented differently for different neural programming models,0
" k is set to 11, to represent the range of 10 possible digits, along with a token representing the end of input",1
" we further have qresult ∈ nu , a scratch-pad which contains the sorted list of vertices at the end of execution, and qstack ∈ nu , which serves the role of the stack s in algorithm 2",1
 stack pop pushes −∞ values to qstacklo and qstackhi,0
"
algorithm 2 depth first search topological sort 1: color all vertices white",0
g,0
"
to perform the verification as described here, it is critical to construct v correctly",1
" if a white vertex is found, set pstart to point to it; this signifies the start of a traversal of a new connected component",0
" partial recursion only adds a tail recursive call to bubblesort after bubble and reset, similar to the tail recursive call described previously for addition",0
" the learned recursive program is different from neural programs learned in all previous work in an important aspect: previous approaches do not explicitly incorporate this abstraction, and hence generalize poorly, whereas our learned neural programs incorporate recursion and achieve perfect generalization",0
" if the step output specifies a primitive function call, we need to reason about how it can affect the environment so as to change the observation in the next step input",0
"
thus far, to evaluate the efficacy of neural models on programming tasks, the only metric that has been used is generalization of expected behavior to inputs of greater complexity (vinyals   (2015), kaiser & sutskever (2015), reed & de freitas (2016), graves   (2016), zaremba   (2016))",0
" as before, the non-recursive learned program lacks a provable guarantee of generalization, whereas we show later that the recursive learned program has one",0
this material is in part based upon work supported by the national science foundation under grant no,1
" in our experiments, we only present dag’s as inputs and represent the vertices as values ranging from 1, ",0
" bstep, used in bubble, compares pairs of numbers, continuously moving the bubble pointers once to the right each time until reaching the end of the array",1
" our implementation uses two stacks qstacklo and
qstackhi, each in ru , that store the arguments to the recursive quicksort calls in algorithm 3; before each recursive call, the appropriate arguments are popped off the stack and written to plo and phi",1
"
we propose and describe our verification procedure",1
" note that recursion helps make this process tractable, because we only need to test a finite number of inputs to show that the model will work correctly on inputs of unbounded complexity",0
" as an application, we incorporate recursion into the neural programmer-interpreter architecture and consider four sample tasks: grade-school addition, bubble sort, topological sort, and quicksort",0
" the write operation has the following arguments:
arg 1 (main action): color curr, color next, active start, active neighb, active stack, save, stack push, stack pop, result
color curr colors vactive, color next colors vertex dag[vactive][childlist[vactive]], active start writes pstart to vactive, active neighb writes dag[vactive][childlist[vactive]] to vactive, active stack writes qstack(pstack) to vactive, save writes vactive to vsave, stack push pushes vactive to the top of the stack, stack pop writes a null value to the top of the stack, and result writes vactive to qresult(presult)",0
" here as a concrete and general example, we consider a general neural programming architecture (npa), similar to neural programmer-interpreter (npi) in reed & de freitas (2016)",1
" both the non-recursive and recursive learned programs generalize on all input lengths we tried, up to 5000 digits",0
" the stack operation has the following arguments:
arg 1 (operation): stack push call1, stack push call2, stack pop
stack push call1 pushes lo and pivot−1 toqstacklo andqstackhi",0
" however, our concept of recursion for neural programs is general",0
" 3: 4: function quicksort(a, lo, hi) 5: if lo < hi: then 6: p = partition(a, lo, hi) 7: quicksort(a, lo, p− 1) 8: quicksort(a, p+ 1, hi) 9: 10: function partition(a, lo, hi) 11: pivot = lo 12: for j ∈ [lo, hi− 1] : do 13: if a[j] ≤ a[hi] then 14: swap a[pivot] with a[j] 15: pivot = pivot+ 1 16: swap a[pivot] with a[hi] 17: return pivot
for quicksort, the domain-specific encoder is
fenc(qarray, qstacklo, qstackhi, plo, phi, pstacklo, pstackhi, ppivot, pj , at) =
mlp ([qarray(pj) ≤ qarray(phi), pj == phi, qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1), pstacklo == 1, at(1), at(2), at(3)]),
whereqarray ∈ ru×11 is a scratch-pad that contains u rows, each containing one of 11 values (one of the numbers 0 through 9 or an invalid state)",1
e,0
" in this version of curriculum learning, even though the inputs are gradually becoming more complex, the semantics of the program is succinct and does not change",1
"
we experiment with two levels of recursion—partial and full",1
" if a white vertex is found, set pstart to point to it; this signifies the start of a traversal of a new connected component",1
" the corresponding edge list is [(1, 2), (1, 5), (2, 4), (2, 5), (3, 5)]",1
" lshift, used in reset, shifts the pointers left until reaching the start token",1
"
it is sufficient to construct problems where every transition between two adjacent columns is covered",0
"
an alternative method is to run p on many different program inputs and then observe step input sequences which occur, to create v ",1
this material is in part based upon work supported by the national science foundation under grant no,0
 we instantiated recursion for the neural programmerinterpreter by changing the training traces,0
 the core controller acts as a dispatcher for the programs,1
"
the three environment observations aid with control flow in algorithm 2",0
" twc-1409915, darpa under grant no",0
"
quicksort",1
 the original version of the bubblesort implementation exposes the values within the array,0
"
arg 2 (increment or decrement): up, down1 // top level topological sort call 2 toposort() { 3 while (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 } 0 11 traverse() { 12 check_child() 13 explore() 14 } 15 16 check_child() { 17 while (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 18 move(childlist[vactive], up) 19 } 20 21 explore() { 22 do 23 if (qcolor(dag[vactive][childlist[vactive]]) is white): 24 write(color_next, color_grey) 25 stack(push) 26 write(save) 27 write(active_neighb) 28 move(childlist[vsave], up) 29 else: 30 write(color_curr, color_black) 31 write(result) 32 move(presult, up) 33 if(pstack == 1): 34 break 35 else: 36 stack(pop) 37 check_child() 38 while (true) 39 } 40 41 stack(op) { 42 if (op == push): 43 write(stack_push) 44 move(pstack, up) 45 46 if (op == pop): 47 write(active_stack) 48 write(stack_pop) 49 move(pstack, down) 50 } 51 52 next_start() { 53 while(qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 54 move(pstart, up) 55 }altered recursive functions  // top level topological sort call 2 toposort() { 3 if (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 toposort() // recursive call 0 } 11 12 check_child() { 13 if (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 14 move(childlist[vactive], up) 15 check_child() // recursive call 16 } 17 18 explore() { 19 if (qcolor(dag[vactive][childlist[vactive]]) is white): 20 write(color_next, color_grey) 21 stack(push) 22 write(save) 23 write(active_neighb) 24 move(childlist[vsave], up) 25 else: 26 write(color_curr, color_black) 27 write(result) 28 move(presult, up) 29 if(pstack == 1): 30 return 31 else: 32 stack(pop) 33 check_child() 34 explore() // recursive call 35 } 36 37 next_start() { 38 if (qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 39 move(pstart, up) 40 next_start() // recursive call 41 }algorithm 4 iterative quicksort 1: initialize an array a to sort and two empty stacks slo and shi",0
" the assumption of leading 0’s can be easily removed, at the cost of slightly increasing the size of the verification set",1
"
the same set of problems was used to generate the training traces for all formulations of the task, for non-recursive and recursive versions",0
5,0
" however, to be sure that the generated v is complete (covers all the cases needed), we need to check all pairs of observations seen in adjacent step inputs (in particular, those before and after a primitive function call), in a similar way as if we were constructing v from scratch",1
" the contents ofqresult andqstack are not exposed directly through the domain-specific encoder; rather, we define primitive functions which manipulate these scratch-pads",0
" both the non-recursive and recursive learned programs generalize on all input lengths we tried, up to 5000 digits",1
 the model described in the training setup of section 4 (trained on 6 traces) was verified to be correct via the matching procedure described in section 4,1
" as a consequence, our learned neural programs empirically achieve perfect generalization from a very small number of training examples",1
" the npi model has three learnable components: a task-agnostic core, a program-key embedding, and domain-specific encoders that allow the npi to operate in diverse environments",0
"
the training set for bubble sort contains 100 traces, with maximum problem length of 2 (e",0
 add1 is a subprogram that adds the current column (writing the appropriate digit to the output row and carrying a bit to the next column if needed),1
" qcolor(pstart) contains the color of the current start vertex, used in the evaluation of the condition in the while loop in line 5 of algorithm 2",1
"
an alternative method is to run p on many different program inputs and then observe step input sequences which occur, to create v ",0
" thus, any general neural programming architecture supporting such a call structure can be made to support recursion",0
" in all experiments, α is set to 0",1
 we created a program set that reflects the semantics of algorithm 2,0
" numerous models have been proposed for learning programs; to name a few, this includes the differentiable neural computer (graves  , 2016), neural turing machine (graves  , 2014), neural gpu (kaiser & sutskever, 2015), neural programmer (neelakantan  , 2015), pointer network (vinyals  , 2015), hierarchical attentive memory (andrychowicz & kurach, 2016), and neural random access machine (kurach  , 2016)",1
"
an alternative way of representing the environment slice is to expose the values of the absolute vertices to the model; however, this makes it difficult to scale the model to larger graphs, since large vertex values are not seen during training time",0
 indentation indicates the stack is one level deeper than before,0
 table 3 presents results on randomly generated arrays of varying length for the learned non-recursive and recursive programs,1
" in the non-recursive trace, there are four functions that can be made recursive—toposort, check child, explore, and next start, and we add a tail recursive call to each of these functions in order to make the recursive trace",0
 this procedure verifies that all base cases and reductions are handled properly by the model via explicit tests,0
" recall that this verification procedure cannot be performed for the non-recursive versions, since the propagation of the hidden state in the core lstm module makes reasoning difficult and so we would need to check an unbounded number of examples",0
"
the training data for the neural programmer-interpreter consists of full execution traces for the program of interest",0
" as with the original npi architecture, the experiments for this paper always used a 3-tuple of integers at = (at(1), at(2), at(3))",0
 our result shows that the training procedure and the npi architecture is capable of generalizing from the step input-output pairs seen in the training data to the unseen ones present in the verification set,0
" formally, the npi is represented by the following set of equations:
st = fenc(et, at)
ht = flstm(st, pt, ht−1)
rt = fend(ht), pt+1 = fprog(ht), at+1 = farg(ht)
t is a subscript denoting the time-step; fenc is a domain-specific encoder (to be described later) that takes in the environment slice et and arguments at; flstm represents the core module, which takes in the state st generated by fenc, a program embedding pt ∈ rp , and hidden lstm state ht; fend decodes the return probability rt; fprog decodes a program key embedding pt+1;1 and farg decodes arguments at+1",0
" as described in the verification procedure, it is possible to prove our learned recursive program generalizes perfectly by testing on an appropriate set of problem inputs, i",0
" 2: initialize lo and hi to be 1 and n, where n is the length of a",1
" it is straightforward to deal with all other operations, which do not induce partial environment states",0
"
in this paper, we propose that recursion is an important concept for neural programs as well",0
"
base cases and reduction rules for topological sort",0
" in this version of curriculum learning, even though the inputs are gradually becoming more complex, the semantics of the program is succinct and does not change",0
" the add reduction rule ensures that each call to add only covers two adjacent columns, and so the lstm only ever runs for a fixed number of steps necessary to process these two columns",0
" the only model, to our knowledge, that does not train on input-output pairs is the neural programmer-interpreter (reed & de freitas, 2016), which trains on synthetic execution traces",1
" empirically, we observe that the learned recursive programs solve all valid inputs with 100% accuracy after training on a very small number of examples, out-performing previous generalization results",0
" we can obtain this mapping by first considering the possible observations that can arise at the beginning of the entry function (add, bubblesort, toposort, and quicksort) for some valid program input, and iteratively applying the observation-to-observation mapping implied by the reference implementation’s step output at that point in the execution",1
 the sequence of primitive operations (move and write operations) for the non-recursive and recursive versions are exactly the same,0
 the environment and return probability are omitted for readability,1
" if the program is primitive, the next environmental state et+1 will be affected by pt and at, i",1
" if the program is primitive, the next environmental state et+1 will be affected by pt and at, i",0
" if it is too small, then execution of the program on some input might require evaluation of m(i) on some i /∈ v , and so the behavior ofm(i) might deviate from p (i)",0
" in particular, this condition is satisfied by npi, and thus the npi model naturally supports recursion (even though the authors of npi did not consider this aspect explicitly)",1
g,1
" in all experiments, α is set to 0",1
" in this version of curriculum learning, even though the inputs are gradually becoming more complex, the semantics of the program is succinct and does not change",1
"
through careful reasoning about the possible set of environment observations created by all valid inputs, and how each of the operations in the execution trace affects the environment, we can construct v using the procedure described in section 3",0
" formally, a function exhibits recursive behavior when it possesses two properties: (1) base cases— terminating scenarios that do not use recursion to produce answers; (2) a set of rules that reduces all other problems toward the base cases",1
" the arrow in the theorem refers to evaluation, as in big-step semantics",1
"
in order to demonstrate that recursion can help learn and generalize better, we trained a non-recursive and recursive model on just a single execution trace generated from a graph containing 5 nodes3 for the topological sort task",0
" in computer science, recursion (as opposed to iteration) involves solving a larger problem by combining solutions to smaller instances of the same problem",1
"
we propose and describe our verification procedure",0
"
quicksort",0
" in future work, we seek to enable more tasks with recursive structure",0
" in this pair, the step input runs a subprogram add1 that has no arguments, and the step output contains a program write that has arguments of out and 1",0
" finally, we implement recursive traces for our own topological sort and quicksort tasks",0
", to change the value of phi) none described belowstack",0
 we leave this as future work to devise a verification procedure more appropriate to this setting,1
" in fact, we argue that recursion is an essential element for neural programs to generalize, and makes it tractable to prove the generalization of neural programs",0
" the core module of the npi is an lstm controller that takes as input a slice of the current external environment, via a set of pointers, and a program and arguments to execute",0
" any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of national science foundation and darpa",0
" this agrees with the generalization of non-recursive addition in reed & de freitas (2016), where they reported generalization up to 3000 digits",1
"
it is important to emphasize that at inference time in the npi, the hidden state of the lstm controller is reset (to zero) at each subprogram call, as in line 3 of algorithm 1 (h ← 0)",0
" via the recursive call, we effectively forget that the column just added exists, since the recursive call to add starts with a new hidden state for the lstm controller",0
" also, compared to the bubble pointers in bubble sort, the pointers that perform the comparison for quicksort (the compswap function) are usually not adjacent to each other, making quicksort less local than bubble sort",0
 a1a0 + bnbn−1 ,0
"6) on the verification set, and ensure they match the traces produced by the true program p ",0
"
after finding v , we construct a set of problem inputs which, when executed on p , create exactly the step input sequences which make up v ",0
" however, our concept of recursion for neural programs is general",0
", an array for quicksort or a dag for topological sort)",0
g,0
", the trace corresponding to the problem “109 + 101”)",0
" the theorem states that for the same sequence of step inputs, the model produces the exact same step output as the target program it aims to learn",0
" observe that partially recursive does slightly better than non-recursive for the setting in which the length of the array is 3, and that the fully recursive version is able to sort every array given to it",0
", pstart or childlist[vactive]) up or down none described belowwrite",0
" the move operation has the following arguments:
arg 1 (pointer): presult, pstack, pstart, childlist[vactive], childlist[vsave]
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 5 values",0
" finally, we implement recursive traces for our own topological sort and quicksort tasks",0
" however, to be sure that the generated v is complete (covers all the cases needed), we need to check all pairs of observations seen in adjacent step inputs (in particular, those before and after a primitive function call), in a similar way as if we were constructing v from scratch",0
"
as mentioned in section 1, all approaches to neural programming today fare poorly on this generalization issue",1
", an array for quicksort or a dag for topological sort)",1
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,0
", the trace corresponding to the problem “109 + 101”)",1
" at end, pointer ppivot is moved to the pivot compswap loop, move pivot lo, move j lo, swap
none
compswap loop runs the for loop inside the partition function compswap, move none compswap compares a[pivot] ≤ a[j]; if so, perform a swap and increment ppivot swap, move none set pivot lo sets ppivot to lo index none none set j lo sets pj to lo index none none set j null sets pj to −∞ none none stack pushes lo/hi states
onto stacks slo and shi according to argument (described below)
write, move described below
move moves pointer one unit up or down none described below swap swaps elements at given array indices none described below write write a value either to stack (e",0
" childlist ∈ nu is a vector of pointers, where childlist[i] points to the next child under consideration for vertex i",1
 this paper is the first (to our knowledge) to investigate the important problem of provable generalization properties of neural programs,0
" in future work, we seek to enable more tasks with recursive structure",0
" as with the original npi architecture, the experiments for this paper always used a 3-tuple of integers at = (at(1), at(2), at(3))",0
" k is set to 11, to represent the range of 10 possible digits, along with a token representing the end of input",0
" the recursive version can generalize perfectly to long problems constructed from these components (such as the sum “822+233”, where “8+2” and “2+3” are in the training set), but the non-recursive version fails to sum these long problems properly",0
", an lstm in npi’s case, but possibly other networks in different cases",0
"
to perform the verification as described here, it is critical to construct v correctly",0
" as an application, we incorporate recursion into the neural programmer-interpreter architecture and consider four sample tasks: grade-school addition, bubble sort, topological sort, and quicksort",1
g,1
"
the npi accesses an external environment, q, which varies according to the task",0
 reset represents a −∞ value,1
" we show that by modifying the training procedure, we enable the npi model to learn recursive neural programs",0
" we performed experiments on the partially recursive version in order to examine what happens when only one recursive call is implemented, when in reality three are required for perfect generalization",0
" finally, the boolean pstack == 1 is used to check whether the stack is empty in line 18 of algorithm 2",1
 2: initialize an empty stack s and a directed acyclic graph dag to traverse,1
" empirically, we observe that the learned recursive programs solve all valid inputs with 100% accuracy after training on a very small number of examples, out-performing previous generalization results",0
 the maximum problem length in this training set is 3 (e,0
" the assumption of leading 0’s can be easily removed, at the cost of slightly increasing the size of the verification set",0
"program descriptions calls arguments toposort perform topological
sort on graph traverse, next start, write, move
none
traverse traverse graph until stack is empty check child, explore none check child check if a white child exists; if so, set childlist[vactive] to point to it move none explore repeatedly traverse subgraphs until stack is empty stack, check child, write, move none stack interact with stack, either pushing or popping write, move push, pop next start move pstart until reaching a white vertex",1
"
note that v should also contain the necessary reductions, which corresponds to making the recursive calls at the correct time, as indicated by p ",1
 ,1
training neural networks to synthesize robust programs from a small number of examples is a challenging task,1
", the verification set",1
"
as in line 13 of the right-hand side of figure 1",0
 table 3 presents results on randomly generated arrays of varying length for the learned non-recursive and recursive programs,0
" at each time step, the core controller can decide to select one of the programs to call with certain arguments",0
" as this complexity interfered with determining the base cases and reductions, we changed the algorithm to its current form",1
" for tasks other than addition, it is difficult to analytically determine the verification set, so instead, we randomly generate input candidates until they completely cover the base cases and reduction rules",0
" m , as described in algorithm 1, processes the sequence of step inputs by using an lstm",0
" in fact, we argue that recursion is an essential element for neural programs to generalize, and makes it tractable to prove the generalization of neural programs",1
 there is a (changing) list of neural programs used to accomplish a given task,1
" as with the others, we apply the procedure described in section 3",0
" , n , where the dag contains n vertices",1
" as with the others, we apply the procedure described in section 3",1
" on the other hand, the recursive programs have learned the true program semantics",0
" empirically, we observe that the learned recursive programs solve all valid inputs with 100% accuracy after training on a very small number of examples, out-performing previous generalization results",0
 we instantiated recursion for the neural programmerinterpreter by changing the training traces,0
" given a precise definition of p , it may be possible to automate the generation of v from p in future work",1
 our result shows that the training procedure and the npi architecture is capable of generalizing from the step input-output pairs seen in the training data to the unseen ones present in the verification set,1
"
the training set for bubble sort contains 100 traces, with maximum problem length of 2 (e",1
" a topological sort is a linear ordering of vertices such that for every directed edge (u, v) from u to v, u comes before v in the ordering",1
 this is due to the properties of the training set – we trained on 2 traces synthesized from arrays of length 7 and 1 trace synthesized from an array of length 6,0
" the npi model has three learnable components: a task-agnostic core, a program-key embedding, and domain-specific encoders that allow the npi to operate in diverse environments",0
" the partial recursion is not enough for perfect generalization, as will be presented later in section 4",0
" recursion is an important concept in programming languages and a critical tool to
reduce the complexity of programs",0
"
the non-recursive trace loops on cycles of bubble and reset, which logically represents one bubble sweep through the array and reset of the two bubble pointers to the very beginning of the array, respectively",0
", to change the value of phi) none described belowstack",0
"
algorithm 3 recursive quicksort 1: initialize an array a to sort",0
" hence, if all subproblems in s are added correctly, we have proven that any member of f will be added correctly, thus eliminating an infinite family of inputs that need to be tested",0
 stack pop pushes −∞ values to qstacklo and qstackhi,1
" recall that this verification procedure cannot be performed for the non-recursive versions, since the propagation of the hidden state in the core lstm module makes reasoning difficult and so we would need to check an unbounded number of examples",1
" twc-1409915, darpa under grant no",1
"
to construct this set, by using the reference implementation of each subprogram, we construct a mapping between two sets of environment observations: the first set consists of all observations that can occur at the beginning of a particular subprogram’s invocation, and the second set contains the observations at the end of that subprogram",0
" via the recursive call, we effectively forget that the column just added exists, since the recursive call to add starts with a new hidden state for the lstm controller",1
", pstart or childlist[vactive]) up or down none described belowwrite",1
 we describe the recursive re-formulation of traces for two tasks from the original npi paper—grade-school addition and bubble sort,0
" in contrast, each call to the recursive add always runs for a fixed number of steps, even on arbitrarily long problems in f , so we can test that it performs correctly on a small, fixed number of step input sequences",1
"
after finding v , we construct a set of problem inputs which, when executed on p , create exactly the step input sequences which make up v ",1
" to make an analogy to mdps, this procedure is analogous to how value iteration obtains the correct value for each state starting from any initialization",0
"we train using the adam optimizer and use a 2-layer lstm and task-specific state encoders for the external environments, as described in reed & de freitas (2016)",0
" the write operation has the following arguments:
arg 1 (main action): color curr, color next, active start, active neighb, active stack, save, stack push, stack pop, result
color curr colors vactive, color next colors vertex dag[vactive][childlist[vactive]], active start writes pstart to vactive, active neighb writes dag[vactive][childlist[vactive]] to vactive, active stack writes qstack(pstack) to vactive, save writes vactive to vsave, stack push pushes vactive to the top of the stack, stack pop writes a null value to the top of the stack, and result writes vactive to qresult(presult)",1
" m , as described in algorithm 1, processes the sequence of step inputs by using an lstm",0
 this makes it difficult to reason about what the model will do when given complex inputs,0
" et+1 ∼ fenv(et, pt, at)",0
" for these models, table 2 presents results on randomly generated dags of varying graph sizes (varying in the number of vertices)",1
", to change the value of phi) none described belowstack",1
" for many problems, the base cases and reduction rules usually consist of a finite (often small) number of cases",1
" if yes, then this indicates that the learned neural program achieves provably perfect generalization",0
" for example, a step input-step output pair in lines 2 and 3 of the left-hand side of figure 1 is (add1, write out 1)",1
"
bubble sort",0
 pstacklo and pstackhi point to the top (empty) positions in qstacklo and qstackhi,0
" note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values, and arg 2 can only take one of 7 values",1
"
algorithm 3 recursive quicksort 1: initialize an array a to sort",0
" for purposes of verification, we replace the domain-specific encoder with the following:
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1) ≤ q(1, i2), 1 ≤ i1 ≤ length, 1 ≤ i2 ≤ length, i3 == length, at(1), at(2), at(3)]),which directly exposes which of the two values pointed to is larger",0
" for grade-school addition, the domain-specific encoder is
fenc(q, i1, i2, i3, i4, at) =mlp ([q(1, i1), q(2, i2), q(3, i3), q(4, i4), at(1), at(2), at(3)]),
where the environment q ∈ r4×n×k is a scratch-pad that contains four rows (the first input number, the second input number, the carry bits, and the output) and n columns",1
 ,1
" via the recursive call, we effectively forget that the column just added exists, since the recursive call to add starts with a new hidden state for the lstm controller",1
" in particular, in the explore function, adding a tail recursive call resets and stores the hidden states associated with vertices in a stack-like fashion",0
 recursion can be implemented differently for different neural programming models,0
" the arrow in the theorem refers to evaluation, as in big-step semantics",0
 these pointers are referred to as bubble pointers,0
" in all experiments, α is set to 0",1
" however, to be sure that the generated v is complete (covers all the cases needed), we need to check all pairs of observations seen in adjacent step inputs (in particular, those before and after a primitive function call), in a similar way as if we were constructing v from scratch",0
 3: begin traversing from vertex 1 in the dag,1
" for each graph size, we test the learned programs on 30 randomly generated dags",1
" in computer science, recursion (as opposed to iteration) involves solving a larger problem by combining solutions to smaller instances of the same problem",0
" in contrast, each call to the recursive add always runs for a fixed number of steps, even on arbitrarily long problems in f , so we can test that it performs correctly on a small, fixed number of step input sequences",1
" formally, a function exhibits recursive behavior when it possesses two properties: (1) base cases— terminating scenarios that do not use recursion to produce answers; (2) a set of rules that reduces all other problems toward the base cases",0
" we note that an earlier version of quicksort we tried lacked primitive operations to directly move a pointer to another, and therefore needed more functions and observations",0
 this makes it difficult to reason about what the model will do when given complex inputs,0
" in particular, given that a recursion is defined by two properties as mentioned before, the base cases and the set of reduction rules, we can prove a recursive neural program generalizes perfectly if we can prove that (1) it performs correctly on the base cases; (2) it learns the reduction rules correctly",1
g,1
g,0
" qcolor(pstart) contains the color of the current start vertex, used in the evaluation of the condition in the while loop in line 5 of algorithm 2",0
"
the training data for the neural programmer-interpreter consists of full execution traces for the program of interest",0
" a single element of an execution trace consists of a step input-step output pair, which can be synthesized from algorithm 1: this corresponds to, for a given time-step, the step input tuple (e, p, a) and step output tuple (r, p2, a2)",0
" fa8750-15-2-0104, and berkeley deep drive",0
"
formally, verification consists of proving the following theorem:
∀i ∈ v,m(i) ⇓ p (i) where i denotes a sequence of step inputs (within one function call), v denotes the set of valid sequences of step inputs, m denotes the neural network model, p denotes the correct program, and p (i) denotes the next step output from the correct program",1
 the logic for the non-recursive trace is shown in algorithm 4 in the appendix,0
" we then construct problem configurations that span all possible valid environment states (for instance, in order to force the carry bit in a column to be 1, one can add the sum “1+9” in the column to the right)",0
" provably perfect generalization implies the model will behave correctly, given any valid input",0
" our implementation uses two stacks qstacklo and
qstackhi, each in ru , that store the arguments to the recursive quicksort calls in algorithm 3; before each recursive call, the appropriate arguments are popped off the stack and written to plo and phi",0
" the write operation has the following arguments:
arg 1 (main action): color curr, color next, active start, active neighb, active stack, save, stack push, stack pop, result
color curr colors vactive, color next colors vertex dag[vactive][childlist[vactive]], active start writes pstart to vactive, active neighb writes dag[vactive][childlist[vactive]] to vactive, active stack writes qstack(pstack) to vactive, save writes vactive to vsave, stack push pushes vactive to the top of the stack, stack pop writes a null value to the top of the stack, and result writes vactive to qresult(presult)",0
" the stack operation has the following arguments:
arg 1 (operation): stack push call1, stack push call2, stack pop
stack push call1 pushes lo and pivot−1 toqstacklo andqstackhi",1
" any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of national science foundation and darpa",1
" the corresponding edge list is [(1, 2), (1, 5), (2, 4), (2, 5), (3, 5)]",1
" for purposes of verification, we replace the domain-specific encoder with the following:
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1) ≤ q(1, i2), 1 ≤ i1 ≤ length, 1 ≤ i2 ≤ length, i3 == length, at(1), at(2), at(3)]),which directly exposes which of the two values pointed to is larger",0
"
algorithm 2 depth first search topological sort 1: color all vertices white",1
" thus, we demonstrate that recursion enables provably perfect generalization for different tasks, including addition, topological sort, quicksort, and a variant of bubble sort",0
" for each task, given the verification set, we check the traces and results of the learned, to-be-verified neural program (described in section 4",1
 the core controller acts as a dispatcher for the programs,0
3,1
 almost all architectures train on program input/output pairs,1
" for many problems, the base cases and reduction rules usually consist of a finite (often small) number of cases",1
" we can obtain this mapping by first considering the possible observations that can arise at the beginning of the entry function (add, bubblesort, toposort, and quicksort) for some valid program input, and iteratively applying the observation-to-observation mapping implied by the reference implementation’s step output at that point in the execution",0
g,1
" even though the earlier version also generalized just as well in practice, relatively small differences in the formulation of the traces and the environment observations can drastically change the difficulty of verification",0
" this is highly undesirable, since being able to provide the correct answer in all possible settings is one of the most important aspects of any learned neural program",0
 this algorithm is a variant of depth first search,1
" in future work, we seek to enable more tasks with recursive structure",1
 pstacklo and pstackhi point to the top (empty) positions in qstacklo and qstackhi,1
" for each length, we test each program on 30 randomly generated problems",1
" for each length, we test each program on 30 randomly generated problems",0
 we use the lomuto partition scheme; the logic for the recursive trace is shown in algorithm 3,0
" in computer science, recursion (as opposed to iteration) involves solving a larger problem by combining solutions to smaller instances of the same problem",0
" we further have qresult ∈ nu , a scratch-pad which contains the sorted list of vertices at the end of execution, and qstack ∈ nu , which serves the role of the stack s in algorithm 2",1
" for example, a step input-step output pair in lines 2 and 3 of the left-hand side of figure 1 is (add1, write out 1)",1
"
to evaluate a neural network that learns a neural program to accomplish a certain task, one common evaluation metric is how well the learned model m generalizes",0
 table 4 also demonstrates that the (verified) recursive program generalizes perfectly,1
" m , as described in algorithm 1, processes the sequence of step inputs by using an lstm",1
" formally, a function exhibits recursive behavior when it possesses two properties: (1) base cases— terminating scenarios that do not use recursion to produce answers; (2) a set of rules that reduces all other problems toward the base cases",0
"
algorithm 1 neural programming inference 1: inputs: environment observation e, program p, arguments a, stop threshold α 2: function run(e, p, a) 3: h← 0, r ← 0 4: while r < α do 5: s← fenc(e, a), h← flstm(s, p, h) 6: r ← fend(h), p2 ← fprog(h), a2 ← farg(h) 7: if p is a primitive function then 8: e← fenv(e, p, a)",0
" in order to claim a proof, we must verify the model produces correct behavior over all base cases and reductions, as described in section 2",1
" without using a recursive program, such a proof is not possible, because the non-recursive program runs on an arbitrarily long addition problem that creates correspondingly long sequences of step inputs; in the non-recursive formulation of addition, add calls add1 a number of times that is dependent on the length of the input",1
e,1
"
in this paper, we propose to resolve these issues by explicitly incorporating recursion into neural architectures",1
 the maximum problem length in this training set is 3 (e,1
as mentioned in section 2,1
"
by reasoning about the possible set of environment observations created by all valid inputs, we construct v using the procedure described in section 3",0
" if it is too small, then execution of the program on some input might require evaluation of m(i) on some i /∈ v , and so the behavior ofm(i) might deviate from p (i)",1
g,1
" for example, a step input-step output pair in lines 2 and 3 of the left-hand side of figure 1 is (add1, write out 1)",1
 the training set for addition contains 200 traces,1
" lshift, used in reset, shifts the pointers left until reaching the start token",1
"
by reasoning about the possible set of environment observations created by all valid inputs, we construct v using the procedure described in section 3",1
" on the other hand, the non-recursive version has low accuracy, beginning from size 5, and fails completely for graphs of size 8 and beyond",0
"
since npi naturally supports the notion of recursion, a key question is how to enable npi to learn recursive programs",0
 the model described in the training setup of section 4 (trained on 6 traces) was verified to be correct via the matching procedure described in section 4,1
" at any given time, the npi has access to the values referred to by two pointers, represented by q(1, i1) and q(1, i2),",1
" we note that an earlier version of quicksort we tried lacked primitive operations to directly move a pointer to another, and therefore needed more functions and observations",0
 this makes it difficult to reason about what the model will do when given complex inputs,0
 we use the lomuto partition scheme; the logic for the recursive trace is shown in algorithm 3,0
g,0
", qstacklo or qstackhi) or to pointer (e",1
" for each length, we test each program on 30 randomly generated problems",0
"
the npi accesses an external environment, q, which varies according to the task",1
" for brevity, we refer the reader to the appendix for further details on the program set and non-recursive and recursive trace-generating functions used for topological sort",1
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,0
"as there is no public implementation of npi, we implemented a version of it in keras that is as faithful to the paper as possible",1
 these pointers are referred to as bubble pointers,1
" the swap operation has the following arguments:
arg 1 (swap object 1): ppivot
arg 2 (swap object 2): phi, pj
write",0
"
an alternative way of representing the environment slice is to expose the values of the absolute vertices to the model; however, this makes it difficult to scale the model to larger graphs, since large vertex values are not seen during training time",1
 we adapt machinery from the original paper slightly to fit our needs,1
"
to evaluate a neural network that learns a neural program to accomplish a certain task, one common evaluation metric is how well the learned model m generalizes",0
"as there is no public implementation of npi, we implemented a version of it in keras that is as faithful to the paper as possible",0
" this guarantees that the step input sequences considered during verification contain all step input sequences which arise during execution of an unseen problem in f , leading to generalization to any problem in f ",0
" 3: 4: function quicksort(a, lo, hi) 5: if lo < hi: then 6: p = partition(a, lo, hi) 7: quicksort(a, lo, p− 1) 8: quicksort(a, p+ 1, hi) 9: 10: function partition(a, lo, hi) 11: pivot = lo 12: for j ∈ [lo, hi− 1] : do 13: if a[j] ≤ a[hi] then 14: swap a[pivot] with a[j] 15: pivot = pivot+ 1 16: swap a[pivot] with a[hi] 17: return pivot
for quicksort, the domain-specific encoder is
fenc(qarray, qstacklo, qstackhi, plo, phi, pstacklo, pstackhi, ppivot, pj , at) =
mlp ([qarray(pj) ≤ qarray(phi), pj == phi, qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1), pstacklo == 1, at(1), at(2), at(3)]),
whereqarray ∈ ru×11 is a scratch-pad that contains u rows, each containing one of 11 values (one of the numbers 0 through 9 or an invalid state)",0
"
in this general neural programming architecture, we show it is easy to support recursion",0
" 16: p = partition(a, lo, hi) 17: push p+ 1 and hi to slo and shi",0
 errors arise due to undesirable and uninterpretable dependencies and associations the architecture learns to store in some high-dimensional hidden state,0
"we show that if we incorporate recursion, the learned npi programs can achieve provably perfect generalization for different tasks",0
" bstep, used in bubble, compares pairs of numbers, continuously moving the bubble pointers once to the right each time until reaching the end of the array",1
"
through careful reasoning about the possible set of environment observations created by all valid inputs, and how each of the operations in the execution trace affects the environment, we can construct v using the procedure described in section 3",0
"
thus far, to evaluate the efficacy of neural models on programming tasks, the only metric that has been used is generalization of expected behavior to inputs of greater complexity (vinyals   (2015), kaiser & sutskever (2015), reed & de freitas (2016), graves   (2016), zaremba   (2016))",0
" in this pair, the step input runs a subprogram add1 that has no arguments, and the step output contains a program write that has arguments of out and 1",0
" in this pair, the step input runs a subprogram add1 that has no arguments, and the step output contains a program write that has arguments of out and 1",1
" the space of possible programs is extremely large, and composing a program that performs robustly on the infinite space of possible inputs is difficult—in part because it is impractical to obtain enough training examples to easily disambiguate amongst all possible programs",1
 add1 is a subprogram that adds the current column (writing the appropriate digit to the output row and carrying a bit to the next column if needed),1
" as described in the verification procedure, it is possible to prove our learned recursive program generalizes perfectly by testing on an appropriate set of problem inputs, i",1
"
base cases and reduction rules for topological sort",0
" in this section, we give a brief review of the npi architecture from reed & de freitas (2016) as background",0
" consequently, it is infeasible to construct a verification set that covers all cases, and the verification procedure we have described is inadequate",1
e,1
" the only model, to our knowledge, that does not train on input-output pairs is the neural programmer-interpreter (reed & de freitas, 2016), which trains on synthetic execution traces",0
"
to construct this set, by using the reference implementation of each subprogram, we construct a mapping between two sets of environment observations: the first set consists of all observations that can occur at the beginning of a particular subprogram’s invocation, and the second set contains the observations at the end of that subprogram",0
" we implement a quicksort task, in order to demonstrate that recursion helps with learning divide-and-conquer algorithms",0
" for instance, for recursive addition, consider the family f of addition problems anan−1 ",0
", the trace corresponding to the problem “109 + 101”)",0
"
the dag is represented as an adjacency list where dag[i][j] refers to the j-th child of vertex i",1
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,1
 recursion enables provably perfect generalization,1
" we make the trace recursive by adding a tail recursive call into the trace for the add program after calling add1 and lshift, the original paper uses k = 10, but we found it necessary to augment the range with an end token, in order to terminate properly",0
"
as in line 13 of the right-hand side of figure 1",0
"we show that if we incorporate recursion, the learned npi programs can achieve provably perfect generalization for different tasks",1
"
note that the training set can often be considerably smaller than the verification set, and despite this, the learned model can still pass the entire verification set",0
" the corresponding edge list is [(1, 2), (1, 5), (2, 4), (2, 5), (3, 5)]",1
"2 at any given time, the npi has access to values pointed to by four pointers in each of the four rows, represented by q(1, i1), q(2, i2), q(3, i3), and q(4, i4)",0
" at any given time, the npi has access to the values referred to by two pointers, represented by q(1, i1) and q(1, i2),",0
" however, the authors of npi did not consider explicitly the notion of recursion and as a consequence, did not learn recursive programs",0
" in all experiments, α is set to 0",0
 this is due to the properties of the training set – we trained on 2 traces synthesized from arrays of length 7 and 1 trace synthesized from an array of length 6,1
 recursion can be implemented differently for different neural programming models,0
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,1
" the move operation has the following arguments:
arg 1 (pointer): presult, pstack, pstart, childlist[vactive], childlist[vsave]
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 5 values",0
" numerous models have been proposed for learning programs; to name a few, this includes the differentiable neural computer (graves  , 2016), neural turing machine (graves  , 2014), neural gpu (kaiser & sutskever, 2015), neural programmer (neelakantan  , 2015), pointer network (vinyals  , 2015), hierarchical attentive memory (andrychowicz & kurach, 2016), and neural random access machine (kurach  , 2016)",1
"
given a verification set, we can then run the model on the verification set to check if the produced traces and results are correct",0
" in particular, we construct new training traces which explicitly contain recursive elements and show that with this type of trace, npi easily learns recursive programs",0
" note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values, and arg 2 can only take one of 7 values",1
" also, compared to the bubble pointers in bubble sort, the pointers that perform the comparison for quicksort (the compswap function) are usually not adjacent to each other, making quicksort less local than bubble sort",1
3,0
"
through careful reasoning about the possible set of environment observations created by all valid inputs, and how each of the operations in the execution trace affects the environment, we can construct v using the procedure described in section 3",1
"
the non-recursive trace loops on cycles of add1 and lshift",0
"
formally, verification consists of proving the following theorem:
∀i ∈ v,m(i) ⇓ p (i) where i denotes a sequence of step inputs (within one function call), v denotes the set of valid sequences of step inputs, m denotes the neural network model, p denotes the correct program, and p (i) denotes the next step output from the correct program",0
" it is straightforward to deal with all other operations, which do not induce partial environment states",0
"
grade school addition",0
 a1a0 + bnbn−1 ,0
" 2: initialize lo and hi to be 1 and n, where n is the length of a",0
" we have three observations: the color of the start node, the color of the active node’s next child to be considered, and whether the stack is empty",0
" the general notion of recursion has been an important concept in many domains, including mathematics and computer science",0
" the outputs rt, pt+1, at+1 are used to determine the next action, as described in algorithm 1",0
5,1
 the environment and return probability are omitted for readability,0
"
arg 2 (increment or decrement): up, down
swap",0
 we leave this as future work to devise a verification procedure more appropriate to this setting,1
"we describe how models trained with recursive traces can be proven to generalize, by using the verification procedure described in section 3",1
", to change the value of phi) none described belowstack",0
" the second is the training procedure, which consists of the form of the training data and the optimization process",0
" as described in the verification procedure, it is possible to prove our learned recursive program generalizes perfectly by testing on an appropriate set of problem inputs, i",0
"
base cases and reduction rules for quicksort",0
"
base cases and reduction rules for bubble sort",0
" the move operation has the following arguments:
arg 1 (pointer): pstacklo, pstackhi, pj , ppivot
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values",0
" an example of part of an addition task trace, written in shorthand, is given in figure 1",0
"
we refer the reader to the appendix for the non-recursive trace generating functions",0
" the recursive version can generalize perfectly to long problems constructed from these components (such as the sum “822+233”, where “8+2” and “2+3” are in the training set), but the non-recursive version fails to sum these long problems properly",1
" by iterating with this procedure, and then running p on the input observation set that we obtain for the entry point function, we can obtain v precisely",1
"as discussed in section 2, the neural programmer-interpreter (npi) is an instance of a neural programmer architecture and hence it naturally supports recursion",0
" in particular, this condition is satisfied by npi, and thus the npi model naturally supports recursion (even though the authors of npi did not consider this aspect explicitly)",0
" qcolor(dag[vactive][childlist[vactive]]) refers to the color of the next child of vactive, used in the evaluation of the condition in the if branch in line 9 of algorithm 2",0
" the space of possible programs is extremely large, and composing a program that performs robustly on the infinite space of possible inputs is difficult—in part because it is impractical to obtain enough training examples to easily disambiguate amongst all possible programs",0
" for bubble sort, the domain-specific encoder is
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1), q(1, i2), i3 == length, at(1), at(2), at(3)]),
where the environment q ∈ r1×n×k is a scratch-pad that contains 1 row, to represent the state of the array as sorting proceeds in-place, andn columns",1
" k is set to 11, to represent the range of 10 possible digits, along with a token representing the end of input",0
 our result shows that the training procedure and the npi architecture is capable of generalizing from the step input-output pairs seen in the training data to the unseen ones present in the verification set,0
" 4: function toposort(dag) 5: while there is still a white vertex u: do 6: color[u] = grey 7: vactive = u 8: do 9: if vactive has a white child v then 10: color[v] = grey 11: push vactive onto s 12: vactive = v 13: else 14: color[vactive] = black 15: write vactive to result 16: if s is empty then pass 17: else pop the top vertex off s and set it to vactive 18: while s is not empty
topological sort",1
" m , as described in algorithm 1, processes the sequence of step inputs by using an lstm",0
" we note that an earlier version of quicksort we tried lacked primitive operations to directly move a pointer to another, and therefore needed more functions and observations",0
" 3: 4: function partition(a, lo, hi) 5: pivot = lo 6: for j ∈ [lo, hi− 1] : do 7: if a[j] ≤ a[hi] then 8: swap a[pivot] with a[j] 9: pivot = pivot+ 1 0: swap a[pivot] with a[hi] 11: return pivot 12: 13: function quicksort(a, lo, hi) 14: while slo and shi are not empty: do 15: pop states off slo and shi, writing them to lo and hi",1
" to the best of our knowledge, this is the first time verification has been applied to a neural program, providing provable guarantees about its behavior",0
 the first is the actual model architecture,0
we emphasize that the notion of a neural recursive program has not been presented in the literature before: this is our main contribution,1
"
in order to demonstrate that recursion can help learn and generalize better, we trained a non-recursive and recursive model on just a single execution trace generated from a graph containing 5 nodes3 for the topological sort task",0
" we prove every member of f is added properly, given that subproblems s = {anan−1 + bnbn−1, an−1an−2 + bn−1bn−2, ",1
" the recursive version can generalize perfectly to long problems constructed from these components (such as the sum “822+233”, where “8+2” and “2+3” are in the training set), but the non-recursive version fails to sum these long problems properly",1
" childlist ∈ nu is a vector of pointers, where childlist[i] points to the next child under consideration for vertex i",0
 a1a0 + bnbn−1 ,0
" we also hope to decrease supervision, for example by training with only partial or non-recursive traces, and to develop novel neural programming architectures integrated directly with a notion of recursion",0
"
quicksort",1
g,0
 the sequence of primitive operations (move and write operations) for the non-recursive and recursive versions are exactly the same,0
" each step during an execution of the program does one of three things: (1) another subprogram along with associated arguments is called, as in line 10, (2) the program writes to the environment if it is primitive, as in line 8, or (3) the loop is terminated if the return probability exceeds a threshold α, after which the stack frame is popped and control is returned to the caller",1
"
an alternative method is to run p on many different program inputs and then observe step input sequences which occur, to create v ",0
 the training set for addition contains 200 traces,0
" nevertheless, we would like the model to quickly learn to represent the right semantics of the underlying program from a small number of training examples, not an exhaustive number of them",1
 ,0
 pstacklo and pstackhi point to the top (empty) positions in qstacklo and qstackhi,1
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,0
" as an application, we incorporate recursion into the neural programmer-interpreter architecture and consider four sample tasks: grade-school addition, bubble sort, topological sort, and quicksort",0
6,1
" we made the assumption of equivalent lengths in order to parametrize the input format with respect to length, but this assumption can be removed as well",0
" nevertheless, we would like the model to quickly learn to represent the right semantics of the underlying program from a small number of training examples, not an exhaustive number of them",1
"6) on the verification set, and ensure they match the traces produced by the true program p ",0
 the program terminates when seeing no numbers in the current column,0
" the general notion of recursion has been an important concept in many domains, including mathematics and computer science",1
" k is set to 11, to represent the range of 10 possible digits, along with a token representing the end of input",0
 we leave this as future work to devise a verification procedure more appropriate to this setting,0
6,1
" the recursive version of topological sort solves all graph instances we tried, from graphs of size 5 through 70",0
" formally, the npi is represented by the following set of equations:
st = fenc(et, at)
ht = flstm(st, pt, ht−1)
rt = fend(ht), pt+1 = fprog(ht), at+1 = farg(ht)
t is a subscript denoting the time-step; fenc is a domain-specific encoder (to be described later) that takes in the environment slice et and arguments at; flstm represents the core module, which takes in the state st generated by fenc, a program embedding pt ∈ rp , and hidden lstm state ht; fend decodes the return probability rt; fprog decodes a program key embedding pt+1;1 and farg decodes arguments at+1",0
" if it is too small, then execution of the program on some input might require evaluation of m(i) on some i /∈ v , and so the behavior ofm(i) might deviate from p (i)",0
" the pointer at index i3 represents a counter internal to the environment that is incremented once after each pass of the algorithm (one cycle of bubble and reset); when incremented a number of times equal to the length of the array, the flag i3 == length becomes true and terminates the entire algorithm ",0
 we use the lomuto partition scheme; the logic for the recursive trace is shown in algorithm 3,1
" we have three observations: the color of the start node, the color of the active node’s next child to be considered, and whether the stack is empty",1
" via the recursive call, we effectively forget that the column just added exists, since the recursive call to add starts with a new hidden state for the lstm controller",0
" the general notion of recursion has been an important concept in many domains, including mathematics and computer science",1
" 4: function toposort(dag) 5: while there is still a white vertex u: do 6: color[u] = grey 7: vactive = u 8: do 9: if vactive has a white child v then 10: color[v] = grey 11: push vactive onto s 12: vactive = v 13: else 14: color[vactive] = black 15: write vactive to result 16: if s is empty then pass 17: else pop the top vertex off s and set it to vactive 18: while s is not empty
topological sort",1
"
the operations we need to be concerned most about are carry and lshift, which induce partial environment states spanning two columns",0
"
after finding v , we construct a set of problem inputs which, when executed on p , create exactly the step input sequences which make up v ",0
"1; and for bubble sort, appendix a",1
" both the non-recursive and recursive learned programs generalize on all input lengths we tried, up to 5000 digits",1
3 to construct v and then empirically create a verification set which covers v ,0
"
grade-school addition",0
"
in order to demonstrate that recursion can help learn and generalize better, we trained a non-recursive and recursive model on just a single execution trace generated from a graph containing 5 nodes3 for the topological sort task",0
"
as a concrete instantiation, we show in this paper that we can enable recursive neural programs in the npi model, and thus enable perfectly generalizable neural programs for tasks such as sorting where the original, non-recursive npi program fails",1
" 3: 4: function partition(a, lo, hi) 5: pivot = lo 6: for j ∈ [lo, hi− 1] : do 7: if a[j] ≤ a[hi] then 8: swap a[pivot] with a[j] 9: pivot = pivot+ 1 0: swap a[pivot] with a[hi] 11: return pivot 12: 13: function quicksort(a, lo, hi) 14: while slo and shi are not empty: do 15: pop states off slo and shi, writing them to lo and hi",0
" however, the authors of npi did not consider explicitly the notion of recursion and as a consequence, did not learn recursive programs",0
" recursion is an important concept in programming languages and a critical tool to
reduce the complexity of programs",0
" empirically, we observe that the learned recursive programs solve all valid inputs with 100% accuracy after training on a very small number of examples, out-performing previous generalization results",0
"
in order to demonstrate that recursion can help learn and generalize better, for addition, we trained only on traces for 5 arbitrarily chosen 1-digit addition sum examples",1
" in this section, we give a brief review of the npi architecture from reed & de freitas (2016) as background",1
 this makes it difficult to reason about what the model will do when given complex inputs,0
" childlist ∈ nu is a vector of pointers, where childlist[i] points to the next child under consideration for vertex i",1
" 4: function toposort(dag) 5: while there is still a white vertex u: do 6: color[u] = grey 7: vactive = u 8: do 9: if vactive has a white child v then 10: color[v] = grey 11: push vactive onto s 12: vactive = v 13: else 14: color[vactive] = black 15: write vactive to result 16: if s is empty then pass 17: else pop the top vertex off s and set it to vactive 18: while s is not empty
topological sort",0
we now report on generalization for the varying tasks,1
" because the context of the caller is stored on a stack when it calls another program and the callee starts in a fresh context, this enables recursion simply by allowing a program to call itself",0
" this is highly undesirable, since being able to provide the correct answer in all possible settings is one of the most important aspects of any learned neural program",1
" for many problems, the base cases and reduction rules usually consist of a finite (often small) number of cases",0
"
the three environment observations aid with control flow in algorithm 2",0
" although the model is exposed to more and more data, it might learn spurious and overly complex representations of the program, as suggested in zaremba   (2016)",1
" the theorem states that for the same sequence of step inputs, the model produces the exact same step output as the target program it aims to learn",1
"
note that the training set can often be considerably smaller than the verification set, and despite this, the learned model can still pass the entire verification set",0
" the arrow in the theorem refers to evaluation, as in big-step semantics",0
" lshift, used in reset, shifts the pointers left until reaching the start token",0
" the write operation has the following arguments:
arg 1 (object to write): env stack lo, env stack hi, phi, plo
env stack lo and env stack hi represent qstacklo(pstacklo) and qstackhi(pstackhi), respectively",0
" this functionality the original npi paper decodes to a program key embedding kt ∈ rk and then computes a program embedding pt+1, which we also did in our implementation, but we omit this for brevity",0
 2: initialize an empty stack s and a directed acyclic graph dag to traverse,0
" as before, the non-recursive learned program lacks a provable guarantee of generalization, whereas we show later that the recursive learned program has one",0
" the general notion of recursion has been an important concept in many domains, including mathematics and computer science",0
 ,0
"
in order to demonstrate that recursion can help learn and generalize better, we trained a non-recursive and recursive model on just a single execution trace generated from a graph containing 5 nodes3 for the topological sort task",0
" in this version, there is a dependence on length: bstep and lshift are called a number of times equivalent to one less than the length of the input array, in bubble and reset respectively",0
 pstacklo and pstackhi point to the top (empty) positions in qstacklo and qstackhi,0
" for the recursive formulation of addition, we analytically construct the set of input problems that cover all base cases and reduction rules",1
" it is straightforward to deal with all other operations, which do not induce partial environment states",1
 stack push call2 pushes pivot + 1 and hi to qstacklo and qstackhi,1
" our implementation uses two stacks qstacklo and
qstackhi, each in ru , that store the arguments to the recursive quicksort calls in algorithm 3; before each recursive call, the appropriate arguments are popped off the stack and written to plo and phi",1
" a topological sort is a linear ordering of vertices such that for every directed edge (u, v) from u to v, u comes before v in the ordering",1
" consequently, it is infeasible to construct a verification set that covers all cases, and the verification procedure we have described is inadequate",1
" for bubble sort, the domain-specific encoder is
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1), q(1, i2), i3 == length, at(1), at(2), at(3)]),
where the environment q ∈ r1×n×k is a scratch-pad that contains 1 row, to represent the state of the array as sorting proceeds in-place, andn columns",1
" because the context of the caller is stored on a stack when it calls another program and the callee starts in a fresh context, this enables recursion simply by allowing a program to call itself",1
"
in what follows, we describe the way in which we constructed npi training traces so as to make them contain recursive elements and thus enable npi to learn recursive programs",1
" for grade-school addition, the domain-specific encoder is
fenc(q, i1, i2, i3, i4, at) =mlp ([q(1, i1), q(2, i2), q(3, i3), q(4, i4), at(1), at(2), at(3)]),
where the environment q ∈ r4×n×k is a scratch-pad that contains four rows (the first input number, the second input number, the carry bits, and the output) and n columns",1
"
we propose and describe our verification procedure",0
" here as a concrete and general example, we consider a general neural programming architecture (npa), similar to neural programmer-interpreter (npi) in reed & de freitas (2016)",0
"
grade school addition",1
", the trace corresponding to the problem “109 + 101”)",1
 this paper is the first (to our knowledge) to investigate the important problem of provable generalization properties of neural programs,1
" given verification sets that cover all the base cases and reduction rules, we can provide proofs that these learned programs generalize perfectly",0
" recall that this verification procedure cannot be performed for the non-recursive versions, since the propagation of the hidden state in the core lstm module makes reasoning difficult and so we would need to check an unbounded number of examples",1
" we performed experiments on the partially recursive version in order to examine what happens when only one recursive call is implemented, when in reality three are required for perfect generalization",1
 we describe the recursive re-formulation of traces for two tasks from the original npi paper—grade-school addition and bubble sort,0
" we implement a quicksort task, in order to demonstrate that recursion helps with learning divide-and-conquer algorithms",0
"
we would also like to point out that in this paper, we provide as an example one way to train a recursive neural program, by providing a certain training execution trace to the npi model",0
" the single-digit multiplication task in zaremba   (2016), the bubble sort task in reed & de freitas (2016), and the graph tasks in graves   (2016))",0
" empirically, existing models suffer from a common limitation—generalization becomes poor beyond a threshold level of complexity",0
"
quicksort",0
" the general notion of recursion has been an important concept in many domains, including mathematics and computer science",0
"
formally, verification consists of proving the following theorem:
∀i ∈ v,m(i) ⇓ p (i) where i denotes a sequence of step inputs (within one function call), v denotes the set of valid sequences of step inputs, m denotes the neural network model, p denotes the correct program, and p (i) denotes the next step output from the correct program",0
"
bubble sort",0
 some functional programming languages go so far as not to define any looping constructs but rely solely on recursion to enable repeated execution of the same code,1
" provably perfect generalization implies the model will behave correctly, given any valid input",0
" childlist ∈ nu is a vector of pointers, where childlist[i] points to the next child under consideration for vertex i",0
 recursion enables provably perfect generalization,1
 the degree for any vertex in the dag is variable,0
"
after finding v , we construct a set of problem inputs which, when executed on p , create exactly the step input sequences which make up v ",1
" for brevity, we refer the reader to the appendix for information about the program set and non-recursive and recursive trace-generating functions for quicksort",0
" for each length, we test each program on 30 randomly generated problems",1
"
as a concrete instantiation, we show in this paper that we can enable recursive neural programs in the npi model, and thus enable perfectly generalizable neural programs for tasks such as sorting where the original, non-recursive npi program fails",0
 the original version of the bubblesort implementation exposes the values within the array,1
 reset represents a −∞ value,0
" we show that by modifying the training procedure, we enable the npi model to learn recursive neural programs",0
 our experiments use a small number of training examples,1
" as with the others, we apply the procedure described in section 3",0
" for grade-school addition, the domain-specific encoder is
fenc(q, i1, i2, i3, i4, at) =mlp ([q(1, i1), q(2, i2), q(3, i3), q(4, i4), at(1), at(2), at(3)]),
where the environment q ∈ r4×n×k is a scratch-pad that contains four rows (the first input number, the second input number, the carry bits, and the output) and n columns",0
"
we refer the reader to the appendix for the non-recursive trace generating functions",1
training neural networks to synthesize robust programs from a small number of examples is a challenging task,1
" partial recursion only adds a tail recursive call to bubblesort after bubble and reset, similar to the tail recursive call described previously for addition",1
" if no white vertex is found, the entire execution is terminated move none write write a value either to environment (e",0
" for these models, table 2 presents results on randomly generated dags of varying graph sizes (varying in the number of vertices)",0
" the move operation has the following arguments:
arg 1 (pointer): presult, pstack, pstart, childlist[vactive], childlist[vsave]
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 5 values",0
 ,0
" plo and phi point to the lo and hi indices of the array, as in algorithm 3",0
2,0
"
formally, verification consists of proving the following theorem:
∀i ∈ v,m(i) ⇓ p (i) where i denotes a sequence of step inputs (within one function call), v denotes the set of valid sequences of step inputs, m denotes the neural network model, p denotes the correct program, and p (i) denotes the next step output from the correct program",0
" the architecture usually possesses some form of memory, which could be internal (such as the hidden state of a recurrent neural network) or external (such as a discrete “scratch pad” or a memory block with differentiable access)",1
" k is set to 11, to denote the range of possible numbers (0 through 9), along with the start/end token (represented with the same encoding) which is observed when a pointer reaches beyond the bounds of the input",1
" for example, a step input-step output pair in lines 2 and 3 of the left-hand side of figure 1 is (add1, write out 1)",1
 this algorithm is a variant of depth first search,1
" as aforementioned, the npi model naturally supports recursion",0
" furthermore, given a verification input set that covers all base cases and reduction rules, we can formally prove that the learned neural programs achieve perfect generalization after verifying its behavior on the verification input set",0
" m is considered to have perfect generalization if m can give the right answer for any input, such as inputs of arbitrary length",0
" as with the original npi architecture, the experiments for this paper always used a 3-tuple of integers at = (at(1), at(2), at(3))",0
" the swap operation has the following arguments:
arg 1 (swap object 1): ppivot
arg 2 (swap object 2): phi, pj
write",1
we emphasize that the notion of a neural recursive program has not been presented in the literature before: this is our main contribution,0
 the sequence of primitive operations (move and write operations) for the non-recursive and recursive versions are exactly the same,1
" a topological sort is a linear ordering of vertices such that for every directed edge (u, v) from u to v, u comes before v in the ordering",1
"
training setup",0
" as this complexity interfered with determining the base cases and reductions, we changed the algorithm to its current form",0
", qstacklo or qstackhi) or to pointer (e",0
" the core lstm module’s hidden state is preserved over all these add1 calls, and it is difficult to interpret with certainty what happens over longer timesteps without concretely evaluating the lstm with an input of that length",0
 we created a program set that reflects the semantics of algorithm 2,0
 ,0
" in this section, we give a brief review of the npi architecture from reed & de freitas (2016) as background",0
" although the model is exposed to more and more data, it might learn spurious and overly complex representations of the program, as suggested in zaremba   (2016)",0
" this agrees with the generalization of non-recursive addition in reed & de freitas (2016), where they reported generalization up to 3000 digits",0
" if the program is primitive, the next environmental state et+1 will be affected by pt and at, i",0
 ,0
" , a1a0 + b1b0} are added properly",1
" the general notion of recursion has been an important concept in many domains, including mathematics and computer science",0
 there is a (changing) list of neural programs used to accomplish a given task,0
 ,1
 some functional programming languages go so far as not to define any looping constructs but rely solely on recursion to enable repeated execution of the same code,0
" because the context of the caller is stored on a stack when it calls another program and the callee starts in a fresh context, this enables recursion simply by allowing a program to call itself",0
 ,1
 table 4 demonstrates that the accuracy of the non-recursive program degrades sharply when moving from arrays of length 7 to arrays of length 8,0
 we leave this as future work to devise a verification procedure more appropriate to this setting,0
" for brevity, we refer the reader to the appendix for information about the program set and non-recursive and recursive trace-generating functions for quicksort",0
" observe that the non-recursive program’s correctness degrades for length 11 and beyond, while the recursive program can sort any given array",1
 a1a0 + bnbn−1 ,0
" for example, for the addition task, the model is trained on short inputs and then tested on its ability to sum inputs with much longer numbers of digits",0
" naı̈vely, we might expect to synthesize and test an input for any sequence created by combining the four possible colors in two variables and another boolean variable for whether the stack is empty (so 32 possible observations at any point), but for various reasons, most of these combinations are impossible to occur at any given point in the execution trace",1
" nevertheless, we would like the model to quickly learn to represent the right semantics of the underlying program from a small number of training examples, not an exhaustive number of them",1
"
inside bubble and reset, there are two operations that can be made recursive",1
" we implement a quicksort task, in order to demonstrate that recursion helps with learning divide-and-conquer algorithms",1
" in future work, we seek to enable more tasks with recursive structure",0
" at end, pointer ppivot is moved to the pivot compswap loop, move pivot lo, move j lo, swap
none
compswap loop runs the for loop inside the partition function compswap, move none compswap compares a[pivot] ≤ a[j]; if so, perform a swap and increment ppivot swap, move none set pivot lo sets ppivot to lo index none none set j lo sets pj to lo index none none set j null sets pj to −∞ none none stack pushes lo/hi states
onto stacks slo and shi according to argument (described below)
write, move described below
move moves pointer one unit up or down none described below swap swaps elements at given array indices none described below write write a value either to stack (e",0
" twc-1409915, darpa under grant no",0
"when constructing a neural network for the purpose of learning a program, there are two orthogonal aspects to consider",1
"
base cases and reduction rules for quicksort",1
" more specifically, when m is trained on simpler inputs, such as inputs of a small length, the generalization metric evaluates how well m will do on more complex inputs, such as inputs of much longer length",0
"1, we hypothesize the non-recursive programs do not generalize well because they have learned spurious dependencies specific to the training set, such as length of the input problems",0
"
we construct input problems by splitting into two cases: one case in which the left column contains a null value and another in which the left column does not contain any null values",1
 ,1
"
as mentioned in section 1, all approaches to neural programming today fare poorly on this generalization issue",0
"
is critical for implementing recursion, since it permits us to restrict our attention to the currently relevant recursive call, ignoring irrelevant details about other contexts",1
" childlist ∈ nu is a vector of pointers, where childlist[i] points to the next child under consideration for vertex i",1
" we then construct problem configurations that span all possible valid environment states (for instance, in order to force the carry bit in a column to be 1, one can add the sum “1+9” in the column to the right)",0
" the second is the training procedure, which consists of the form of the training data and the optimization process",1
" in particular, this condition is satisfied by npi, and thus the npi model naturally supports recursion (even though the authors of npi did not consider this aspect explicitly)",1
" while this matches the description from reed & de freitas (2016), we found that this causes an unnecessary blowup in the size of v and makes it much more difficult to construct the verification set",0
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,1
" given a precise definition of p , it may be possible to automate the generation of v from p in future work",1
" that is to say, the network does not learn the true program semantics",0
we now report on generalization for the varying tasks,0
"when constructing a neural network for the purpose of learning a program, there are two orthogonal aspects to consider",0
" this is highly undesirable, since being able to provide the correct answer in all possible settings is one of the most important aspects of any learned neural program",0
" formally, a function exhibits recursive behavior when it possesses two properties: (1) base cases— terminating scenarios that do not use recursion to produce answers; (2) a set of rules that reduces all other problems toward the base cases",0
" this is possible if and only if the graph has no directed cycles; that is to say, it must be a directed acyclic graph (dag)",0
" qcolor(dag[vactive][childlist[vactive]]) refers to the color of the next child of vactive, used in the evaluation of the condition in the if branch in line 9 of algorithm 2",0
 3: begin traversing from vertex 1 in the dag,0
"2 at any given time, the npi has access to values pointed to by four pointers in each of the four rows, represented by q(1, i1), q(2, i2), q(3, i3), and q(4, i4)",0
3,0
"
in this paper, we propose to resolve these issues by explicitly incorporating recursion into neural architectures",1
" without using a recursive program, such a proof is not possible, because the non-recursive program runs on an arbitrarily long addition problem that creates correspondingly long sequences of step inputs; in the non-recursive formulation of addition, add calls add1 a number of times that is dependent on the length of the input",1
" when the program is called, the current context including the caller’s memory state is stored on a stack; when the program returns, the stored context is popped off the stack to resume execution in the previous caller’s context",1
" that is to say, the network does not learn the true program semantics",1
"we show that if we incorporate recursion, the learned npi programs can achieve provably perfect generalization for different tasks",0
 ,0
" naı̈vely, we might expect to synthesize and test an input for any sequence created by combining the four possible colors in two variables and another boolean variable for whether the stack is empty (so 32 possible observations at any point), but for various reasons, most of these combinations are impossible to occur at any given point in the execution trace",1
" for each function we use to implement the recursive version of topological sort, we need to consider the set of possible environment observation sequences we can create from all valid inputs and test that the learned program produces the correct behavior on each of these inputs",1
6,0
" k is set to 11, to represent the range of 10 possible digits, along with a token representing the end of input",0
"
arg 2 (object to copy): env stack lo peek, env stack hi peek, phi, plo, ppivot − 1, ppivot + 1, reset
env stack lo peek and env stack hi peek represent qstacklo(pstacklo − 1) and qstackhi(pstackhi − 1), respectively",1
 our experiments use a small number of training examples,1
" given verification sets that cover all the base cases and reduction rules, we can provide proofs that these learned programs generalize perfectly",1
" we show that by modifying the training procedure, we enable the npi model to learn recursive neural programs",0
" in particular, in the explore function, adding a tail recursive call resets and stores the hidden states associated with vertices in a stack-like fashion",1
 the first is the actual model architecture,1
"in this paper, we propose that the key abstraction of recursion is necessary for neural programs to generalize",0
" the npi model has three learnable components: a task-agnostic core, a program-key embedding, and domain-specific encoders that allow the npi to operate in diverse environments",1
6,0
" that is to say, the network does not learn the true program semantics",0
" in order to compensate for this, ppivot and pj require special functions (move pivot lo and move j lo) to properly set them to lo in lines 11 and 12 of the partition function in algorithm 3",1
" consequently, it is infeasible to construct a verification set that covers all cases, and the verification procedure we have described is inadequate",1
we emphasize the overall goal of this work is to enable the learning of a recursive program,1
" the stack operation has the following arguments:
arg 1 (operation): stack push call1, stack push call2, stack pop
stack push call1 pushes lo and pivot−1 toqstacklo andqstackhi",1
 we choose to implement a topological sort task for graphs,1
" the add reduction rule ensures that each call to add only covers two adjacent columns, and so the lstm only ever runs for a fixed number of steps necessary to process these two columns",1
e,1
" qcolor(dag[vactive][childlist[vactive]]) refers to the color of the next child of vactive, used in the evaluation of the condition in the if branch in line 9 of algorithm 2",1
" the general notion of recursion has been an important concept in many domains, including mathematics and computer science",1
 this procedure verifies that all base cases and reductions are handled properly by the model via explicit tests,0
" we implement a quicksort task, in order to demonstrate that recursion helps with learning divide-and-conquer algorithms",0
 we find that recursion makes it easier for the network to learn the right program and generalize to unknown situations,1
" the write operation has the following arguments:
arg 1 (main action): color curr, color next, active start, active neighb, active stack, save, stack push, stack pop, result
color curr colors vactive, color next colors vertex dag[vactive][childlist[vactive]], active start writes pstart to vactive, active neighb writes dag[vactive][childlist[vactive]] to vactive, active stack writes qstack(pstack) to vactive, save writes vactive to vsave, stack push pushes vactive to the top of the stack, stack pop writes a null value to the top of the stack, and result writes vactive to qresult(presult)",1
 there are 2 variables (vactive and vsave); vactive holds the active vertex (as in algorithm 2) and vsave holds the value of vactive before executing line 12 of algorithm 2,1
", an array for quicksort or a dag for topological sort)",0
"
given a verification set, we can then run the model on the verification set to check if the produced traces and results are correct",1
" in this pair, the step input runs a subprogram add1 that has no arguments, and the step output contains a program write that has arguments of out and 1",1
" in our experiments, we only present dag’s as inputs and represent the vertices as values ranging from 1, ",0
"
we would also like to point out that in this paper, we provide as an example one way to train a recursive neural program, by providing a certain training execution trace to the npi model",0
"
it is sufficient to construct problems where every transition between two adjacent columns is covered",0
" 2: initialize lo and hi to be 1 and n, where n is the length of a",1
"
it is sufficient to construct problems where every transition between two adjacent columns is covered",1
" however, note that there is no provable guarantee that the non-recursive learned program will generalize to all inputs, whereas we show later that the recursive learned program has a provable guarantee of perfect generalization",0
" the move operation has the following arguments:
arg 1 (pointer): presult, pstack, pstart, childlist[vactive], childlist[vsave]
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 5 values",0
"program descriptions calls arguments quicksort run the quicksort
routine in place for the array a, for indices from lo to hi non-recursive: partition, stack, write recursive: same as non-recursive version, along with quicksort implicitly: array a to sort, lo, hi
partition runs the partition function",0
 this verification phase only needs to be performed once after training,0
"
we refer the reader to the appendix for the non-recursive trace generating functions",0
 2: initialize an empty stack s and a directed acyclic graph dag to traverse,1
 the sequence of primitive operations (move and write operations) for the non-recursive and recursive versions are exactly the same,0
 the original version of the bubblesort implementation exposes the values within the array,0
"
under the assumption that there are no leading 0’s (except in the case of single digits) and the two numbers to be added have the same number of digits, the verification set for addition contains 20,181 input problems",1
 table 4 also demonstrates that the (verified) recursive program generalizes perfectly,1
"as discussed in section 2, the neural programmer-interpreter (npi) is an instance of a neural programmer architecture and hence it naturally supports recursion",1
" the partial recursion is not enough for perfect generalization, as will be presented later in section 4",1
 we outline how to construct this set,1
 our experiments use a small number of training examples,0
 almost all architectures train on program input/output pairs,1
" thus, any general neural programming architecture supporting such a call structure can be made to support recursion",0
" 9: else 10: function run(e, p2, a2)
a description of the inference procedure is given in algorithm 1",0
"
base cases and reduction rules for quicksort",1
" on the other hand, the non-recursive version has low accuracy, beginning from size 5, and fails completely for graphs of size 8 and beyond",0
"
we also report on generalization results for the non-recursive and recursive versions of this variant of bubble sort",0
"
note that the recursion for quicksort is not purely tail recursive and therefore represents a more complex kind of recursion that is harder to learn than in the previous tasks",0
" the non-recursive and partially recursive versions are unable to sort long arrays, beyond length 8",1
", to change the value of vactive) none described below move move a pointer (e",1
" the pointer at index i3 represents a counter internal to the environment that is incremented once after each pass of the algorithm (one cycle of bubble and reset); when incremented a number of times equal to the length of the array, the flag i3 == length becomes true and terminates the entire algorithm ",1
" this functionality the original npi paper decodes to a program key embedding kt ∈ rk and then computes a program embedding pt+1, which we also did in our implementation, but we omit this for brevity",1
"
as in line 13 of the right-hand side of figure 1",1
" for each task, given the verification set, we check the traces and results of the learned, to-be-verified neural program (described in section 4",0
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,1
" at any given time, the npi has access to the values referred to by two pointers, represented by q(1, i1) and q(1, i2),",1
" twc-1409915, darpa under grant no",0
" m , as described in algorithm 1, processes the sequence of step inputs by using an lstm",1
 we created a program set that reflects the semantics of algorithm 2,0
" the pointer at index i3 represents a counter internal to the environment that is incremented once after each pass of the algorithm (one cycle of bubble and reset); when incremented a number of times equal to the length of the array, the flag i3 == length becomes true and terminates the entire algorithm ",0
" training on the full recursive trace leads to perfect generalization, as shown in section 4",0
 our experiments use a small number of training examples,0
" the add reduction rule ensures that each call to add only covers two adjacent columns, and so the lstm only ever runs for a fixed number of steps necessary to process these two columns",0
"
inside bubble and reset, there are two operations that can be made recursive",0
"
figure 1 shows examples of non-recursive and recursive addition traces",1
" also the dag can have potentially more than one connected component, meaning it is necessary to transition between these components appropriately",1
" 3: 4: function partition(a, lo, hi) 5: pivot = lo 6: for j ∈ [lo, hi− 1] : do 7: if a[j] ≤ a[hi] then 8: swap a[pivot] with a[j] 9: pivot = pivot+ 1 0: swap a[pivot] with a[hi] 11: return pivot 12: 13: function quicksort(a, lo, hi) 14: while slo and shi are not empty: do 15: pop states off slo and shi, writing them to lo and hi",0
" if yes, then this indicates that the learned neural program achieves provably perfect generalization",0
3,0
" in order to claim a proof, we must verify the model produces correct behavior over all base cases and reductions, as described in section 2",1
" any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of national science foundation and darpa",1
 this modification also enables us to sort arrays containing arbitrary comparable elements,0
"
note that the recursion for quicksort is not purely tail recursive and therefore represents a more complex kind of recursion that is harder to learn than in the previous tasks",0
" the swap operation has the following arguments:
arg 1 (swap object 1): ppivot
arg 2 (swap object 2): phi, pj
write",1
 this algorithm is a variant of depth first search,0
" 9: else 10: function run(e, p2, a2)
a description of the inference procedure is given in algorithm 1",0
" a single element of an execution trace consists of a step input-step output pair, which can be synthesized from algorithm 1: this corresponds to, for a given time-step, the step input tuple (e, p, a) and step output tuple (r, p2, a2)",0
 stack pop pushes −∞ values to qstacklo and qstackhi,1
"
it is important to emphasize that at inference time in the npi, the hidden state of the lstm controller is reset (to zero) at each subprogram call, as in line 3 of algorithm 1 (h ← 0)",0
" using this modification, we constructed a verification set consisting of one array of size 10",0
" by iterating with this procedure, and then running p on the input observation set that we obtain for the entry point function, we can obtain v precisely",0
"when constructing a neural network for the purpose of learning a program, there are two orthogonal aspects to consider",0
" the theorem states that for the same sequence of step inputs, the model produces the exact same step output as the target program it aims to learn",0
" while this matches the description from reed & de freitas (2016), we found that this causes an unnecessary blowup in the size of v and makes it much more difficult to construct the verification set",0
"
to construct this set, by using the reference implementation of each subprogram, we construct a mapping between two sets of environment observations: the first set consists of all observations that can occur at the beginning of a particular subprogram’s invocation, and the second set contains the observations at the end of that subprogram",0
"
after finding v , we construct a set of problem inputs which, when executed on p , create exactly the step input sequences which make up v ",1
"
there are 6 pointers (plo, phi, pstacklo, pstackhi, ppivot, pj)",1
"
the dag is represented as an adjacency list where dag[i][j] refers to the j-th child of vertex i",0
" if no white vertex is found, the entire execution is terminated move none write write a value either to environment (e",1
" for each graph size, we test the learned programs on 30 randomly generated dags",1
" note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values, and arg 2 can only take one of 7 values",1
" because the context of the caller is stored on a stack when it calls another program and the callee starts in a fresh context, this enables recursion simply by allowing a program to call itself",1
 this is due to the properties of the training set – we trained on 2 traces synthesized from arrays of length 7 and 1 trace synthesized from an array of length 6,0
" in order to claim a proof, we must verify the model produces correct behavior over all base cases and reductions, as described in section 2",1
" the space of possible programs is extremely large, and composing a program that performs robustly on the infinite space of possible inputs is difficult—in part because it is impractical to obtain enough training examples to easily disambiguate amongst all possible programs",0
" on the other hand, the non-recursive version has low accuracy, beginning from size 5, and fails completely for graphs of size 8 and beyond",0
" for brevity, we refer the reader to the appendix for further details on the program set and non-recursive and recursive trace-generating functions used for topological sort",0
" in particular, given that a recursion is defined by two properties as mentioned before, the base cases and the set of reduction rules, we can prove a recursive neural program generalizes perfectly if we can prove that (1) it performs correctly on the base cases; (2) it learns the reduction rules correctly",0
" for each length, we test each program on 30 randomly generated problems",0
"
we refer the reader to the appendix for the non-recursive trace generating functions",1
" the non-recursive and partially recursive versions are unable to sort long arrays, beyond length 8",0
" in particular, recursion can be implemented as a program calling itself",0
" m , as described in algorithm 1, processes the sequence of step inputs by using an lstm",0
 this verification phase only needs to be performed once after training,0
"
in order to demonstrate that recursion can help learn and generalize better, we trained a non-recursive and recursive model on just a single execution trace generated from a graph containing 5 nodes3 for the topological sort task",1
"
note that the recursion for quicksort is not purely tail recursive and therefore represents a more complex kind of recursion that is harder to learn than in the previous tasks",0
g,1
"
topological sort",1
"
quicksort",0
"
by nature, recursion reduces the complexity of a problem to simpler instances",0
"
training setup",0
", the verification set",1
" for brevity, we refer the reader to the appendix for information about the program set and non-recursive and recursive trace-generating functions for quicksort",0
"
figure 1 shows examples of non-recursive and recursive addition traces",1
" observe that partially recursive does slightly better than non-recursive for the setting in which the length of the array is 3, and that the fully recursive version is able to sort every array given to it",0
"
we construct input problems by splitting into two cases: one case in which the left column contains a null value and another in which the left column does not contain any null values",0
" the core lstm module’s hidden state is preserved over all these add1 calls, and it is difficult to interpret with certainty what happens over longer timesteps without concretely evaluating the lstm with an input of that length",0
"1; and for bubble sort, appendix a",1
 we instantiated recursion for the neural programmerinterpreter by changing the training traces,0
 we choose to implement a topological sort task for graphs,1
", qstacklo or qstackhi) or to pointer (e",1
 we find that recursion makes it easier for the network to learn the right program and generalize to unknown situations,1
" 4: function toposort(dag) 5: while there is still a white vertex u: do 6: color[u] = grey 7: vactive = u 8: do 9: if vactive has a white child v then 10: color[v] = grey 11: push vactive onto s 12: vactive = v 13: else 14: color[vactive] = black 15: write vactive to result 16: if s is empty then pass 17: else pop the top vertex off s and set it to vactive 18: while s is not empty
topological sort",0
" via the recursive call, we effectively forget that the column just added exists, since the recursive call to add starts with a new hidden state for the lstm controller",1
" we performed experiments on the partially recursive version in order to examine what happens when only one recursive call is implemented, when in reality three are required for perfect generalization",0
 ,0
"
bubble sort",0
"
as mentioned in section 1, all approaches to neural programming today fare poorly on this generalization issue",0
g,1
 table 3 presents results on randomly generated arrays of varying length for the learned non-recursive and recursive programs,0
" the verification set can be very small, as we found a 10-element array ([8,2,1,2,0,8,5,8,3,7]) is sufficient to cover all of v ",0
" as a consequence, our learned neural programs empirically achieve perfect generalization from a very small number of training examples",0
" we prove every member of f is added properly, given that subproblems s = {anan−1 + bnbn−1, an−1an−2 + bn−1bn−2, ",1
" each step during an execution of the program does one of three things: (1) another subprogram along with associated arguments is called, as in line 10, (2) the program writes to the environment if it is primitive, as in line 8, or (3) the loop is terminated if the return probability exceeds a threshold α, after which the stack frame is popped and control is returned to the caller",1
" , n , where the dag contains n vertices",1
" although the model is exposed to more and more data, it might learn spurious and overly complex representations of the program, as suggested in zaremba   (2016)",0
"
for topological sort, the domain-specific encoder is
fenc(dag,qcolor, pstack, pstart, vactive, childlist, at)
=mlp ([qcolor(pstart), qcolor(dag[vactive][childlist[vactive]]), pstack == 1, at(1), at(2), at(3)]),
where qcolor ∈ ru×4 is a scratch-pad that contains u rows, each containing one of four colors (white, gray, black, invalid) with one-hot encoding",1
 ,0
" the outputs rt, pt+1, at+1 are used to determine the next action, as described in algorithm 1",0
"
we describe the base cases, reduction rules, and the verification set for each task in appendix a",0
2,1
"
through careful reasoning about the possible set of environment observations created by all valid inputs, and how each of the operations in the execution trace affects the environment, we can construct v using the procedure described in section 3",1
"
an alternative way of representing the environment slice is to expose the values of the absolute vertices to the model; however, this makes it difficult to scale the model to larger graphs, since large vertex values are not seen during training time",0
"
bubble sort",1
 we adapt machinery from the original paper slightly to fit our needs,0
" the theorem states that for the same sequence of step inputs, the model produces the exact same step output as the target program it aims to learn",0
 this verification phase only needs to be performed once after training,0
" as described in the verification procedure, it is possible to prove our learned recursive program generalizes perfectly by testing on an appropriate set of problem inputs, i",1
" table 1 presents results on randomly generated arrays of varying length for the learned non-recursive, partially recursive, and full recursive programs",0
" as aforementioned, the npi model naturally supports recursion",0
we emphasize the overall goal of this work is to enable the learning of a recursive program,1
" finally, we implement recursive traces for our own topological sort and quicksort tasks",1
" 3: 4: function quicksort(a, lo, hi) 5: if lo < hi: then 6: p = partition(a, lo, hi) 7: quicksort(a, lo, p− 1) 8: quicksort(a, p+ 1, hi) 9: 10: function partition(a, lo, hi) 11: pivot = lo 12: for j ∈ [lo, hi− 1] : do 13: if a[j] ≤ a[hi] then 14: swap a[pivot] with a[j] 15: pivot = pivot+ 1 16: swap a[pivot] with a[hi] 17: return pivot
for quicksort, the domain-specific encoder is
fenc(qarray, qstacklo, qstackhi, plo, phi, pstacklo, pstackhi, ppivot, pj , at) =
mlp ([qarray(pj) ≤ qarray(phi), pj == phi, qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1), pstacklo == 1, at(1), at(2), at(3)]),
whereqarray ∈ ru×11 is a scratch-pad that contains u rows, each containing one of 11 values (one of the numbers 0 through 9 or an invalid state)",1
 ,0
"
in order to demonstrate that recursion can help learn and generalize better, we trained a non-recursive and recursive model on just a single execution trace generated from a graph containing 5 nodes3 for the topological sort task",0
" for each task, given the verification set, we check the traces and results of the learned, to-be-verified neural program (described in section 4",1
3,0
"
to perform the verification as described here, it is critical to construct v correctly",0
" we call this set of inputs the verification set, sv ",0
" for purposes of verification, we replace the domain-specific encoder with the following:
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1) ≤ q(1, i2), 1 ≤ i1 ≤ length, 1 ≤ i2 ≤ length, i3 == length, at(1), at(2), at(3)]),which directly exposes which of the two values pointed to is larger",0
" using this modification, we constructed a verification set consisting of one array of size 10",0
"
arg 2 (auxiliary variable): color grey, color black
color grey and color black color the given vertex grey and black, respectively",0
" the add reduction rule ensures that each call to add only covers two adjacent columns, and so the lstm only ever runs for a fixed number of steps necessary to process these two columns",1
 we leave this as future work to devise a verification procedure more appropriate to this setting,0
" to the best of our knowledge, this is the first time verification has been applied to a neural program, providing provable guarantees about its behavior",1
"
the npi accesses an external environment, q, which varies according to the task",1
" also, compared to the bubble pointers in bubble sort, the pointers that perform the comparison for quicksort (the compswap function) are usually not adjacent to each other, making quicksort less local than bubble sort",1
" the move operation has the following arguments:
arg 1 (pointer): presult, pstack, pstart, childlist[vactive], childlist[vsave]
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 5 values",1
"
as a concrete instantiation, we show in this paper that we can enable recursive neural programs in the npi model, and thus enable perfectly generalizable neural programs for tasks such as sorting where the original, non-recursive npi program fails",0
"
training setup",1
" in particular, this condition is satisfied by npi, and thus the npi model naturally supports recursion (even though the authors of npi did not consider this aspect explicitly)",0
" if the program is primitive, the next environmental state et+1 will be affected by pt and at, i",0
"
note that the recursion for quicksort is not purely tail recursive and therefore represents a more complex kind of recursion that is harder to learn than in the previous tasks",0
" as before, the non-recursive learned program lacks a provable guarantee of generalization, whereas we show later that the recursive learned program has one",0
" at end, pointer ppivot is moved to the pivot compswap loop, move pivot lo, move j lo, swap
none
compswap loop runs the for loop inside the partition function compswap, move none compswap compares a[pivot] ≤ a[j]; if so, perform a swap and increment ppivot swap, move none set pivot lo sets ppivot to lo index none none set j lo sets pj to lo index none none set j null sets pj to −∞ none none stack pushes lo/hi states
onto stacks slo and shi according to argument (described below)
write, move described below
move moves pointer one unit up or down none described below swap swaps elements at given array indices none described below write write a value either to stack (e",0
" in particular, given that a recursion is defined by two properties as mentioned before, the base cases and the set of reduction rules, we can prove a recursive neural program generalizes perfectly if we can prove that (1) it performs correctly on the base cases; (2) it learns the reduction rules correctly",0
" full recursion, in addition to making the aforementioned tail recursive call, adds two additional recursive calls; bstep and lshift are made tail recursive",1
" full recursion, in addition to making the aforementioned tail recursive call, adds two additional recursive calls; bstep and lshift are made tail recursive",1
" for example, for the addition task, the model is trained on short inputs and then tested on its ability to sum inputs with much longer numbers of digits",0
"
formally, verification consists of proving the following theorem:
∀i ∈ v,m(i) ⇓ p (i) where i denotes a sequence of step inputs (within one function call), v denotes the set of valid sequences of step inputs, m denotes the neural network model, p denotes the correct program, and p (i) denotes the next step output from the correct program",1
" the recursive version of topological sort solves all graph instances we tried, from graphs of size 5 through 70",1
" given a precise definition of p , it may be possible to automate the generation of v from p in future work",1
" in particular, recursion can be implemented as a program calling itself",0
" on the other hand, the recursive programs have learned the true program semantics",0
"
quicksort",1
 table 3 presents results on randomly generated arrays of varying length for the learned non-recursive and recursive programs,0
g,1
" the npi model has three learnable components: a task-agnostic core, a program-key embedding, and domain-specific encoders that allow the npi to operate in diverse environments",0
we now report on generalization for the varying tasks,1
 npi then outputs the return probability and next program and arguments to execute,1
" by iterating with this procedure, and then running p on the input observation set that we obtain for the entry point function, we can obtain v precisely",0
 these pointers are referred to as bubble pointers,1
" we made the assumption of equivalent lengths in order to parametrize the input format with respect to length, but this assumption can be removed as well",1
" formally, the npi is represented by the following set of equations:
st = fenc(et, at)
ht = flstm(st, pt, ht−1)
rt = fend(ht), pt+1 = fprog(ht), at+1 = farg(ht)
t is a subscript denoting the time-step; fenc is a domain-specific encoder (to be described later) that takes in the environment slice et and arguments at; flstm represents the core module, which takes in the state st generated by fenc, a program embedding pt ∈ rp , and hidden lstm state ht; fend decodes the return probability rt; fprog decodes a program key embedding pt+1;1 and farg decodes arguments at+1",1
" for example, for the addition task, the model is trained on short inputs and then tested on its ability to sum inputs with much longer numbers of digits",0
"
recursion drastically reduces the number of configurations we need to consider during the verification phase and makes the proof tractable, because it introduces structure that eliminates infinitely long sequences of step inputs that would otherwise need to be considered",0
"
after finding v , we construct a set of problem inputs which, when executed on p , create exactly the step input sequences which make up v ",0
" 4: function toposort(dag) 5: while there is still a white vertex u: do 6: color[u] = grey 7: vactive = u 8: do 9: if vactive has a white child v then 10: color[v] = grey 11: push vactive onto s 12: vactive = v 13: else 14: color[vactive] = black 15: write vactive to result 16: if s is empty then pass 17: else pop the top vertex off s and set it to vactive 18: while s is not empty
topological sort",1
" the general notion of recursion has been an important concept in many domains, including mathematics and computer science",0
", to change the value of phi) none described belowstack",1
 the memory updates of these neural programs are so complex and interdependent that it is difficult to reason about the behaviors of the learned neural program under previously unseen situations (such as problems with longer inputs),0
"
there are 6 pointers (plo, phi, pstacklo, pstackhi, ppivot, pj)",1
 stack push call2 pushes pivot + 1 and hi to qstacklo and qstackhi,1
 we created a program set that reflects the semantics of algorithm 2,1
"
move",0
" nevertheless, we would like the model to quickly learn to represent the right semantics of the underlying program from a small number of training examples, not an exhaustive number of them",1
" the recursive version can generalize perfectly to long problems constructed from these components (such as the sum “822+233”, where “8+2” and “2+3” are in the training set), but the non-recursive version fails to sum these long problems properly",1
" for the recursive formulation of addition, we analytically construct the set of input problems that cover all base cases and reduction rules",1
" if the program is primitive, the next environmental state et+1 will be affected by pt and at, i",0
"
we describe the base cases, reduction rules, and the verification set for each task in appendix a",0
"
in order to demonstrate that recursion can help learn and generalize better, for addition, we trained only on traces for 5 arbitrarily chosen 1-digit addition sum examples",0
" the second is the training procedure, which consists of the form of the training data and the optimization process",1
3,0
"
base cases and reduction rules for addition",0
" table 1 presents results on randomly generated arrays of varying length for the learned non-recursive, partially recursive, and full recursive programs",0
" even though the earlier version also generalized just as well in practice, relatively small differences in the formulation of the traces and the environment observations can drastically change the difficulty of verification",1
 18: push lo and p− 1 to slo and shi,1
" for non-primitive subprograms, we can update the observation-to-observation mapping currently associated with the subprogram and then apply that mapping to the current set",0
" given a precise definition of p , it may be possible to automate the generation of v from p in future work",1
"
for addition, we analytically determine the verification set",1
" we performed experiments on the partially recursive version in order to examine what happens when only one recursive call is implemented, when in reality three are required for perfect generalization",0
"
move",0
" for the recursive formulation of addition, we analytically construct the set of input problems that cover all base cases and reduction rules",1
"
bubble sort",1
" if yes, then this indicates that the learned neural program achieves provably perfect generalization",0
" via the recursive call, we effectively forget that the column just added exists, since the recursive call to add starts with a new hidden state for the lstm controller",1
"
training setup",0
" for these programs, we re-use the appropriate program sets (the associated subprograms), and we refer the reader to the appendix of reed & de freitas (2016) for further details on the subprograms used in addition and bubble sort",0
" also the dag can have potentially more than one connected component, meaning it is necessary to transition between these components appropriately",0
" as an application, we incorporate recursion into the neural programmer-interpreter architecture and consider four sample tasks: grade-school addition, bubble sort, topological sort, and quicksort",0
"
directed acyclic graphs are structurally more diverse than inputs in the two tasks of grade-school addition and bubble sort",1
" in particular, given that a recursion is defined by two properties as mentioned before, the base cases and the set of reduction rules, we can prove a recursive neural program generalizes perfectly if we can prove that (1) it performs correctly on the base cases; (2) it learns the reduction rules correctly",1
 the sequence of primitive operations (move and write operations) for the non-recursive and recursive versions are exactly the same,0
" to the best of our knowledge, this is the first time verification has been applied to a neural program, providing provable guarantees about its behavior",0
"
the training set for topological sort contains 6 traces, with one synthesized from a graph of size 5 and the rest synthesized from graphs of size 7",0
" as aforementioned, the npi model naturally supports recursion",1
" observe that partially recursive does slightly better than non-recursive for the setting in which the length of the array is 3, and that the fully recursive version is able to sort every array given to it",0
", the trace corresponding to the problem “109 + 101”)",1
"
we would also like to point out that in this paper, we provide as an example one way to train a recursive neural program, by providing a certain training execution trace to the npi model",1
" in particular, we construct new training traces which explicitly contain recursive elements and show that with this type of trace, npi easily learns recursive programs",0
" if it is too small, then execution of the program on some input might require evaluation of m(i) on some i /∈ v , and so the behavior ofm(i) might deviate from p (i)",0
3,1
"
it is important to emphasize that at inference time in the npi, the hidden state of the lstm controller is reset (to zero) at each subprogram call, as in line 3 of algorithm 1 (h ← 0)",1
" we implement a quicksort task, in order to demonstrate that recursion helps with learning divide-and-conquer algorithms",1
", the trace corresponding to the problem “109 + 101”)",0
" in all experiments, α is set to 0",0
" on the other hand, the recursive programs have learned the true program semantics",1
g,0
" in this architecture, we consider a core controller, e",0
we emphasize the overall goal of this work is to enable the learning of a recursive program,0
" thus, any general neural programming architecture supporting such a call structure can be made to support recursion",0
" the outputs rt, pt+1, at+1 are used to determine the next action, as described in algorithm 1",1
 2: initialize an empty stack s and a directed acyclic graph dag to traverse,0
" twc-1409915, darpa under grant no",1
" finally, we implement recursive traces for our own topological sort and quicksort tasks",1
"
to construct this set, by using the reference implementation of each subprogram, we construct a mapping between two sets of environment observations: the first set consists of all observations that can occur at the beginning of a particular subprogram’s invocation, and the second set contains the observations at the end of that subprogram",0
"1; and for bubble sort, appendix a",0
" k is set to 11, to represent the range of 10 possible digits, along with a token representing the end of input",1
 these pointers are referred to as bubble pointers,0
"
the training set for quicksort contains 4 traces, synthesized from arrays of length 5",0
" the write operation has the following arguments:
arg 1 (object to write): env stack lo, env stack hi, phi, plo
env stack lo and env stack hi represent qstacklo(pstacklo) and qstackhi(pstackhi), respectively",1
"
through careful reasoning about the possible set of environment observations created by all valid inputs, and how each of the operations in the execution trace affects the environment, we can construct v using the procedure described in section 3",0
" each step during an execution of the program does one of three things: (1) another subprogram along with associated arguments is called, as in line 10, (2) the program writes to the environment if it is primitive, as in line 8, or (3) the loop is terminated if the return probability exceeds a threshold α, after which the stack frame is popped and control is returned to the caller",0
" at any given time, the npi has access to the values referred to by two pointers, represented by q(1, i1) and q(1, i2),",0
 stack push call2 pushes pivot + 1 and hi to qstacklo and qstackhi,1
"
bubble sort",1
"
algorithm 1 neural programming inference 1: inputs: environment observation e, program p, arguments a, stop threshold α 2: function run(e, p, a) 3: h← 0, r ← 0 4: while r < α do 5: s← fenc(e, a), h← flstm(s, p, h) 6: r ← fend(h), p2 ← fprog(h), a2 ← farg(h) 7: if p is a primitive function then 8: e← fenv(e, p, a)",0
"
figure 1 shows examples of non-recursive and recursive addition traces",0
 this is due to the properties of the training set – we trained on 2 traces synthesized from arrays of length 7 and 1 trace synthesized from an array of length 6,1
" in this pair, the step input runs a subprogram add1 that has no arguments, and the step output contains a program write that has arguments of out and 1",0
 some functional programming languages go so far as not to define any looping constructs but rely solely on recursion to enable repeated execution of the same code,0
 the core controller acts as a dispatcher for the programs,1
" we make the trace recursive by adding a tail recursive call into the trace for the add program after calling add1 and lshift, the original paper uses k = 10, but we found it necessary to augment the range with an end token, in order to terminate properly",1
" the verification set can be very small, as we found a 10-element array ([8,2,1,2,0,8,5,8,3,7]) is sufficient to cover all of v ",0
", pstart or childlist[vactive]) up or down none described belowwrite",1
" nevertheless, we would like the model to quickly learn to represent the right semantics of the underlying program from a small number of training examples, not an exhaustive number of them",1
" we then construct problem configurations that span all possible valid environment states (for instance, in order to force the carry bit in a column to be 1, one can add the sum “1+9” in the column to the right)",1
" this is highly undesirable, since being able to provide the correct answer in all possible settings is one of the most important aspects of any learned neural program",0
" however, note that there is no provable guarantee that the non-recursive learned program will generalize to all inputs, whereas we show later that the recursive learned program has a provable guarantee of perfect generalization",0
" this is possible if and only if the graph has no directed cycles; that is to say, it must be a directed acyclic graph (dag)",0
" the only model, to our knowledge, that does not train on input-output pairs is the neural programmer-interpreter (reed & de freitas, 2016), which trains on synthetic execution traces",1
 ,0
" to make an analogy to mdps, this procedure is analogous to how value iteration obtains the correct value for each state starting from any initialization",0
"
by reasoning about the possible set of environment observations created by all valid inputs, we construct v using the procedure described in section 3",0
" the corresponding edge list is [(1, 2), (1, 5), (2, 4), (2, 5), (3, 5)]",0
 the maximum problem length in this training set is 3 (e,1
"
note that v should also contain the necessary reductions, which corresponds to making the recursive calls at the correct time, as indicated by p ",0
" et+1 ∼ fenv(et, pt, at)",0
training neural networks to synthesize robust programs from a small number of examples is a challenging task,0
" the assumption of leading 0’s can be easily removed, at the cost of slightly increasing the size of the verification set",0
"as there is no public implementation of npi, we implemented a version of it in keras that is as faithful to the paper as possible",0
" finally, we implement recursive traces for our own topological sort and quicksort tasks",1
 the sequence of primitive operations (move and write operations) for the non-recursive and recursive versions are exactly the same,1
" table 1 presents results on randomly generated arrays of varying length for the learned non-recursive, partially recursive, and full recursive programs",1
 ,0
" thus, recursion helps decompose a problem and makes it easier to reason about a program’s behavior for previously unseen situations such as longer inputs",1
 there is a (changing) list of neural programs used to accomplish a given task,0
 2: initialize an empty stack s and a directed acyclic graph dag to traverse,1
" we then construct problem configurations that span all possible valid environment states (for instance, in order to force the carry bit in a column to be 1, one can add the sum “1+9” in the column to the right)",0
" in particular, we construct new training traces which explicitly contain recursive elements and show that with this type of trace, npi easily learns recursive programs",0
" the assumption of leading 0’s can be easily removed, at the cost of slightly increasing the size of the verification set",0
" the only model, to our knowledge, that does not train on input-output pairs is the neural programmer-interpreter (reed & de freitas, 2016), which trains on synthetic execution traces",0
" more specifically, when m is trained on simpler inputs, such as inputs of a small length, the generalization metric evaluates how well m will do on more complex inputs, such as inputs of much longer length",1
"
for addition, we analytically determine the verification set",0
 the degree for any vertex in the dag is variable,1
" a topological sort is a linear ordering of vertices such that for every directed edge (u, v) from u to v, u comes before v in the ordering",0
"
arg 2 (auxiliary variable): color grey, color black
color grey and color black color the given vertex grey and black, respectively",0
" any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of national science foundation and darpa",0
" also, compared to the bubble pointers in bubble sort, the pointers that perform the comparison for quicksort (the compswap function) are usually not adjacent to each other, making quicksort less local than bubble sort",0
 some functional programming languages go so far as not to define any looping constructs but rely solely on recursion to enable repeated execution of the same code,1
this material is in part based upon work supported by the national science foundation under grant no,0
" for bubble sort, the domain-specific encoder is
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1), q(1, i2), i3 == length, at(1), at(2), at(3)]),
where the environment q ∈ r1×n×k is a scratch-pad that contains 1 row, to represent the state of the array as sorting proceeds in-place, andn columns",0
 ,1
 table 4 also demonstrates that the (verified) recursive program generalizes perfectly,0
" both the non-recursive and recursive learned programs generalize on all input lengths we tried, up to 5000 digits",1
"
one common strategy to improve generalization is to use curriculum learning, where the model is trained on inputs of gradually increasing complexity",1
" when the program is called, the current context including the caller’s memory state is stored on a stack; when the program returns, the stored context is popped off the stack to resume execution in the previous caller’s context",0
"
to perform the verification as described here, it is critical to construct v correctly",1
", an lstm in npi’s case, but possibly other networks in different cases",1
" in particular, we construct new training traces which explicitly contain recursive elements and show that with this type of trace, npi easily learns recursive programs",0
" 4: function toposort(dag) 5: while there is still a white vertex u: do 6: color[u] = grey 7: vactive = u 8: do 9: if vactive has a white child v then 10: color[v] = grey 11: push vactive onto s 12: vactive = v 13: else 14: color[vactive] = black 15: write vactive to result 16: if s is empty then pass 17: else pop the top vertex off s and set it to vactive 18: while s is not empty
topological sort",1
e,0
" if a white vertex is found, set pstart to point to it; this signifies the start of a traversal of a new connected component",1
 we created a program set that reflects the semantics of algorithm 2,0
"
bubble sort",0
" the write operation has the following arguments:
arg 1 (object to write): env stack lo, env stack hi, phi, plo
env stack lo and env stack hi represent qstacklo(pstacklo) and qstackhi(pstackhi), respectively",1
 the degree for any vertex in the dag is variable,0
" we also hope to decrease supervision, for example by training with only partial or non-recursive traces, and to develop novel neural programming architectures integrated directly with a notion of recursion",0
" the recursive version of topological sort solves all graph instances we tried, from graphs of size 5 through 70",0
" each time a subprogram is called, the stack depth increases",0
 we describe the recursive re-formulation of traces for two tasks from the original npi paper—grade-school addition and bubble sort,1
 the original version of the bubblesort implementation exposes the values within the array,1
 this modification also enables us to sort arrays containing arbitrary comparable elements,0
"
given a verification set, we can then run the model on the verification set to check if the produced traces and results are correct",1
"
the three environment observations aid with control flow in algorithm 2",0
" a topological sort is a linear ordering of vertices such that for every directed edge (u, v) from u to v, u comes before v in the ordering",0
", the trace corresponding to the problem “109 + 101”)",1
"
the training set for bubble sort contains 100 traces, with maximum problem length of 2 (e",1
" ppivot and pj point to the pivot and j indices of the array, used in the partition function in algorithm 3",1
" in particular, recursion can be implemented as a program calling itself",1
 ,0
" we make the trace recursive by adding a tail recursive call into the trace for the add program after calling add1 and lshift, the original paper uses k = 10, but we found it necessary to augment the range with an end token, in order to terminate properly",0
 ,1
 recursion enables provable guarantees on neural programs’ behavior without needing to exhaustively enumerate all possible inputs to the programs,1
 the first is the actual model architecture,0
" in particular, recursion can be implemented as a program calling itself",1
" it is straightforward to deal with all other operations, which do not induce partial environment states",1
 18: push lo and p− 1 to slo and shi,0
", the trace corresponding to the array [3,2])",1
" this is highly undesirable, since being able to provide the correct answer in all possible settings is one of the most important aspects of any learned neural program",0
"
through careful reasoning about the possible set of environment observations created by all valid inputs, and how each of the operations in the execution trace affects the environment, we can construct v using the procedure described in section 3",0
" for each length, we test each program on 30 randomly generated problems",1
", to color a vertex) or variable (e",0
 ,1
"
figure 1 shows examples of non-recursive and recursive addition traces",0
" a single element of an execution trace consists of a step input-step output pair, which can be synthesized from algorithm 1: this corresponds to, for a given time-step, the step input tuple (e, p, a) and step output tuple (r, p2, a2)",0
"
figure 1 shows examples of non-recursive and recursive addition traces",1
 this modification also enables us to sort arrays containing arbitrary comparable elements,1
" in this version of curriculum learning, even though the inputs are gradually becoming more complex, the semantics of the program is succinct and does not change",0
" the stack operation has the following arguments:
arg 1 (operation): stack push call1, stack push call2, stack pop
stack push call1 pushes lo and pivot−1 toqstacklo andqstackhi",1
" provably perfect generalization implies the model will behave correctly, given any valid input",0
" more specifically, when m is trained on simpler inputs, such as inputs of a small length, the generalization metric evaluates how well m will do on more complex inputs, such as inputs of much longer length",0
" as described in the verification procedure, it is possible to prove our learned recursive program generalizes perfectly by testing on an appropriate set of problem inputs, i",1
 pstacklo and pstackhi point to the top (empty) positions in qstacklo and qstackhi,0
" we call this set of inputs the verification set, sv ",0
"
formally, verification consists of proving the following theorem:
∀i ∈ v,m(i) ⇓ p (i) where i denotes a sequence of step inputs (within one function call), v denotes the set of valid sequences of step inputs, m denotes the neural network model, p denotes the correct program, and p (i) denotes the next step output from the correct program",1
" provably perfect generalization implies the model will behave correctly, given any valid input",1
" in fact, we argue that recursion is an essential element for neural programs to generalize, and makes it tractable to prove the generalization of neural programs",1
", the trace corresponding to the array [3,2])",1
 we adapt machinery from the original paper slightly to fit our needs,0
 ,1
"
is critical for implementing recursion, since it permits us to restrict our attention to the currently relevant recursive call, ignoring irrelevant details about other contexts",0
 we instantiated recursion for the neural programmerinterpreter by changing the training traces,1
", to color a vertex) or variable (e",1
 this makes it difficult to reason about what the model will do when given complex inputs,1
" our results show that for all learned, to-be-verified neural programs, they all produced the same traces as those produced by p on the verification set",1
" for tasks other than addition, it is difficult to analytically determine the verification set, so instead, we randomly generate input candidates until they completely cover the base cases and reduction rules",1
" the add reduction rule ensures that each call to add only covers two adjacent columns, and so the lstm only ever runs for a fixed number of steps necessary to process these two columns",0
as mentioned in section 2,1
"
we propose and describe our verification procedure",1
"
we propose and describe our verification procedure",0
"
grade-school addition",1
" for non-primitive subprograms, we can update the observation-to-observation mapping currently associated with the subprogram and then apply that mapping to the current set",0
g,1
" the npi model has three learnable components: a task-agnostic core, a program-key embedding, and domain-specific encoders that allow the npi to operate in diverse environments",0
" in our experiments, we only present dag’s as inputs and represent the vertices as values ranging from 1, ",1
"
inside bubble and reset, there are two operations that can be made recursive",1
"as discussed in section 2, the neural programmer-interpreter (npi) is an instance of a neural programmer architecture and hence it naturally supports recursion",0
" for instance, for recursive addition, consider the family f of addition problems anan−1 ",1
 the memory updates of these neural programs are so complex and interdependent that it is difficult to reason about the behaviors of the learned neural program under previously unseen situations (such as problems with longer inputs),1
"
note that the recursion for quicksort is not purely tail recursive and therefore represents a more complex kind of recursion that is harder to learn than in the previous tasks",0
"
in what follows, we describe the way in which we constructed npi training traces so as to make them contain recursive elements and thus enable npi to learn recursive programs",1
"
by reasoning about the possible set of environment observations created by all valid inputs, we construct v using the procedure described in section 3",1
" the assumption of leading 0’s can be easily removed, at the cost of slightly increasing the size of the verification set",0
" consequently, it is infeasible to construct a verification set that covers all cases, and the verification procedure we have described is inadequate",0
"
move",0
" the recursive version can generalize perfectly to long problems constructed from these components (such as the sum “822+233”, where “8+2” and “2+3” are in the training set), but the non-recursive version fails to sum these long problems properly",0
" without using a recursive program, such a proof is not possible, because the non-recursive program runs on an arbitrarily long addition problem that creates correspondingly long sequences of step inputs; in the non-recursive formulation of addition, add calls add1 a number of times that is dependent on the length of the input",1
 ,1
"
we refer the reader to the appendix for the non-recursive trace generating functions",0
" it is straightforward to deal with all other operations, which do not induce partial environment states",0
" the move operation has the following arguments:
arg 1 (pointer): pstacklo, pstackhi, pj , ppivot
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values",1
" if it is too large, then the semantics of p might not be well-defined on some elements in v , or the spurious step input sequences may not be reachable from any valid problem input (e",0
" this functionality the original npi paper decodes to a program key embedding kt ∈ rk and then computes a program embedding pt+1, which we also did in our implementation, but we omit this for brevity",0
 the core controller acts as a dispatcher for the programs,1
" table 1 presents results on randomly generated arrays of varying length for the learned non-recursive, partially recursive, and full recursive programs",0
 this procedure verifies that all base cases and reductions are handled properly by the model via explicit tests,1
", to color a vertex) or variable (e",0
" m is considered to have perfect generalization if m can give the right answer for any input, such as inputs of arbitrary length",0
"6) on the verification set, and ensure they match the traces produced by the true program p ",1
" an example of part of an addition task trace, written in shorthand, is given in figure 1",0
g,1
" in computer science, recursion (as opposed to iteration) involves solving a larger problem by combining solutions to smaller instances of the same problem",1
"
in this paper, we propose that recursion is an important concept for neural programs as well",1
" the single-digit multiplication task in zaremba   (2016), the bubble sort task in reed & de freitas (2016), and the graph tasks in graves   (2016))",1
" for example, a step input-step output pair in lines 2 and 3 of the left-hand side of figure 1 is (add1, write out 1)",1
2,0
 recursion can be implemented differently for different neural programming models,0
"
grade-school addition",0
" we made the assumption of equivalent lengths in order to parametrize the input format with respect to length, but this assumption can be removed as well",0
" in order to claim a proof, we must verify the model produces correct behavior over all base cases and reductions, as described in section 2",0
" empirically, existing models suffer from a common limitation—generalization becomes poor beyond a threshold level of complexity",1
 we created a program set that reflects the semantics of algorithm 2,0
"
the training set for bubble sort contains 100 traces, with maximum problem length of 2 (e",0
"
quicksort",0
"
the same set of problems was used to generate the training traces for all formulations of the task, for non-recursive and recursive versions",1
" the 4 environment observations aid with control flow; qstacklo(pstacklo− 1) < qstackhi(pstackhi− 1) implements the lo < hi comparison in line 5 of algorithm 3, pstacklo == 1 checks if the stacks are empty in line 18 of algorithm 4, and the other observations (all involving ppivot or pj) deal with logic in the partition function",0
 this algorithm is a variant of depth first search,0
"
after finding v , we construct a set of problem inputs which, when executed on p , create exactly the step input sequences which make up v ",1
" for each graph size, we test the learned programs on 30 randomly generated dags",0
g,0
" each step during an execution of the program does one of three things: (1) another subprogram along with associated arguments is called, as in line 10, (2) the program writes to the environment if it is primitive, as in line 8, or (3) the loop is terminated if the return probability exceeds a threshold α, after which the stack frame is popped and control is returned to the caller",0
"
it is important to emphasize that at inference time in the npi, the hidden state of the lstm controller is reset (to zero) at each subprogram call, as in line 3 of algorithm 1 (h ← 0)",0
" here as a concrete and general example, we consider a general neural programming architecture (npa), similar to neural programmer-interpreter (npi) in reed & de freitas (2016)",0
"
the three environment observations aid with control flow in algorithm 2",0
" the 4 environment observations aid with control flow; qstacklo(pstacklo− 1) < qstackhi(pstackhi− 1) implements the lo < hi comparison in line 5 of algorithm 3, pstacklo == 1 checks if the stacks are empty in line 18 of algorithm 4, and the other observations (all involving ppivot or pj) deal with logic in the partition function",1
" the partial recursion is not enough for perfect generalization, as will be presented later in section 4",0
"
bubble sort",1
"
as a concrete instantiation, we show in this paper that we can enable recursive neural programs in the npi model, and thus enable perfectly generalizable neural programs for tasks such as sorting where the original, non-recursive npi program fails",0
"
as in line 13 of the right-hand side of figure 1",0
"
training setup",0
" the pointers at index i1 and i2 are used to compare the pair of numbers considered during the bubble sweep, swapping them if the number at i1 is greater than that in i2",1
 we created a program set that reflects the semantics of algorithm 2,0
" 3: 4: function partition(a, lo, hi) 5: pivot = lo 6: for j ∈ [lo, hi− 1] : do 7: if a[j] ≤ a[hi] then 8: swap a[pivot] with a[j] 9: pivot = pivot+ 1 0: swap a[pivot] with a[hi] 11: return pivot 12: 13: function quicksort(a, lo, hi) 14: while slo and shi are not empty: do 15: pop states off slo and shi, writing them to lo and hi",0
" in this pair, the step input runs a subprogram add1 that has no arguments, and the step output contains a program write that has arguments of out and 1",0
" in particular, we construct new training traces which explicitly contain recursive elements and show that with this type of trace, npi easily learns recursive programs",0
"
the non-recursive trace loops on cycles of add1 and lshift",0
" the arrow in the theorem refers to evaluation, as in big-step semantics",1
" a topological sort is a linear ordering of vertices such that for every directed edge (u, v) from u to v, u comes before v in the ordering",0
" for non-primitive subprograms, we can update the observation-to-observation mapping currently associated with the subprogram and then apply that mapping to the current set",0
"
we note that for tasks with very large input domains, such as ones involving mnist digits or speech samples, the state space of base cases and reduction rules could be prohibitively large, possibly infinite",0
"
in what follows, we describe the way in which we constructed npi training traces so as to make them contain recursive elements and thus enable npi to learn recursive programs",1
" furthermore, given a verification input set that covers all base cases and reduction rules, we can formally prove that the learned neural programs achieve perfect generalization after verifying its behavior on the verification input set",0
" note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values, and arg 2 can only take one of 7 values",1
"
we propose and describe our verification procedure",0
" both the non-recursive and recursive learned programs generalize on all input lengths we tried, up to 5000 digits",0
" for problems where the base cases may be extremely large or infinite, such as certain forms of motor control, recursion can still help reduce the problem of generalization to these two aspects and make the generalization problem significantly simpler to handle and reason about",0
training neural networks to synthesize robust programs from a small number of examples is a challenging task,1
"
under the assumption that there are no leading 0’s (except in the case of single digits) and the two numbers to be added have the same number of digits, the verification set for addition contains 20,181 input problems",0
this material is in part based upon work supported by the national science foundation under grant no,1
" for instance, for recursive addition, consider the family f of addition problems anan−1 ",1
" as aforementioned, the npi model naturally supports recursion",0
"
we describe the base cases, reduction rules, and the verification set for each task in appendix a",1
 we choose to implement a topological sort task for graphs,1
" for purposes of verification, we replace the domain-specific encoder with the following:
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1) ≤ q(1, i2), 1 ≤ i1 ≤ length, 1 ≤ i2 ≤ length, i3 == length, at(1), at(2), at(3)]),which directly exposes which of the two values pointed to is larger",1
" the learned recursive program is different from neural programs learned in all previous work in an important aspect: previous approaches do not explicitly incorporate this abstraction, and hence generalize poorly, whereas our learned neural programs incorporate recursion and achieve perfect generalization",0
" lshift moves the four pointers to the left, to move to the next column",0
" training on the full recursive trace leads to perfect generalization, as shown in section 4",1
 table 3 presents results on randomly generated arrays of varying length for the learned non-recursive and recursive programs,1
"
by reasoning about the possible set of environment observations created by all valid inputs, we construct v using the procedure described in section 3",0
 ,1
" for these programs, we re-use the appropriate program sets (the associated subprograms), and we refer the reader to the appendix of reed & de freitas (2016) for further details on the subprograms used in addition and bubble sort",0
" the swap operation has the following arguments:
arg 1 (swap object 1): ppivot
arg 2 (swap object 2): phi, pj
write",0
"
we would also like to point out that in this paper, we provide as an example one way to train a recursive neural program, by providing a certain training execution trace to the npi model",0
" childlist ∈ nu is a vector of pointers, where childlist[i] points to the next child under consideration for vertex i",0
" for brevity, we refer the reader to the appendix for further details on the program set and non-recursive and recursive trace-generating functions used for topological sort",0
" fa8750-15-2-0104, and berkeley deep drive",0
"program descriptions calls arguments toposort perform topological
sort on graph traverse, next start, write, move
none
traverse traverse graph until stack is empty check child, explore none check child check if a white child exists; if so, set childlist[vactive] to point to it move none explore repeatedly traverse subgraphs until stack is empty stack, check child, write, move none stack interact with stack, either pushing or popping write, move push, pop next start move pstart until reaching a white vertex",1
 b1b0 where no carry operations occur,0
" for many problems, the base cases and reduction rules usually consist of a finite (often small) number of cases",0
" the non-recursive and partially recursive versions are unable to sort long arrays, beyond length 8",1
" qcolor(dag[vactive][childlist[vactive]]) refers to the color of the next child of vactive, used in the evaluation of the condition in the if branch in line 9 of algorithm 2",0
"
we describe the base cases, reduction rules, and the verification set for each task in appendix a",0
" the add reduction rule ensures that each call to add only covers two adjacent columns, and so the lstm only ever runs for a fixed number of steps necessary to process these two columns",0
e,0
3,0
" this functionality the original npi paper decodes to a program key embedding kt ∈ rk and then computes a program embedding pt+1, which we also did in our implementation, but we omit this for brevity",0
"
the operations we need to be concerned most about are carry and lshift, which induce partial environment states spanning two columns",1
we emphasize the overall goal of this work is to enable the learning of a recursive program,0
" in this version of curriculum learning, even though the inputs are gradually becoming more complex, the semantics of the program is succinct and does not change",0
 indentation indicates the stack is one level deeper than before,1
"
the non-recursive trace loops on cycles of bubble and reset, which logically represents one bubble sweep through the array and reset of the two bubble pointers to the very beginning of the array, respectively",0
" the corresponding edge list is [(1, 2), (1, 5), (2, 4), (2, 5), (3, 5)]",1
" we can obtain this mapping by first considering the possible observations that can arise at the beginning of the entry function (add, bubblesort, toposort, and quicksort) for some valid program input, and iteratively applying the observation-to-observation mapping implied by the reference implementation’s step output at that point in the execution",0
" the general notion of recursion has been an important concept in many domains, including mathematics and computer science",0
" m is considered to have perfect generalization if m can give the right answer for any input, such as inputs of arbitrary length",1
g,1
" to make an analogy to mdps, this procedure is analogous to how value iteration obtains the correct value for each state starting from any initialization",0
" the architecture usually possesses some form of memory, which could be internal (such as the hidden state of a recurrent neural network) or external (such as a discrete “scratch pad” or a memory block with differentiable access)",0
" recall that this verification procedure cannot be performed for the non-recursive versions, since the propagation of the hidden state in the core lstm module makes reasoning difficult and so we would need to check an unbounded number of examples",1
" in fact, it is one of our future directions to explore new ways to train a recursive neural program without providing explicit training execution traces or with only partial or non-recursive traces",1
 b1b0 where no carry operations occur,0
" the partial recursion is not enough for perfect generalization, as will be presented later in section 4",1
" the add reduction rule ensures that each call to add only covers two adjacent columns, and so the lstm only ever runs for a fixed number of steps necessary to process these two columns",1
" if the step output specifies a primitive function call, we need to reason about how it can affect the environment so as to change the observation in the next step input",0
" the pointers at index i1 and i2 are used to compare the pair of numbers considered during the bubble sweep, swapping them if the number at i1 is greater than that in i2",0
" each time a subprogram is called, the stack depth increases",1
"
inside bubble and reset, there are two operations that can be made recursive",0
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,0
"
the training set for bubble sort contains 100 traces, with maximum problem length of 2 (e",0
" the pointer at index i3 represents a counter internal to the environment that is incremented once after each pass of the algorithm (one cycle of bubble and reset); when incremented a number of times equal to the length of the array, the flag i3 == length becomes true and terminates the entire algorithm ",0
" in computer science, recursion (as opposed to iteration) involves solving a larger problem by combining solutions to smaller instances of the same problem",0
" this makes it so that we only need to consider the vertices in the subgraph that are currently relevant for computing the sort, allowing simpler reasoning about behavior for large graphs",0
we emphasize the overall goal of this work is to enable the learning of a recursive program,0
" the architecture usually possesses some form of memory, which could be internal (such as the hidden state of a recurrent neural network) or external (such as a discrete “scratch pad” or a memory block with differentiable access)",0
" the write operation has the following arguments:
arg 1 (object to write): env stack lo, env stack hi, phi, plo
env stack lo and env stack hi represent qstacklo(pstacklo) and qstackhi(pstackhi), respectively",0
g,0
 the core controller acts as a dispatcher for the programs,0
", the trace corresponding to the problem “109 + 101”)",1
" in order to compensate for this, ppivot and pj require special functions (move pivot lo and move j lo) to properly set them to lo in lines 11 and 12 of the partition function in algorithm 3",1
"
note that the training set can often be considerably smaller than the verification set, and despite this, the learned model can still pass the entire verification set",0
" the stack operation has the following arguments:
arg 1 (operation): stack push call1, stack push call2, stack pop
stack push call1 pushes lo and pivot−1 toqstacklo andqstackhi",0
"as discussed in section 2, the neural programmer-interpreter (npi) is an instance of a neural programmer architecture and hence it naturally supports recursion",0
 pstacklo and pstackhi point to the top (empty) positions in qstacklo and qstackhi,0
" finally, we implement recursive traces for our own topological sort and quicksort tasks",0
" k is set to 11, to denote the range of possible numbers (0 through 9), along with the start/end token (represented with the same encoding) which is observed when a pointer reaches beyond the bounds of the input",1
" as before, the non-recursive learned program lacks a provable guarantee of generalization, whereas we show later that the recursive learned program has one",1
"
by nature, recursion reduces the complexity of a problem to simpler instances",0
" plo and phi point to the lo and hi indices of the array, as in algorithm 3",1
"
algorithm 2 shows the topological sort task of interest",0
3,0
" in this pair, the step input runs a subprogram add1 that has no arguments, and the step output contains a program write that has arguments of out and 1",1
this material is in part based upon work supported by the national science foundation under grant no,1
" the corresponding edge list is [(1, 2), (1, 5), (2, 4), (2, 5), (3, 5)]",1
" at end, pointer ppivot is moved to the pivot compswap loop, move pivot lo, move j lo, swap
none
compswap loop runs the for loop inside the partition function compswap, move none compswap compares a[pivot] ≤ a[j]; if so, perform a swap and increment ppivot swap, move none set pivot lo sets ppivot to lo index none none set j lo sets pj to lo index none none set j null sets pj to −∞ none none stack pushes lo/hi states
onto stacks slo and shi according to argument (described below)
write, move described below
move moves pointer one unit up or down none described below swap swaps elements at given array indices none described below write write a value either to stack (e",1
" in future work, we would like to decrease supervision and construct models that are capable of coming up with recursive abstractions themselves",0
", qstacklo or qstackhi) or to pointer (e",0
" the write operation has the following arguments:
arg 1 (object to write): env stack lo, env stack hi, phi, plo
env stack lo and env stack hi represent qstacklo(pstacklo) and qstackhi(pstackhi), respectively",0
3,1
" empirically, existing models suffer from a common limitation—generalization becomes poor beyond a threshold level of complexity",1
" without using a recursive program, such a proof is not possible, because the non-recursive program runs on an arbitrarily long addition problem that creates correspondingly long sequences of step inputs; in the non-recursive formulation of addition, add calls add1 a number of times that is dependent on the length of the input",0
" given verification sets that cover all the base cases and reduction rules, we can provide proofs that these learned programs generalize perfectly",0
g,0
", to change the value of phi) none described belowstack",0
" in computer science, recursion (as opposed to iteration) involves solving a larger problem by combining solutions to smaller instances of the same problem",1
" the move operation has the following arguments:
arg 1 (pointer): pstacklo, pstackhi, pj , ppivot
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values",1
"
grade school addition",0
" in this version, there is a dependence on length: bstep and lshift are called a number of times equivalent to one less than the length of the input array, in bubble and reset respectively",0
" if no white vertex is found, the entire execution is terminated move none write write a value either to environment (e",0
" as with the original npi architecture, the experiments for this paper always used a 3-tuple of integers at = (at(1), at(2), at(3))",1
 this makes it difficult to reason about what the model will do when given complex inputs,1
" we make the trace recursive by adding a tail recursive call into the trace for the add program after calling add1 and lshift, the original paper uses k = 10, but we found it necessary to augment the range with an end token, in order to terminate properly",0
"
we note that for tasks with very large input domains, such as ones involving mnist digits or speech samples, the state space of base cases and reduction rules could be prohibitively large, possibly infinite",1
" as with the others, we apply the procedure described in section 3",1
" the write operation has the following arguments:
arg 1 (main action): color curr, color next, active start, active neighb, active stack, save, stack push, stack pop, result
color curr colors vactive, color next colors vertex dag[vactive][childlist[vactive]], active start writes pstart to vactive, active neighb writes dag[vactive][childlist[vactive]] to vactive, active stack writes qstack(pstack) to vactive, save writes vactive to vsave, stack push pushes vactive to the top of the stack, stack pop writes a null value to the top of the stack, and result writes vactive to qresult(presult)",1
" m is considered to have perfect generalization if m can give the right answer for any input, such as inputs of arbitrary length",1
3,0
 we outline how to construct this set,1
" in particular, we construct new training traces which explicitly contain recursive elements and show that with this type of trace, npi easily learns recursive programs",1
" et+1 ∼ fenv(et, pt, at)",1
" the non-recursive and partially recursive versions are unable to sort long arrays, beyond length 8",0
", the verification set",0
" there are 3 pointers (presult, pstack, pstart), presult points to the next empty location in qresult,
pstack points to the top of the stack in qstack, and pstart points to the candidate starting node for a connected component",1
" we can obtain this mapping by first considering the possible observations that can arise at the beginning of the entry function (add, bubblesort, toposort, and quicksort) for some valid program input, and iteratively applying the observation-to-observation mapping implied by the reference implementation’s step output at that point in the execution",1
" our implementation uses two stacks qstacklo and
qstackhi, each in ru , that store the arguments to the recursive quicksort calls in algorithm 3; before each recursive call, the appropriate arguments are popped off the stack and written to plo and phi",0
" for each graph size, we test the learned programs on 30 randomly generated dags",1
" we note that an earlier version of quicksort we tried lacked primitive operations to directly move a pointer to another, and therefore needed more functions and observations",1
" furthermore, given a verification input set that covers all base cases and reduction rules, we can formally prove that the learned neural programs achieve perfect generalization after verifying its behavior on the verification input set",0
"
recursion drastically reduces the number of configurations we need to consider during the verification phase and makes the proof tractable, because it introduces structure that eliminates infinitely long sequences of step inputs that would otherwise need to be considered",1
 npi then outputs the return probability and next program and arguments to execute,0
" twc-1409915, darpa under grant no",1
" in fact, we argue that recursion is an essential element for neural programs to generalize, and makes it tractable to prove the generalization of neural programs",0
"
as mentioned in section 1, all approaches to neural programming today fare poorly on this generalization issue",0
", an lstm in npi’s case, but possibly other networks in different cases",0
"
the operations we need to be concerned most about are carry and lshift, which induce partial environment states spanning two columns",0
" empirically, existing models suffer from a common limitation—generalization becomes poor beyond a threshold level of complexity",1
 table 4 also demonstrates that the (verified) recursive program generalizes perfectly,0
"
we also report on generalization results for the non-recursive and recursive versions of this variant of bubble sort",0
" an example of part of an addition task trace, written in shorthand, is given in figure 1",0
" also the dag can have potentially more than one connected component, meaning it is necessary to transition between these components appropriately",0
g,0
"
as a concrete instantiation, we show in this paper that we can enable recursive neural programs in the npi model, and thus enable perfectly generalizable neural programs for tasks such as sorting where the original, non-recursive npi program fails",0
 recursion enables provable guarantees on neural programs’ behavior without needing to exhaustively enumerate all possible inputs to the programs,1
 ,0
" the npi model has three learnable components: a task-agnostic core, a program-key embedding, and domain-specific encoders that allow the npi to operate in diverse environments",1
" the corresponding edge list is [(1, 2), (1, 5), (2, 4), (2, 5), (3, 5)]",1
" the single-digit multiplication task in zaremba   (2016), the bubble sort task in reed & de freitas (2016), and the graph tasks in graves   (2016))",1
 table 3 presents results on randomly generated arrays of varying length for the learned non-recursive and recursive programs,0
" however, models that make use of this strategy eventually fail after a certain level of complexity (e",1
 ,0
 stack pop pushes −∞ values to qstacklo and qstackhi,1
"
we experiment with two levels of recursion—partial and full",0
" for brevity, we refer the reader to the appendix for information about the program set and non-recursive and recursive trace-generating functions for quicksort",0
", to change the value of vactive) none described below move move a pointer (e",0
", to color a vertex) or variable (e",0
 this makes it difficult to reason about what the model will do when given complex inputs,1
"
base cases and reduction rules for bubble sort",1
"
we refer the reader to the appendix for the non-recursive trace generating functions",1
" the outputs rt, pt+1, at+1 are used to determine the next action, as described in algorithm 1",0
" 2: initialize lo and hi to be 1 and n, where n is the length of a",0
" the core lstm module’s hidden state is preserved over all these add1 calls, and it is difficult to interpret with certainty what happens over longer timesteps without concretely evaluating the lstm with an input of that length",0
"
in order to demonstrate that recursion can help learn and generalize better, for addition, we trained only on traces for 5 arbitrarily chosen 1-digit addition sum examples",1
"2 at any given time, the npi has access to values pointed to by four pointers in each of the four rows, represented by q(1, i1), q(2, i2), q(3, i3), and q(4, i4)",0
" the corresponding edge list is [(1, 2), (1, 5), (2, 4), (2, 5), (3, 5)]",1
" in order to claim a proof, we must verify the model produces correct behavior over all base cases and reductions, as described in section 2",0
" as aforementioned, the npi model naturally supports recursion",1
" in future work, we would like to decrease supervision and construct models that are capable of coming up with recursive abstractions themselves",0
"when constructing a neural network for the purpose of learning a program, there are two orthogonal aspects to consider",0
" given a precise definition of p , it may be possible to automate the generation of v from p in future work",0
" any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of national science foundation and darpa",1
" for problems where the base cases may be extremely large or infinite, such as certain forms of motor control, recursion can still help reduce the problem of generalization to these two aspects and make the generalization problem significantly simpler to handle and reason about",1
 some functional programming languages go so far as not to define any looping constructs but rely solely on recursion to enable repeated execution of the same code,0
" for example, for the addition task, the model is trained on short inputs and then tested on its ability to sum inputs with much longer numbers of digits",0
" the write operation has the following arguments:
arg 1 (object to write): env stack lo, env stack hi, phi, plo
env stack lo and env stack hi represent qstacklo(pstacklo) and qstackhi(pstackhi), respectively",0
 ,0
" however, models that make use of this strategy eventually fail after a certain level of complexity (e",0
" 2: initialize lo and hi to be 1 and n, where n is the length of a",0
 there is a (changing) list of neural programs used to accomplish a given task,1
"program descriptions calls arguments quicksort run the quicksort
routine in place for the array a, for indices from lo to hi non-recursive: partition, stack, write recursive: same as non-recursive version, along with quicksort implicitly: array a to sort, lo, hi
partition runs the partition function",0
"
grade-school addition",0
" for grade-school addition, the domain-specific encoder is
fenc(q, i1, i2, i3, i4, at) =mlp ([q(1, i1), q(2, i2), q(3, i3), q(4, i4), at(1), at(2), at(3)]),
where the environment q ∈ r4×n×k is a scratch-pad that contains four rows (the first input number, the second input number, the carry bits, and the output) and n columns",1
"
note that v should also contain the necessary reductions, which corresponds to making the recursive calls at the correct time, as indicated by p ",0
" twc-1409915, darpa under grant no",0
"
move",1
 this verification phase only needs to be performed once after training,0
" the learned recursive program is different from neural programs learned in all previous work in an important aspect: previous approaches do not explicitly incorporate this abstraction, and hence generalize poorly, whereas our learned neural programs incorporate recursion and achieve perfect generalization",0
"as there is no public implementation of npi, we implemented a version of it in keras that is as faithful to the paper as possible",0
"we train using the adam optimizer and use a 2-layer lstm and task-specific state encoders for the external environments, as described in reed & de freitas (2016)",1
" in all experiments, α is set to 0",0
 we found that changing the npi training traces is a simple way to enable this,1
 b1b0 where no carry operations occur,0
" naı̈vely, we might expect to synthesize and test an input for any sequence created by combining the four possible colors in two variables and another boolean variable for whether the stack is empty (so 32 possible observations at any point), but for various reasons, most of these combinations are impossible to occur at any given point in the execution trace",0
" for non-primitive subprograms, we can update the observation-to-observation mapping currently associated with the subprogram and then apply that mapping to the current set",1
 there are 2 variables (vactive and vsave); vactive holds the active vertex (as in algorithm 2) and vsave holds the value of vactive before executing line 12 of algorithm 2,1
" in particular, recursion can be implemented as a program calling itself",1
 18: push lo and p− 1 to slo and shi,0
" we can obtain this mapping by first considering the possible observations that can arise at the beginning of the entry function (add, bubblesort, toposort, and quicksort) for some valid program input, and iteratively applying the observation-to-observation mapping implied by the reference implementation’s step output at that point in the execution",0
 ,0
" the architecture usually possesses some form of memory, which could be internal (such as the hidden state of a recurrent neural network) or external (such as a discrete “scratch pad” or a memory block with differentiable access)",1
", an array for quicksort or a dag for topological sort)",1
" here as a concrete and general example, we consider a general neural programming architecture (npa), similar to neural programmer-interpreter (npi) in reed & de freitas (2016)",0
" consequently, there is no concept of length relevant to the problem, which has traditionally been an important focus of length-based curriculum learning",0
" the non-recursive and partially recursive versions are unable to sort long arrays, beyond length 8",0
e,1
"
topological sort",1
" for brevity, we refer the reader to the appendix for information about the program set and non-recursive and recursive trace-generating functions for quicksort",1
 this paper is the first (to our knowledge) to investigate the important problem of provable generalization properties of neural programs,1
" at each time step, the core controller can decide to select one of the programs to call with certain arguments",0
" also the dag can have potentially more than one connected component, meaning it is necessary to transition between these components appropriately",1
" plo and phi point to the lo and hi indices of the array, as in algorithm 3",0
 this makes it difficult to reason about what the model will do when given complex inputs,1
" for each function we use to implement the recursive version of topological sort, we need to consider the set of possible environment observation sequences we can create from all valid inputs and test that the learned program produces the correct behavior on each of these inputs",0
 this algorithm is a variant of depth first search,1
"
in order to demonstrate that recursion can help learn and generalize better, we trained a non-recursive and recursive model on just a single execution trace generated from a graph containing 5 nodes3 for the topological sort task",0
" an example of part of an addition task trace, written in shorthand, is given in figure 1",0
" empirically, existing models suffer from a common limitation—generalization becomes poor beyond a threshold level of complexity",1
"
as in line 13 of the right-hand side of figure 1",1
 ,1
" here as a concrete and general example, we consider a general neural programming architecture (npa), similar to neural programmer-interpreter (npi) in reed & de freitas (2016)",1
 this procedure verifies that all base cases and reductions are handled properly by the model via explicit tests,1
" numerous models have been proposed for learning programs; to name a few, this includes the differentiable neural computer (graves  , 2016), neural turing machine (graves  , 2014), neural gpu (kaiser & sutskever, 2015), neural programmer (neelakantan  , 2015), pointer network (vinyals  , 2015), hierarchical attentive memory (andrychowicz & kurach, 2016), and neural random access machine (kurach  , 2016)",0
"
algorithm 3 recursive quicksort 1: initialize an array a to sort",0
"
under the assumption that there are no leading 0’s (except in the case of single digits) and the two numbers to be added have the same number of digits, the verification set for addition contains 20,181 input problems",0
 stack push call2 pushes pivot + 1 and hi to qstacklo and qstackhi,0
"when constructing a neural network for the purpose of learning a program, there are two orthogonal aspects to consider",0
" thus, recursion helps decompose a problem and makes it easier to reason about a program’s behavior for previously unseen situations such as longer inputs",0
" the recursive version can generalize perfectly to long problems constructed from these components (such as the sum “822+233”, where “8+2” and “2+3” are in the training set), but the non-recursive version fails to sum these long problems properly",0
" the theorem states that for the same sequence of step inputs, the model produces the exact same step output as the target program it aims to learn",0
" by iterating with this procedure, and then running p on the input observation set that we obtain for the entry point function, we can obtain v precisely",0
" at any given time, the npi has access to the values referred to by two pointers, represented by q(1, i1) and q(1, i2),",0
" k is set to 11, to denote the range of possible numbers (0 through 9), along with the start/end token (represented with the same encoding) which is observed when a pointer reaches beyond the bounds of the input",0
 stack pop pushes −∞ values to qstacklo and qstackhi,0
" as aforementioned, the npi model naturally supports recursion",0
" on the other hand, the non-recursive version has low accuracy, beginning from size 5, and fails completely for graphs of size 8 and beyond",1
" any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of national science foundation and darpa",0
" the partial recursion is not enough for perfect generalization, as will be presented later in section 4",0
", pstart or childlist[vactive]) up or down none described belowwrite",0
" there are 3 pointers (presult, pstack, pstart), presult points to the next empty location in qresult,
pstack points to the top of the stack in qstack, and pstart points to the candidate starting node for a connected component",0
" as a consequence, our learned neural programs empirically achieve perfect generalization from a very small number of training examples",0
we now report on generalization for the varying tasks,1
"
as mentioned in section 1, all approaches to neural programming today fare poorly on this generalization issue",0
" an example of part of an addition task trace, written in shorthand, is given in figure 1",0
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,0
" we make the trace recursive by adding a tail recursive call into the trace for the add program after calling add1 and lshift, the original paper uses k = 10, but we found it necessary to augment the range with an end token, in order to terminate properly",0
 this modification also enables us to sort arrays containing arbitrary comparable elements,1
"program descriptions calls arguments toposort perform topological
sort on graph traverse, next start, write, move
none
traverse traverse graph until stack is empty check child, explore none check child check if a white child exists; if so, set childlist[vactive] to point to it move none explore repeatedly traverse subgraphs until stack is empty stack, check child, write, move none stack interact with stack, either pushing or popping write, move push, pop next start move pstart until reaching a white vertex",1
" formally, a function exhibits recursive behavior when it possesses two properties: (1) base cases— terminating scenarios that do not use recursion to produce answers; (2) a set of rules that reduces all other problems toward the base cases",0
" et+1 ∼ fenv(et, pt, at)",0
 recursion can be implemented differently for different neural programming models,0
" for many problems, the base cases and reduction rules usually consist of a finite (often small) number of cases",0
" in particular, recursion can be implemented as a program calling itself",0
"
grade school addition",1
"
base cases and reduction rules for bubble sort",0
"
topological sort",0
" for brevity, we refer the reader to the appendix for further details on the program set and non-recursive and recursive trace-generating functions used for topological sort",0
 we instantiated recursion for the neural programmerinterpreter by changing the training traces,1
" note that recursion helps make this process tractable, because we only need to test a finite number of inputs to show that the model will work correctly on inputs of unbounded complexity",0
"
to construct this set, by using the reference implementation of each subprogram, we construct a mapping between two sets of environment observations: the first set consists of all observations that can occur at the beginning of a particular subprogram’s invocation, and the second set contains the observations at the end of that subprogram",1
" if a white vertex is found, set pstart to point to it; this signifies the start of a traversal of a new connected component",1
"in this paper, we propose that the key abstraction of recursion is necessary for neural programs to generalize",0
 ,0
" training on the full recursive trace leads to perfect generalization, as shown in section 4",1
"
it is important to emphasize that at inference time in the npi, the hidden state of the lstm controller is reset (to zero) at each subprogram call, as in line 3 of algorithm 1 (h ← 0)",1
 our experiments use a small number of training examples,1
" the core lstm module’s hidden state is preserved over all these add1 calls, and it is difficult to interpret with certainty what happens over longer timesteps without concretely evaluating the lstm with an input of that length",1
" recall that this verification procedure cannot be performed for the non-recursive versions, since the propagation of the hidden state in the core lstm module makes reasoning difficult and so we would need to check an unbounded number of examples",1
 we found that changing the npi training traces is a simple way to enable this,0
", to color a vertex) or variable (e",1
", pstart or childlist[vactive]) up or down none described belowwrite",0
"
the same set of problems was used to generate the training traces for all formulations of the task, for non-recursive and recursive versions",0
" nevertheless, we would like the model to quickly learn to represent the right semantics of the underlying program from a small number of training examples, not an exhaustive number of them",0
g,0
"
the npi accesses an external environment, q, which varies according to the task",1
" thus, any general neural programming architecture supporting such a call structure can be made to support recursion",0
" naı̈vely, we might expect to synthesize and test an input for any sequence created by combining the four possible colors in two variables and another boolean variable for whether the stack is empty (so 32 possible observations at any point), but for various reasons, most of these combinations are impossible to occur at any given point in the execution trace",0
" in the non-recursive trace, there are four functions that can be made recursive—toposort, check child, explore, and next start, and we add a tail recursive call to each of these functions in order to make the recursive trace",1
" the contents ofqresult andqstack are not exposed directly through the domain-specific encoder; rather, we define primitive functions which manipulate these scratch-pads",1
" 16: p = partition(a, lo, hi) 17: push p+ 1 and hi to slo and shi",0
2,0
"
in what follows, we describe the way in which we constructed npi training traces so as to make them contain recursive elements and thus enable npi to learn recursive programs",0
"
in this paper, we propose that recursion is an important concept for neural programs as well",0
" for each length, we test each program on 30 randomly generated problems",0
" the move operation has the following arguments:
arg 1 (pointer): pstacklo, pstackhi, pj , ppivot
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values",1
" in particular, this condition is satisfied by npi, and thus the npi model naturally supports recursion (even though the authors of npi did not consider this aspect explicitly)",0
 table 4 demonstrates that the accuracy of the non-recursive program degrades sharply when moving from arrays of length 7 to arrays of length 8,1
"
we would also like to point out that in this paper, we provide as an example one way to train a recursive neural program, by providing a certain training execution trace to the npi model",0
" the swap operation has the following arguments:
arg 1 (swap object 1): ppivot
arg 2 (swap object 2): phi, pj
write",1
 the logic for the non-recursive trace is shown in algorithm 4 in the appendix,0
" in this section, we give a brief review of the npi architecture from reed & de freitas (2016) as background",0
"
as in line 13 of the right-hand side of figure 1",0
" both the non-recursive and recursive learned programs generalize on all input lengths we tried, up to 5000 digits",1
" this is possible if and only if the graph has no directed cycles; that is to say, it must be a directed acyclic graph (dag)",0
", the trace corresponding to the problem “109 + 101”)",0
g,1
" that is to say, the network does not learn the true program semantics",0
"
the training set for quicksort contains 4 traces, synthesized from arrays of length 5",1
 the sequence of primitive operations (move and write operations) for the non-recursive and recursive versions are exactly the same,0
" at end, pointer ppivot is moved to the pivot compswap loop, move pivot lo, move j lo, swap
none
compswap loop runs the for loop inside the partition function compswap, move none compswap compares a[pivot] ≤ a[j]; if so, perform a swap and increment ppivot swap, move none set pivot lo sets ppivot to lo index none none set j lo sets pj to lo index none none set j null sets pj to −∞ none none stack pushes lo/hi states
onto stacks slo and shi according to argument (described below)
write, move described below
move moves pointer one unit up or down none described below swap swaps elements at given array indices none described below write write a value either to stack (e",0
" nevertheless, we would like the model to quickly learn to represent the right semantics of the underlying program from a small number of training examples, not an exhaustive number of them",0
" here as a concrete and general example, we consider a general neural programming architecture (npa), similar to neural programmer-interpreter (npi) in reed & de freitas (2016)",0
" 3: 4: function partition(a, lo, hi) 5: pivot = lo 6: for j ∈ [lo, hi− 1] : do 7: if a[j] ≤ a[hi] then 8: swap a[pivot] with a[j] 9: pivot = pivot+ 1 0: swap a[pivot] with a[hi] 11: return pivot 12: 13: function quicksort(a, lo, hi) 14: while slo and shi are not empty: do 15: pop states off slo and shi, writing them to lo and hi",0
" we have three observations: the color of the start node, the color of the active node’s next child to be considered, and whether the stack is empty",0
" this guarantees that the step input sequences considered during verification contain all step input sequences which arise during execution of an unseen problem in f , leading to generalization to any problem in f ",0
" the swap operation has the following arguments:
arg 1 (swap object 1): ppivot
arg 2 (swap object 2): phi, pj
write",1
", an array for quicksort or a dag for topological sort)",0
" we have three observations: the color of the start node, the color of the active node’s next child to be considered, and whether the stack is empty",0
" we also hope to decrease supervision, for example by training with only partial or non-recursive traces, and to develop novel neural programming architectures integrated directly with a notion of recursion",0
" in particular, recursion can be implemented as a program calling itself",0
" this agrees with the generalization of non-recursive addition in reed & de freitas (2016), where they reported generalization up to 3000 digits",0
" thus, we demonstrate that recursion enables provably perfect generalization for different tasks, including addition, topological sort, quicksort, and a variant of bubble sort",1
"
the operations we need to be concerned most about are carry and lshift, which induce partial environment states spanning two columns",1
"
note that the recursion for quicksort is not purely tail recursive and therefore represents a more complex kind of recursion that is harder to learn than in the previous tasks",0
" the swap operation has the following arguments:
arg 1 (swap object 1): ppivot
arg 2 (swap object 2): phi, pj
write",0
 we created a program set that reflects the semantics of algorithm 2,0
" we hypothesize that the reason for this is that the neural network learns to spuriously depend on specific characteristics of the training examples that are irrelevant to the true program semantics, such as length of the training inputs, and thus fails to generalize to more complex inputs",0
", an lstm in npi’s case, but possibly other networks in different cases",0
"
in order to demonstrate that recursion can help learn and generalize better, we trained a non-recursive and recursive model on just a single execution trace generated from a graph containing 5 nodes3 for the topological sort task",0
 we outline how to construct this set,0
this material is in part based upon work supported by the national science foundation under grant no,0
"
algorithm 3 recursive quicksort 1: initialize an array a to sort",0
 we found that changing the npi training traces is a simple way to enable this,1
" bstep, used in bubble, compares pairs of numbers, continuously moving the bubble pointers once to the right each time until reaching the end of the array",0
 we created a program set that reflects the semantics of algorithm 2,1
" the move operation has the following arguments:
arg 1 (pointer): presult, pstack, pstart, childlist[vactive], childlist[vsave]
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 5 values",1
"
topological sort",1
"
base cases and reduction rules for topological sort",0
" the verification set can be very small, as we found a 10-element array ([8,2,1,2,0,8,5,8,3,7]) is sufficient to cover all of v ",1
"
in order to demonstrate that recursion can help learn and generalize better, for addition, we trained only on traces for 5 arbitrarily chosen 1-digit addition sum examples",1
" the pointers at index i1 and i2 are used to compare the pair of numbers considered during the bubble sweep, swapping them if the number at i1 is greater than that in i2",1
 we instantiated recursion for the neural programmerinterpreter by changing the training traces,0
", to change the value of phi) none described belowstack",1
" the second is the training procedure, which consists of the form of the training data and the optimization process",0
" the 4 environment observations aid with control flow; qstacklo(pstacklo− 1) < qstackhi(pstackhi− 1) implements the lo < hi comparison in line 5 of algorithm 3, pstacklo == 1 checks if the stacks are empty in line 18 of algorithm 4, and the other observations (all involving ppivot or pj) deal with logic in the partition function",0
g,0
"
is critical for implementing recursion, since it permits us to restrict our attention to the currently relevant recursive call, ignoring irrelevant details about other contexts",1
"
algorithm 2 shows the topological sort task of interest",1
" if the program is primitive, the next environmental state et+1 will be affected by pt and at, i",0
 the first is the actual model architecture,1
"
topological sort",0
 errors arise due to undesirable and uninterpretable dependencies and associations the architecture learns to store in some high-dimensional hidden state,1
" however, models that make use of this strategy eventually fail after a certain level of complexity (e",1
" our implementation uses two stacks qstacklo and
qstackhi, each in ru , that store the arguments to the recursive quicksort calls in algorithm 3; before each recursive call, the appropriate arguments are popped off the stack and written to plo and phi",1
" as with the others, we apply the procedure described in section 3",0
we emphasize the overall goal of this work is to enable the learning of a recursive program,1
"we describe how models trained with recursive traces can be proven to generalize, by using the verification procedure described in section 3",0
" for tasks other than addition, it is difficult to analytically determine the verification set, so instead, we randomly generate input candidates until they completely cover the base cases and reduction rules",0
"
arg 2 (auxiliary variable): color grey, color black
color grey and color black color the given vertex grey and black, respectively",1
"
the training set for quicksort contains 4 traces, synthesized from arrays of length 5",1
" finally, we implement recursive traces for our own topological sort and quicksort tasks",1
" a single element of an execution trace consists of a step input-step output pair, which can be synthesized from algorithm 1: this corresponds to, for a given time-step, the step input tuple (e, p, a) and step output tuple (r, p2, a2)",1
g,1
" to make an analogy to mdps, this procedure is analogous to how value iteration obtains the correct value for each state starting from any initialization",0
" bstep, used in bubble, compares pairs of numbers, continuously moving the bubble pointers once to the right each time until reaching the end of the array",0
"
figure 1 shows examples of non-recursive and recursive addition traces",0
" in our experiments, we only present dag’s as inputs and represent the vertices as values ranging from 1, ",0
"
the dag is represented as an adjacency list where dag[i][j] refers to the j-th child of vertex i",0
" for each function we use to implement the recursive version of topological sort, we need to consider the set of possible environment observation sequences we can create from all valid inputs and test that the learned program produces the correct behavior on each of these inputs",0
" given a precise definition of p , it may be possible to automate the generation of v from p in future work",1
"
is critical for implementing recursion, since it permits us to restrict our attention to the currently relevant recursive call, ignoring irrelevant details about other contexts",0
"
base cases and reduction rules for bubble sort",1
" any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of national science foundation and darpa",0
" twc-1409915, darpa under grant no",0
" furthermore, given a verification input set that covers all base cases and reduction rules, we can formally prove that the learned neural programs achieve perfect generalization after verifying its behavior on the verification input set",0
", pstart or childlist[vactive]) up or down none described belowwrite",0
" we have three observations: the color of the start node, the color of the active node’s next child to be considered, and whether the stack is empty",0
", the trace corresponding to the problem “109 + 101”)",1
" we prove every member of f is added properly, given that subproblems s = {anan−1 + bnbn−1, an−1an−2 + bn−1bn−2, ",0
" the partial recursion is not enough for perfect generalization, as will be presented later in section 4",0
" using this modification, we constructed a verification set consisting of one array of size 10",1
"
we propose and describe our verification procedure",1
" in particular, given that a recursion is defined by two properties as mentioned before, the base cases and the set of reduction rules, we can prove a recursive neural program generalizes perfectly if we can prove that (1) it performs correctly on the base cases; (2) it learns the reduction rules correctly",1
" thus, recursion helps decompose a problem and makes it easier to reason about a program’s behavior for previously unseen situations such as longer inputs",1
" as a consequence, our learned neural programs empirically achieve perfect generalization from a very small number of training examples",0
" we performed experiments on the partially recursive version in order to examine what happens when only one recursive call is implemented, when in reality three are required for perfect generalization",0
" in this pair, the step input runs a subprogram add1 that has no arguments, and the step output contains a program write that has arguments of out and 1",1
" this agrees with the generalization of non-recursive addition in reed & de freitas (2016), where they reported generalization up to 3000 digits",1
" while this matches the description from reed & de freitas (2016), we found that this causes an unnecessary blowup in the size of v and makes it much more difficult to construct the verification set",1
"
as a concrete instantiation, we show in this paper that we can enable recursive neural programs in the npi model, and thus enable perfectly generalizable neural programs for tasks such as sorting where the original, non-recursive npi program fails",1
" full recursion, in addition to making the aforementioned tail recursive call, adds two additional recursive calls; bstep and lshift are made tail recursive",1
" via the recursive call, we effectively forget that the column just added exists, since the recursive call to add starts with a new hidden state for the lstm controller",0
" we performed experiments on the partially recursive version in order to examine what happens when only one recursive call is implemented, when in reality three are required for perfect generalization",0
"
we experiment with two levels of recursion—partial and full",0
"we describe how models trained with recursive traces can be proven to generalize, by using the verification procedure described in section 3",0
" in particular, recursion can be implemented as a program calling itself",1
" in order to claim a proof, we must verify the model produces correct behavior over all base cases and reductions, as described in section 2",0
"
grade-school addition",0
"
it is important to emphasize that at inference time in the npi, the hidden state of the lstm controller is reset (to zero) at each subprogram call, as in line 3 of algorithm 1 (h ← 0)",0
" empirically, existing models suffer from a common limitation—generalization becomes poor beyond a threshold level of complexity",0
" lshift, used in reset, shifts the pointers left until reaching the start token",0
" for non-primitive subprograms, we can update the observation-to-observation mapping currently associated with the subprogram and then apply that mapping to the current set",0
" the core lstm module’s hidden state is preserved over all these add1 calls, and it is difficult to interpret with certainty what happens over longer timesteps without concretely evaluating the lstm with an input of that length",1
as mentioned in section 2,0
we now report on generalization for the varying tasks,0
"1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 while (pstacklo 6= 1): 6 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 7 stack(stack_pop) 8 else: 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 stack(stack_pop) 12 partition() 13 stack(stack_push_call2) 14 stack(stack_push_call1) 15 } 16 17 partition() { 18 set_pivot_lo() 19 set_j_lo() 20 compswap_loop() 21 swap(ppivot, phi) 22 set_j_null() 23 } 24 25 compswap_loop() { 26 while (pj 6= phi): 27 compswap() 28 move(pj, up) 29 } 30 31 compswap() { 32 if (a[pj ] ≤ a[phi]): 33 swap(ppivot, pj) 34 move(ppivot, up) 35 } 36 37 stack(op) { 38 if (op == stack_push_call1): 39 write(env_stack_lo, plo) 40 write(env_stack_hi, ppivot − 1) 41 move(pstacklo, up) 42 move(pstackhi, up) 43 44 if (op == stack_push_call2): 45 write(env_stack_lo, ppivot + 1) 46 write(env_stack_hi, phi) 47 move(pstacklo, up) 48 move(pstackhi, up) 49 50 if (op == stack_pop): 51 write(env_stack_lo, reset) 52 write(env_stack_hi, reset) 53 move(pstacklo, down) 54 move(pstackhi, down) 55 }altered recursive functions 1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 6 partition() 7 stack(stack_push_call2) 8 stack(stack_push_call1) 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 quicksort() // recursive call 12 stack(stack_pop) 13 write(phi, env_stack_hi_peek) 14 write(plo, env_stack_lo_peek) 15 quicksort() // recursive call 16 stack(stack_pop) 17 } 18 19 compswap_loop() { 20 if (pj 6= phi): 21 compswap() 22 move(pj, up) 23 compswap_loop() // recursive call 24 }in this section, we describe the space of base cases and reduction rules that must be covered for each of the four sample tasks, in order to create the verification set",0
 we then construct a verification set of size 73 by ensuring that randomly generated graphs cover the analytically derived v ,0
"
we note that for tasks with very large input domains, such as ones involving mnist digits or speech samples, the state space of base cases and reduction rules could be prohibitively large, possibly infinite",1
 2: initialize an empty stack s and a directed acyclic graph dag to traverse,1
" 3: 4: function quicksort(a, lo, hi) 5: if lo < hi: then 6: p = partition(a, lo, hi) 7: quicksort(a, lo, p− 1) 8: quicksort(a, p+ 1, hi) 9: 10: function partition(a, lo, hi) 11: pivot = lo 12: for j ∈ [lo, hi− 1] : do 13: if a[j] ≤ a[hi] then 14: swap a[pivot] with a[j] 15: pivot = pivot+ 1 16: swap a[pivot] with a[hi] 17: return pivot
for quicksort, the domain-specific encoder is
fenc(qarray, qstacklo, qstackhi, plo, phi, pstacklo, pstackhi, ppivot, pj , at) =
mlp ([qarray(pj) ≤ qarray(phi), pj == phi, qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1), pstacklo == 1, at(1), at(2), at(3)]),
whereqarray ∈ ru×11 is a scratch-pad that contains u rows, each containing one of 11 values (one of the numbers 0 through 9 or an invalid state)",1
 almost all architectures train on program input/output pairs,0
"
in order to demonstrate that recursion can help learn and generalize better, we trained a non-recursive and recursive model on just a single execution trace generated from a graph containing 5 nodes3 for the topological sort task",0
" for example, for the addition task, the model is trained on short inputs and then tested on its ability to sum inputs with much longer numbers of digits",1
" a single element of an execution trace consists of a step input-step output pair, which can be synthesized from algorithm 1: this corresponds to, for a given time-step, the step input tuple (e, p, a) and step output tuple (r, p2, a2)",0
 ,0
" we prove every member of f is added properly, given that subproblems s = {anan−1 + bnbn−1, an−1an−2 + bn−1bn−2, ",0
" for tasks other than addition, it is difficult to analytically determine the verification set, so instead, we randomly generate input candidates until they completely cover the base cases and reduction rules",1
" even though the earlier version also generalized just as well in practice, relatively small differences in the formulation of the traces and the environment observations can drastically change the difficulty of verification",1
" the partial recursion is not enough for perfect generalization, as will be presented later in section 4",0
 the program terminates when seeing no numbers in the current column,1
"
given a verification set, we can then run the model on the verification set to check if the produced traces and results are correct",0
" there are 3 pointers (presult, pstack, pstart), presult points to the next empty location in qresult,
pstack points to the top of the stack in qstack, and pstart points to the candidate starting node for a connected component",0
g,0
"
we construct input problems by splitting into two cases: one case in which the left column contains a null value and another in which the left column does not contain any null values",0
" we show that by modifying the training procedure, we enable the npi model to learn recursive neural programs",0
"
we refer the reader to the appendix for the non-recursive trace generating functions",1
 we leave this as future work to devise a verification procedure more appropriate to this setting,0
as mentioned in section 2,0
" given verification sets that cover all the base cases and reduction rules, we can provide proofs that these learned programs generalize perfectly",0
"
algorithm 2 shows the topological sort task of interest",0
 these pointers are referred to as bubble pointers,1
"
for topological sort, the domain-specific encoder is
fenc(dag,qcolor, pstack, pstart, vactive, childlist, at)
=mlp ([qcolor(pstart), qcolor(dag[vactive][childlist[vactive]]), pstack == 1, at(1), at(2), at(3)]),
where qcolor ∈ ru×4 is a scratch-pad that contains u rows, each containing one of four colors (white, gray, black, invalid) with one-hot encoding",1
" although the model is exposed to more and more data, it might learn spurious and overly complex representations of the program, as suggested in zaremba   (2016)",1
"
the same set of problems was used to generate the training traces for all formulations of the task, for non-recursive and recursive versions",0
"
base cases and reduction rules for topological sort",0
" 3: 4: function quicksort(a, lo, hi) 5: if lo < hi: then 6: p = partition(a, lo, hi) 7: quicksort(a, lo, p− 1) 8: quicksort(a, p+ 1, hi) 9: 10: function partition(a, lo, hi) 11: pivot = lo 12: for j ∈ [lo, hi− 1] : do 13: if a[j] ≤ a[hi] then 14: swap a[pivot] with a[j] 15: pivot = pivot+ 1 16: swap a[pivot] with a[hi] 17: return pivot
for quicksort, the domain-specific encoder is
fenc(qarray, qstacklo, qstackhi, plo, phi, pstacklo, pstackhi, ppivot, pj , at) =
mlp ([qarray(pj) ≤ qarray(phi), pj == phi, qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1), pstacklo == 1, at(1), at(2), at(3)]),
whereqarray ∈ ru×11 is a scratch-pad that contains u rows, each containing one of 11 values (one of the numbers 0 through 9 or an invalid state)",0
"
we also report on generalization results for the non-recursive and recursive versions of this variant of bubble sort",0
" the learned recursive program is different from neural programs learned in all previous work in an important aspect: previous approaches do not explicitly incorporate this abstraction, and hence generalize poorly, whereas our learned neural programs incorporate recursion and achieve perfect generalization",1
 indentation indicates the stack is one level deeper than before,0
" finally, we implement recursive traces for our own topological sort and quicksort tasks",1
" , n , where the dag contains n vertices",0
" in particular, we construct new training traces which explicitly contain recursive elements and show that with this type of trace, npi easily learns recursive programs",0
we now report on generalization for the varying tasks,0
" if the program is primitive, the next environmental state et+1 will be affected by pt and at, i",1
"
the three environment observations aid with control flow in algorithm 2",1
" for the recursive formulation of addition, we analytically construct the set of input problems that cover all base cases and reduction rules",0
" we performed experiments on the partially recursive version in order to examine what happens when only one recursive call is implemented, when in reality three are required for perfect generalization",1
 the original version of the bubblesort implementation exposes the values within the array,0
" the space of possible programs is extremely large, and composing a program that performs robustly on the infinite space of possible inputs is difficult—in part because it is impractical to obtain enough training examples to easily disambiguate amongst all possible programs",1
 table 4 also demonstrates that the (verified) recursive program generalizes perfectly,1
" we call this set of inputs the verification set, sv ",0
" the core lstm module’s hidden state is preserved over all these add1 calls, and it is difficult to interpret with certainty what happens over longer timesteps without concretely evaluating the lstm with an input of that length",1
"
in addition, none of the current approaches to neural programming provide a method or even aim to enable provable guarantees about generalization",0
g,0
" full recursion, in addition to making the aforementioned tail recursive call, adds two additional recursive calls; bstep and lshift are made tail recursive",0
" the architecture usually possesses some form of memory, which could be internal (such as the hidden state of a recurrent neural network) or external (such as a discrete “scratch pad” or a memory block with differentiable access)",1
"
an alternative method is to run p on many different program inputs and then observe step input sequences which occur, to create v ",0
 2: initialize an empty stack s and a directed acyclic graph dag to traverse,1
" the core lstm module’s hidden state is preserved over all these add1 calls, and it is difficult to interpret with certainty what happens over longer timesteps without concretely evaluating the lstm with an input of that length",1
"
bubble sort",0
" the assumption of leading 0’s can be easily removed, at the cost of slightly increasing the size of the verification set",0
"
formally, verification consists of proving the following theorem:
∀i ∈ v,m(i) ⇓ p (i) where i denotes a sequence of step inputs (within one function call), v denotes the set of valid sequences of step inputs, m denotes the neural network model, p denotes the correct program, and p (i) denotes the next step output from the correct program",1
"
as mentioned in section 1, all approaches to neural programming today fare poorly on this generalization issue",1
" k is set to 11, to represent the range of 10 possible digits, along with a token representing the end of input",0
"
grade-school addition",1
 ,1
", to change the value of vactive) none described below move move a pointer (e",0
" lshift moves the four pointers to the left, to move to the next column",1
"
we describe the details of the npi model relevant to our contributions",1
 stack push call2 pushes pivot + 1 and hi to qstacklo and qstackhi,0
" 3: 4: function partition(a, lo, hi) 5: pivot = lo 6: for j ∈ [lo, hi− 1] : do 7: if a[j] ≤ a[hi] then 8: swap a[pivot] with a[j] 9: pivot = pivot+ 1 0: swap a[pivot] with a[hi] 11: return pivot 12: 13: function quicksort(a, lo, hi) 14: while slo and shi are not empty: do 15: pop states off slo and shi, writing them to lo and hi",0
" for instance, for recursive addition, consider the family f of addition problems anan−1 ",0
 this is due to the properties of the training set – we trained on 2 traces synthesized from arrays of length 7 and 1 trace synthesized from an array of length 6,0
"
we construct input problems by splitting into two cases: one case in which the left column contains a null value and another in which the left column does not contain any null values",0
"
the training data for the neural programmer-interpreter consists of full execution traces for the program of interest",1
"
note that the training set can often be considerably smaller than the verification set, and despite this, the learned model can still pass the entire verification set",1
" for problems where the base cases may be extremely large or infinite, such as certain forms of motor control, recursion can still help reduce the problem of generalization to these two aspects and make the generalization problem significantly simpler to handle and reason about",1
" the write operation has the following arguments:
arg 1 (object to write): env stack lo, env stack hi, phi, plo
env stack lo and env stack hi represent qstacklo(pstacklo) and qstackhi(pstackhi), respectively",1
", an lstm in npi’s case, but possibly other networks in different cases",1
" for problems where the base cases may be extremely large or infinite, such as certain forms of motor control, recursion can still help reduce the problem of generalization to these two aspects and make the generalization problem significantly simpler to handle and reason about",1
" for each graph size, we test the learned programs on 30 randomly generated dags",0
" a single element of an execution trace consists of a step input-step output pair, which can be synthesized from algorithm 1: this corresponds to, for a given time-step, the step input tuple (e, p, a) and step output tuple (r, p2, a2)",0
" for non-primitive subprograms, we can update the observation-to-observation mapping currently associated with the subprogram and then apply that mapping to the current set",0
"
arg 2 (increment or decrement): up, down1 // top level topological sort call 2 toposort() { 3 while (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 } 0 11 traverse() { 12 check_child() 13 explore() 14 } 15 16 check_child() { 17 while (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 18 move(childlist[vactive], up) 19 } 20 21 explore() { 22 do 23 if (qcolor(dag[vactive][childlist[vactive]]) is white): 24 write(color_next, color_grey) 25 stack(push) 26 write(save) 27 write(active_neighb) 28 move(childlist[vsave], up) 29 else: 30 write(color_curr, color_black) 31 write(result) 32 move(presult, up) 33 if(pstack == 1): 34 break 35 else: 36 stack(pop) 37 check_child() 38 while (true) 39 } 40 41 stack(op) { 42 if (op == push): 43 write(stack_push) 44 move(pstack, up) 45 46 if (op == pop): 47 write(active_stack) 48 write(stack_pop) 49 move(pstack, down) 50 } 51 52 next_start() { 53 while(qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 54 move(pstart, up) 55 }altered recursive functions  // top level topological sort call 2 toposort() { 3 if (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 toposort() // recursive call 0 } 11 12 check_child() { 13 if (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 14 move(childlist[vactive], up) 15 check_child() // recursive call 16 } 17 18 explore() { 19 if (qcolor(dag[vactive][childlist[vactive]]) is white): 20 write(color_next, color_grey) 21 stack(push) 22 write(save) 23 write(active_neighb) 24 move(childlist[vsave], up) 25 else: 26 write(color_curr, color_black) 27 write(result) 28 move(presult, up) 29 if(pstack == 1): 30 return 31 else: 32 stack(pop) 33 check_child() 34 explore() // recursive call 35 } 36 37 next_start() { 38 if (qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 39 move(pstart, up) 40 next_start() // recursive call 41 }algorithm 4 iterative quicksort 1: initialize an array a to sort and two empty stacks slo and shi",1
" fa8750-15-2-0104, and berkeley deep drive",1
 stack push call2 pushes pivot + 1 and hi to qstacklo and qstackhi,0
 the maximum problem length in this training set is 3 (e,0
"
the three environment observations aid with control flow in algorithm 2",1
 stack pop pushes −∞ values to qstacklo and qstackhi,0
" the add reduction rule ensures that each call to add only covers two adjacent columns, and so the lstm only ever runs for a fixed number of steps necessary to process these two columns",0
" finally, the boolean pstack == 1 is used to check whether the stack is empty in line 18 of algorithm 2",0
" lshift moves the four pointers to the left, to move to the next column",0
" we make the trace recursive by adding a tail recursive call into the trace for the add program after calling add1 and lshift, the original paper uses k = 10, but we found it necessary to augment the range with an end token, in order to terminate properly",1
"
base cases and reduction rules for topological sort",1
"as there is no public implementation of npi, we implemented a version of it in keras that is as faithful to the paper as possible",0
" numerous models have been proposed for learning programs; to name a few, this includes the differentiable neural computer (graves  , 2016), neural turing machine (graves  , 2014), neural gpu (kaiser & sutskever, 2015), neural programmer (neelakantan  , 2015), pointer network (vinyals  , 2015), hierarchical attentive memory (andrychowicz & kurach, 2016), and neural random access machine (kurach  , 2016)",1
", the verification set",1
we now report on generalization for the varying tasks,0
" 3: 4: function partition(a, lo, hi) 5: pivot = lo 6: for j ∈ [lo, hi− 1] : do 7: if a[j] ≤ a[hi] then 8: swap a[pivot] with a[j] 9: pivot = pivot+ 1 0: swap a[pivot] with a[hi] 11: return pivot 12: 13: function quicksort(a, lo, hi) 14: while slo and shi are not empty: do 15: pop states off slo and shi, writing them to lo and hi",0
 there is a (changing) list of neural programs used to accomplish a given task,1
" at end, pointer ppivot is moved to the pivot compswap loop, move pivot lo, move j lo, swap
none
compswap loop runs the for loop inside the partition function compswap, move none compswap compares a[pivot] ≤ a[j]; if so, perform a swap and increment ppivot swap, move none set pivot lo sets ppivot to lo index none none set j lo sets pj to lo index none none set j null sets pj to −∞ none none stack pushes lo/hi states
onto stacks slo and shi according to argument (described below)
write, move described below
move moves pointer one unit up or down none described below swap swaps elements at given array indices none described below write write a value either to stack (e",0
" the core module of the npi is an lstm controller that takes as input a slice of the current external environment, via a set of pointers, and a program and arguments to execute",0
" while this matches the description from reed & de freitas (2016), we found that this causes an unnecessary blowup in the size of v and makes it much more difficult to construct the verification set",1
", the trace corresponding to the array [3,2])",0
 stack pop pushes −∞ values to qstacklo and qstackhi,0
" our implementation uses two stacks qstacklo and
qstackhi, each in ru , that store the arguments to the recursive quicksort calls in algorithm 3; before each recursive call, the appropriate arguments are popped off the stack and written to plo and phi",1
" the assumption of leading 0’s can be easily removed, at the cost of slightly increasing the size of the verification set",0
" for tasks other than addition, it is difficult to analytically determine the verification set, so instead, we randomly generate input candidates until they completely cover the base cases and reduction rules",1
 we created a program set that reflects the semantics of algorithm 2,1
 we find that recursion makes it easier for the network to learn the right program and generalize to unknown situations,0
" this agrees with the generalization of non-recursive addition in reed & de freitas (2016), where they reported generalization up to 3000 digits",0
"
in this paper, we propose to resolve these issues by explicitly incorporating recursion into neural architectures",0
 ,0
 this verification phase only needs to be performed once after training,1
"
move",1
 reset represents a −∞ value,0
 2: initialize an empty stack s and a directed acyclic graph dag to traverse,1
"
an alternative way of representing the environment slice is to expose the values of the absolute vertices to the model; however, this makes it difficult to scale the model to larger graphs, since large vertex values are not seen during training time",0
" the architecture usually possesses some form of memory, which could be internal (such as the hidden state of a recurrent neural network) or external (such as a discrete “scratch pad” or a memory block with differentiable access)",1
" for these programs, we re-use the appropriate program sets (the associated subprograms), and we refer the reader to the appendix of reed & de freitas (2016) for further details on the subprograms used in addition and bubble sort",1
" et+1 ∼ fenv(et, pt, at)",0
" et+1 ∼ fenv(et, pt, at)",1
2,1
"
one common strategy to improve generalization is to use curriculum learning, where the model is trained on inputs of gradually increasing complexity",0
" in all experiments, α is set to 0",1
"
it is important to emphasize that at inference time in the npi, the hidden state of the lstm controller is reset (to zero) at each subprogram call, as in line 3 of algorithm 1 (h ← 0)",1
g,0
"
bubble sort",1
" the stack operation has the following arguments:
arg 1 (operation): stack push call1, stack push call2, stack pop
stack push call1 pushes lo and pivot−1 toqstacklo andqstackhi",1
 ,0
 we use the lomuto partition scheme; the logic for the recursive trace is shown in algorithm 3,0
" we show that by modifying the training procedure, we enable the npi model to learn recursive neural programs",1
" empirically, existing models suffer from a common limitation—generalization becomes poor beyond a threshold level of complexity",0
 this procedure verifies that all base cases and reductions are handled properly by the model via explicit tests,0
" m is considered to have perfect generalization if m can give the right answer for any input, such as inputs of arbitrary length",0
" partial recursion only adds a tail recursive call to bubblesort after bubble and reset, similar to the tail recursive call described previously for addition",1
" given a precise definition of p , it may be possible to automate the generation of v from p in future work",0
"
the training set for topological sort contains 6 traces, with one synthesized from a graph of size 5 and the rest synthesized from graphs of size 7",0
" given verification sets that cover all the base cases and reduction rules, we can provide proofs that these learned programs generalize perfectly",1
"
recursion drastically reduces the number of configurations we need to consider during the verification phase and makes the proof tractable, because it introduces structure that eliminates infinitely long sequences of step inputs that would otherwise need to be considered",0
g,1
"
grade school addition",1
 ,0
 the program terminates when seeing no numbers in the current column,0
" formally, a function exhibits recursive behavior when it possesses two properties: (1) base cases— terminating scenarios that do not use recursion to produce answers; (2) a set of rules that reduces all other problems toward the base cases",1
" if no white vertex is found, the entire execution is terminated move none write write a value either to environment (e",1
" observe that partially recursive does slightly better than non-recursive for the setting in which the length of the array is 3, and that the fully recursive version is able to sort every array given to it",1
" in order to compensate for this, ppivot and pj require special functions (move pivot lo and move j lo) to properly set them to lo in lines 11 and 12 of the partition function in algorithm 3",0
g,0
" plo and phi point to the lo and hi indices of the array, as in algorithm 3",0
" in practice, we can additionally use tail recursion optimization to avoid problems with the call stack growing too deep",0
"
the three environment observations aid with control flow in algorithm 2",1
", the trace corresponding to the array [3,2])",0
"
arg 2 (increment or decrement): up, down
swap",1
" for tasks other than addition, it is difficult to analytically determine the verification set, so instead, we randomly generate input candidates until they completely cover the base cases and reduction rules",1
" if it is too large, then the semantics of p might not be well-defined on some elements in v , or the spurious step input sequences may not be reachable from any valid problem input (e",0
" if it is too large, then the semantics of p might not be well-defined on some elements in v , or the spurious step input sequences may not be reachable from any valid problem input (e",1
" in this architecture, we consider a core controller, e",0
"in this paper, we propose that the key abstraction of recursion is necessary for neural programs to generalize",0
" thus, any general neural programming architecture supporting such a call structure can be made to support recursion",0
3,0
"
arg 2 (increment or decrement): up, down1 // top level topological sort call 2 toposort() { 3 while (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 } 0 11 traverse() { 12 check_child() 13 explore() 14 } 15 16 check_child() { 17 while (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 18 move(childlist[vactive], up) 19 } 20 21 explore() { 22 do 23 if (qcolor(dag[vactive][childlist[vactive]]) is white): 24 write(color_next, color_grey) 25 stack(push) 26 write(save) 27 write(active_neighb) 28 move(childlist[vsave], up) 29 else: 30 write(color_curr, color_black) 31 write(result) 32 move(presult, up) 33 if(pstack == 1): 34 break 35 else: 36 stack(pop) 37 check_child() 38 while (true) 39 } 40 41 stack(op) { 42 if (op == push): 43 write(stack_push) 44 move(pstack, up) 45 46 if (op == pop): 47 write(active_stack) 48 write(stack_pop) 49 move(pstack, down) 50 } 51 52 next_start() { 53 while(qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 54 move(pstart, up) 55 }altered recursive functions  // top level topological sort call 2 toposort() { 3 if (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 toposort() // recursive call 0 } 11 12 check_child() { 13 if (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 14 move(childlist[vactive], up) 15 check_child() // recursive call 16 } 17 18 explore() { 19 if (qcolor(dag[vactive][childlist[vactive]]) is white): 20 write(color_next, color_grey) 21 stack(push) 22 write(save) 23 write(active_neighb) 24 move(childlist[vsave], up) 25 else: 26 write(color_curr, color_black) 27 write(result) 28 move(presult, up) 29 if(pstack == 1): 30 return 31 else: 32 stack(pop) 33 check_child() 34 explore() // recursive call 35 } 36 37 next_start() { 38 if (qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 39 move(pstart, up) 40 next_start() // recursive call 41 }algorithm 4 iterative quicksort 1: initialize an array a to sort and two empty stacks slo and shi",1
" the theorem states that for the same sequence of step inputs, the model produces the exact same step output as the target program it aims to learn",0
 pstacklo and pstackhi point to the top (empty) positions in qstacklo and qstackhi,0
" 9: else 10: function run(e, p2, a2)
a description of the inference procedure is given in algorithm 1",0
 we adapt machinery from the original paper slightly to fit our needs,1
" the architecture usually possesses some form of memory, which could be internal (such as the hidden state of a recurrent neural network) or external (such as a discrete “scratch pad” or a memory block with differentiable access)",0
g,0
"
algorithm 1 neural programming inference 1: inputs: environment observation e, program p, arguments a, stop threshold α 2: function run(e, p, a) 3: h← 0, r ← 0 4: while r < α do 5: s← fenc(e, a), h← flstm(s, p, h) 6: r ← fend(h), p2 ← fprog(h), a2 ← farg(h) 7: if p is a primitive function then 8: e← fenv(e, p, a)",1
" here as a concrete and general example, we consider a general neural programming architecture (npa), similar to neural programmer-interpreter (npi) in reed & de freitas (2016)",1
" at any given time, the npi has access to the values referred to by two pointers, represented by q(1, i1) and q(1, i2),",0
 almost all architectures train on program input/output pairs,1
g,1
"
one common strategy to improve generalization is to use curriculum learning, where the model is trained on inputs of gradually increasing complexity",1
"
arg 2 (increment or decrement): up, down
swap",0
e,1
" qcolor(dag[vactive][childlist[vactive]]) refers to the color of the next child of vactive, used in the evaluation of the condition in the if branch in line 9 of algorithm 2",1
" without using a recursive program, such a proof is not possible, because the non-recursive program runs on an arbitrarily long addition problem that creates correspondingly long sequences of step inputs; in the non-recursive formulation of addition, add calls add1 a number of times that is dependent on the length of the input",0
3,1
3,1
g,0
", to color a vertex) or variable (e",0
"2 at any given time, the npi has access to values pointed to by four pointers in each of the four rows, represented by q(1, i1), q(2, i2), q(3, i3), and q(4, i4)",0
"
an alternative method is to run p on many different program inputs and then observe step input sequences which occur, to create v ",1
 ,0
 almost all architectures train on program input/output pairs,0
e,0
" if the step output specifies a primitive function call, we need to reason about how it can affect the environment so as to change the observation in the next step input",1
e,0
" this agrees with the generalization of non-recursive addition in reed & de freitas (2016), where they reported generalization up to 3000 digits",1
" our implementation uses two stacks qstacklo and
qstackhi, each in ru , that store the arguments to the recursive quicksort calls in algorithm 3; before each recursive call, the appropriate arguments are popped off the stack and written to plo and phi",1
"
base cases and reduction rules for quicksort",0
 we outline how to construct this set,1
" as a consequence, our learned neural programs empirically achieve perfect generalization from a very small number of training examples",1
 this algorithm is a variant of depth first search,1
"
by reasoning about the possible set of environment observations created by all valid inputs, we construct v using the procedure described in section 3",1
" for example, for the addition task, the model is trained on short inputs and then tested on its ability to sum inputs with much longer numbers of digits",0
5,0
", an array for quicksort or a dag for topological sort)",0
" in particular, this condition is satisfied by npi, and thus the npi model naturally supports recursion (even though the authors of npi did not consider this aspect explicitly)",0
"
grade-school addition",0
 the memory updates of these neural programs are so complex and interdependent that it is difficult to reason about the behaviors of the learned neural program under previously unseen situations (such as problems with longer inputs),1
 stack push call2 pushes pivot + 1 and hi to qstacklo and qstackhi,1
"
recursion drastically reduces the number of configurations we need to consider during the verification phase and makes the proof tractable, because it introduces structure that eliminates infinitely long sequences of step inputs that would otherwise need to be considered",0
g,0
", to change the value of vactive) none described below move move a pointer (e",1
" the corresponding edge list is [(1, 2), (1, 5), (2, 4), (2, 5), (3, 5)]",1
" the core module of the npi is an lstm controller that takes as input a slice of the current external environment, via a set of pointers, and a program and arguments to execute",1
 there is a (changing) list of neural programs used to accomplish a given task,0
"
in this paper, we propose to resolve these issues by explicitly incorporating recursion into neural architectures",0
"
move",0
" numerous models have been proposed for learning programs; to name a few, this includes the differentiable neural computer (graves  , 2016), neural turing machine (graves  , 2014), neural gpu (kaiser & sutskever, 2015), neural programmer (neelakantan  , 2015), pointer network (vinyals  , 2015), hierarchical attentive memory (andrychowicz & kurach, 2016), and neural random access machine (kurach  , 2016)",1
 we found that changing the npi training traces is a simple way to enable this,0
" this is possible if and only if the graph has no directed cycles; that is to say, it must be a directed acyclic graph (dag)",1
" to make an analogy to mdps, this procedure is analogous to how value iteration obtains the correct value for each state starting from any initialization",0
"
arg 2 (increment or decrement): up, down
swap",0
", qstacklo or qstackhi) or to pointer (e",1
"
we describe the details of the npi model relevant to our contributions",1
"1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 while (pstacklo 6= 1): 6 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 7 stack(stack_pop) 8 else: 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 stack(stack_pop) 12 partition() 13 stack(stack_push_call2) 14 stack(stack_push_call1) 15 } 16 17 partition() { 18 set_pivot_lo() 19 set_j_lo() 20 compswap_loop() 21 swap(ppivot, phi) 22 set_j_null() 23 } 24 25 compswap_loop() { 26 while (pj 6= phi): 27 compswap() 28 move(pj, up) 29 } 30 31 compswap() { 32 if (a[pj ] ≤ a[phi]): 33 swap(ppivot, pj) 34 move(ppivot, up) 35 } 36 37 stack(op) { 38 if (op == stack_push_call1): 39 write(env_stack_lo, plo) 40 write(env_stack_hi, ppivot − 1) 41 move(pstacklo, up) 42 move(pstackhi, up) 43 44 if (op == stack_push_call2): 45 write(env_stack_lo, ppivot + 1) 46 write(env_stack_hi, phi) 47 move(pstacklo, up) 48 move(pstackhi, up) 49 50 if (op == stack_pop): 51 write(env_stack_lo, reset) 52 write(env_stack_hi, reset) 53 move(pstacklo, down) 54 move(pstackhi, down) 55 }altered recursive functions 1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 6 partition() 7 stack(stack_push_call2) 8 stack(stack_push_call1) 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 quicksort() // recursive call 12 stack(stack_pop) 13 write(phi, env_stack_hi_peek) 14 write(plo, env_stack_lo_peek) 15 quicksort() // recursive call 16 stack(stack_pop) 17 } 18 19 compswap_loop() { 20 if (pj 6= phi): 21 compswap() 22 move(pj, up) 23 compswap_loop() // recursive call 24 }in this section, we describe the space of base cases and reduction rules that must be covered for each of the four sample tasks, in order to create the verification set",1
we emphasize that the notion of a neural recursive program has not been presented in the literature before: this is our main contribution,0
 ,1
" consequently, there is no concept of length relevant to the problem, which has traditionally been an important focus of length-based curriculum learning",0
" in this version, there is a dependence on length: bstep and lshift are called a number of times equivalent to one less than the length of the input array, in bubble and reset respectively",1
g,0
" to the best of our knowledge, this is the first time verification has been applied to a neural program, providing provable guarantees about its behavior",1
" without using a recursive program, such a proof is not possible, because the non-recursive program runs on an arbitrarily long addition problem that creates correspondingly long sequences of step inputs; in the non-recursive formulation of addition, add calls add1 a number of times that is dependent on the length of the input",0
", an lstm in npi’s case, but possibly other networks in different cases",1
" the npi model has three learnable components: a task-agnostic core, a program-key embedding, and domain-specific encoders that allow the npi to operate in diverse environments",1
 stack pop pushes −∞ values to qstacklo and qstackhi,0
" if yes, then this indicates that the learned neural program achieves provably perfect generalization",0
"1; and for bubble sort, appendix a",0
g,1
 the first is the actual model architecture,1
 table 4 demonstrates that the accuracy of the non-recursive program degrades sharply when moving from arrays of length 7 to arrays of length 8,1
" a topological sort is a linear ordering of vertices such that for every directed edge (u, v) from u to v, u comes before v in the ordering",0
 the memory updates of these neural programs are so complex and interdependent that it is difficult to reason about the behaviors of the learned neural program under previously unseen situations (such as problems with longer inputs),1
" bstep, used in bubble, compares pairs of numbers, continuously moving the bubble pointers once to the right each time until reaching the end of the array",0
" thus, we demonstrate that recursion enables provably perfect generalization for different tasks, including addition, topological sort, quicksort, and a variant of bubble sort",1
" in computer science, recursion (as opposed to iteration) involves solving a larger problem by combining solutions to smaller instances of the same problem",0
" in this architecture, we consider a core controller, e",0
" a single element of an execution trace consists of a step input-step output pair, which can be synthesized from algorithm 1: this corresponds to, for a given time-step, the step input tuple (e, p, a) and step output tuple (r, p2, a2)",0
"1; and for bubble sort, appendix a",0
" full recursion, in addition to making the aforementioned tail recursive call, adds two additional recursive calls; bstep and lshift are made tail recursive",0
"
in order to demonstrate that recursion can help learn and generalize better, we trained a non-recursive and recursive model on just a single execution trace generated from a graph containing 5 nodes3 for the topological sort task",0
g,0
" empirically, we observe that the learned recursive programs solve all valid inputs with 100% accuracy after training on a very small number of examples, out-performing previous generalization results",1
" given verification sets that cover all the base cases and reduction rules, we can provide proofs that these learned programs generalize perfectly",0
" for many problems, the base cases and reduction rules usually consist of a finite (often small) number of cases",0
" the arrow in the theorem refers to evaluation, as in big-step semantics",0
" for brevity, we refer the reader to the appendix for information about the program set and non-recursive and recursive trace-generating functions for quicksort",0
" et+1 ∼ fenv(et, pt, at)",0
 3: begin traversing from vertex 1 in the dag,0
" furthermore, given a verification input set that covers all base cases and reduction rules, we can formally prove that the learned neural programs achieve perfect generalization after verifying its behavior on the verification input set",1
" the core module of the npi is an lstm controller that takes as input a slice of the current external environment, via a set of pointers, and a program and arguments to execute",0
"1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 while (pstacklo 6= 1): 6 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 7 stack(stack_pop) 8 else: 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 stack(stack_pop) 12 partition() 13 stack(stack_push_call2) 14 stack(stack_push_call1) 15 } 16 17 partition() { 18 set_pivot_lo() 19 set_j_lo() 20 compswap_loop() 21 swap(ppivot, phi) 22 set_j_null() 23 } 24 25 compswap_loop() { 26 while (pj 6= phi): 27 compswap() 28 move(pj, up) 29 } 30 31 compswap() { 32 if (a[pj ] ≤ a[phi]): 33 swap(ppivot, pj) 34 move(ppivot, up) 35 } 36 37 stack(op) { 38 if (op == stack_push_call1): 39 write(env_stack_lo, plo) 40 write(env_stack_hi, ppivot − 1) 41 move(pstacklo, up) 42 move(pstackhi, up) 43 44 if (op == stack_push_call2): 45 write(env_stack_lo, ppivot + 1) 46 write(env_stack_hi, phi) 47 move(pstacklo, up) 48 move(pstackhi, up) 49 50 if (op == stack_pop): 51 write(env_stack_lo, reset) 52 write(env_stack_hi, reset) 53 move(pstacklo, down) 54 move(pstackhi, down) 55 }altered recursive functions 1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 6 partition() 7 stack(stack_push_call2) 8 stack(stack_push_call1) 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 quicksort() // recursive call 12 stack(stack_pop) 13 write(phi, env_stack_hi_peek) 14 write(plo, env_stack_lo_peek) 15 quicksort() // recursive call 16 stack(stack_pop) 17 } 18 19 compswap_loop() { 20 if (pj 6= phi): 21 compswap() 22 move(pj, up) 23 compswap_loop() // recursive call 24 }in this section, we describe the space of base cases and reduction rules that must be covered for each of the four sample tasks, in order to create the verification set",0
" in particular, given that a recursion is defined by two properties as mentioned before, the base cases and the set of reduction rules, we can prove a recursive neural program generalizes perfectly if we can prove that (1) it performs correctly on the base cases; (2) it learns the reduction rules correctly",0
" any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of national science foundation and darpa",0
" however, the authors of npi did not consider explicitly the notion of recursion and as a consequence, did not learn recursive programs",1
"
arg 2 (auxiliary variable): color grey, color black
color grey and color black color the given vertex grey and black, respectively",0
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,0
"
the npi accesses an external environment, q, which varies according to the task",1
"1; and for bubble sort, appendix a",0
 these pointers are referred to as bubble pointers,0
" a single element of an execution trace consists of a step input-step output pair, which can be synthesized from algorithm 1: this corresponds to, for a given time-step, the step input tuple (e, p, a) and step output tuple (r, p2, a2)",0
" for each task, given the verification set, we check the traces and results of the learned, to-be-verified neural program (described in section 4",1
", an lstm in npi’s case, but possibly other networks in different cases",0
" in future work, we would like to decrease supervision and construct models that are capable of coming up with recursive abstractions themselves",1
 our experiments use a small number of training examples,1
" for grade-school addition, the domain-specific encoder is
fenc(q, i1, i2, i3, i4, at) =mlp ([q(1, i1), q(2, i2), q(3, i3), q(4, i4), at(1), at(2), at(3)]),
where the environment q ∈ r4×n×k is a scratch-pad that contains four rows (the first input number, the second input number, the carry bits, and the output) and n columns",1
"
bubble sort",0
" the second is the training procedure, which consists of the form of the training data and the optimization process",1
" , n , where the dag contains n vertices",0
" we note that an earlier version of quicksort we tried lacked primitive operations to directly move a pointer to another, and therefore needed more functions and observations",1
 there are 2 variables (vactive and vsave); vactive holds the active vertex (as in algorithm 2) and vsave holds the value of vactive before executing line 12 of algorithm 2,1
" we prove every member of f is added properly, given that subproblems s = {anan−1 + bnbn−1, an−1an−2 + bn−1bn−2, ",1
", pstart or childlist[vactive]) up or down none described belowwrite",1
" if the program is primitive, the next environmental state et+1 will be affected by pt and at, i",0
"
to construct this set, by using the reference implementation of each subprogram, we construct a mapping between two sets of environment observations: the first set consists of all observations that can occur at the beginning of a particular subprogram’s invocation, and the second set contains the observations at the end of that subprogram",1
"1, we hypothesize the non-recursive programs do not generalize well because they have learned spurious dependencies specific to the training set, such as length of the input problems",1
"
after finding v , we construct a set of problem inputs which, when executed on p , create exactly the step input sequences which make up v ",0
" plo and phi point to the lo and hi indices of the array, as in algorithm 3",0
" each time a subprogram is called, the stack depth increases",0
"
we refer the reader to the appendix for the non-recursive trace generating functions",0
2,1
" this is highly undesirable, since being able to provide the correct answer in all possible settings is one of the most important aspects of any learned neural program",1
" furthermore, given a verification input set that covers all base cases and reduction rules, we can formally prove that the learned neural programs achieve perfect generalization after verifying its behavior on the verification input set",0
" the theorem states that for the same sequence of step inputs, the model produces the exact same step output as the target program it aims to learn",0
" here as a concrete and general example, we consider a general neural programming architecture (npa), similar to neural programmer-interpreter (npi) in reed & de freitas (2016)",0
as mentioned in section 2,0
"
the operations we need to be concerned most about are carry and lshift, which induce partial environment states spanning two columns",0
" as with the others, we apply the procedure described in section 3",1
"
in this paper, we propose to resolve these issues by explicitly incorporating recursion into neural architectures",0
we emphasize the overall goal of this work is to enable the learning of a recursive program,1
" on the other hand, the non-recursive version has low accuracy, beginning from size 5, and fails completely for graphs of size 8 and beyond",1
" in this architecture, we consider a core controller, e",0
"
is critical for implementing recursion, since it permits us to restrict our attention to the currently relevant recursive call, ignoring irrelevant details about other contexts",0
" each step during an execution of the program does one of three things: (1) another subprogram along with associated arguments is called, as in line 10, (2) the program writes to the environment if it is primitive, as in line 8, or (3) the loop is terminated if the return probability exceeds a threshold α, after which the stack frame is popped and control is returned to the caller",0
"
move",0
" as before, the non-recursive learned program lacks a provable guarantee of generalization, whereas we show later that the recursive learned program has one",1
 ,1
"
for addition, we analytically determine the verification set",0
" in order to compensate for this, ppivot and pj require special functions (move pivot lo and move j lo) to properly set them to lo in lines 11 and 12 of the partition function in algorithm 3",0
"
figure 1 shows examples of non-recursive and recursive addition traces",1
"
inside bubble and reset, there are two operations that can be made recursive",0
" qcolor(dag[vactive][childlist[vactive]]) refers to the color of the next child of vactive, used in the evaluation of the condition in the if branch in line 9 of algorithm 2",1
" in particular, recursion can be implemented as a program calling itself",1
"
note that the training set can often be considerably smaller than the verification set, and despite this, the learned model can still pass the entire verification set",0
"
the dag is represented as an adjacency list where dag[i][j] refers to the j-th child of vertex i",0
" here as a concrete and general example, we consider a general neural programming architecture (npa), similar to neural programmer-interpreter (npi) in reed & de freitas (2016)",0
" in particular, in the explore function, adding a tail recursive call resets and stores the hidden states associated with vertices in a stack-like fashion",0
"
in order to demonstrate that recursion can help learn and generalize better, for addition, we trained only on traces for 5 arbitrarily chosen 1-digit addition sum examples",1
" we implement a quicksort task, in order to demonstrate that recursion helps with learning divide-and-conquer algorithms",0
" as described in the verification procedure, it is possible to prove our learned recursive program generalizes perfectly by testing on an appropriate set of problem inputs, i",1
" via the recursive call, we effectively forget that the column just added exists, since the recursive call to add starts with a new hidden state for the lstm controller",0
"
arg 2 (object to copy): env stack lo peek, env stack hi peek, phi, plo, ppivot − 1, ppivot + 1, reset
env stack lo peek and env stack hi peek represent qstacklo(pstacklo − 1) and qstackhi(pstackhi − 1), respectively",0
" we prove every member of f is added properly, given that subproblems s = {anan−1 + bnbn−1, an−1an−2 + bn−1bn−2, ",1
 indentation indicates the stack is one level deeper than before,1
"
bubble sort",0
" if the step output specifies a primitive function call, we need to reason about how it can affect the environment so as to change the observation in the next step input",1
 the training set for addition contains 200 traces,0
" for problems where the base cases may be extremely large or infinite, such as certain forms of motor control, recursion can still help reduce the problem of generalization to these two aspects and make the generalization problem significantly simpler to handle and reason about",0
" each step during an execution of the program does one of three things: (1) another subprogram along with associated arguments is called, as in line 10, (2) the program writes to the environment if it is primitive, as in line 8, or (3) the loop is terminated if the return probability exceeds a threshold α, after which the stack frame is popped and control is returned to the caller",1
 ,1
" however, our concept of recursion for neural programs is general",0
"
by nature, recursion reduces the complexity of a problem to simpler instances",1
" for bubble sort, the domain-specific encoder is
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1), q(1, i2), i3 == length, at(1), at(2), at(3)]),
where the environment q ∈ r1×n×k is a scratch-pad that contains 1 row, to represent the state of the array as sorting proceeds in-place, andn columns",0
" observe that the non-recursive program’s correctness degrades for length 11 and beyond, while the recursive program can sort any given array",1
 we found that changing the npi training traces is a simple way to enable this,0
 errors arise due to undesirable and uninterpretable dependencies and associations the architecture learns to store in some high-dimensional hidden state,0
" for grade-school addition, the domain-specific encoder is
fenc(q, i1, i2, i3, i4, at) =mlp ([q(1, i1), q(2, i2), q(3, i3), q(4, i4), at(1), at(2), at(3)]),
where the environment q ∈ r4×n×k is a scratch-pad that contains four rows (the first input number, the second input number, the carry bits, and the output) and n columns",0
 stack push call2 pushes pivot + 1 and hi to qstacklo and qstackhi,0
 we leave this as future work to devise a verification procedure more appropriate to this setting,1
" consequently, there is no concept of length relevant to the problem, which has traditionally been an important focus of length-based curriculum learning",1
" the general notion of recursion has been an important concept in many domains, including mathematics and computer science",1
 we use the lomuto partition scheme; the logic for the recursive trace is shown in algorithm 3,1
" thus, any general neural programming architecture supporting such a call structure can be made to support recursion",0
" plo and phi point to the lo and hi indices of the array, as in algorithm 3",1
5,0
" the npi model has three learnable components: a task-agnostic core, a program-key embedding, and domain-specific encoders that allow the npi to operate in diverse environments",1
" the 4 environment observations aid with control flow; qstacklo(pstacklo− 1) < qstackhi(pstackhi− 1) implements the lo < hi comparison in line 5 of algorithm 3, pstacklo == 1 checks if the stacks are empty in line 18 of algorithm 4, and the other observations (all involving ppivot or pj) deal with logic in the partition function",0
"
an alternative method is to run p on many different program inputs and then observe step input sequences which occur, to create v ",0
" if it is too large, then the semantics of p might not be well-defined on some elements in v , or the spurious step input sequences may not be reachable from any valid problem input (e",0
", qstacklo or qstackhi) or to pointer (e",1
g,0
" an example of part of an addition task trace, written in shorthand, is given in figure 1",1
" using this modification, we constructed a verification set consisting of one array of size 10",1
" consequently, it is infeasible to construct a verification set that covers all cases, and the verification procedure we have described is inadequate",0
" we further have qresult ∈ nu , a scratch-pad which contains the sorted list of vertices at the end of execution, and qstack ∈ nu , which serves the role of the stack s in algorithm 2",1
" at each time step, the core controller can decide to select one of the programs to call with certain arguments",1
"
an alternative way of representing the environment slice is to expose the values of the absolute vertices to the model; however, this makes it difficult to scale the model to larger graphs, since large vertex values are not seen during training time",0
"
we describe the base cases, reduction rules, and the verification set for each task in appendix a",0
" the npi model has three learnable components: a task-agnostic core, a program-key embedding, and domain-specific encoders that allow the npi to operate in diverse environments",0
" for instance, for recursive addition, consider the family f of addition problems anan−1 ",0
 we find that recursion makes it easier for the network to learn the right program and generalize to unknown situations,0
"
after finding v , we construct a set of problem inputs which, when executed on p , create exactly the step input sequences which make up v ",1
" for each function we use to implement the recursive version of topological sort, we need to consider the set of possible environment observation sequences we can create from all valid inputs and test that the learned program produces the correct behavior on each of these inputs",1
" lshift, used in reset, shifts the pointers left until reaching the start token",1
" on the other hand, the recursive programs have learned the true program semantics",1
" also, compared to the bubble pointers in bubble sort, the pointers that perform the comparison for quicksort (the compswap function) are usually not adjacent to each other, making quicksort less local than bubble sort",1
 we find that recursion makes it easier for the network to learn the right program and generalize to unknown situations,1
" 4: function toposort(dag) 5: while there is still a white vertex u: do 6: color[u] = grey 7: vactive = u 8: do 9: if vactive has a white child v then 10: color[v] = grey 11: push vactive onto s 12: vactive = v 13: else 14: color[vactive] = black 15: write vactive to result 16: if s is empty then pass 17: else pop the top vertex off s and set it to vactive 18: while s is not empty
topological sort",1
" the npi model has three learnable components: a task-agnostic core, a program-key embedding, and domain-specific encoders that allow the npi to operate in diverse environments",0
" thus, recursion helps decompose a problem and makes it easier to reason about a program’s behavior for previously unseen situations such as longer inputs",0
"we show that if we incorporate recursion, the learned npi programs can achieve provably perfect generalization for different tasks",0
 the sequence of primitive operations (move and write operations) for the non-recursive and recursive versions are exactly the same,1
" the single-digit multiplication task in zaremba   (2016), the bubble sort task in reed & de freitas (2016), and the graph tasks in graves   (2016))",0
"
the npi accesses an external environment, q, which varies according to the task",0
3,0
" we make the trace recursive by adding a tail recursive call into the trace for the add program after calling add1 and lshift, the original paper uses k = 10, but we found it necessary to augment the range with an end token, in order to terminate properly",0
" 2: initialize lo and hi to be 1 and n, where n is the length of a",0
"
under the assumption that there are no leading 0’s (except in the case of single digits) and the two numbers to be added have the same number of digits, the verification set for addition contains 20,181 input problems",1
" if it is too small, then execution of the program on some input might require evaluation of m(i) on some i /∈ v , and so the behavior ofm(i) might deviate from p (i)",1
" we have three observations: the color of the start node, the color of the active node’s next child to be considered, and whether the stack is empty",1
this material is in part based upon work supported by the national science foundation under grant no,0
" the write operation has the following arguments:
arg 1 (main action): color curr, color next, active start, active neighb, active stack, save, stack push, stack pop, result
color curr colors vactive, color next colors vertex dag[vactive][childlist[vactive]], active start writes pstart to vactive, active neighb writes dag[vactive][childlist[vactive]] to vactive, active stack writes qstack(pstack) to vactive, save writes vactive to vsave, stack push pushes vactive to the top of the stack, stack pop writes a null value to the top of the stack, and result writes vactive to qresult(presult)",0
" the move operation has the following arguments:
arg 1 (pointer): pstacklo, pstackhi, pj , ppivot
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values",0
"
it is sufficient to construct problems where every transition between two adjacent columns is covered",0
 stack pop pushes −∞ values to qstacklo and qstackhi,1
"
it is important to emphasize that at inference time in the npi, the hidden state of the lstm controller is reset (to zero) at each subprogram call, as in line 3 of algorithm 1 (h ← 0)",1
training neural networks to synthesize robust programs from a small number of examples is a challenging task,1
" the single-digit multiplication task in zaremba   (2016), the bubble sort task in reed & de freitas (2016), and the graph tasks in graves   (2016))",0
 the maximum problem length in this training set is 3 (e,1
"
figure 1 shows examples of non-recursive and recursive addition traces",1
"as there is no public implementation of npi, we implemented a version of it in keras that is as faithful to the paper as possible",1
"
base cases and reduction rules for bubble sort",0
" in this architecture, we consider a core controller, e",0
" observe that the non-recursive program’s correctness degrades for length 11 and beyond, while the recursive program can sort any given array",0
" ppivot and pj point to the pivot and j indices of the array, used in the partition function in algorithm 3",0
" observe that partially recursive does slightly better than non-recursive for the setting in which the length of the array is 3, and that the fully recursive version is able to sort every array given to it",1
"
for addition, we analytically determine the verification set",0
" the outputs rt, pt+1, at+1 are used to determine the next action, as described in algorithm 1",1
" for problems where the base cases may be extremely large or infinite, such as certain forms of motor control, recursion can still help reduce the problem of generalization to these two aspects and make the generalization problem significantly simpler to handle and reason about",0
" however, note that there is no provable guarantee that the non-recursive learned program will generalize to all inputs, whereas we show later that the recursive learned program has a provable guarantee of perfect generalization",0
"
the operations we need to be concerned most about are carry and lshift, which induce partial environment states spanning two columns",0
we emphasize that the notion of a neural recursive program has not been presented in the literature before: this is our main contribution,0
 this modification also enables us to sort arrays containing arbitrary comparable elements,0
" , a1a0 + b1b0} are added properly",1
"
move",0
"
we would also like to point out that in this paper, we provide as an example one way to train a recursive neural program, by providing a certain training execution trace to the npi model",1
" empirically, we observe that the learned recursive programs solve all valid inputs with 100% accuracy after training on a very small number of examples, out-performing previous generalization results",1
"
the npi accesses an external environment, q, which varies according to the task",1
" in this pair, the step input runs a subprogram add1 that has no arguments, and the step output contains a program write that has arguments of out and 1",0
g,1
" thus, recursion helps decompose a problem and makes it easier to reason about a program’s behavior for previously unseen situations such as longer inputs",0
" in this pair, the step input runs a subprogram add1 that has no arguments, and the step output contains a program write that has arguments of out and 1",0
"
the training set for bubble sort contains 100 traces, with maximum problem length of 2 (e",0
" thus, recursion helps decompose a problem and makes it easier to reason about a program’s behavior for previously unseen situations such as longer inputs",0
 pstacklo and pstackhi point to the top (empty) positions in qstacklo and qstackhi,1
" formally, the npi is represented by the following set of equations:
st = fenc(et, at)
ht = flstm(st, pt, ht−1)
rt = fend(ht), pt+1 = fprog(ht), at+1 = farg(ht)
t is a subscript denoting the time-step; fenc is a domain-specific encoder (to be described later) that takes in the environment slice et and arguments at; flstm represents the core module, which takes in the state st generated by fenc, a program embedding pt ∈ rp , and hidden lstm state ht; fend decodes the return probability rt; fprog decodes a program key embedding pt+1;1 and farg decodes arguments at+1",0
" provably perfect generalization implies the model will behave correctly, given any valid input",0
" finally, we implement recursive traces for our own topological sort and quicksort tasks",0
 the memory updates of these neural programs are so complex and interdependent that it is difficult to reason about the behaviors of the learned neural program under previously unseen situations (such as problems with longer inputs),0
" plo and phi point to the lo and hi indices of the array, as in algorithm 3",1
" here as a concrete and general example, we consider a general neural programming architecture (npa), similar to neural programmer-interpreter (npi) in reed & de freitas (2016)",0
" the core lstm module’s hidden state is preserved over all these add1 calls, and it is difficult to interpret with certainty what happens over longer timesteps without concretely evaluating the lstm with an input of that length",0
 18: push lo and p− 1 to slo and shi,0
 we created a program set that reflects the semantics of algorithm 2,0
" in particular, we construct new training traces which explicitly contain recursive elements and show that with this type of trace, npi easily learns recursive programs",1
" however, the authors of npi did not consider explicitly the notion of recursion and as a consequence, did not learn recursive programs",1
"
topological sort",1
"
in addition, none of the current approaches to neural programming provide a method or even aim to enable provable guarantees about generalization",1
" without using a recursive program, such a proof is not possible, because the non-recursive program runs on an arbitrarily long addition problem that creates correspondingly long sequences of step inputs; in the non-recursive formulation of addition, add calls add1 a number of times that is dependent on the length of the input",1
" partial recursion only adds a tail recursive call to bubblesort after bubble and reset, similar to the tail recursive call described previously for addition",0
 b1b0 where no carry operations occur,0
" we also hope to decrease supervision, for example by training with only partial or non-recursive traces, and to develop novel neural programming architectures integrated directly with a notion of recursion",1
"
move",0
" for example, a step input-step output pair in lines 2 and 3 of the left-hand side of figure 1 is (add1, write out 1)",0
" consequently, there is no concept of length relevant to the problem, which has traditionally been an important focus of length-based curriculum learning",1
" for the recursive formulation of addition, we analytically construct the set of input problems that cover all base cases and reduction rules",0
" if a white vertex is found, set pstart to point to it; this signifies the start of a traversal of a new connected component",0
"
the training set for quicksort contains 4 traces, synthesized from arrays of length 5",0
 the environment and return probability are omitted for readability,0
 we leave this as future work to devise a verification procedure more appropriate to this setting,0
" plo and phi point to the lo and hi indices of the array, as in algorithm 3",0
"
as a concrete instantiation, we show in this paper that we can enable recursive neural programs in the npi model, and thus enable perfectly generalizable neural programs for tasks such as sorting where the original, non-recursive npi program fails",0
" in future work, we seek to enable more tasks with recursive structure",0
g,0
" 4: function toposort(dag) 5: while there is still a white vertex u: do 6: color[u] = grey 7: vactive = u 8: do 9: if vactive has a white child v then 10: color[v] = grey 11: push vactive onto s 12: vactive = v 13: else 14: color[vactive] = black 15: write vactive to result 16: if s is empty then pass 17: else pop the top vertex off s and set it to vactive 18: while s is not empty
topological sort",0
" using this modification, we constructed a verification set consisting of one array of size 10",0
", qstacklo or qstackhi) or to pointer (e",0
"
we describe the base cases, reduction rules, and the verification set for each task in appendix a",1
 add1 is a subprogram that adds the current column (writing the appropriate digit to the output row and carrying a bit to the next column if needed),0
 ,0
" formally, a function exhibits recursive behavior when it possesses two properties: (1) base cases— terminating scenarios that do not use recursion to produce answers; (2) a set of rules that reduces all other problems toward the base cases",0
g,1
" for each length, we test each program on 30 randomly generated problems",1
" for tasks other than addition, it is difficult to analytically determine the verification set, so instead, we randomly generate input candidates until they completely cover the base cases and reduction rules",0
" at each time step, the core controller can decide to select one of the programs to call with certain arguments",1
 we describe the recursive re-formulation of traces for two tasks from the original npi paper—grade-school addition and bubble sort,0
"
it is sufficient to construct problems where every transition between two adjacent columns is covered",0
", an array for quicksort or a dag for topological sort)",0
"
for addition, we analytically determine the verification set",0
3,1
 we found that changing the npi training traces is a simple way to enable this,0
 this is due to the properties of the training set – we trained on 2 traces synthesized from arrays of length 7 and 1 trace synthesized from an array of length 6,1
" for example, for the addition task, the model is trained on short inputs and then tested on its ability to sum inputs with much longer numbers of digits",1
"
we construct input problems by splitting into two cases: one case in which the left column contains a null value and another in which the left column does not contain any null values",0
" et+1 ∼ fenv(et, pt, at)",0
" at end, pointer ppivot is moved to the pivot compswap loop, move pivot lo, move j lo, swap
none
compswap loop runs the for loop inside the partition function compswap, move none compswap compares a[pivot] ≤ a[j]; if so, perform a swap and increment ppivot swap, move none set pivot lo sets ppivot to lo index none none set j lo sets pj to lo index none none set j null sets pj to −∞ none none stack pushes lo/hi states
onto stacks slo and shi according to argument (described below)
write, move described below
move moves pointer one unit up or down none described below swap swaps elements at given array indices none described below write write a value either to stack (e",1
" 3: 4: function partition(a, lo, hi) 5: pivot = lo 6: for j ∈ [lo, hi− 1] : do 7: if a[j] ≤ a[hi] then 8: swap a[pivot] with a[j] 9: pivot = pivot+ 1 0: swap a[pivot] with a[hi] 11: return pivot 12: 13: function quicksort(a, lo, hi) 14: while slo and shi are not empty: do 15: pop states off slo and shi, writing them to lo and hi",0
" in fact, we argue that recursion is an essential element for neural programs to generalize, and makes it tractable to prove the generalization of neural programs",1
" the space of possible programs is extremely large, and composing a program that performs robustly on the infinite space of possible inputs is difficult—in part because it is impractical to obtain enough training examples to easily disambiguate amongst all possible programs",0
g,1
" for instance, for recursive addition, consider the family f of addition problems anan−1 ",0
e,0
"
base cases and reduction rules for addition",0
"
it is important to emphasize that at inference time in the npi, the hidden state of the lstm controller is reset (to zero) at each subprogram call, as in line 3 of algorithm 1 (h ← 0)",0
" using this modification, we constructed a verification set consisting of one array of size 10",1
" if a white vertex is found, set pstart to point to it; this signifies the start of a traversal of a new connected component",0
" as with the original npi architecture, the experiments for this paper always used a 3-tuple of integers at = (at(1), at(2), at(3))",0
" that is to say, the network does not learn the true program semantics",0
" the space of possible programs is extremely large, and composing a program that performs robustly on the infinite space of possible inputs is difficult—in part because it is impractical to obtain enough training examples to easily disambiguate amongst all possible programs",1
"when constructing a neural network for the purpose of learning a program, there are two orthogonal aspects to consider",0
" observe that partially recursive does slightly better than non-recursive for the setting in which the length of the array is 3, and that the fully recursive version is able to sort every array given to it",1
" if no white vertex is found, the entire execution is terminated move none write write a value either to environment (e",0
" by iterating with this procedure, and then running p on the input observation set that we obtain for the entry point function, we can obtain v precisely",1
" even though the earlier version also generalized just as well in practice, relatively small differences in the formulation of the traces and the environment observations can drastically change the difficulty of verification",0
" however, note that there is no provable guarantee that the non-recursive learned program will generalize to all inputs, whereas we show later that the recursive learned program has a provable guarantee of perfect generalization",0
" the core lstm module’s hidden state is preserved over all these add1 calls, and it is difficult to interpret with certainty what happens over longer timesteps without concretely evaluating the lstm with an input of that length",1
" lshift, used in reset, shifts the pointers left until reaching the start token",0
" we prove every member of f is added properly, given that subproblems s = {anan−1 + bnbn−1, an−1an−2 + bn−1bn−2, ",1
" more specifically, when m is trained on simpler inputs, such as inputs of a small length, the generalization metric evaluates how well m will do on more complex inputs, such as inputs of much longer length",1
" we have three observations: the color of the start node, the color of the active node’s next child to be considered, and whether the stack is empty",1
"
the non-recursive trace loops on cycles of add1 and lshift",0
" in this section, we give a brief review of the npi architecture from reed & de freitas (2016) as background",0
" however, the authors of npi did not consider explicitly the notion of recursion and as a consequence, did not learn recursive programs",0
3,0
"
base cases and reduction rules for topological sort",0
"we train using the adam optimizer and use a 2-layer lstm and task-specific state encoders for the external environments, as described in reed & de freitas (2016)",0
" when the program is called, the current context including the caller’s memory state is stored on a stack; when the program returns, the stored context is popped off the stack to resume execution in the previous caller’s context",0
"
we construct input problems by splitting into two cases: one case in which the left column contains a null value and another in which the left column does not contain any null values",1
" the stack operation has the following arguments:
arg 1 (operation): stack push call1, stack push call2, stack pop
stack push call1 pushes lo and pivot−1 toqstacklo andqstackhi",0
" we made the assumption of equivalent lengths in order to parametrize the input format with respect to length, but this assumption can be removed as well",1
 ,0
g,0
g,0
"
the training set for bubble sort contains 100 traces, with maximum problem length of 2 (e",0
"as discussed in section 2, the neural programmer-interpreter (npi) is an instance of a neural programmer architecture and hence it naturally supports recursion",0
" 9: else 10: function run(e, p2, a2)
a description of the inference procedure is given in algorithm 1",0
"
we note that for tasks with very large input domains, such as ones involving mnist digits or speech samples, the state space of base cases and reduction rules could be prohibitively large, possibly infinite",0
" in order to claim a proof, we must verify the model produces correct behavior over all base cases and reductions, as described in section 2",0
"
note that the recursion for quicksort is not purely tail recursive and therefore represents a more complex kind of recursion that is harder to learn than in the previous tasks",0
" finally, we implement recursive traces for our own topological sort and quicksort tasks",0
" we made the assumption of equivalent lengths in order to parametrize the input format with respect to length, but this assumption can be removed as well",0
" k is set to 11, to represent the range of 10 possible digits, along with a token representing the end of input",0
" if no white vertex is found, the entire execution is terminated move none write write a value either to environment (e",0
" the theorem states that for the same sequence of step inputs, the model produces the exact same step output as the target program it aims to learn",1
we emphasize that the notion of a neural recursive program has not been presented in the literature before: this is our main contribution,0
" observe that the non-recursive program’s correctness degrades for length 11 and beyond, while the recursive program can sort any given array",0
"
algorithm 2 depth first search topological sort 1: color all vertices white",0
"
we construct input problems by splitting into two cases: one case in which the left column contains a null value and another in which the left column does not contain any null values",0
" to the best of our knowledge, this is the first time verification has been applied to a neural program, providing provable guarantees about its behavior",0
 recursion enables provable guarantees on neural programs’ behavior without needing to exhaustively enumerate all possible inputs to the programs,0
" if the step output specifies a primitive function call, we need to reason about how it can affect the environment so as to change the observation in the next step input",1
"as there is no public implementation of npi, we implemented a version of it in keras that is as faithful to the paper as possible",1
" the second is the training procedure, which consists of the form of the training data and the optimization process",1
", the trace corresponding to the problem “109 + 101”)",0
" in this architecture, we consider a core controller, e",1
"
as a concrete instantiation, we show in this paper that we can enable recursive neural programs in the npi model, and thus enable perfectly generalizable neural programs for tasks such as sorting where the original, non-recursive npi program fails",0
 ,0
" for each function we use to implement the recursive version of topological sort, we need to consider the set of possible environment observation sequences we can create from all valid inputs and test that the learned program produces the correct behavior on each of these inputs",0
"
in this paper, we propose to resolve these issues by explicitly incorporating recursion into neural architectures",0
 this modification also enables us to sort arrays containing arbitrary comparable elements,0
" for instance, for recursive addition, consider the family f of addition problems anan−1 ",0
"
move",0
"
through careful reasoning about the possible set of environment observations created by all valid inputs, and how each of the operations in the execution trace affects the environment, we can construct v using the procedure described in section 3",0
 errors arise due to undesirable and uninterpretable dependencies and associations the architecture learns to store in some high-dimensional hidden state,1
 add1 is a subprogram that adds the current column (writing the appropriate digit to the output row and carrying a bit to the next column if needed),0
g,0
" finally, the boolean pstack == 1 is used to check whether the stack is empty in line 18 of algorithm 2",1
" the recursive version of topological sort solves all graph instances we tried, from graphs of size 5 through 70",1
 these pointers are referred to as bubble pointers,0
"
an alternative method is to run p on many different program inputs and then observe step input sequences which occur, to create v ",0
e,0
" in fact, it is one of our future directions to explore new ways to train a recursive neural program without providing explicit training execution traces or with only partial or non-recursive traces",0
as mentioned in section 2,1
" m is considered to have perfect generalization if m can give the right answer for any input, such as inputs of arbitrary length",0
" 16: p = partition(a, lo, hi) 17: push p+ 1 and hi to slo and shi",1
" each step during an execution of the program does one of three things: (1) another subprogram along with associated arguments is called, as in line 10, (2) the program writes to the environment if it is primitive, as in line 8, or (3) the loop is terminated if the return probability exceeds a threshold α, after which the stack frame is popped and control is returned to the caller",0
" for these programs, we re-use the appropriate program sets (the associated subprograms), and we refer the reader to the appendix of reed & de freitas (2016) for further details on the subprograms used in addition and bubble sort",1
" ppivot and pj point to the pivot and j indices of the array, used in the partition function in algorithm 3",0
"
to perform the verification as described here, it is critical to construct v correctly",0
"1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 while (pstacklo 6= 1): 6 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 7 stack(stack_pop) 8 else: 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 stack(stack_pop) 12 partition() 13 stack(stack_push_call2) 14 stack(stack_push_call1) 15 } 16 17 partition() { 18 set_pivot_lo() 19 set_j_lo() 20 compswap_loop() 21 swap(ppivot, phi) 22 set_j_null() 23 } 24 25 compswap_loop() { 26 while (pj 6= phi): 27 compswap() 28 move(pj, up) 29 } 30 31 compswap() { 32 if (a[pj ] ≤ a[phi]): 33 swap(ppivot, pj) 34 move(ppivot, up) 35 } 36 37 stack(op) { 38 if (op == stack_push_call1): 39 write(env_stack_lo, plo) 40 write(env_stack_hi, ppivot − 1) 41 move(pstacklo, up) 42 move(pstackhi, up) 43 44 if (op == stack_push_call2): 45 write(env_stack_lo, ppivot + 1) 46 write(env_stack_hi, phi) 47 move(pstacklo, up) 48 move(pstackhi, up) 49 50 if (op == stack_pop): 51 write(env_stack_lo, reset) 52 write(env_stack_hi, reset) 53 move(pstacklo, down) 54 move(pstackhi, down) 55 }altered recursive functions 1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 6 partition() 7 stack(stack_push_call2) 8 stack(stack_push_call1) 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 quicksort() // recursive call 12 stack(stack_pop) 13 write(phi, env_stack_hi_peek) 14 write(plo, env_stack_lo_peek) 15 quicksort() // recursive call 16 stack(stack_pop) 17 } 18 19 compswap_loop() { 20 if (pj 6= phi): 21 compswap() 22 move(pj, up) 23 compswap_loop() // recursive call 24 }in this section, we describe the space of base cases and reduction rules that must be covered for each of the four sample tasks, in order to create the verification set",0
" we prove every member of f is added properly, given that subproblems s = {anan−1 + bnbn−1, an−1an−2 + bn−1bn−2, ",0
" , a1a0 + b1b0} are added properly",0
g,1
 figure 2 shows examples of traces for the different versions of bubble sort,0
 recursion enables provable guarantees on neural programs’ behavior without needing to exhaustively enumerate all possible inputs to the programs,0
" at any given time, the npi has access to the values referred to by two pointers, represented by q(1, i1) and q(1, i2),",0
 this modification also enables us to sort arrays containing arbitrary comparable elements,1
"
we describe the details of the npi model relevant to our contributions",1
" recursion is an important concept in programming languages and a critical tool to
reduce the complexity of programs",1
" however, our concept of recursion for neural programs is general",0
3,1
" for each task, given the verification set, we check the traces and results of the learned, to-be-verified neural program (described in section 4",0
 we use the lomuto partition scheme; the logic for the recursive trace is shown in algorithm 3,0
" naı̈vely, we might expect to synthesize and test an input for any sequence created by combining the four possible colors in two variables and another boolean variable for whether the stack is empty (so 32 possible observations at any point), but for various reasons, most of these combinations are impossible to occur at any given point in the execution trace",0
"
move",0
 recursion can be implemented differently for different neural programming models,0
 almost all architectures train on program input/output pairs,0
"we describe how models trained with recursive traces can be proven to generalize, by using the verification procedure described in section 3",0
 this paper is the first (to our knowledge) to investigate the important problem of provable generalization properties of neural programs,0
" in this pair, the step input runs a subprogram add1 that has no arguments, and the step output contains a program write that has arguments of out and 1",0
"
one common strategy to improve generalization is to use curriculum learning, where the model is trained on inputs of gradually increasing complexity",1
" empirically, we observe that the learned recursive programs solve all valid inputs with 100% accuracy after training on a very small number of examples, out-performing previous generalization results",1
" the contents ofqresult andqstack are not exposed directly through the domain-specific encoder; rather, we define primitive functions which manipulate these scratch-pads",1
"
in this paper, we propose to resolve these issues by explicitly incorporating recursion into neural architectures",0
" thus, any general neural programming architecture supporting such a call structure can be made to support recursion",0
" our results show that for all learned, to-be-verified neural programs, they all produced the same traces as those produced by p on the verification set",0
" in order to compensate for this, ppivot and pj require special functions (move pivot lo and move j lo) to properly set them to lo in lines 11 and 12 of the partition function in algorithm 3",0
"
we experiment with two levels of recursion—partial and full",1
" in particular, we construct new training traces which explicitly contain recursive elements and show that with this type of trace, npi easily learns recursive programs",0
 npi then outputs the return probability and next program and arguments to execute,0
g,1
"1, we hypothesize the non-recursive programs do not generalize well because they have learned spurious dependencies specific to the training set, such as length of the input problems",0
" here as a concrete and general example, we consider a general neural programming architecture (npa), similar to neural programmer-interpreter (npi) in reed & de freitas (2016)",0
" the swap operation has the following arguments:
arg 1 (swap object 1): ppivot
arg 2 (swap object 2): phi, pj
write",0
"
arg 2 (increment or decrement): up, down
swap",1
" the verification set can be very small, as we found a 10-element array ([8,2,1,2,0,8,5,8,3,7]) is sufficient to cover all of v ",1
" both the non-recursive and recursive learned programs generalize on all graphs we tried, up to 120 vertices",1
" the pointer at index i3 represents a counter internal to the environment that is incremented once after each pass of the algorithm (one cycle of bubble and reset); when incremented a number of times equal to the length of the array, the flag i3 == length becomes true and terminates the entire algorithm ",0
" while this matches the description from reed & de freitas (2016), we found that this causes an unnecessary blowup in the size of v and makes it much more difficult to construct the verification set",0
" the space of possible programs is extremely large, and composing a program that performs robustly on the infinite space of possible inputs is difficult—in part because it is impractical to obtain enough training examples to easily disambiguate amongst all possible programs",0
"2 at any given time, the npi has access to values pointed to by four pointers in each of the four rows, represented by q(1, i1), q(2, i2), q(3, i3), and q(4, i4)",0
" the space of possible programs is extremely large, and composing a program that performs robustly on the infinite space of possible inputs is difficult—in part because it is impractical to obtain enough training examples to easily disambiguate amongst all possible programs",0
" note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values, and arg 2 can only take one of 7 values",0
" finally, we implement recursive traces for our own topological sort and quicksort tasks",0
" m is considered to have perfect generalization if m can give the right answer for any input, such as inputs of arbitrary length",1
" however, models that make use of this strategy eventually fail after a certain level of complexity (e",0
" 2: initialize lo and hi to be 1 and n, where n is the length of a",0
" however, our concept of recursion for neural programs is general",0
" if the step output specifies a primitive function call, we need to reason about how it can affect the environment so as to change the observation in the next step input",0
" consequently, there is no concept of length relevant to the problem, which has traditionally been an important focus of length-based curriculum learning",0
", the verification set",0
" for these programs, we re-use the appropriate program sets (the associated subprograms), and we refer the reader to the appendix of reed & de freitas (2016) for further details on the subprograms used in addition and bubble sort",1
" if yes, then this indicates that the learned neural program achieves provably perfect generalization",1
" as a consequence, our learned neural programs empirically achieve perfect generalization from a very small number of training examples",0
"when constructing a neural network for the purpose of learning a program, there are two orthogonal aspects to consider",0
" the partial recursion is not enough for perfect generalization, as will be presented later in section 4",1
" the single-digit multiplication task in zaremba   (2016), the bubble sort task in reed & de freitas (2016), and the graph tasks in graves   (2016))",0
" the outputs rt, pt+1, at+1 are used to determine the next action, as described in algorithm 1",0
" the core module of the npi is an lstm controller that takes as input a slice of the current external environment, via a set of pointers, and a program and arguments to execute",1
" for example, a step input-step output pair in lines 2 and 3 of the left-hand side of figure 1 is (add1, write out 1)",0
" full recursion, in addition to making the aforementioned tail recursive call, adds two additional recursive calls; bstep and lshift are made tail recursive",0
"
we construct input problems by splitting into two cases: one case in which the left column contains a null value and another in which the left column does not contain any null values",1
" we make the trace recursive by adding a tail recursive call into the trace for the add program after calling add1 and lshift, the original paper uses k = 10, but we found it necessary to augment the range with an end token, in order to terminate properly",0
 we instantiated recursion for the neural programmerinterpreter by changing the training traces,1
" 9: else 10: function run(e, p2, a2)
a description of the inference procedure is given in algorithm 1",0
"
by nature, recursion reduces the complexity of a problem to simpler instances",0
 table 4 demonstrates that the accuracy of the non-recursive program degrades sharply when moving from arrays of length 7 to arrays of length 8,1
" in this pair, the step input runs a subprogram add1 that has no arguments, and the step output contains a program write that has arguments of out and 1",0
" lshift moves the four pointers to the left, to move to the next column",0
" in future work, we would like to decrease supervision and construct models that are capable of coming up with recursive abstractions themselves",1
 indentation indicates the stack is one level deeper than before,0
" in contrast, each call to the recursive add always runs for a fixed number of steps, even on arbitrarily long problems in f , so we can test that it performs correctly on a small, fixed number of step input sequences",1
" finally, we implement recursive traces for our own topological sort and quicksort tasks",0
" when the program is called, the current context including the caller’s memory state is stored on a stack; when the program returns, the stored context is popped off the stack to resume execution in the previous caller’s context",1
e,1
g,0
 we instantiated recursion for the neural programmerinterpreter by changing the training traces,1
"
inside bubble and reset, there are two operations that can be made recursive",0
 this is due to the properties of the training set – we trained on 2 traces synthesized from arrays of length 7 and 1 trace synthesized from an array of length 6,0
" as aforementioned, the npi model naturally supports recursion",1
"
formally, verification consists of proving the following theorem:
∀i ∈ v,m(i) ⇓ p (i) where i denotes a sequence of step inputs (within one function call), v denotes the set of valid sequences of step inputs, m denotes the neural network model, p denotes the correct program, and p (i) denotes the next step output from the correct program",0
" on the other hand, the recursive programs have learned the true program semantics",0
"
note that the recursion for quicksort is not purely tail recursive and therefore represents a more complex kind of recursion that is harder to learn than in the previous tasks",0
" the corresponding edge list is [(1, 2), (1, 5), (2, 4), (2, 5), (3, 5)]",0
" the verification set can be very small, as we found a 10-element array ([8,2,1,2,0,8,5,8,3,7]) is sufficient to cover all of v ",1
" the verification set can be very small, as we found a 10-element array ([8,2,1,2,0,8,5,8,3,7]) is sufficient to cover all of v ",1
" the corresponding edge list is [(1, 2), (1, 5), (2, 4), (2, 5), (3, 5)]",1
 we describe the recursive re-formulation of traces for two tasks from the original npi paper—grade-school addition and bubble sort,0
" in future work, we would like to decrease supervision and construct models that are capable of coming up with recursive abstractions themselves",0
" recall that this verification procedure cannot be performed for the non-recursive versions, since the propagation of the hidden state in the core lstm module makes reasoning difficult and so we would need to check an unbounded number of examples",1
" 9: else 10: function run(e, p2, a2)
a description of the inference procedure is given in algorithm 1",1
" even though the earlier version also generalized just as well in practice, relatively small differences in the formulation of the traces and the environment observations can drastically change the difficulty of verification",0
"
as in line 13 of the right-hand side of figure 1",0
" the assumption of leading 0’s can be easily removed, at the cost of slightly increasing the size of the verification set",1
" 9: else 10: function run(e, p2, a2)
a description of the inference procedure is given in algorithm 1",0
"
given a verification set, we can then run the model on the verification set to check if the produced traces and results are correct",1
" formally, a function exhibits recursive behavior when it possesses two properties: (1) base cases— terminating scenarios that do not use recursion to produce answers; (2) a set of rules that reduces all other problems toward the base cases",1
" if the program is primitive, the next environmental state et+1 will be affected by pt and at, i",0
", to change the value of vactive) none described below move move a pointer (e",0
 our result shows that the training procedure and the npi architecture is capable of generalizing from the step input-output pairs seen in the training data to the unseen ones present in the verification set,0
" that is to say, the network does not learn the true program semantics",1
" we call this set of inputs the verification set, sv ",0
 the maximum problem length in this training set is 3 (e,0
" this guarantees that the step input sequences considered during verification contain all step input sequences which arise during execution of an unseen problem in f , leading to generalization to any problem in f ",1
", the trace corresponding to the problem “109 + 101”)",0
"
is critical for implementing recursion, since it permits us to restrict our attention to the currently relevant recursive call, ignoring irrelevant details about other contexts",1
" if it is too large, then the semantics of p might not be well-defined on some elements in v , or the spurious step input sequences may not be reachable from any valid problem input (e",0
" for bubble sort, the domain-specific encoder is
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1), q(1, i2), i3 == length, at(1), at(2), at(3)]),
where the environment q ∈ r1×n×k is a scratch-pad that contains 1 row, to represent the state of the array as sorting proceeds in-place, andn columns",1
" for bubble sort, the domain-specific encoder is
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1), q(1, i2), i3 == length, at(1), at(2), at(3)]),
where the environment q ∈ r1×n×k is a scratch-pad that contains 1 row, to represent the state of the array as sorting proceeds in-place, andn columns",1
" the verification set can be very small, as we found a 10-element array ([8,2,1,2,0,8,5,8,3,7]) is sufficient to cover all of v ",1
"
since npi naturally supports the notion of recursion, a key question is how to enable npi to learn recursive programs",1
as mentioned in section 2,0
 figure 2 shows examples of traces for the different versions of bubble sort,0
" the non-recursive and partially recursive versions are unable to sort long arrays, beyond length 8",0
" in particular, recursion can be implemented as a program calling itself",1
" the pointers at index i1 and i2 are used to compare the pair of numbers considered during the bubble sweep, swapping them if the number at i1 is greater than that in i2",1
" the partial recursion is not enough for perfect generalization, as will be presented later in section 4",0
" empirically, we observe that the learned recursive programs solve all valid inputs with 100% accuracy after training on a very small number of examples, out-performing previous generalization results",1
"
the same set of problems was used to generate the training traces for all formulations of the task, for non-recursive and recursive versions",1
 this verification phase only needs to be performed once after training,1
 3: begin traversing from vertex 1 in the dag,0
" here as a concrete and general example, we consider a general neural programming architecture (npa), similar to neural programmer-interpreter (npi) in reed & de freitas (2016)",0
 npi then outputs the return probability and next program and arguments to execute,0
", the trace corresponding to the array [3,2])",1
"
we refer the reader to the appendix for the non-recursive trace generating functions",0
 18: push lo and p− 1 to slo and shi,0
"
we propose and describe our verification procedure",0
" 2: initialize lo and hi to be 1 and n, where n is the length of a",0
 ,1
 the program terminates when seeing no numbers in the current column,0
" without using a recursive program, such a proof is not possible, because the non-recursive program runs on an arbitrarily long addition problem that creates correspondingly long sequences of step inputs; in the non-recursive formulation of addition, add calls add1 a number of times that is dependent on the length of the input",1
" for purposes of verification, we replace the domain-specific encoder with the following:
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1) ≤ q(1, i2), 1 ≤ i1 ≤ length, 1 ≤ i2 ≤ length, i3 == length, at(1), at(2), at(3)]),which directly exposes which of the two values pointed to is larger",0
" formally, a function exhibits recursive behavior when it possesses two properties: (1) base cases— terminating scenarios that do not use recursion to produce answers; (2) a set of rules that reduces all other problems toward the base cases",1
g,1
" in particular, we construct new training traces which explicitly contain recursive elements and show that with this type of trace, npi easily learns recursive programs",0
 ,1
"
in addition, none of the current approaches to neural programming provide a method or even aim to enable provable guarantees about generalization",1
"
the same set of problems was used to generate the training traces for all formulations of the task, for non-recursive and recursive versions",0
"
the dag is represented as an adjacency list where dag[i][j] refers to the j-th child of vertex i",0
", to color a vertex) or variable (e",0
 b1b0 where no carry operations occur,0
"1; and for bubble sort, appendix a",1
6,0
" in contrast, each call to the recursive add always runs for a fixed number of steps, even on arbitrarily long problems in f , so we can test that it performs correctly on a small, fixed number of step input sequences",0
training neural networks to synthesize robust programs from a small number of examples is a challenging task,1
"
one common strategy to improve generalization is to use curriculum learning, where the model is trained on inputs of gradually increasing complexity",0
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,1
"
after finding v , we construct a set of problem inputs which, when executed on p , create exactly the step input sequences which make up v ",0
 almost all architectures train on program input/output pairs,0
" in contrast, each call to the recursive add always runs for a fixed number of steps, even on arbitrarily long problems in f , so we can test that it performs correctly on a small, fixed number of step input sequences",1
"
arg 2 (increment or decrement): up, down
swap",1
"
note that the recursion for quicksort is not purely tail recursive and therefore represents a more complex kind of recursion that is harder to learn than in the previous tasks",0
" 9: else 10: function run(e, p2, a2)
a description of the inference procedure is given in algorithm 1",1
"
under the assumption that there are no leading 0’s (except in the case of single digits) and the two numbers to be added have the same number of digits, the verification set for addition contains 20,181 input problems",0
" m is considered to have perfect generalization if m can give the right answer for any input, such as inputs of arbitrary length",0
" on the other hand, the non-recursive version has low accuracy, beginning from size 5, and fails completely for graphs of size 8 and beyond",1
"
training setup",0
" the contents ofqresult andqstack are not exposed directly through the domain-specific encoder; rather, we define primitive functions which manipulate these scratch-pads",0
 we adapt machinery from the original paper slightly to fit our needs,0
"
in addition, none of the current approaches to neural programming provide a method or even aim to enable provable guarantees about generalization",0
" in this pair, the step input runs a subprogram add1 that has no arguments, and the step output contains a program write that has arguments of out and 1",1
" here as a concrete and general example, we consider a general neural programming architecture (npa), similar to neural programmer-interpreter (npi) in reed & de freitas (2016)",0
 the program terminates when seeing no numbers in the current column,0
"
base cases and reduction rules for bubble sort",1
" k is set to 11, to denote the range of possible numbers (0 through 9), along with the start/end token (represented with the same encoding) which is observed when a pointer reaches beyond the bounds of the input",1
" thus, any general neural programming architecture supporting such a call structure can be made to support recursion",1
g,1
" 2: initialize lo and hi to be 1 and n, where n is the length of a",0
 u varies with the number of vertices in the graph,0
g,0
 we adapt machinery from the original paper slightly to fit our needs,0
" et+1 ∼ fenv(et, pt, at)",0
" finally, we implement recursive traces for our own topological sort and quicksort tasks",0
"as there is no public implementation of npi, we implemented a version of it in keras that is as faithful to the paper as possible",0
" however, note that there is no provable guarantee that the non-recursive learned program will generalize to all inputs, whereas we show later that the recursive learned program has a provable guarantee of perfect generalization",0
 there are 2 variables (vactive and vsave); vactive holds the active vertex (as in algorithm 2) and vsave holds the value of vactive before executing line 12 of algorithm 2,0
"
arg 2 (increment or decrement): up, down1 // top level topological sort call 2 toposort() { 3 while (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 } 0 11 traverse() { 12 check_child() 13 explore() 14 } 15 16 check_child() { 17 while (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 18 move(childlist[vactive], up) 19 } 20 21 explore() { 22 do 23 if (qcolor(dag[vactive][childlist[vactive]]) is white): 24 write(color_next, color_grey) 25 stack(push) 26 write(save) 27 write(active_neighb) 28 move(childlist[vsave], up) 29 else: 30 write(color_curr, color_black) 31 write(result) 32 move(presult, up) 33 if(pstack == 1): 34 break 35 else: 36 stack(pop) 37 check_child() 38 while (true) 39 } 40 41 stack(op) { 42 if (op == push): 43 write(stack_push) 44 move(pstack, up) 45 46 if (op == pop): 47 write(active_stack) 48 write(stack_pop) 49 move(pstack, down) 50 } 51 52 next_start() { 53 while(qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 54 move(pstart, up) 55 }altered recursive functions  // top level topological sort call 2 toposort() { 3 if (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 toposort() // recursive call 0 } 11 12 check_child() { 13 if (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 14 move(childlist[vactive], up) 15 check_child() // recursive call 16 } 17 18 explore() { 19 if (qcolor(dag[vactive][childlist[vactive]]) is white): 20 write(color_next, color_grey) 21 stack(push) 22 write(save) 23 write(active_neighb) 24 move(childlist[vsave], up) 25 else: 26 write(color_curr, color_black) 27 write(result) 28 move(presult, up) 29 if(pstack == 1): 30 return 31 else: 32 stack(pop) 33 check_child() 34 explore() // recursive call 35 } 36 37 next_start() { 38 if (qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 39 move(pstart, up) 40 next_start() // recursive call 41 }algorithm 4 iterative quicksort 1: initialize an array a to sort and two empty stacks slo and shi",0
"
grade-school addition",0
"
in this general neural programming architecture, we show it is easy to support recursion",0
" the add reduction rule ensures that each call to add only covers two adjacent columns, and so the lstm only ever runs for a fixed number of steps necessary to process these two columns",0
" the non-recursive and partially recursive versions are unable to sort long arrays, beyond length 8",1
"
note that the recursion for quicksort is not purely tail recursive and therefore represents a more complex kind of recursion that is harder to learn than in the previous tasks",0
3,0
" lshift moves the four pointers to the left, to move to the next column",0
" for each task, given the verification set, we check the traces and results of the learned, to-be-verified neural program (described in section 4",0
"we describe how models trained with recursive traces can be proven to generalize, by using the verification procedure described in section 3",0
 3: begin traversing from vertex 1 in the dag,0
" to the best of our knowledge, this is the first time verification has been applied to a neural program, providing provable guarantees about its behavior",0
"
we also report on generalization results for the non-recursive and recursive versions of this variant of bubble sort",1
" we implement a quicksort task, in order to demonstrate that recursion helps with learning divide-and-conquer algorithms",0
" the move operation has the following arguments:
arg 1 (pointer): presult, pstack, pstart, childlist[vactive], childlist[vsave]
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 5 values",0
", to change the value of phi) none described belowstack",0
" given a precise definition of p , it may be possible to automate the generation of v from p in future work",0
"
in addition, none of the current approaches to neural programming provide a method or even aim to enable provable guarantees about generalization",0
" 2: initialize lo and hi to be 1 and n, where n is the length of a",1
 the memory updates of these neural programs are so complex and interdependent that it is difficult to reason about the behaviors of the learned neural program under previously unseen situations (such as problems with longer inputs),0
" lshift moves the four pointers to the left, to move to the next column",1
g,0
" also the dag can have potentially more than one connected component, meaning it is necessary to transition between these components appropriately",1
 recursion enables provably perfect generalization,0
" childlist ∈ nu is a vector of pointers, where childlist[i] points to the next child under consideration for vertex i",1
"
algorithm 2 depth first search topological sort 1: color all vertices white",0
 b1b0 where no carry operations occur,0
 our experiments use a small number of training examples,1
 almost all architectures train on program input/output pairs,1
" our results show that for all learned, to-be-verified neural programs, they all produced the same traces as those produced by p on the verification set",1
"
note that the training set can often be considerably smaller than the verification set, and despite this, the learned model can still pass the entire verification set",0
", to color a vertex) or variable (e",0
" in contrast, each call to the recursive add always runs for a fixed number of steps, even on arbitrarily long problems in f , so we can test that it performs correctly on a small, fixed number of step input sequences",0
" the only model, to our knowledge, that does not train on input-output pairs is the neural programmer-interpreter (reed & de freitas, 2016), which trains on synthetic execution traces",1
"
the npi accesses an external environment, q, which varies according to the task",0
" however, to be sure that the generated v is complete (covers all the cases needed), we need to check all pairs of observations seen in adjacent step inputs (in particular, those before and after a primitive function call), in a similar way as if we were constructing v from scratch",0
" this guarantees that the step input sequences considered during verification contain all step input sequences which arise during execution of an unseen problem in f , leading to generalization to any problem in f ",1
" for these models, table 2 presents results on randomly generated dags of varying graph sizes (varying in the number of vertices)",1
", to change the value of phi) none described belowstack",0
 indentation indicates the stack is one level deeper than before,0
"
the npi accesses an external environment, q, which varies according to the task",0
" the non-recursive and partially recursive versions are unable to sort long arrays, beyond length 8",1
g,1
 stack pop pushes −∞ values to qstacklo and qstackhi,0
"
algorithm 1 neural programming inference 1: inputs: environment observation e, program p, arguments a, stop threshold α 2: function run(e, p, a) 3: h← 0, r ← 0 4: while r < α do 5: s← fenc(e, a), h← flstm(s, p, h) 6: r ← fend(h), p2 ← fprog(h), a2 ← farg(h) 7: if p is a primitive function then 8: e← fenv(e, p, a)",0
" qcolor(pstart) contains the color of the current start vertex, used in the evaluation of the condition in the while loop in line 5 of algorithm 2",0
" to make an analogy to mdps, this procedure is analogous to how value iteration obtains the correct value for each state starting from any initialization",1
"
the training set for bubble sort contains 100 traces, with maximum problem length of 2 (e",0
 there are 2 variables (vactive and vsave); vactive holds the active vertex (as in algorithm 2) and vsave holds the value of vactive before executing line 12 of algorithm 2,1
" although the model is exposed to more and more data, it might learn spurious and overly complex representations of the program, as suggested in zaremba   (2016)",0
 the maximum problem length in this training set is 3 (e,1
"
note that the recursion for quicksort is not purely tail recursive and therefore represents a more complex kind of recursion that is harder to learn than in the previous tasks",1
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,0
"
for topological sort, the domain-specific encoder is
fenc(dag,qcolor, pstack, pstart, vactive, childlist, at)
=mlp ([qcolor(pstart), qcolor(dag[vactive][childlist[vactive]]), pstack == 1, at(1), at(2), at(3)]),
where qcolor ∈ ru×4 is a scratch-pad that contains u rows, each containing one of four colors (white, gray, black, invalid) with one-hot encoding",1
" we can obtain this mapping by first considering the possible observations that can arise at the beginning of the entry function (add, bubblesort, toposort, and quicksort) for some valid program input, and iteratively applying the observation-to-observation mapping implied by the reference implementation’s step output at that point in the execution",0
" to the best of our knowledge, this is the first time verification has been applied to a neural program, providing provable guarantees about its behavior",0
"
in this paper, we propose to resolve these issues by explicitly incorporating recursion into neural architectures",0
" here as a concrete and general example, we consider a general neural programming architecture (npa), similar to neural programmer-interpreter (npi) in reed & de freitas (2016)",0
 18: push lo and p− 1 to slo and shi,0
"
it is sufficient to construct problems where every transition between two adjacent columns is covered",0
 recursion enables provable guarantees on neural programs’ behavior without needing to exhaustively enumerate all possible inputs to the programs,0
" finally, we implement recursive traces for our own topological sort and quicksort tasks",0
"
in order to demonstrate that recursion can help learn and generalize better, we trained a non-recursive and recursive model on just a single execution trace generated from a graph containing 5 nodes3 for the topological sort task",0
" the learned recursive program is different from neural programs learned in all previous work in an important aspect: previous approaches do not explicitly incorporate this abstraction, and hence generalize poorly, whereas our learned neural programs incorporate recursion and achieve perfect generalization",0
g,1
 we use the lomuto partition scheme; the logic for the recursive trace is shown in algorithm 3,0
" because the context of the caller is stored on a stack when it calls another program and the callee starts in a fresh context, this enables recursion simply by allowing a program to call itself",0
" in practice, we can additionally use tail recursion optimization to avoid problems with the call stack growing too deep",0
"as there is no public implementation of npi, we implemented a version of it in keras that is as faithful to the paper as possible",0
", the verification set",0
" provably perfect generalization implies the model will behave correctly, given any valid input",0
" if no white vertex is found, the entire execution is terminated move none write write a value either to environment (e",0
" at any given time, the npi has access to the values referred to by two pointers, represented by q(1, i1) and q(1, i2),",0
"
arg 2 (auxiliary variable): color grey, color black
color grey and color black color the given vertex grey and black, respectively",1
"we train using the adam optimizer and use a 2-layer lstm and task-specific state encoders for the external environments, as described in reed & de freitas (2016)",0
" for each function we use to implement the recursive version of topological sort, we need to consider the set of possible environment observation sequences we can create from all valid inputs and test that the learned program produces the correct behavior on each of these inputs",1
 some functional programming languages go so far as not to define any looping constructs but rely solely on recursion to enable repeated execution of the same code,0
" in order to claim a proof, we must verify the model produces correct behavior over all base cases and reductions, as described in section 2",1
" our implementation uses two stacks qstacklo and
qstackhi, each in ru , that store the arguments to the recursive quicksort calls in algorithm 3; before each recursive call, the appropriate arguments are popped off the stack and written to plo and phi",0
 this algorithm is a variant of depth first search,0
" the non-recursive and partially recursive versions are unable to sort long arrays, beyond length 8",1
" at each time step, the core controller can decide to select one of the programs to call with certain arguments",0
"
we experiment with two levels of recursion—partial and full",0
 this modification also enables us to sort arrays containing arbitrary comparable elements,1
 3: begin traversing from vertex 1 in the dag,0
" note that recursion helps make this process tractable, because we only need to test a finite number of inputs to show that the model will work correctly on inputs of unbounded complexity",0
"
formally, verification consists of proving the following theorem:
∀i ∈ v,m(i) ⇓ p (i) where i denotes a sequence of step inputs (within one function call), v denotes the set of valid sequences of step inputs, m denotes the neural network model, p denotes the correct program, and p (i) denotes the next step output from the correct program",0
"in this paper, we propose that the key abstraction of recursion is necessary for neural programs to generalize",0
" as an application, we incorporate recursion into the neural programmer-interpreter architecture and consider four sample tasks: grade-school addition, bubble sort, topological sort, and quicksort",1
", an array for quicksort or a dag for topological sort)",1
" in all experiments, α is set to 0",0
" 16: p = partition(a, lo, hi) 17: push p+ 1 and hi to slo and shi",0
" the 4 environment observations aid with control flow; qstacklo(pstacklo− 1) < qstackhi(pstackhi− 1) implements the lo < hi comparison in line 5 of algorithm 3, pstacklo == 1 checks if the stacks are empty in line 18 of algorithm 4, and the other observations (all involving ppivot or pj) deal with logic in the partition function",0
3,1
"
formally, verification consists of proving the following theorem:
∀i ∈ v,m(i) ⇓ p (i) where i denotes a sequence of step inputs (within one function call), v denotes the set of valid sequences of step inputs, m denotes the neural network model, p denotes the correct program, and p (i) denotes the next step output from the correct program",1
"
as mentioned in section 1, all approaches to neural programming today fare poorly on this generalization issue",0
" hence, if all subproblems in s are added correctly, we have proven that any member of f will be added correctly, thus eliminating an infinite family of inputs that need to be tested",1
" if a white vertex is found, set pstart to point to it; this signifies the start of a traversal of a new connected component",0
e,0
" for problems where the base cases may be extremely large or infinite, such as certain forms of motor control, recursion can still help reduce the problem of generalization to these two aspects and make the generalization problem significantly simpler to handle and reason about",0
" if yes, then this indicates that the learned neural program achieves provably perfect generalization",0
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,1
" in particular, given that a recursion is defined by two properties as mentioned before, the base cases and the set of reduction rules, we can prove a recursive neural program generalizes perfectly if we can prove that (1) it performs correctly on the base cases; (2) it learns the reduction rules correctly",1
" lshift, used in reset, shifts the pointers left until reaching the start token",0
 table 4 demonstrates that the accuracy of the non-recursive program degrades sharply when moving from arrays of length 7 to arrays of length 8,1
g,1
"
we construct input problems by splitting into two cases: one case in which the left column contains a null value and another in which the left column does not contain any null values",1
" in this version, there is a dependence on length: bstep and lshift are called a number of times equivalent to one less than the length of the input array, in bubble and reset respectively",0
 add1 is a subprogram that adds the current column (writing the appropriate digit to the output row and carrying a bit to the next column if needed),1
" the learned recursive program is different from neural programs learned in all previous work in an important aspect: previous approaches do not explicitly incorporate this abstraction, and hence generalize poorly, whereas our learned neural programs incorporate recursion and achieve perfect generalization",1
"1, we hypothesize the non-recursive programs do not generalize well because they have learned spurious dependencies specific to the training set, such as length of the input problems",1
" if no white vertex is found, the entire execution is terminated move none write write a value either to environment (e",0
"
there are 6 pointers (plo, phi, pstacklo, pstackhi, ppivot, pj)",1
" by iterating with this procedure, and then running p on the input observation set that we obtain for the entry point function, we can obtain v precisely",0
" et+1 ∼ fenv(et, pt, at)",0
" the contents ofqresult andqstack are not exposed directly through the domain-specific encoder; rather, we define primitive functions which manipulate these scratch-pads",0
" for each length, we test each program on 30 randomly generated problems",1
"
base cases and reduction rules for addition",1
 we then construct a verification set of size 73 by ensuring that randomly generated graphs cover the analytically derived v ,0
" for example, a step input-step output pair in lines 2 and 3 of the left-hand side of figure 1 is (add1, write out 1)",0
" the pointer at index i3 represents a counter internal to the environment that is incremented once after each pass of the algorithm (one cycle of bubble and reset); when incremented a number of times equal to the length of the array, the flag i3 == length becomes true and terminates the entire algorithm ",0
"
arg 2 (object to copy): env stack lo peek, env stack hi peek, phi, plo, ppivot − 1, ppivot + 1, reset
env stack lo peek and env stack hi peek represent qstacklo(pstacklo − 1) and qstackhi(pstackhi − 1), respectively",0
" the learned recursive program is different from neural programs learned in all previous work in an important aspect: previous approaches do not explicitly incorporate this abstraction, and hence generalize poorly, whereas our learned neural programs incorporate recursion and achieve perfect generalization",1
" m , as described in algorithm 1, processes the sequence of step inputs by using an lstm",0
" given a precise definition of p , it may be possible to automate the generation of v from p in future work",1
 the training set for addition contains 200 traces,1
", to color a vertex) or variable (e",1
 ,0
"
through careful reasoning about the possible set of environment observations created by all valid inputs, and how each of the operations in the execution trace affects the environment, we can construct v using the procedure described in section 3",1
" plo and phi point to the lo and hi indices of the array, as in algorithm 3",1
"
to evaluate a neural network that learns a neural program to accomplish a certain task, one common evaluation metric is how well the learned model m generalizes",0
 this procedure verifies that all base cases and reductions are handled properly by the model via explicit tests,0
g,0
"
the training data for the neural programmer-interpreter consists of full execution traces for the program of interest",1
" our results show that for all learned, to-be-verified neural programs, they all produced the same traces as those produced by p on the verification set",0
" consequently, it is infeasible to construct a verification set that covers all cases, and the verification procedure we have described is inadequate",1
" hence, if all subproblems in s are added correctly, we have proven that any member of f will be added correctly, thus eliminating an infinite family of inputs that need to be tested",1
 we choose to implement a topological sort task for graphs,0
" full recursion, in addition to making the aforementioned tail recursive call, adds two additional recursive calls; bstep and lshift are made tail recursive",0
e,1
 this modification also enables us to sort arrays containing arbitrary comparable elements,0
" the stack operation has the following arguments:
arg 1 (operation): stack push call1, stack push call2, stack pop
stack push call1 pushes lo and pivot−1 toqstacklo andqstackhi",1
" fa8750-15-2-0104, and berkeley deep drive",0
" the pointer at index i3 represents a counter internal to the environment that is incremented once after each pass of the algorithm (one cycle of bubble and reset); when incremented a number of times equal to the length of the array, the flag i3 == length becomes true and terminates the entire algorithm ",1
 we then construct a verification set of size 73 by ensuring that randomly generated graphs cover the analytically derived v ,0
" we have three observations: the color of the start node, the color of the active node’s next child to be considered, and whether the stack is empty",0
3,1
 there is a (changing) list of neural programs used to accomplish a given task,0
" the partial recursion is not enough for perfect generalization, as will be presented later in section 4",1
g,0
"
base cases and reduction rules for quicksort",0
" recursion is an important concept in programming languages and a critical tool to
reduce the complexity of programs",0
" formally, a function exhibits recursive behavior when it possesses two properties: (1) base cases— terminating scenarios that do not use recursion to produce answers; (2) a set of rules that reduces all other problems toward the base cases",0
" lshift moves the four pointers to the left, to move to the next column",1
"program descriptions calls arguments quicksort run the quicksort
routine in place for the array a, for indices from lo to hi non-recursive: partition, stack, write recursive: same as non-recursive version, along with quicksort implicitly: array a to sort, lo, hi
partition runs the partition function",0
e,0
" we can obtain this mapping by first considering the possible observations that can arise at the beginning of the entry function (add, bubblesort, toposort, and quicksort) for some valid program input, and iteratively applying the observation-to-observation mapping implied by the reference implementation’s step output at that point in the execution",0
 3: begin traversing from vertex 1 in the dag,1
"1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 while (pstacklo 6= 1): 6 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 7 stack(stack_pop) 8 else: 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 stack(stack_pop) 12 partition() 13 stack(stack_push_call2) 14 stack(stack_push_call1) 15 } 16 17 partition() { 18 set_pivot_lo() 19 set_j_lo() 20 compswap_loop() 21 swap(ppivot, phi) 22 set_j_null() 23 } 24 25 compswap_loop() { 26 while (pj 6= phi): 27 compswap() 28 move(pj, up) 29 } 30 31 compswap() { 32 if (a[pj ] ≤ a[phi]): 33 swap(ppivot, pj) 34 move(ppivot, up) 35 } 36 37 stack(op) { 38 if (op == stack_push_call1): 39 write(env_stack_lo, plo) 40 write(env_stack_hi, ppivot − 1) 41 move(pstacklo, up) 42 move(pstackhi, up) 43 44 if (op == stack_push_call2): 45 write(env_stack_lo, ppivot + 1) 46 write(env_stack_hi, phi) 47 move(pstacklo, up) 48 move(pstackhi, up) 49 50 if (op == stack_pop): 51 write(env_stack_lo, reset) 52 write(env_stack_hi, reset) 53 move(pstacklo, down) 54 move(pstackhi, down) 55 }altered recursive functions 1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 6 partition() 7 stack(stack_push_call2) 8 stack(stack_push_call1) 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 quicksort() // recursive call 12 stack(stack_pop) 13 write(phi, env_stack_hi_peek) 14 write(plo, env_stack_lo_peek) 15 quicksort() // recursive call 16 stack(stack_pop) 17 } 18 19 compswap_loop() { 20 if (pj 6= phi): 21 compswap() 22 move(pj, up) 23 compswap_loop() // recursive call 24 }in this section, we describe the space of base cases and reduction rules that must be covered for each of the four sample tasks, in order to create the verification set",0
" ppivot and pj point to the pivot and j indices of the array, used in the partition function in algorithm 3",0
"
the non-recursive trace loops on cycles of add1 and lshift",1
"
recursion drastically reduces the number of configurations we need to consider during the verification phase and makes the proof tractable, because it introduces structure that eliminates infinitely long sequences of step inputs that would otherwise need to be considered",0
" both the non-recursive and recursive learned programs generalize on all graphs we tried, up to 120 vertices",0
" the stack operation has the following arguments:
arg 1 (operation): stack push call1, stack push call2, stack pop
stack push call1 pushes lo and pivot−1 toqstacklo andqstackhi",0
"
through careful reasoning about the possible set of environment observations created by all valid inputs, and how each of the operations in the execution trace affects the environment, we can construct v using the procedure described in section 3",0
" for each length, we test each program on 30 randomly generated problems",0
" we then construct problem configurations that span all possible valid environment states (for instance, in order to force the carry bit in a column to be 1, one can add the sum “1+9” in the column to the right)",0
 a1a0 + bnbn−1 ,0
"program descriptions calls arguments quicksort run the quicksort
routine in place for the array a, for indices from lo to hi non-recursive: partition, stack, write recursive: same as non-recursive version, along with quicksort implicitly: array a to sort, lo, hi
partition runs the partition function",0
" the general notion of recursion has been an important concept in many domains, including mathematics and computer science",0
" lshift, used in reset, shifts the pointers left until reaching the start token",0
"
base cases and reduction rules for addition",0
"
we describe the details of the npi model relevant to our contributions",0
" also, compared to the bubble pointers in bubble sort, the pointers that perform the comparison for quicksort (the compswap function) are usually not adjacent to each other, making quicksort less local than bubble sort",0
"
quicksort",1
" there are 3 pointers (presult, pstack, pstart), presult points to the next empty location in qresult,
pstack points to the top of the stack in qstack, and pstart points to the candidate starting node for a connected component",0
" each step during an execution of the program does one of three things: (1) another subprogram along with associated arguments is called, as in line 10, (2) the program writes to the environment if it is primitive, as in line 8, or (3) the loop is terminated if the return probability exceeds a threshold α, after which the stack frame is popped and control is returned to the caller",0
 we use the lomuto partition scheme; the logic for the recursive trace is shown in algorithm 3,0
we emphasize the overall goal of this work is to enable the learning of a recursive program,1
" we show that by modifying the training procedure, we enable the npi model to learn recursive neural programs",1
" the move operation has the following arguments:
arg 1 (pointer): pstacklo, pstackhi, pj , ppivot
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values",0
 ,0
" it is straightforward to deal with all other operations, which do not induce partial environment states",0
 the core controller acts as a dispatcher for the programs,1
"
note that v should also contain the necessary reductions, which corresponds to making the recursive calls at the correct time, as indicated by p ",0
" partial recursion only adds a tail recursive call to bubblesort after bubble and reset, similar to the tail recursive call described previously for addition",1
" formally, the npi is represented by the following set of equations:
st = fenc(et, at)
ht = flstm(st, pt, ht−1)
rt = fend(ht), pt+1 = fprog(ht), at+1 = farg(ht)
t is a subscript denoting the time-step; fenc is a domain-specific encoder (to be described later) that takes in the environment slice et and arguments at; flstm represents the core module, which takes in the state st generated by fenc, a program embedding pt ∈ rp , and hidden lstm state ht; fend decodes the return probability rt; fprog decodes a program key embedding pt+1;1 and farg decodes arguments at+1",1
" via the recursive call, we effectively forget that the column just added exists, since the recursive call to add starts with a new hidden state for the lstm controller",0
"
through careful reasoning about the possible set of environment observations created by all valid inputs, and how each of the operations in the execution trace affects the environment, we can construct v using the procedure described in section 3",0
" the add reduction rule ensures that each call to add only covers two adjacent columns, and so the lstm only ever runs for a fixed number of steps necessary to process these two columns",0
" table 1 presents results on randomly generated arrays of varying length for the learned non-recursive, partially recursive, and full recursive programs",1
" there are 3 pointers (presult, pstack, pstart), presult points to the next empty location in qresult,
pstack points to the top of the stack in qstack, and pstart points to the candidate starting node for a connected component",0
"
it is sufficient to construct problems where every transition between two adjacent columns is covered",1
g,1
"
the operations we need to be concerned most about are carry and lshift, which induce partial environment states spanning two columns",0
" however, note that there is no provable guarantee that the non-recursive learned program will generalize to all inputs, whereas we show later that the recursive learned program has a provable guarantee of perfect generalization",1
g,0
"
there are 6 pointers (plo, phi, pstacklo, pstackhi, ppivot, pj)",0
" by iterating with this procedure, and then running p on the input observation set that we obtain for the entry point function, we can obtain v precisely",0
"
bubble sort",0
"
to construct this set, by using the reference implementation of each subprogram, we construct a mapping between two sets of environment observations: the first set consists of all observations that can occur at the beginning of a particular subprogram’s invocation, and the second set contains the observations at the end of that subprogram",1
 table 4 also demonstrates that the (verified) recursive program generalizes perfectly,0
g,0
 our experiments use a small number of training examples,0
"
there are 6 pointers (plo, phi, pstacklo, pstackhi, ppivot, pj)",0
" k is set to 11, to denote the range of possible numbers (0 through 9), along with the start/end token (represented with the same encoding) which is observed when a pointer reaches beyond the bounds of the input",0
 this verification phase only needs to be performed once after training,1
" the non-recursive and partially recursive versions are unable to sort long arrays, beyond length 8",1
", an lstm in npi’s case, but possibly other networks in different cases",0
" naı̈vely, we might expect to synthesize and test an input for any sequence created by combining the four possible colors in two variables and another boolean variable for whether the stack is empty (so 32 possible observations at any point), but for various reasons, most of these combinations are impossible to occur at any given point in the execution trace",0
g,0
" we have three observations: the color of the start node, the color of the active node’s next child to be considered, and whether the stack is empty",0
"
to evaluate a neural network that learns a neural program to accomplish a certain task, one common evaluation metric is how well the learned model m generalizes",0
" in our experiments, we only present dag’s as inputs and represent the vertices as values ranging from 1, ",0
"
by reasoning about the possible set of environment observations created by all valid inputs, we construct v using the procedure described in section 3",1
 ,1
"
through careful reasoning about the possible set of environment observations created by all valid inputs, and how each of the operations in the execution trace affects the environment, we can construct v using the procedure described in section 3",0
" bstep, used in bubble, compares pairs of numbers, continuously moving the bubble pointers once to the right each time until reaching the end of the array",0
" recursion is an important concept in programming languages and a critical tool to
reduce the complexity of programs",1
" twc-1409915, darpa under grant no",0
 our experiments use a small number of training examples,1
"
recursion drastically reduces the number of configurations we need to consider during the verification phase and makes the proof tractable, because it introduces structure that eliminates infinitely long sequences of step inputs that would otherwise need to be considered",0
 the training set for addition contains 200 traces,1
" 9: else 10: function run(e, p2, a2)
a description of the inference procedure is given in algorithm 1",0
" if the program is primitive, the next environmental state et+1 will be affected by pt and at, i",0
"
note that the training set can often be considerably smaller than the verification set, and despite this, the learned model can still pass the entire verification set",0
g,0
" the single-digit multiplication task in zaremba   (2016), the bubble sort task in reed & de freitas (2016), and the graph tasks in graves   (2016))",1
 table 4 also demonstrates that the (verified) recursive program generalizes perfectly,0
" as an application, we incorporate recursion into the neural programmer-interpreter architecture and consider four sample tasks: grade-school addition, bubble sort, topological sort, and quicksort",0
", qstacklo or qstackhi) or to pointer (e",0
" the only model, to our knowledge, that does not train on input-output pairs is the neural programmer-interpreter (reed & de freitas, 2016), which trains on synthetic execution traces",0
" the corresponding edge list is [(1, 2), (1, 5), (2, 4), (2, 5), (3, 5)]",0
" in order to claim a proof, we must verify the model produces correct behavior over all base cases and reductions, as described in section 2",1
"
as in line 13 of the right-hand side of figure 1",0
 u varies with the number of vertices in the graph,1
 we leave this as future work to devise a verification procedure more appropriate to this setting,0
g,0
"
arg 2 (increment or decrement): up, down1 // top level topological sort call 2 toposort() { 3 while (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 } 0 11 traverse() { 12 check_child() 13 explore() 14 } 15 16 check_child() { 17 while (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 18 move(childlist[vactive], up) 19 } 20 21 explore() { 22 do 23 if (qcolor(dag[vactive][childlist[vactive]]) is white): 24 write(color_next, color_grey) 25 stack(push) 26 write(save) 27 write(active_neighb) 28 move(childlist[vsave], up) 29 else: 30 write(color_curr, color_black) 31 write(result) 32 move(presult, up) 33 if(pstack == 1): 34 break 35 else: 36 stack(pop) 37 check_child() 38 while (true) 39 } 40 41 stack(op) { 42 if (op == push): 43 write(stack_push) 44 move(pstack, up) 45 46 if (op == pop): 47 write(active_stack) 48 write(stack_pop) 49 move(pstack, down) 50 } 51 52 next_start() { 53 while(qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 54 move(pstart, up) 55 }altered recursive functions  // top level topological sort call 2 toposort() { 3 if (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 toposort() // recursive call 0 } 11 12 check_child() { 13 if (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 14 move(childlist[vactive], up) 15 check_child() // recursive call 16 } 17 18 explore() { 19 if (qcolor(dag[vactive][childlist[vactive]]) is white): 20 write(color_next, color_grey) 21 stack(push) 22 write(save) 23 write(active_neighb) 24 move(childlist[vsave], up) 25 else: 26 write(color_curr, color_black) 27 write(result) 28 move(presult, up) 29 if(pstack == 1): 30 return 31 else: 32 stack(pop) 33 check_child() 34 explore() // recursive call 35 } 36 37 next_start() { 38 if (qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 39 move(pstart, up) 40 next_start() // recursive call 41 }algorithm 4 iterative quicksort 1: initialize an array a to sort and two empty stacks slo and shi",0
" we performed experiments on the partially recursive version in order to examine what happens when only one recursive call is implemented, when in reality three are required for perfect generalization",0
as mentioned in section 2,1
" via the recursive call, we effectively forget that the column just added exists, since the recursive call to add starts with a new hidden state for the lstm controller",1
"
in addition, none of the current approaches to neural programming provide a method or even aim to enable provable guarantees about generalization",1
" also, compared to the bubble pointers in bubble sort, the pointers that perform the comparison for quicksort (the compswap function) are usually not adjacent to each other, making quicksort less local than bubble sort",0
 ,0
" fa8750-15-2-0104, and berkeley deep drive",0
" the npi model has three learnable components: a task-agnostic core, a program-key embedding, and domain-specific encoders that allow the npi to operate in diverse environments",0
" consequently, there is no concept of length relevant to the problem, which has traditionally been an important focus of length-based curriculum learning",1
" even though the earlier version also generalized just as well in practice, relatively small differences in the formulation of the traces and the environment observations can drastically change the difficulty of verification",1
" for each length, we test each program on 30 randomly generated problems",0
" formally, a function exhibits recursive behavior when it possesses two properties: (1) base cases— terminating scenarios that do not use recursion to produce answers; (2) a set of rules that reduces all other problems toward the base cases",0
" for many problems, the base cases and reduction rules usually consist of a finite (often small) number of cases",1
" in this section, we give a brief review of the npi architecture from reed & de freitas (2016) as background",1
"we show that if we incorporate recursion, the learned npi programs can achieve provably perfect generalization for different tasks",1
"
inside bubble and reset, there are two operations that can be made recursive",0
"
there are 6 pointers (plo, phi, pstacklo, pstackhi, ppivot, pj)",0
" the learned recursive program is different from neural programs learned in all previous work in an important aspect: previous approaches do not explicitly incorporate this abstraction, and hence generalize poorly, whereas our learned neural programs incorporate recursion and achieve perfect generalization",0
"
the operations we need to be concerned most about are carry and lshift, which induce partial environment states spanning two columns",1
" in our experiments, we only present dag’s as inputs and represent the vertices as values ranging from 1, ",0
" however, the authors of npi did not consider explicitly the notion of recursion and as a consequence, did not learn recursive programs",0
" we note that an earlier version of quicksort we tried lacked primitive operations to directly move a pointer to another, and therefore needed more functions and observations",1
" m is considered to have perfect generalization if m can give the right answer for any input, such as inputs of arbitrary length",0
 this verification phase only needs to be performed once after training,0
" as a consequence, our learned neural programs empirically achieve perfect generalization from a very small number of training examples",0
" both the non-recursive and recursive learned programs generalize on all graphs we tried, up to 120 vertices",0
" if it is too large, then the semantics of p might not be well-defined on some elements in v , or the spurious step input sequences may not be reachable from any valid problem input (e",0
" in our experiments, we only present dag’s as inputs and represent the vertices as values ranging from 1, ",0
"
inside bubble and reset, there are two operations that can be made recursive",1
" 3: 4: function quicksort(a, lo, hi) 5: if lo < hi: then 6: p = partition(a, lo, hi) 7: quicksort(a, lo, p− 1) 8: quicksort(a, p+ 1, hi) 9: 10: function partition(a, lo, hi) 11: pivot = lo 12: for j ∈ [lo, hi− 1] : do 13: if a[j] ≤ a[hi] then 14: swap a[pivot] with a[j] 15: pivot = pivot+ 1 16: swap a[pivot] with a[hi] 17: return pivot
for quicksort, the domain-specific encoder is
fenc(qarray, qstacklo, qstackhi, plo, phi, pstacklo, pstackhi, ppivot, pj , at) =
mlp ([qarray(pj) ≤ qarray(phi), pj == phi, qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1), pstacklo == 1, at(1), at(2), at(3)]),
whereqarray ∈ ru×11 is a scratch-pad that contains u rows, each containing one of 11 values (one of the numbers 0 through 9 or an invalid state)",0
 the environment and return probability are omitted for readability,0
"
as in line 13 of the right-hand side of figure 1",0
"
by reasoning about the possible set of environment observations created by all valid inputs, we construct v using the procedure described in section 3",1
" lshift moves the four pointers to the left, to move to the next column",0
" for bubble sort, the domain-specific encoder is
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1), q(1, i2), i3 == length, at(1), at(2), at(3)]),
where the environment q ∈ r1×n×k is a scratch-pad that contains 1 row, to represent the state of the array as sorting proceeds in-place, andn columns",0
" as before, the non-recursive learned program lacks a provable guarantee of generalization, whereas we show later that the recursive learned program has one",0
"
grade school addition",1
 3: begin traversing from vertex 1 in the dag,0
" this guarantees that the step input sequences considered during verification contain all step input sequences which arise during execution of an unseen problem in f , leading to generalization to any problem in f ",0
" for each task, given the verification set, we check the traces and results of the learned, to-be-verified neural program (described in section 4",1
" in order to claim a proof, we must verify the model produces correct behavior over all base cases and reductions, as described in section 2",0
"
as mentioned in section 1, all approaches to neural programming today fare poorly on this generalization issue",1
" the npi model has three learnable components: a task-agnostic core, a program-key embedding, and domain-specific encoders that allow the npi to operate in diverse environments",1
 the sequence of primitive operations (move and write operations) for the non-recursive and recursive versions are exactly the same,0
3,0
" in particular, recursion can be implemented as a program calling itself",0
e,0
" a single element of an execution trace consists of a step input-step output pair, which can be synthesized from algorithm 1: this corresponds to, for a given time-step, the step input tuple (e, p, a) and step output tuple (r, p2, a2)",0
" the recursive version of topological sort solves all graph instances we tried, from graphs of size 5 through 70",0
"
inside bubble and reset, there are two operations that can be made recursive",1
" as with the original npi architecture, the experiments for this paper always used a 3-tuple of integers at = (at(1), at(2), at(3))",1
"
since npi naturally supports the notion of recursion, a key question is how to enable npi to learn recursive programs",0
 u varies with the number of vertices in the graph,1
" training on the full recursive trace leads to perfect generalization, as shown in section 4",0
" the learned recursive program is different from neural programs learned in all previous work in an important aspect: previous approaches do not explicitly incorporate this abstraction, and hence generalize poorly, whereas our learned neural programs incorporate recursion and achieve perfect generalization",0
"program descriptions calls arguments quicksort run the quicksort
routine in place for the array a, for indices from lo to hi non-recursive: partition, stack, write recursive: same as non-recursive version, along with quicksort implicitly: array a to sort, lo, hi
partition runs the partition function",0
" for problems where the base cases may be extremely large or infinite, such as certain forms of motor control, recursion can still help reduce the problem of generalization to these two aspects and make the generalization problem significantly simpler to handle and reason about",0
" our results show that for all learned, to-be-verified neural programs, they all produced the same traces as those produced by p on the verification set",0
" the npi model has three learnable components: a task-agnostic core, a program-key embedding, and domain-specific encoders that allow the npi to operate in diverse environments",0
"
base cases and reduction rules for addition",1
 we created a program set that reflects the semantics of algorithm 2,1
" when the program is called, the current context including the caller’s memory state is stored on a stack; when the program returns, the stored context is popped off the stack to resume execution in the previous caller’s context",0
" the recursive version of topological sort solves all graph instances we tried, from graphs of size 5 through 70",0
"
it is important to emphasize that at inference time in the npi, the hidden state of the lstm controller is reset (to zero) at each subprogram call, as in line 3 of algorithm 1 (h ← 0)",0
" the core module of the npi is an lstm controller that takes as input a slice of the current external environment, via a set of pointers, and a program and arguments to execute",0
" plo and phi point to the lo and hi indices of the array, as in algorithm 3",0
 the model described in the training setup of section 4 (trained on 6 traces) was verified to be correct via the matching procedure described in section 4,0
" the swap operation has the following arguments:
arg 1 (swap object 1): ppivot
arg 2 (swap object 2): phi, pj
write",0
"
to construct this set, by using the reference implementation of each subprogram, we construct a mapping between two sets of environment observations: the first set consists of all observations that can occur at the beginning of a particular subprogram’s invocation, and the second set contains the observations at the end of that subprogram",0
" for each function we use to implement the recursive version of topological sort, we need to consider the set of possible environment observation sequences we can create from all valid inputs and test that the learned program produces the correct behavior on each of these inputs",0
" when the program is called, the current context including the caller’s memory state is stored on a stack; when the program returns, the stored context is popped off the stack to resume execution in the previous caller’s context",0
" while this matches the description from reed & de freitas (2016), we found that this causes an unnecessary blowup in the size of v and makes it much more difficult to construct the verification set",1
" as this complexity interfered with determining the base cases and reductions, we changed the algorithm to its current form",0
" numerous models have been proposed for learning programs; to name a few, this includes the differentiable neural computer (graves  , 2016), neural turing machine (graves  , 2014), neural gpu (kaiser & sutskever, 2015), neural programmer (neelakantan  , 2015), pointer network (vinyals  , 2015), hierarchical attentive memory (andrychowicz & kurach, 2016), and neural random access machine (kurach  , 2016)",0
" that is to say, the network does not learn the true program semantics",0
 ,0
" for instance, for recursive addition, consider the family f of addition problems anan−1 ",1
 table 3 presents results on randomly generated arrays of varying length for the learned non-recursive and recursive programs,1
" both the non-recursive and recursive learned programs generalize on all graphs we tried, up to 120 vertices",1
 we then construct a verification set of size 73 by ensuring that randomly generated graphs cover the analytically derived v ,0
"
arg 2 (increment or decrement): up, down
swap",0
" m , as described in algorithm 1, processes the sequence of step inputs by using an lstm",0
3,0
" qcolor(pstart) contains the color of the current start vertex, used in the evaluation of the condition in the while loop in line 5 of algorithm 2",1
" in particular, given that a recursion is defined by two properties as mentioned before, the base cases and the set of reduction rules, we can prove a recursive neural program generalizes perfectly if we can prove that (1) it performs correctly on the base cases; (2) it learns the reduction rules correctly",0
 pstacklo and pstackhi point to the top (empty) positions in qstacklo and qstackhi,1
" as this complexity interfered with determining the base cases and reductions, we changed the algorithm to its current form",0
", pstart or childlist[vactive]) up or down none described belowwrite",0
" in future work, we would like to decrease supervision and construct models that are capable of coming up with recursive abstractions themselves",0
g,0
" a topological sort is a linear ordering of vertices such that for every directed edge (u, v) from u to v, u comes before v in the ordering",0
"
grade school addition",0
"2 at any given time, the npi has access to values pointed to by four pointers in each of the four rows, represented by q(1, i1), q(2, i2), q(3, i3), and q(4, i4)",0
 the logic for the non-recursive trace is shown in algorithm 4 in the appendix,0
" 2: initialize lo and hi to be 1 and n, where n is the length of a",0
 some functional programming languages go so far as not to define any looping constructs but rely solely on recursion to enable repeated execution of the same code,0
" k is set to 11, to denote the range of possible numbers (0 through 9), along with the start/end token (represented with the same encoding) which is observed when a pointer reaches beyond the bounds of the input",0
" as an application, we incorporate recursion into the neural programmer-interpreter architecture and consider four sample tasks: grade-school addition, bubble sort, topological sort, and quicksort",1
g,0
"
grade school addition",0
"
the npi accesses an external environment, q, which varies according to the task",0
" the move operation has the following arguments:
arg 1 (pointer): pstacklo, pstackhi, pj , ppivot
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values",0
"
the training set for topological sort contains 6 traces, with one synthesized from a graph of size 5 and the rest synthesized from graphs of size 7",0
" for brevity, we refer the reader to the appendix for further details on the program set and non-recursive and recursive trace-generating functions used for topological sort",0
as mentioned in section 2,0
" we hypothesize that the reason for this is that the neural network learns to spuriously depend on specific characteristics of the training examples that are irrelevant to the true program semantics, such as length of the training inputs, and thus fails to generalize to more complex inputs",0
" for these models, table 2 presents results on randomly generated dags of varying graph sizes (varying in the number of vertices)",1
 we use the lomuto partition scheme; the logic for the recursive trace is shown in algorithm 3,0
" this is highly undesirable, since being able to provide the correct answer in all possible settings is one of the most important aspects of any learned neural program",1
" bstep, used in bubble, compares pairs of numbers, continuously moving the bubble pointers once to the right each time until reaching the end of the array",1
" et+1 ∼ fenv(et, pt, at)",1
"
algorithm 1 neural programming inference 1: inputs: environment observation e, program p, arguments a, stop threshold α 2: function run(e, p, a) 3: h← 0, r ← 0 4: while r < α do 5: s← fenc(e, a), h← flstm(s, p, h) 6: r ← fend(h), p2 ← fprog(h), a2 ← farg(h) 7: if p is a primitive function then 8: e← fenv(e, p, a)",0
 ,0
" in this version of curriculum learning, even though the inputs are gradually becoming more complex, the semantics of the program is succinct and does not change",0
" we implement a quicksort task, in order to demonstrate that recursion helps with learning divide-and-conquer algorithms",0
" empirically, existing models suffer from a common limitation—generalization becomes poor beyond a threshold level of complexity",0
", to color a vertex) or variable (e",1
"
the training set for bubble sort contains 100 traces, with maximum problem length of 2 (e",0
" at each time step, the core controller can decide to select one of the programs to call with certain arguments",1
 there are 2 variables (vactive and vsave); vactive holds the active vertex (as in algorithm 2) and vsave holds the value of vactive before executing line 12 of algorithm 2,1
 a1a0 + bnbn−1 ,1
 b1b0 where no carry operations occur,0
" for non-primitive subprograms, we can update the observation-to-observation mapping currently associated with the subprogram and then apply that mapping to the current set",0
" this is possible if and only if the graph has no directed cycles; that is to say, it must be a directed acyclic graph (dag)",0
"
we note that for tasks with very large input domains, such as ones involving mnist digits or speech samples, the state space of base cases and reduction rules could be prohibitively large, possibly infinite",0
" because the context of the caller is stored on a stack when it calls another program and the callee starts in a fresh context, this enables recursion simply by allowing a program to call itself",1
" 4: function toposort(dag) 5: while there is still a white vertex u: do 6: color[u] = grey 7: vactive = u 8: do 9: if vactive has a white child v then 10: color[v] = grey 11: push vactive onto s 12: vactive = v 13: else 14: color[vactive] = black 15: write vactive to result 16: if s is empty then pass 17: else pop the top vertex off s and set it to vactive 18: while s is not empty
topological sort",1
" in all experiments, α is set to 0",0
" for bubble sort, the domain-specific encoder is
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1), q(1, i2), i3 == length, at(1), at(2), at(3)]),
where the environment q ∈ r1×n×k is a scratch-pad that contains 1 row, to represent the state of the array as sorting proceeds in-place, andn columns",1
" in future work, we would like to decrease supervision and construct models that are capable of coming up with recursive abstractions themselves",0
" in the non-recursive trace, there are four functions that can be made recursive—toposort, check child, explore, and next start, and we add a tail recursive call to each of these functions in order to make the recursive trace",0
" formally, a function exhibits recursive behavior when it possesses two properties: (1) base cases— terminating scenarios that do not use recursion to produce answers; (2) a set of rules that reduces all other problems toward the base cases",0
 we found that changing the npi training traces is a simple way to enable this,1
" this agrees with the generalization of non-recursive addition in reed & de freitas (2016), where they reported generalization up to 3000 digits",0
" however, models that make use of this strategy eventually fail after a certain level of complexity (e",0
" both the non-recursive and recursive learned programs generalize on all input lengths we tried, up to 5000 digits",0
"
base cases and reduction rules for addition",1
" the write operation has the following arguments:
arg 1 (main action): color curr, color next, active start, active neighb, active stack, save, stack push, stack pop, result
color curr colors vactive, color next colors vertex dag[vactive][childlist[vactive]], active start writes pstart to vactive, active neighb writes dag[vactive][childlist[vactive]] to vactive, active stack writes qstack(pstack) to vactive, save writes vactive to vsave, stack push pushes vactive to the top of the stack, stack pop writes a null value to the top of the stack, and result writes vactive to qresult(presult)",1
" to make an analogy to mdps, this procedure is analogous to how value iteration obtains the correct value for each state starting from any initialization",1
", the verification set",1
"
quicksort",0
" note that recursion helps make this process tractable, because we only need to test a finite number of inputs to show that the model will work correctly on inputs of unbounded complexity",1
 reset represents a −∞ value,0
" for these models, table 2 presents results on randomly generated dags of varying graph sizes (varying in the number of vertices)",0
"in this paper, we propose that the key abstraction of recursion is necessary for neural programs to generalize",1
we now report on generalization for the varying tasks,0
"
note that the training set can often be considerably smaller than the verification set, and despite this, the learned model can still pass the entire verification set",0
"
topological sort",1
 this verification phase only needs to be performed once after training,1
" both the non-recursive and recursive learned programs generalize on all input lengths we tried, up to 5000 digits",1
"
training setup",0
"
to perform the verification as described here, it is critical to construct v correctly",0
"
an alternative method is to run p on many different program inputs and then observe step input sequences which occur, to create v ",1
"
under the assumption that there are no leading 0’s (except in the case of single digits) and the two numbers to be added have the same number of digits, the verification set for addition contains 20,181 input problems",0
" for example, a step input-step output pair in lines 2 and 3 of the left-hand side of figure 1 is (add1, write out 1)",1
" furthermore, given a verification input set that covers all base cases and reduction rules, we can formally prove that the learned neural programs achieve perfect generalization after verifying its behavior on the verification input set",1
 there is a (changing) list of neural programs used to accomplish a given task,0
"
arg 2 (increment or decrement): up, down
swap",1
this material is in part based upon work supported by the national science foundation under grant no,0
" the contents ofqresult andqstack are not exposed directly through the domain-specific encoder; rather, we define primitive functions which manipulate these scratch-pads",1
"
the three environment observations aid with control flow in algorithm 2",1
 this verification phase only needs to be performed once after training,1
"
in order to demonstrate that recursion can help learn and generalize better, for addition, we trained only on traces for 5 arbitrarily chosen 1-digit addition sum examples",1
"
move",1
" recall that this verification procedure cannot be performed for the non-recursive versions, since the propagation of the hidden state in the core lstm module makes reasoning difficult and so we would need to check an unbounded number of examples",0
" empirically, existing models suffer from a common limitation—generalization becomes poor beyond a threshold level of complexity",0
"
to evaluate a neural network that learns a neural program to accomplish a certain task, one common evaluation metric is how well the learned model m generalizes",0
 reset represents a −∞ value,0
" we implement a quicksort task, in order to demonstrate that recursion helps with learning divide-and-conquer algorithms",0
"program descriptions calls arguments quicksort run the quicksort
routine in place for the array a, for indices from lo to hi non-recursive: partition, stack, write recursive: same as non-recursive version, along with quicksort implicitly: array a to sort, lo, hi
partition runs the partition function",0
"
it is important to emphasize that at inference time in the npi, the hidden state of the lstm controller is reset (to zero) at each subprogram call, as in line 3 of algorithm 1 (h ← 0)",1
 we then construct a verification set of size 73 by ensuring that randomly generated graphs cover the analytically derived v ,1
" that is to say, the network does not learn the true program semantics",1
" bstep, used in bubble, compares pairs of numbers, continuously moving the bubble pointers once to the right each time until reaching the end of the array",0
" 3: 4: function partition(a, lo, hi) 5: pivot = lo 6: for j ∈ [lo, hi− 1] : do 7: if a[j] ≤ a[hi] then 8: swap a[pivot] with a[j] 9: pivot = pivot+ 1 0: swap a[pivot] with a[hi] 11: return pivot 12: 13: function quicksort(a, lo, hi) 14: while slo and shi are not empty: do 15: pop states off slo and shi, writing them to lo and hi",1
 ,0
 this paper is the first (to our knowledge) to investigate the important problem of provable generalization properties of neural programs,0
"program descriptions calls arguments quicksort run the quicksort
routine in place for the array a, for indices from lo to hi non-recursive: partition, stack, write recursive: same as non-recursive version, along with quicksort implicitly: array a to sort, lo, hi
partition runs the partition function",1
" the non-recursive and partially recursive versions are unable to sort long arrays, beyond length 8",1
"
algorithm 1 neural programming inference 1: inputs: environment observation e, program p, arguments a, stop threshold α 2: function run(e, p, a) 3: h← 0, r ← 0 4: while r < α do 5: s← fenc(e, a), h← flstm(s, p, h) 6: r ← fend(h), p2 ← fprog(h), a2 ← farg(h) 7: if p is a primitive function then 8: e← fenv(e, p, a)",0
 ,0
" a single element of an execution trace consists of a step input-step output pair, which can be synthesized from algorithm 1: this corresponds to, for a given time-step, the step input tuple (e, p, a) and step output tuple (r, p2, a2)",0
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,0
 this paper is the first (to our knowledge) to investigate the important problem of provable generalization properties of neural programs,0
"
arg 2 (object to copy): env stack lo peek, env stack hi peek, phi, plo, ppivot − 1, ppivot + 1, reset
env stack lo peek and env stack hi peek represent qstacklo(pstacklo − 1) and qstackhi(pstackhi − 1), respectively",0
"
as a concrete instantiation, we show in this paper that we can enable recursive neural programs in the npi model, and thus enable perfectly generalizable neural programs for tasks such as sorting where the original, non-recursive npi program fails",1
" qcolor(dag[vactive][childlist[vactive]]) refers to the color of the next child of vactive, used in the evaluation of the condition in the if branch in line 9 of algorithm 2",0
" nevertheless, we would like the model to quickly learn to represent the right semantics of the underlying program from a small number of training examples, not an exhaustive number of them",0
"
figure 1 shows examples of non-recursive and recursive addition traces",0
"
by reasoning about the possible set of environment observations created by all valid inputs, we construct v using the procedure described in section 3",0
" as described in the verification procedure, it is possible to prove our learned recursive program generalizes perfectly by testing on an appropriate set of problem inputs, i",0
" m , as described in algorithm 1, processes the sequence of step inputs by using an lstm",1
"
directed acyclic graphs are structurally more diverse than inputs in the two tasks of grade-school addition and bubble sort",1
" the single-digit multiplication task in zaremba   (2016), the bubble sort task in reed & de freitas (2016), and the graph tasks in graves   (2016))",0
"
algorithm 1 neural programming inference 1: inputs: environment observation e, program p, arguments a, stop threshold α 2: function run(e, p, a) 3: h← 0, r ← 0 4: while r < α do 5: s← fenc(e, a), h← flstm(s, p, h) 6: r ← fend(h), p2 ← fprog(h), a2 ← farg(h) 7: if p is a primitive function then 8: e← fenv(e, p, a)",1
" even though the earlier version also generalized just as well in practice, relatively small differences in the formulation of the traces and the environment observations can drastically change the difficulty of verification",1
 the training set for addition contains 200 traces,1
"
under the assumption that there are no leading 0’s (except in the case of single digits) and the two numbers to be added have the same number of digits, the verification set for addition contains 20,181 input problems",1
" however, models that make use of this strategy eventually fail after a certain level of complexity (e",1
" on the other hand, the non-recursive version has low accuracy, beginning from size 5, and fails completely for graphs of size 8 and beyond",0
 some functional programming languages go so far as not to define any looping constructs but rely solely on recursion to enable repeated execution of the same code,0
 recursion enables provably perfect generalization,0
 our result shows that the training procedure and the npi architecture is capable of generalizing from the step input-output pairs seen in the training data to the unseen ones present in the verification set,0
"
base cases and reduction rules for quicksort",0
 some functional programming languages go so far as not to define any looping constructs but rely solely on recursion to enable repeated execution of the same code,0
" formally, the npi is represented by the following set of equations:
st = fenc(et, at)
ht = flstm(st, pt, ht−1)
rt = fend(ht), pt+1 = fprog(ht), at+1 = farg(ht)
t is a subscript denoting the time-step; fenc is a domain-specific encoder (to be described later) that takes in the environment slice et and arguments at; flstm represents the core module, which takes in the state st generated by fenc, a program embedding pt ∈ rp , and hidden lstm state ht; fend decodes the return probability rt; fprog decodes a program key embedding pt+1;1 and farg decodes arguments at+1",1
" thus, we demonstrate that recursion enables provably perfect generalization for different tasks, including addition, topological sort, quicksort, and a variant of bubble sort",1
" each step during an execution of the program does one of three things: (1) another subprogram along with associated arguments is called, as in line 10, (2) the program writes to the environment if it is primitive, as in line 8, or (3) the loop is terminated if the return probability exceeds a threshold α, after which the stack frame is popped and control is returned to the caller",0
" however, note that there is no provable guarantee that the non-recursive learned program will generalize to all inputs, whereas we show later that the recursive learned program has a provable guarantee of perfect generalization",0
 there are 2 variables (vactive and vsave); vactive holds the active vertex (as in algorithm 2) and vsave holds the value of vactive before executing line 12 of algorithm 2,0
 the model described in the training setup of section 4 (trained on 6 traces) was verified to be correct via the matching procedure described in section 4,0
" this is possible if and only if the graph has no directed cycles; that is to say, it must be a directed acyclic graph (dag)",1
" although the model is exposed to more and more data, it might learn spurious and overly complex representations of the program, as suggested in zaremba   (2016)",0
" when the program is called, the current context including the caller’s memory state is stored on a stack; when the program returns, the stored context is popped off the stack to resume execution in the previous caller’s context",0
"
as a concrete instantiation, we show in this paper that we can enable recursive neural programs in the npi model, and thus enable perfectly generalizable neural programs for tasks such as sorting where the original, non-recursive npi program fails",0
" in this section, we give a brief review of the npi architecture from reed & de freitas (2016) as background",1
" for each task, given the verification set, we check the traces and results of the learned, to-be-verified neural program (described in section 4",0
" the corresponding edge list is [(1, 2), (1, 5), (2, 4), (2, 5), (3, 5)]",0
"
arg 2 (increment or decrement): up, down1 // top level topological sort call 2 toposort() { 3 while (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 } 0 11 traverse() { 12 check_child() 13 explore() 14 } 15 16 check_child() { 17 while (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 18 move(childlist[vactive], up) 19 } 20 21 explore() { 22 do 23 if (qcolor(dag[vactive][childlist[vactive]]) is white): 24 write(color_next, color_grey) 25 stack(push) 26 write(save) 27 write(active_neighb) 28 move(childlist[vsave], up) 29 else: 30 write(color_curr, color_black) 31 write(result) 32 move(presult, up) 33 if(pstack == 1): 34 break 35 else: 36 stack(pop) 37 check_child() 38 while (true) 39 } 40 41 stack(op) { 42 if (op == push): 43 write(stack_push) 44 move(pstack, up) 45 46 if (op == pop): 47 write(active_stack) 48 write(stack_pop) 49 move(pstack, down) 50 } 51 52 next_start() { 53 while(qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 54 move(pstart, up) 55 }altered recursive functions  // top level topological sort call 2 toposort() { 3 if (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 toposort() // recursive call 0 } 11 12 check_child() { 13 if (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 14 move(childlist[vactive], up) 15 check_child() // recursive call 16 } 17 18 explore() { 19 if (qcolor(dag[vactive][childlist[vactive]]) is white): 20 write(color_next, color_grey) 21 stack(push) 22 write(save) 23 write(active_neighb) 24 move(childlist[vsave], up) 25 else: 26 write(color_curr, color_black) 27 write(result) 28 move(presult, up) 29 if(pstack == 1): 30 return 31 else: 32 stack(pop) 33 check_child() 34 explore() // recursive call 35 } 36 37 next_start() { 38 if (qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 39 move(pstart, up) 40 next_start() // recursive call 41 }algorithm 4 iterative quicksort 1: initialize an array a to sort and two empty stacks slo and shi",1
g,0
" finally, the boolean pstack == 1 is used to check whether the stack is empty in line 18 of algorithm 2",1
" even though the earlier version also generalized just as well in practice, relatively small differences in the formulation of the traces and the environment observations can drastically change the difficulty of verification",1
" however, the authors of npi did not consider explicitly the notion of recursion and as a consequence, did not learn recursive programs",1
", to color a vertex) or variable (e",0
" in this pair, the step input runs a subprogram add1 that has no arguments, and the step output contains a program write that has arguments of out and 1",0
" naı̈vely, we might expect to synthesize and test an input for any sequence created by combining the four possible colors in two variables and another boolean variable for whether the stack is empty (so 32 possible observations at any point), but for various reasons, most of these combinations are impossible to occur at any given point in the execution trace",1
g,0
 this makes it difficult to reason about what the model will do when given complex inputs,0
 add1 is a subprogram that adds the current column (writing the appropriate digit to the output row and carrying a bit to the next column if needed),0
g,1
" for each graph size, we test the learned programs on 30 randomly generated dags",1
"
for topological sort, the domain-specific encoder is
fenc(dag,qcolor, pstack, pstart, vactive, childlist, at)
=mlp ([qcolor(pstart), qcolor(dag[vactive][childlist[vactive]]), pstack == 1, at(1), at(2), at(3)]),
where qcolor ∈ ru×4 is a scratch-pad that contains u rows, each containing one of four colors (white, gray, black, invalid) with one-hot encoding",0
"1, we hypothesize the non-recursive programs do not generalize well because they have learned spurious dependencies specific to the training set, such as length of the input problems",0
" in our experiments, we only present dag’s as inputs and represent the vertices as values ranging from 1, ",1
" 9: else 10: function run(e, p2, a2)
a description of the inference procedure is given in algorithm 1",0
 npi then outputs the return probability and next program and arguments to execute,0
e,0
" any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of national science foundation and darpa",1
 our result shows that the training procedure and the npi architecture is capable of generalizing from the step input-output pairs seen in the training data to the unseen ones present in the verification set,0
" a topological sort is a linear ordering of vertices such that for every directed edge (u, v) from u to v, u comes before v in the ordering",0
 the model described in the training setup of section 4 (trained on 6 traces) was verified to be correct via the matching procedure described in section 4,0
" in this architecture, we consider a core controller, e",0
" in this version of curriculum learning, even though the inputs are gradually becoming more complex, the semantics of the program is succinct and does not change",1
6,1
" also the dag can have potentially more than one connected component, meaning it is necessary to transition between these components appropriately",0
"
grade-school addition",1
"
since npi naturally supports the notion of recursion, a key question is how to enable npi to learn recursive programs",1
"
it is important to emphasize that at inference time in the npi, the hidden state of the lstm controller is reset (to zero) at each subprogram call, as in line 3 of algorithm 1 (h ← 0)",0
" to make an analogy to mdps, this procedure is analogous to how value iteration obtains the correct value for each state starting from any initialization",0
" also the dag can have potentially more than one connected component, meaning it is necessary to transition between these components appropriately",0
" plo and phi point to the lo and hi indices of the array, as in algorithm 3",0
"
directed acyclic graphs are structurally more diverse than inputs in the two tasks of grade-school addition and bubble sort",0
"
for topological sort, the domain-specific encoder is
fenc(dag,qcolor, pstack, pstart, vactive, childlist, at)
=mlp ([qcolor(pstart), qcolor(dag[vactive][childlist[vactive]]), pstack == 1, at(1), at(2), at(3)]),
where qcolor ∈ ru×4 is a scratch-pad that contains u rows, each containing one of four colors (white, gray, black, invalid) with one-hot encoding",0
" because the context of the caller is stored on a stack when it calls another program and the callee starts in a fresh context, this enables recursion simply by allowing a program to call itself",1
" consequently, it is infeasible to construct a verification set that covers all cases, and the verification procedure we have described is inadequate",0
" the corresponding edge list is [(1, 2), (1, 5), (2, 4), (2, 5), (3, 5)]",1
" for brevity, we refer the reader to the appendix for information about the program set and non-recursive and recursive trace-generating functions for quicksort",1
 the sequence of primitive operations (move and write operations) for the non-recursive and recursive versions are exactly the same,0
"
we describe the base cases, reduction rules, and the verification set for each task in appendix a",1
" , n , where the dag contains n vertices",0
" in fact, we argue that recursion is an essential element for neural programs to generalize, and makes it tractable to prove the generalization of neural programs",1
" finally, the boolean pstack == 1 is used to check whether the stack is empty in line 18 of algorithm 2",0
 recursion enables provably perfect generalization,1
"
in this paper, we propose that recursion is an important concept for neural programs as well",0
" thus, any general neural programming architecture supporting such a call structure can be made to support recursion",0
" however, the authors of npi did not consider explicitly the notion of recursion and as a consequence, did not learn recursive programs",1
" the npi model has three learnable components: a task-agnostic core, a program-key embedding, and domain-specific encoders that allow the npi to operate in diverse environments",0
 2: initialize an empty stack s and a directed acyclic graph dag to traverse,1
" to make an analogy to mdps, this procedure is analogous to how value iteration obtains the correct value for each state starting from any initialization",1
" in particular, given that a recursion is defined by two properties as mentioned before, the base cases and the set of reduction rules, we can prove a recursive neural program generalizes perfectly if we can prove that (1) it performs correctly on the base cases; (2) it learns the reduction rules correctly",0
"as discussed in section 2, the neural programmer-interpreter (npi) is an instance of a neural programmer architecture and hence it naturally supports recursion",0
" lshift moves the four pointers to the left, to move to the next column",0
" for purposes of verification, we replace the domain-specific encoder with the following:
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1) ≤ q(1, i2), 1 ≤ i1 ≤ length, 1 ≤ i2 ≤ length, i3 == length, at(1), at(2), at(3)]),which directly exposes which of the two values pointed to is larger",0
"
base cases and reduction rules for quicksort",1
"
note that the training set can often be considerably smaller than the verification set, and despite this, the learned model can still pass the entire verification set",0
"
since npi naturally supports the notion of recursion, a key question is how to enable npi to learn recursive programs",1
"
we note that for tasks with very large input domains, such as ones involving mnist digits or speech samples, the state space of base cases and reduction rules could be prohibitively large, possibly infinite",0
3,1
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,0
"
is critical for implementing recursion, since it permits us to restrict our attention to the currently relevant recursive call, ignoring irrelevant details about other contexts",0
" the write operation has the following arguments:
arg 1 (main action): color curr, color next, active start, active neighb, active stack, save, stack push, stack pop, result
color curr colors vactive, color next colors vertex dag[vactive][childlist[vactive]], active start writes pstart to vactive, active neighb writes dag[vactive][childlist[vactive]] to vactive, active stack writes qstack(pstack) to vactive, save writes vactive to vsave, stack push pushes vactive to the top of the stack, stack pop writes a null value to the top of the stack, and result writes vactive to qresult(presult)",0
"
note that v should also contain the necessary reductions, which corresponds to making the recursive calls at the correct time, as indicated by p ",0
" k is set to 11, to denote the range of possible numbers (0 through 9), along with the start/end token (represented with the same encoding) which is observed when a pointer reaches beyond the bounds of the input",0
"
we note that for tasks with very large input domains, such as ones involving mnist digits or speech samples, the state space of base cases and reduction rules could be prohibitively large, possibly infinite",0
" the single-digit multiplication task in zaremba   (2016), the bubble sort task in reed & de freitas (2016), and the graph tasks in graves   (2016))",0
 reset represents a −∞ value,1
" training on the full recursive trace leads to perfect generalization, as shown in section 4",1
 ,1
" the arrow in the theorem refers to evaluation, as in big-step semantics",0
" on the other hand, the recursive programs have learned the true program semantics",1
" the core module of the npi is an lstm controller that takes as input a slice of the current external environment, via a set of pointers, and a program and arguments to execute",0
 this modification also enables us to sort arrays containing arbitrary comparable elements,1
g,0
" , a1a0 + b1b0} are added properly",0
"
we note that for tasks with very large input domains, such as ones involving mnist digits or speech samples, the state space of base cases and reduction rules could be prohibitively large, possibly infinite",0
" the non-recursive and partially recursive versions are unable to sort long arrays, beyond length 8",1
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,1
" fa8750-15-2-0104, and berkeley deep drive",0
 we found that changing the npi training traces is a simple way to enable this,1
"program descriptions calls arguments toposort perform topological
sort on graph traverse, next start, write, move
none
traverse traverse graph until stack is empty check child, explore none check child check if a white child exists; if so, set childlist[vactive] to point to it move none explore repeatedly traverse subgraphs until stack is empty stack, check child, write, move none stack interact with stack, either pushing or popping write, move push, pop next start move pstart until reaching a white vertex",0
"
base cases and reduction rules for addition",0
"
grade school addition",0
" although the model is exposed to more and more data, it might learn spurious and overly complex representations of the program, as suggested in zaremba   (2016)",0
" even though the earlier version also generalized just as well in practice, relatively small differences in the formulation of the traces and the environment observations can drastically change the difficulty of verification",1
"2 at any given time, the npi has access to values pointed to by four pointers in each of the four rows, represented by q(1, i1), q(2, i2), q(3, i3), and q(4, i4)",1
" m is considered to have perfect generalization if m can give the right answer for any input, such as inputs of arbitrary length",1
"
after finding v , we construct a set of problem inputs which, when executed on p , create exactly the step input sequences which make up v ",0
"
directed acyclic graphs are structurally more diverse than inputs in the two tasks of grade-school addition and bubble sort",0
" formally, the npi is represented by the following set of equations:
st = fenc(et, at)
ht = flstm(st, pt, ht−1)
rt = fend(ht), pt+1 = fprog(ht), at+1 = farg(ht)
t is a subscript denoting the time-step; fenc is a domain-specific encoder (to be described later) that takes in the environment slice et and arguments at; flstm represents the core module, which takes in the state st generated by fenc, a program embedding pt ∈ rp , and hidden lstm state ht; fend decodes the return probability rt; fprog decodes a program key embedding pt+1;1 and farg decodes arguments at+1",0
"
to evaluate a neural network that learns a neural program to accomplish a certain task, one common evaluation metric is how well the learned model m generalizes",1
" in particular, given that a recursion is defined by two properties as mentioned before, the base cases and the set of reduction rules, we can prove a recursive neural program generalizes perfectly if we can prove that (1) it performs correctly on the base cases; (2) it learns the reduction rules correctly",0
e,1
"
since npi naturally supports the notion of recursion, a key question is how to enable npi to learn recursive programs",1
" we note that an earlier version of quicksort we tried lacked primitive operations to directly move a pointer to another, and therefore needed more functions and observations",1
 this modification also enables us to sort arrays containing arbitrary comparable elements,0
" we show that by modifying the training procedure, we enable the npi model to learn recursive neural programs",0
"
it is important to emphasize that at inference time in the npi, the hidden state of the lstm controller is reset (to zero) at each subprogram call, as in line 3 of algorithm 1 (h ← 0)",0
" more specifically, when m is trained on simpler inputs, such as inputs of a small length, the generalization metric evaluates how well m will do on more complex inputs, such as inputs of much longer length",1
this material is in part based upon work supported by the national science foundation under grant no,0
" et+1 ∼ fenv(et, pt, at)",0
" a single element of an execution trace consists of a step input-step output pair, which can be synthesized from algorithm 1: this corresponds to, for a given time-step, the step input tuple (e, p, a) and step output tuple (r, p2, a2)",1
" without using a recursive program, such a proof is not possible, because the non-recursive program runs on an arbitrarily long addition problem that creates correspondingly long sequences of step inputs; in the non-recursive formulation of addition, add calls add1 a number of times that is dependent on the length of the input",0
" lshift, used in reset, shifts the pointers left until reaching the start token",0
"2 at any given time, the npi has access to values pointed to by four pointers in each of the four rows, represented by q(1, i1), q(2, i2), q(3, i3), and q(4, i4)",0
"
we experiment with two levels of recursion—partial and full",0
", the verification set",0
" for brevity, we refer the reader to the appendix for information about the program set and non-recursive and recursive trace-generating functions for quicksort",0
 ,1
" it is straightforward to deal with all other operations, which do not induce partial environment states",1
" in practice, we can additionally use tail recursion optimization to avoid problems with the call stack growing too deep",0
" in particular, we construct new training traces which explicitly contain recursive elements and show that with this type of trace, npi easily learns recursive programs",1
" if it is too small, then execution of the program on some input might require evaluation of m(i) on some i /∈ v , and so the behavior ofm(i) might deviate from p (i)",1
" while this matches the description from reed & de freitas (2016), we found that this causes an unnecessary blowup in the size of v and makes it much more difficult to construct the verification set",1
" we can obtain this mapping by first considering the possible observations that can arise at the beginning of the entry function (add, bubblesort, toposort, and quicksort) for some valid program input, and iteratively applying the observation-to-observation mapping implied by the reference implementation’s step output at that point in the execution",0
" for example, for the addition task, the model is trained on short inputs and then tested on its ability to sum inputs with much longer numbers of digits",1
g,1
" we call this set of inputs the verification set, sv ",1
"
the training set for bubble sort contains 100 traces, with maximum problem length of 2 (e",0
training neural networks to synthesize robust programs from a small number of examples is a challenging task,0
"
the training set for bubble sort contains 100 traces, with maximum problem length of 2 (e",1
 we adapt machinery from the original paper slightly to fit our needs,1
"
the three environment observations aid with control flow in algorithm 2",1
" on the other hand, the non-recursive version has low accuracy, beginning from size 5, and fails completely for graphs of size 8 and beyond",0
"
it is important to emphasize that at inference time in the npi, the hidden state of the lstm controller is reset (to zero) at each subprogram call, as in line 3 of algorithm 1 (h ← 0)",0
"
an alternative way of representing the environment slice is to expose the values of the absolute vertices to the model; however, this makes it difficult to scale the model to larger graphs, since large vertex values are not seen during training time",1
 the core controller acts as a dispatcher for the programs,1
" as with the others, we apply the procedure described in section 3",0
 recursion enables provably perfect generalization,1
"
directed acyclic graphs are structurally more diverse than inputs in the two tasks of grade-school addition and bubble sort",0
" observe that partially recursive does slightly better than non-recursive for the setting in which the length of the array is 3, and that the fully recursive version is able to sort every array given to it",1
"
the same set of problems was used to generate the training traces for all formulations of the task, for non-recursive and recursive versions",0
" this is highly undesirable, since being able to provide the correct answer in all possible settings is one of the most important aspects of any learned neural program",0
"
recursion drastically reduces the number of configurations we need to consider during the verification phase and makes the proof tractable, because it introduces structure that eliminates infinitely long sequences of step inputs that would otherwise need to be considered",1
" in this pair, the step input runs a subprogram add1 that has no arguments, and the step output contains a program write that has arguments of out and 1",0
" consequently, it is infeasible to construct a verification set that covers all cases, and the verification procedure we have described is inadequate",1
" in computer science, recursion (as opposed to iteration) involves solving a larger problem by combining solutions to smaller instances of the same problem",0
 this makes it difficult to reason about what the model will do when given complex inputs,0
" as before, the non-recursive learned program lacks a provable guarantee of generalization, whereas we show later that the recursive learned program has one",1
 recursion enables provable guarantees on neural programs’ behavior without needing to exhaustively enumerate all possible inputs to the programs,0
" partial recursion only adds a tail recursive call to bubblesort after bubble and reset, similar to the tail recursive call described previously for addition",0
 our result shows that the training procedure and the npi architecture is capable of generalizing from the step input-output pairs seen in the training data to the unseen ones present in the verification set,1
"
arg 2 (increment or decrement): up, down
swap",1
 this modification also enables us to sort arrays containing arbitrary comparable elements,1
"
in this paper, we propose to resolve these issues by explicitly incorporating recursion into neural architectures",0
5,1
" 3: 4: function quicksort(a, lo, hi) 5: if lo < hi: then 6: p = partition(a, lo, hi) 7: quicksort(a, lo, p− 1) 8: quicksort(a, p+ 1, hi) 9: 10: function partition(a, lo, hi) 11: pivot = lo 12: for j ∈ [lo, hi− 1] : do 13: if a[j] ≤ a[hi] then 14: swap a[pivot] with a[j] 15: pivot = pivot+ 1 16: swap a[pivot] with a[hi] 17: return pivot
for quicksort, the domain-specific encoder is
fenc(qarray, qstacklo, qstackhi, plo, phi, pstacklo, pstackhi, ppivot, pj , at) =
mlp ([qarray(pj) ≤ qarray(phi), pj == phi, qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1), pstacklo == 1, at(1), at(2), at(3)]),
whereqarray ∈ ru×11 is a scratch-pad that contains u rows, each containing one of 11 values (one of the numbers 0 through 9 or an invalid state)",0
"
quicksort",1
" the arrow in the theorem refers to evaluation, as in big-step semantics",1
" the only model, to our knowledge, that does not train on input-output pairs is the neural programmer-interpreter (reed & de freitas, 2016), which trains on synthetic execution traces",1
" the outputs rt, pt+1, at+1 are used to determine the next action, as described in algorithm 1",0
 we find that recursion makes it easier for the network to learn the right program and generalize to unknown situations,1
"
there are 6 pointers (plo, phi, pstacklo, pstackhi, ppivot, pj)",0
" the space of possible programs is extremely large, and composing a program that performs robustly on the infinite space of possible inputs is difficult—in part because it is impractical to obtain enough training examples to easily disambiguate amongst all possible programs",0
 recursion enables provably perfect generalization,0
" our results show that for all learned, to-be-verified neural programs, they all produced the same traces as those produced by p on the verification set",1
" in our experiments, we only present dag’s as inputs and represent the vertices as values ranging from 1, ",0
" m is considered to have perfect generalization if m can give the right answer for any input, such as inputs of arbitrary length",1
"
in what follows, we describe the way in which we constructed npi training traces so as to make them contain recursive elements and thus enable npi to learn recursive programs",1
" if the step output specifies a primitive function call, we need to reason about how it can affect the environment so as to change the observation in the next step input",0
" the core module of the npi is an lstm controller that takes as input a slice of the current external environment, via a set of pointers, and a program and arguments to execute",1
" et+1 ∼ fenv(et, pt, at)",1
" the write operation has the following arguments:
arg 1 (main action): color curr, color next, active start, active neighb, active stack, save, stack push, stack pop, result
color curr colors vactive, color next colors vertex dag[vactive][childlist[vactive]], active start writes pstart to vactive, active neighb writes dag[vactive][childlist[vactive]] to vactive, active stack writes qstack(pstack) to vactive, save writes vactive to vsave, stack push pushes vactive to the top of the stack, stack pop writes a null value to the top of the stack, and result writes vactive to qresult(presult)",0
" on the other hand, the non-recursive version has low accuracy, beginning from size 5, and fails completely for graphs of size 8 and beyond",1
 we outline how to construct this set,1
" it is straightforward to deal with all other operations, which do not induce partial environment states",1
 the program terminates when seeing no numbers in the current column,1
" consequently, there is no concept of length relevant to the problem, which has traditionally been an important focus of length-based curriculum learning",0
 we find that recursion makes it easier for the network to learn the right program and generalize to unknown situations,0
" 16: p = partition(a, lo, hi) 17: push p+ 1 and hi to slo and shi",0
" recall that this verification procedure cannot be performed for the non-recursive versions, since the propagation of the hidden state in the core lstm module makes reasoning difficult and so we would need to check an unbounded number of examples",1
" finally, the boolean pstack == 1 is used to check whether the stack is empty in line 18 of algorithm 2",0
" for each task, given the verification set, we check the traces and results of the learned, to-be-verified neural program (described in section 4",0
3,1
"
the training data for the neural programmer-interpreter consists of full execution traces for the program of interest",0
" in the non-recursive trace, there are four functions that can be made recursive—toposort, check child, explore, and next start, and we add a tail recursive call to each of these functions in order to make the recursive trace",0
" it is straightforward to deal with all other operations, which do not induce partial environment states",1
" as before, the non-recursive learned program lacks a provable guarantee of generalization, whereas we show later that the recursive learned program has one",0
" as a consequence, our learned neural programs empirically achieve perfect generalization from a very small number of training examples",0
" in fact, it is one of our future directions to explore new ways to train a recursive neural program without providing explicit training execution traces or with only partial or non-recursive traces",1
 some functional programming languages go so far as not to define any looping constructs but rely solely on recursion to enable repeated execution of the same code,1
 we outline how to construct this set,0
 we outline how to construct this set,0
" note that recursion helps make this process tractable, because we only need to test a finite number of inputs to show that the model will work correctly on inputs of unbounded complexity",1
g,0
"
directed acyclic graphs are structurally more diverse than inputs in the two tasks of grade-school addition and bubble sort",0
" if yes, then this indicates that the learned neural program achieves provably perfect generalization",0
" k is set to 11, to denote the range of possible numbers (0 through 9), along with the start/end token (represented with the same encoding) which is observed when a pointer reaches beyond the bounds of the input",0
" however, to be sure that the generated v is complete (covers all the cases needed), we need to check all pairs of observations seen in adjacent step inputs (in particular, those before and after a primitive function call), in a similar way as if we were constructing v from scratch",1
" both the non-recursive and recursive learned programs generalize on all input lengths we tried, up to 5000 digits",1
"
algorithm 2 shows the topological sort task of interest",0
as mentioned in section 2,1
" , a1a0 + b1b0} are added properly",0
" we show that by modifying the training procedure, we enable the npi model to learn recursive neural programs",0
" in particular, we construct new training traces which explicitly contain recursive elements and show that with this type of trace, npi easily learns recursive programs",0
" partial recursion only adds a tail recursive call to bubblesort after bubble and reset, similar to the tail recursive call described previously for addition",0
"
under the assumption that there are no leading 0’s (except in the case of single digits) and the two numbers to be added have the same number of digits, the verification set for addition contains 20,181 input problems",0
"
arg 2 (object to copy): env stack lo peek, env stack hi peek, phi, plo, ppivot − 1, ppivot + 1, reset
env stack lo peek and env stack hi peek represent qstacklo(pstacklo − 1) and qstackhi(pstackhi − 1), respectively",0
 there are 2 variables (vactive and vsave); vactive holds the active vertex (as in algorithm 2) and vsave holds the value of vactive before executing line 12 of algorithm 2,0
" in our experiments, we only present dag’s as inputs and represent the vertices as values ranging from 1, ",0
 this is due to the properties of the training set – we trained on 2 traces synthesized from arrays of length 7 and 1 trace synthesized from an array of length 6,0
" this makes it so that we only need to consider the vertices in the subgraph that are currently relevant for computing the sort, allowing simpler reasoning about behavior for large graphs",1
training neural networks to synthesize robust programs from a small number of examples is a challenging task,0
"
directed acyclic graphs are structurally more diverse than inputs in the two tasks of grade-school addition and bubble sort",0
" the assumption of leading 0’s can be easily removed, at the cost of slightly increasing the size of the verification set",0
" both the non-recursive and recursive learned programs generalize on all input lengths we tried, up to 5000 digits",1
"
given a verification set, we can then run the model on the verification set to check if the produced traces and results are correct",0
" qcolor(dag[vactive][childlist[vactive]]) refers to the color of the next child of vactive, used in the evaluation of the condition in the if branch in line 9 of algorithm 2",0
"
arg 2 (auxiliary variable): color grey, color black
color grey and color black color the given vertex grey and black, respectively",0
"
by reasoning about the possible set of environment observations created by all valid inputs, we construct v using the procedure described in section 3",0
" lshift moves the four pointers to the left, to move to the next column",0
" nevertheless, we would like the model to quickly learn to represent the right semantics of the underlying program from a small number of training examples, not an exhaustive number of them",1
" finally, we implement recursive traces for our own topological sort and quicksort tasks",0
" at each time step, the core controller can decide to select one of the programs to call with certain arguments",0
" for tasks other than addition, it is difficult to analytically determine the verification set, so instead, we randomly generate input candidates until they completely cover the base cases and reduction rules",0
 reset represents a −∞ value,0
 the environment and return probability are omitted for readability,1
"
arg 2 (object to copy): env stack lo peek, env stack hi peek, phi, plo, ppivot − 1, ppivot + 1, reset
env stack lo peek and env stack hi peek represent qstacklo(pstacklo − 1) and qstackhi(pstackhi − 1), respectively",0
2,0
" we hypothesize that the reason for this is that the neural network learns to spuriously depend on specific characteristics of the training examples that are irrelevant to the true program semantics, such as length of the training inputs, and thus fails to generalize to more complex inputs",0
" if no white vertex is found, the entire execution is terminated move none write write a value either to environment (e",1
" because the context of the caller is stored on a stack when it calls another program and the callee starts in a fresh context, this enables recursion simply by allowing a program to call itself",0
"
as in line 13 of the right-hand side of figure 1",1
" at end, pointer ppivot is moved to the pivot compswap loop, move pivot lo, move j lo, swap
none
compswap loop runs the for loop inside the partition function compswap, move none compswap compares a[pivot] ≤ a[j]; if so, perform a swap and increment ppivot swap, move none set pivot lo sets ppivot to lo index none none set j lo sets pj to lo index none none set j null sets pj to −∞ none none stack pushes lo/hi states
onto stacks slo and shi according to argument (described below)
write, move described below
move moves pointer one unit up or down none described below swap swaps elements at given array indices none described below write write a value either to stack (e",0
"
topological sort",0
" the general notion of recursion has been an important concept in many domains, including mathematics and computer science",1
" training on the full recursive trace leads to perfect generalization, as shown in section 4",0
"
training setup",0
"
in this general neural programming architecture, we show it is easy to support recursion",0
"
the same set of problems was used to generate the training traces for all formulations of the task, for non-recursive and recursive versions",1
" as this complexity interfered with determining the base cases and reductions, we changed the algorithm to its current form",0
" as aforementioned, the npi model naturally supports recursion",1
" there are 3 pointers (presult, pstack, pstart), presult points to the next empty location in qresult,
pstack points to the top of the stack in qstack, and pstart points to the candidate starting node for a connected component",1
" in the non-recursive trace, there are four functions that can be made recursive—toposort, check child, explore, and next start, and we add a tail recursive call to each of these functions in order to make the recursive trace",1
"
topological sort",0
"
we also report on generalization results for the non-recursive and recursive versions of this variant of bubble sort",1
" in all experiments, α is set to 0",0
" we prove every member of f is added properly, given that subproblems s = {anan−1 + bnbn−1, an−1an−2 + bn−1bn−2, ",0
" the corresponding edge list is [(1, 2), (1, 5), (2, 4), (2, 5), (3, 5)]",0
" empirically, existing models suffer from a common limitation—generalization becomes poor beyond a threshold level of complexity",1
", to color a vertex) or variable (e",0
" provably perfect generalization implies the model will behave correctly, given any valid input",0
" because the context of the caller is stored on a stack when it calls another program and the callee starts in a fresh context, this enables recursion simply by allowing a program to call itself",0
"
an alternative method is to run p on many different program inputs and then observe step input sequences which occur, to create v ",0
"
algorithm 2 shows the topological sort task of interest",0
" for purposes of verification, we replace the domain-specific encoder with the following:
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1) ≤ q(1, i2), 1 ≤ i1 ≤ length, 1 ≤ i2 ≤ length, i3 == length, at(1), at(2), at(3)]),which directly exposes which of the two values pointed to is larger",1
" we then construct problem configurations that span all possible valid environment states (for instance, in order to force the carry bit in a column to be 1, one can add the sum “1+9” in the column to the right)",0
 reset represents a −∞ value,0
"2 at any given time, the npi has access to values pointed to by four pointers in each of the four rows, represented by q(1, i1), q(2, i2), q(3, i3), and q(4, i4)",0
" for the recursive formulation of addition, we analytically construct the set of input problems that cover all base cases and reduction rules",1
 table 4 also demonstrates that the (verified) recursive program generalizes perfectly,0
 indentation indicates the stack is one level deeper than before,1
 ,0
", to change the value of phi) none described belowstack",0
" as this complexity interfered with determining the base cases and reductions, we changed the algorithm to its current form",0
" for each function we use to implement the recursive version of topological sort, we need to consider the set of possible environment observation sequences we can create from all valid inputs and test that the learned program produces the correct behavior on each of these inputs",0
" if a white vertex is found, set pstart to point to it; this signifies the start of a traversal of a new connected component",0
" 3: 4: function partition(a, lo, hi) 5: pivot = lo 6: for j ∈ [lo, hi− 1] : do 7: if a[j] ≤ a[hi] then 8: swap a[pivot] with a[j] 9: pivot = pivot+ 1 0: swap a[pivot] with a[hi] 11: return pivot 12: 13: function quicksort(a, lo, hi) 14: while slo and shi are not empty: do 15: pop states off slo and shi, writing them to lo and hi",0
" the move operation has the following arguments:
arg 1 (pointer): presult, pstack, pstart, childlist[vactive], childlist[vsave]
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 5 values",0
"
inside bubble and reset, there are two operations that can be made recursive",1
"when constructing a neural network for the purpose of learning a program, there are two orthogonal aspects to consider",0
" in computer science, recursion (as opposed to iteration) involves solving a larger problem by combining solutions to smaller instances of the same problem",0
" if a white vertex is found, set pstart to point to it; this signifies the start of a traversal of a new connected component",1
" in particular, recursion can be implemented as a program calling itself",0
 this is due to the properties of the training set – we trained on 2 traces synthesized from arrays of length 7 and 1 trace synthesized from an array of length 6,1
 errors arise due to undesirable and uninterpretable dependencies and associations the architecture learns to store in some high-dimensional hidden state,1
" we hypothesize that the reason for this is that the neural network learns to spuriously depend on specific characteristics of the training examples that are irrelevant to the true program semantics, such as length of the training inputs, and thus fails to generalize to more complex inputs",0
" however, the authors of npi did not consider explicitly the notion of recursion and as a consequence, did not learn recursive programs",0
 the degree for any vertex in the dag is variable,0
" the assumption of leading 0’s can be easily removed, at the cost of slightly increasing the size of the verification set",1
"
by reasoning about the possible set of environment observations created by all valid inputs, we construct v using the procedure described in section 3",1
3,0
g,1
"
for addition, we analytically determine the verification set",1
 b1b0 where no carry operations occur,0
" this makes it so that we only need to consider the vertices in the subgraph that are currently relevant for computing the sort, allowing simpler reasoning about behavior for large graphs",1
" as an application, we incorporate recursion into the neural programmer-interpreter architecture and consider four sample tasks: grade-school addition, bubble sort, topological sort, and quicksort",0
" for brevity, we refer the reader to the appendix for further details on the program set and non-recursive and recursive trace-generating functions used for topological sort",1
" also the dag can have potentially more than one connected component, meaning it is necessary to transition between these components appropriately",0
"
arg 2 (increment or decrement): up, down1 // top level topological sort call 2 toposort() { 3 while (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 } 0 11 traverse() { 12 check_child() 13 explore() 14 } 15 16 check_child() { 17 while (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 18 move(childlist[vactive], up) 19 } 20 21 explore() { 22 do 23 if (qcolor(dag[vactive][childlist[vactive]]) is white): 24 write(color_next, color_grey) 25 stack(push) 26 write(save) 27 write(active_neighb) 28 move(childlist[vsave], up) 29 else: 30 write(color_curr, color_black) 31 write(result) 32 move(presult, up) 33 if(pstack == 1): 34 break 35 else: 36 stack(pop) 37 check_child() 38 while (true) 39 } 40 41 stack(op) { 42 if (op == push): 43 write(stack_push) 44 move(pstack, up) 45 46 if (op == pop): 47 write(active_stack) 48 write(stack_pop) 49 move(pstack, down) 50 } 51 52 next_start() { 53 while(qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 54 move(pstart, up) 55 }altered recursive functions  // top level topological sort call 2 toposort() { 3 if (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 toposort() // recursive call 0 } 11 12 check_child() { 13 if (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 14 move(childlist[vactive], up) 15 check_child() // recursive call 16 } 17 18 explore() { 19 if (qcolor(dag[vactive][childlist[vactive]]) is white): 20 write(color_next, color_grey) 21 stack(push) 22 write(save) 23 write(active_neighb) 24 move(childlist[vsave], up) 25 else: 26 write(color_curr, color_black) 27 write(result) 28 move(presult, up) 29 if(pstack == 1): 30 return 31 else: 32 stack(pop) 33 check_child() 34 explore() // recursive call 35 } 36 37 next_start() { 38 if (qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 39 move(pstart, up) 40 next_start() // recursive call 41 }algorithm 4 iterative quicksort 1: initialize an array a to sort and two empty stacks slo and shi",0
" each step during an execution of the program does one of three things: (1) another subprogram along with associated arguments is called, as in line 10, (2) the program writes to the environment if it is primitive, as in line 8, or (3) the loop is terminated if the return probability exceeds a threshold α, after which the stack frame is popped and control is returned to the caller",0
 indentation indicates the stack is one level deeper than before,0
 ,0
"
quicksort",1
 ,0
"
recursion drastically reduces the number of configurations we need to consider during the verification phase and makes the proof tractable, because it introduces structure that eliminates infinitely long sequences of step inputs that would otherwise need to be considered",1
" if it is too small, then execution of the program on some input might require evaluation of m(i) on some i /∈ v , and so the behavior ofm(i) might deviate from p (i)",1
 stack push call2 pushes pivot + 1 and hi to qstacklo and qstackhi,1
 there is a (changing) list of neural programs used to accomplish a given task,1
" we show that by modifying the training procedure, we enable the npi model to learn recursive neural programs",0
" ppivot and pj point to the pivot and j indices of the array, used in the partition function in algorithm 3",0
" we prove every member of f is added properly, given that subproblems s = {anan−1 + bnbn−1, an−1an−2 + bn−1bn−2, ",0
g,0
"
bubble sort",0
" that is to say, the network does not learn the true program semantics",0
"
arg 2 (increment or decrement): up, down
swap",0
" ppivot and pj point to the pivot and j indices of the array, used in the partition function in algorithm 3",0
g,1
 almost all architectures train on program input/output pairs,0
" at each time step, the core controller can decide to select one of the programs to call with certain arguments",1
 ,1
 we adapt machinery from the original paper slightly to fit our needs,0
" thus, any general neural programming architecture supporting such a call structure can be made to support recursion",0
"
in what follows, we describe the way in which we constructed npi training traces so as to make them contain recursive elements and thus enable npi to learn recursive programs",0
we emphasize that the notion of a neural recursive program has not been presented in the literature before: this is our main contribution,0
 ,0
"
the training set for quicksort contains 4 traces, synthesized from arrays of length 5",1
 the program terminates when seeing no numbers in the current column,1
" we hypothesize that the reason for this is that the neural network learns to spuriously depend on specific characteristics of the training examples that are irrelevant to the true program semantics, such as length of the training inputs, and thus fails to generalize to more complex inputs",0
" the corresponding edge list is [(1, 2), (1, 5), (2, 4), (2, 5), (3, 5)]",0
 2: initialize an empty stack s and a directed acyclic graph dag to traverse,0
 we find that recursion makes it easier for the network to learn the right program and generalize to unknown situations,1
" formally, a function exhibits recursive behavior when it possesses two properties: (1) base cases— terminating scenarios that do not use recursion to produce answers; (2) a set of rules that reduces all other problems toward the base cases",1
"
formally, verification consists of proving the following theorem:
∀i ∈ v,m(i) ⇓ p (i) where i denotes a sequence of step inputs (within one function call), v denotes the set of valid sequences of step inputs, m denotes the neural network model, p denotes the correct program, and p (i) denotes the next step output from the correct program",0
" the write operation has the following arguments:
arg 1 (object to write): env stack lo, env stack hi, phi, plo
env stack lo and env stack hi represent qstacklo(pstacklo) and qstackhi(pstackhi), respectively",1
 ,1
 ,1
" formally, a function exhibits recursive behavior when it possesses two properties: (1) base cases— terminating scenarios that do not use recursion to produce answers; (2) a set of rules that reduces all other problems toward the base cases",0
", the trace corresponding to the array [3,2])",1
 this verification phase only needs to be performed once after training,0
 ,1
"
note that v should also contain the necessary reductions, which corresponds to making the recursive calls at the correct time, as indicated by p ",0
 the maximum problem length in this training set is 3 (e,1
"
we propose and describe our verification procedure",0
" for problems where the base cases may be extremely large or infinite, such as certain forms of motor control, recursion can still help reduce the problem of generalization to these two aspects and make the generalization problem significantly simpler to handle and reason about",1
" for tasks other than addition, it is difficult to analytically determine the verification set, so instead, we randomly generate input candidates until they completely cover the base cases and reduction rules",0
" observe that the non-recursive program’s correctness degrades for length 11 and beyond, while the recursive program can sort any given array",1
"
to evaluate a neural network that learns a neural program to accomplish a certain task, one common evaluation metric is how well the learned model m generalizes",0
" for each length, we test each program on 30 randomly generated problems",1
 add1 is a subprogram that adds the current column (writing the appropriate digit to the output row and carrying a bit to the next column if needed),0
" as this complexity interfered with determining the base cases and reductions, we changed the algorithm to its current form",0
", the trace corresponding to the array [3,2])",1
", to color a vertex) or variable (e",1
" as before, the non-recursive learned program lacks a provable guarantee of generalization, whereas we show later that the recursive learned program has one",1
"program descriptions calls arguments toposort perform topological
sort on graph traverse, next start, write, move
none
traverse traverse graph until stack is empty check child, explore none check child check if a white child exists; if so, set childlist[vactive] to point to it move none explore repeatedly traverse subgraphs until stack is empty stack, check child, write, move none stack interact with stack, either pushing or popping write, move push, pop next start move pstart until reaching a white vertex",0
" we made the assumption of equivalent lengths in order to parametrize the input format with respect to length, but this assumption can be removed as well",1
 we describe the recursive re-formulation of traces for two tasks from the original npi paper—grade-school addition and bubble sort,0
" a single element of an execution trace consists of a step input-step output pair, which can be synthesized from algorithm 1: this corresponds to, for a given time-step, the step input tuple (e, p, a) and step output tuple (r, p2, a2)",1
 b1b0 where no carry operations occur,1
 we leave this as future work to devise a verification procedure more appropriate to this setting,0
training neural networks to synthesize robust programs from a small number of examples is a challenging task,0
 this makes it difficult to reason about what the model will do when given complex inputs,1
"
to evaluate a neural network that learns a neural program to accomplish a certain task, one common evaluation metric is how well the learned model m generalizes",0
" the recursive version of topological sort solves all graph instances we tried, from graphs of size 5 through 70",0
 these pointers are referred to as bubble pointers,1
" thus, we demonstrate that recursion enables provably perfect generalization for different tasks, including addition, topological sort, quicksort, and a variant of bubble sort",0
" in fact, we argue that recursion is an essential element for neural programs to generalize, and makes it tractable to prove the generalization of neural programs",1
"
the three environment observations aid with control flow in algorithm 2",0
" if no white vertex is found, the entire execution is terminated move none write write a value either to environment (e",1
"
figure 1 shows examples of non-recursive and recursive addition traces",1
" empirically, existing models suffer from a common limitation—generalization becomes poor beyond a threshold level of complexity",1
" the verification set can be very small, as we found a 10-element array ([8,2,1,2,0,8,5,8,3,7]) is sufficient to cover all of v ",0
 the training set for addition contains 200 traces,1
", to change the value of vactive) none described below move move a pointer (e",0
 table 4 also demonstrates that the (verified) recursive program generalizes perfectly,0
" for instance, for recursive addition, consider the family f of addition problems anan−1 ",0
" here as a concrete and general example, we consider a general neural programming architecture (npa), similar to neural programmer-interpreter (npi) in reed & de freitas (2016)",1
" this guarantees that the step input sequences considered during verification contain all step input sequences which arise during execution of an unseen problem in f , leading to generalization to any problem in f ",0
 there are 2 variables (vactive and vsave); vactive holds the active vertex (as in algorithm 2) and vsave holds the value of vactive before executing line 12 of algorithm 2,0
"
the same set of problems was used to generate the training traces for all formulations of the task, for non-recursive and recursive versions",0
"
in addition, none of the current approaches to neural programming provide a method or even aim to enable provable guarantees about generalization",0
"1, we hypothesize the non-recursive programs do not generalize well because they have learned spurious dependencies specific to the training set, such as length of the input problems",0
", to change the value of vactive) none described below move move a pointer (e",0
" we have three observations: the color of the start node, the color of the active node’s next child to be considered, and whether the stack is empty",1
"
grade school addition",0
"
base cases and reduction rules for addition",0
"
since npi naturally supports the notion of recursion, a key question is how to enable npi to learn recursive programs",0
" on the other hand, the non-recursive version has low accuracy, beginning from size 5, and fails completely for graphs of size 8 and beyond",0
" , n , where the dag contains n vertices",0
g,0
"
arg 2 (auxiliary variable): color grey, color black
color grey and color black color the given vertex grey and black, respectively",0
" we made the assumption of equivalent lengths in order to parametrize the input format with respect to length, but this assumption can be removed as well",0
"
the training set for bubble sort contains 100 traces, with maximum problem length of 2 (e",1
"
the non-recursive trace loops on cycles of bubble and reset, which logically represents one bubble sweep through the array and reset of the two bubble pointers to the very beginning of the array, respectively",0
" recursion is an important concept in programming languages and a critical tool to
reduce the complexity of programs",0
" we make the trace recursive by adding a tail recursive call into the trace for the add program after calling add1 and lshift, the original paper uses k = 10, but we found it necessary to augment the range with an end token, in order to terminate properly",0
" however, models that make use of this strategy eventually fail after a certain level of complexity (e",1
"
the training set for bubble sort contains 100 traces, with maximum problem length of 2 (e",1
" as with the others, we apply the procedure described in section 3",0
" we also hope to decrease supervision, for example by training with only partial or non-recursive traces, and to develop novel neural programming architectures integrated directly with a notion of recursion",0
" recall that this verification procedure cannot be performed for the non-recursive versions, since the propagation of the hidden state in the core lstm module makes reasoning difficult and so we would need to check an unbounded number of examples",1
" the swap operation has the following arguments:
arg 1 (swap object 1): ppivot
arg 2 (swap object 2): phi, pj
write",0
" for each function we use to implement the recursive version of topological sort, we need to consider the set of possible environment observation sequences we can create from all valid inputs and test that the learned program produces the correct behavior on each of these inputs",0
" qcolor(dag[vactive][childlist[vactive]]) refers to the color of the next child of vactive, used in the evaluation of the condition in the if branch in line 9 of algorithm 2",0
" 3: 4: function partition(a, lo, hi) 5: pivot = lo 6: for j ∈ [lo, hi− 1] : do 7: if a[j] ≤ a[hi] then 8: swap a[pivot] with a[j] 9: pivot = pivot+ 1 0: swap a[pivot] with a[hi] 11: return pivot 12: 13: function quicksort(a, lo, hi) 14: while slo and shi are not empty: do 15: pop states off slo and shi, writing them to lo and hi",1
 the environment and return probability are omitted for readability,0
" in particular, recursion can be implemented as a program calling itself",0
 the original version of the bubblesort implementation exposes the values within the array,1
" if it is too large, then the semantics of p might not be well-defined on some elements in v , or the spurious step input sequences may not be reachable from any valid problem input (e",0
" however, the authors of npi did not consider explicitly the notion of recursion and as a consequence, did not learn recursive programs",0
" furthermore, given a verification input set that covers all base cases and reduction rules, we can formally prove that the learned neural programs achieve perfect generalization after verifying its behavior on the verification input set",0
"
for topological sort, the domain-specific encoder is
fenc(dag,qcolor, pstack, pstart, vactive, childlist, at)
=mlp ([qcolor(pstart), qcolor(dag[vactive][childlist[vactive]]), pstack == 1, at(1), at(2), at(3)]),
where qcolor ∈ ru×4 is a scratch-pad that contains u rows, each containing one of four colors (white, gray, black, invalid) with one-hot encoding",0
"6) on the verification set, and ensure they match the traces produced by the true program p ",1
" in fact, we argue that recursion is an essential element for neural programs to generalize, and makes it tractable to prove the generalization of neural programs",0
 we use the lomuto partition scheme; the logic for the recursive trace is shown in algorithm 3,1
" as aforementioned, the npi model naturally supports recursion",0
"
topological sort",0
" formally, a function exhibits recursive behavior when it possesses two properties: (1) base cases— terminating scenarios that do not use recursion to produce answers; (2) a set of rules that reduces all other problems toward the base cases",0
"we describe how models trained with recursive traces can be proven to generalize, by using the verification procedure described in section 3",1
 we leave this as future work to devise a verification procedure more appropriate to this setting,1
 we find that recursion makes it easier for the network to learn the right program and generalize to unknown situations,0
"
figure 1 shows examples of non-recursive and recursive addition traces",0
" in this section, we give a brief review of the npi architecture from reed & de freitas (2016) as background",0
" numerous models have been proposed for learning programs; to name a few, this includes the differentiable neural computer (graves  , 2016), neural turing machine (graves  , 2014), neural gpu (kaiser & sutskever, 2015), neural programmer (neelakantan  , 2015), pointer network (vinyals  , 2015), hierarchical attentive memory (andrychowicz & kurach, 2016), and neural random access machine (kurach  , 2016)",0
3,0
5,1
" we make the trace recursive by adding a tail recursive call into the trace for the add program after calling add1 and lshift, the original paper uses k = 10, but we found it necessary to augment the range with an end token, in order to terminate properly",1
" both the non-recursive and recursive learned programs generalize on all graphs we tried, up to 120 vertices",1
"
base cases and reduction rules for bubble sort",1
" in this pair, the step input runs a subprogram add1 that has no arguments, and the step output contains a program write that has arguments of out and 1",0
", to color a vertex) or variable (e",0
 18: push lo and p− 1 to slo and shi,0
" if a white vertex is found, set pstart to point to it; this signifies the start of a traversal of a new connected component",0
" a topological sort is a linear ordering of vertices such that for every directed edge (u, v) from u to v, u comes before v in the ordering",0
" the move operation has the following arguments:
arg 1 (pointer): pstacklo, pstackhi, pj , ppivot
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values",0
"
the non-recursive trace loops on cycles of bubble and reset, which logically represents one bubble sweep through the array and reset of the two bubble pointers to the very beginning of the array, respectively",0
 we found that changing the npi training traces is a simple way to enable this,0
"
the non-recursive trace loops on cycles of bubble and reset, which logically represents one bubble sweep through the array and reset of the two bubble pointers to the very beginning of the array, respectively",1
"
bubble sort",1
"
the training data for the neural programmer-interpreter consists of full execution traces for the program of interest",1
" recursion is an important concept in programming languages and a critical tool to
reduce the complexity of programs",0
"
since npi naturally supports the notion of recursion, a key question is how to enable npi to learn recursive programs",1
g,0
" on the other hand, the recursive programs have learned the true program semantics",1
" k is set to 11, to represent the range of 10 possible digits, along with a token representing the end of input",1
" using this modification, we constructed a verification set consisting of one array of size 10",1
 the memory updates of these neural programs are so complex and interdependent that it is difficult to reason about the behaviors of the learned neural program under previously unseen situations (such as problems with longer inputs),0
", an lstm in npi’s case, but possibly other networks in different cases",0
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,0
"1; and for bubble sort, appendix a",0
" although the model is exposed to more and more data, it might learn spurious and overly complex representations of the program, as suggested in zaremba   (2016)",1
 almost all architectures train on program input/output pairs,1
 ,0
"
quicksort",0
" the pointers at index i1 and i2 are used to compare the pair of numbers considered during the bubble sweep, swapping them if the number at i1 is greater than that in i2",0
" in this section, we give a brief review of the npi architecture from reed & de freitas (2016) as background",1
" in contrast, each call to the recursive add always runs for a fixed number of steps, even on arbitrarily long problems in f , so we can test that it performs correctly on a small, fixed number of step input sequences",0
"
the three environment observations aid with control flow in algorithm 2",0
"
after finding v , we construct a set of problem inputs which, when executed on p , create exactly the step input sequences which make up v ",1
"
move",0
"
the non-recursive trace loops on cycles of add1 and lshift",1
" m , as described in algorithm 1, processes the sequence of step inputs by using an lstm",0
"
as mentioned in section 1, all approaches to neural programming today fare poorly on this generalization issue",0
" qcolor(pstart) contains the color of the current start vertex, used in the evaluation of the condition in the while loop in line 5 of algorithm 2",0
" in the non-recursive trace, there are four functions that can be made recursive—toposort, check child, explore, and next start, and we add a tail recursive call to each of these functions in order to make the recursive trace",0
" the stack operation has the following arguments:
arg 1 (operation): stack push call1, stack push call2, stack pop
stack push call1 pushes lo and pivot−1 toqstacklo andqstackhi",1
" the write operation has the following arguments:
arg 1 (main action): color curr, color next, active start, active neighb, active stack, save, stack push, stack pop, result
color curr colors vactive, color next colors vertex dag[vactive][childlist[vactive]], active start writes pstart to vactive, active neighb writes dag[vactive][childlist[vactive]] to vactive, active stack writes qstack(pstack) to vactive, save writes vactive to vsave, stack push pushes vactive to the top of the stack, stack pop writes a null value to the top of the stack, and result writes vactive to qresult(presult)",0
 add1 is a subprogram that adds the current column (writing the appropriate digit to the output row and carrying a bit to the next column if needed),0
 the environment and return probability are omitted for readability,0
 stack pop pushes −∞ values to qstacklo and qstackhi,1
" in order to claim a proof, we must verify the model produces correct behavior over all base cases and reductions, as described in section 2",0
" in practice, we can additionally use tail recursion optimization to avoid problems with the call stack growing too deep",0
" the corresponding edge list is [(1, 2), (1, 5), (2, 4), (2, 5), (3, 5)]",1
" the move operation has the following arguments:
arg 1 (pointer): presult, pstack, pstart, childlist[vactive], childlist[vsave]
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 5 values",1
 pstacklo and pstackhi point to the top (empty) positions in qstacklo and qstackhi,0
 we leave this as future work to devise a verification procedure more appropriate to this setting,0
" to the best of our knowledge, this is the first time verification has been applied to a neural program, providing provable guarantees about its behavior",0
" consequently, it is infeasible to construct a verification set that covers all cases, and the verification procedure we have described is inadequate",0
 18: push lo and p− 1 to slo and shi,1
" thus, recursion helps decompose a problem and makes it easier to reason about a program’s behavior for previously unseen situations such as longer inputs",1
"program descriptions calls arguments toposort perform topological
sort on graph traverse, next start, write, move
none
traverse traverse graph until stack is empty check child, explore none check child check if a white child exists; if so, set childlist[vactive] to point to it move none explore repeatedly traverse subgraphs until stack is empty stack, check child, write, move none stack interact with stack, either pushing or popping write, move push, pop next start move pstart until reaching a white vertex",1
" however, to be sure that the generated v is complete (covers all the cases needed), we need to check all pairs of observations seen in adjacent step inputs (in particular, those before and after a primitive function call), in a similar way as if we were constructing v from scratch",0
 we choose to implement a topological sort task for graphs,0
" in particular, this condition is satisfied by npi, and thus the npi model naturally supports recursion (even though the authors of npi did not consider this aspect explicitly)",1
" the outputs rt, pt+1, at+1 are used to determine the next action, as described in algorithm 1",0
" we show that by modifying the training procedure, we enable the npi model to learn recursive neural programs",0
" bstep, used in bubble, compares pairs of numbers, continuously moving the bubble pointers once to the right each time until reaching the end of the array",1
" 16: p = partition(a, lo, hi) 17: push p+ 1 and hi to slo and shi",1
" to the best of our knowledge, this is the first time verification has been applied to a neural program, providing provable guarantees about its behavior",1
" for non-primitive subprograms, we can update the observation-to-observation mapping currently associated with the subprogram and then apply that mapping to the current set",1
 reset represents a −∞ value,0
" the single-digit multiplication task in zaremba   (2016), the bubble sort task in reed & de freitas (2016), and the graph tasks in graves   (2016))",1
" more specifically, when m is trained on simpler inputs, such as inputs of a small length, the generalization metric evaluates how well m will do on more complex inputs, such as inputs of much longer length",0
"
arg 2 (object to copy): env stack lo peek, env stack hi peek, phi, plo, ppivot − 1, ppivot + 1, reset
env stack lo peek and env stack hi peek represent qstacklo(pstacklo − 1) and qstackhi(pstackhi − 1), respectively",1
 this modification also enables us to sort arrays containing arbitrary comparable elements,1
"
recursion drastically reduces the number of configurations we need to consider during the verification phase and makes the proof tractable, because it introduces structure that eliminates infinitely long sequences of step inputs that would otherwise need to be considered",0
" in this pair, the step input runs a subprogram add1 that has no arguments, and the step output contains a program write that has arguments of out and 1",0
"
note that the recursion for quicksort is not purely tail recursive and therefore represents a more complex kind of recursion that is harder to learn than in the previous tasks",0
" in computer science, recursion (as opposed to iteration) involves solving a larger problem by combining solutions to smaller instances of the same problem",1
 we use the lomuto partition scheme; the logic for the recursive trace is shown in algorithm 3,0
", to change the value of vactive) none described below move move a pointer (e",0
" however, models that make use of this strategy eventually fail after a certain level of complexity (e",1
" recall that this verification procedure cannot be performed for the non-recursive versions, since the propagation of the hidden state in the core lstm module makes reasoning difficult and so we would need to check an unbounded number of examples",1
 these pointers are referred to as bubble pointers,1
g,0
" the corresponding edge list is [(1, 2), (1, 5), (2, 4), (2, 5), (3, 5)]",0
" the recursive version can generalize perfectly to long problems constructed from these components (such as the sum “822+233”, where “8+2” and “2+3” are in the training set), but the non-recursive version fails to sum these long problems properly",0
"
as mentioned in section 1, all approaches to neural programming today fare poorly on this generalization issue",1
"
note that the training set can often be considerably smaller than the verification set, and despite this, the learned model can still pass the entire verification set",1
" the 4 environment observations aid with control flow; qstacklo(pstacklo− 1) < qstackhi(pstackhi− 1) implements the lo < hi comparison in line 5 of algorithm 3, pstacklo == 1 checks if the stacks are empty in line 18 of algorithm 4, and the other observations (all involving ppivot or pj) deal with logic in the partition function",1
 we leave this as future work to devise a verification procedure more appropriate to this setting,0
"
thus far, to evaluate the efficacy of neural models on programming tasks, the only metric that has been used is generalization of expected behavior to inputs of greater complexity (vinyals   (2015), kaiser & sutskever (2015), reed & de freitas (2016), graves   (2016), zaremba   (2016))",1
 the first is the actual model architecture,0
"
we refer the reader to the appendix for the non-recursive trace generating functions",1
" et+1 ∼ fenv(et, pt, at)",1
 this modification also enables us to sort arrays containing arbitrary comparable elements,1
" the non-recursive and partially recursive versions are unable to sort long arrays, beyond length 8",0
this material is in part based upon work supported by the national science foundation under grant no,1
" in contrast, each call to the recursive add always runs for a fixed number of steps, even on arbitrarily long problems in f , so we can test that it performs correctly on a small, fixed number of step input sequences",0
" full recursion, in addition to making the aforementioned tail recursive call, adds two additional recursive calls; bstep and lshift are made tail recursive",0
 ,1
" as an application, we incorporate recursion into the neural programmer-interpreter architecture and consider four sample tasks: grade-school addition, bubble sort, topological sort, and quicksort",1
" both the non-recursive and recursive learned programs generalize on all input lengths we tried, up to 5000 digits",0
"
the training set for topological sort contains 6 traces, with one synthesized from a graph of size 5 and the rest synthesized from graphs of size 7",1
" plo and phi point to the lo and hi indices of the array, as in algorithm 3",1
 the maximum problem length in this training set is 3 (e,0
" a single element of an execution trace consists of a step input-step output pair, which can be synthesized from algorithm 1: this corresponds to, for a given time-step, the step input tuple (e, p, a) and step output tuple (r, p2, a2)",0
" for non-primitive subprograms, we can update the observation-to-observation mapping currently associated with the subprogram and then apply that mapping to the current set",0
"
we refer the reader to the appendix for the non-recursive trace generating functions",1
"
arg 2 (object to copy): env stack lo peek, env stack hi peek, phi, plo, ppivot − 1, ppivot + 1, reset
env stack lo peek and env stack hi peek represent qstacklo(pstacklo − 1) and qstackhi(pstackhi − 1), respectively",0
" formally, the npi is represented by the following set of equations:
st = fenc(et, at)
ht = flstm(st, pt, ht−1)
rt = fend(ht), pt+1 = fprog(ht), at+1 = farg(ht)
t is a subscript denoting the time-step; fenc is a domain-specific encoder (to be described later) that takes in the environment slice et and arguments at; flstm represents the core module, which takes in the state st generated by fenc, a program embedding pt ∈ rp , and hidden lstm state ht; fend decodes the return probability rt; fprog decodes a program key embedding pt+1;1 and farg decodes arguments at+1",0
" in fact, it is one of our future directions to explore new ways to train a recursive neural program without providing explicit training execution traces or with only partial or non-recursive traces",0
" in particular, given that a recursion is defined by two properties as mentioned before, the base cases and the set of reduction rules, we can prove a recursive neural program generalizes perfectly if we can prove that (1) it performs correctly on the base cases; (2) it learns the reduction rules correctly",1
" qcolor(dag[vactive][childlist[vactive]]) refers to the color of the next child of vactive, used in the evaluation of the condition in the if branch in line 9 of algorithm 2",1
" for non-primitive subprograms, we can update the observation-to-observation mapping currently associated with the subprogram and then apply that mapping to the current set",0
" full recursion, in addition to making the aforementioned tail recursive call, adds two additional recursive calls; bstep and lshift are made tail recursive",0
" even though the earlier version also generalized just as well in practice, relatively small differences in the formulation of the traces and the environment observations can drastically change the difficulty of verification",0
 this is due to the properties of the training set – we trained on 2 traces synthesized from arrays of length 7 and 1 trace synthesized from an array of length 6,1
 the model described in the training setup of section 4 (trained on 6 traces) was verified to be correct via the matching procedure described in section 4,1
"
thus far, to evaluate the efficacy of neural models on programming tasks, the only metric that has been used is generalization of expected behavior to inputs of greater complexity (vinyals   (2015), kaiser & sutskever (2015), reed & de freitas (2016), graves   (2016), zaremba   (2016))",1
 we use the lomuto partition scheme; the logic for the recursive trace is shown in algorithm 3,1
6,1
" this is possible if and only if the graph has no directed cycles; that is to say, it must be a directed acyclic graph (dag)",0
g,0
" the only model, to our knowledge, that does not train on input-output pairs is the neural programmer-interpreter (reed & de freitas, 2016), which trains on synthetic execution traces",0
" if a white vertex is found, set pstart to point to it; this signifies the start of a traversal of a new connected component",0
e,1
 we leave this as future work to devise a verification procedure more appropriate to this setting,0
"
for addition, we analytically determine the verification set",1
" in fact, it is one of our future directions to explore new ways to train a recursive neural program without providing explicit training execution traces or with only partial or non-recursive traces",0
"
topological sort",0
 the program terminates when seeing no numbers in the current column,1
" in particular, in the explore function, adding a tail recursive call resets and stores the hidden states associated with vertices in a stack-like fashion",0
" given verification sets that cover all the base cases and reduction rules, we can provide proofs that these learned programs generalize perfectly",0
"
move",1
" in contrast, each call to the recursive add always runs for a fixed number of steps, even on arbitrarily long problems in f , so we can test that it performs correctly on a small, fixed number of step input sequences",1
" it is straightforward to deal with all other operations, which do not induce partial environment states",0
"
in order to demonstrate that recursion can help learn and generalize better, we trained a non-recursive and recursive model on just a single execution trace generated from a graph containing 5 nodes3 for the topological sort task",1
" although the model is exposed to more and more data, it might learn spurious and overly complex representations of the program, as suggested in zaremba   (2016)",1
" for each task, given the verification set, we check the traces and results of the learned, to-be-verified neural program (described in section 4",1
" consequently, it is infeasible to construct a verification set that covers all cases, and the verification procedure we have described is inadequate",0
 the original version of the bubblesort implementation exposes the values within the array,0
"
by nature, recursion reduces the complexity of a problem to simpler instances",1
g,1
"
algorithm 2 depth first search topological sort 1: color all vertices white",0
 we instantiated recursion for the neural programmerinterpreter by changing the training traces,0
" hence, if all subproblems in s are added correctly, we have proven that any member of f will be added correctly, thus eliminating an infinite family of inputs that need to be tested",0
" formally, a function exhibits recursive behavior when it possesses two properties: (1) base cases— terminating scenarios that do not use recursion to produce answers; (2) a set of rules that reduces all other problems toward the base cases",1
"
an alternative way of representing the environment slice is to expose the values of the absolute vertices to the model; however, this makes it difficult to scale the model to larger graphs, since large vertex values are not seen during training time",1
"
we would also like to point out that in this paper, we provide as an example one way to train a recursive neural program, by providing a certain training execution trace to the npi model",0
" on the other hand, the non-recursive version has low accuracy, beginning from size 5, and fails completely for graphs of size 8 and beyond",0
"
in what follows, we describe the way in which we constructed npi training traces so as to make them contain recursive elements and thus enable npi to learn recursive programs",0
"2 at any given time, the npi has access to values pointed to by four pointers in each of the four rows, represented by q(1, i1), q(2, i2), q(3, i3), and q(4, i4)",0
"
it is sufficient to construct problems where every transition between two adjacent columns is covered",1
"
the training set for quicksort contains 4 traces, synthesized from arrays of length 5",0
" recursion is an important concept in programming languages and a critical tool to
reduce the complexity of programs",0
 the logic for the non-recursive trace is shown in algorithm 4 in the appendix,1
" training on the full recursive trace leads to perfect generalization, as shown in section 4",1
" table 1 presents results on randomly generated arrays of varying length for the learned non-recursive, partially recursive, and full recursive programs",0
" in future work, we would like to decrease supervision and construct models that are capable of coming up with recursive abstractions themselves",0
" m , as described in algorithm 1, processes the sequence of step inputs by using an lstm",0
" in the non-recursive trace, there are four functions that can be made recursive—toposort, check child, explore, and next start, and we add a tail recursive call to each of these functions in order to make the recursive trace",1
", pstart or childlist[vactive]) up or down none described belowwrite",0
" for instance, for recursive addition, consider the family f of addition problems anan−1 ",1
g,1
g,0
" using this modification, we constructed a verification set consisting of one array of size 10",0
 table 4 also demonstrates that the (verified) recursive program generalizes perfectly,0
"
the non-recursive trace loops on cycles of bubble and reset, which logically represents one bubble sweep through the array and reset of the two bubble pointers to the very beginning of the array, respectively",0
"
after finding v , we construct a set of problem inputs which, when executed on p , create exactly the step input sequences which make up v ",0
e,1
 our result shows that the training procedure and the npi architecture is capable of generalizing from the step input-output pairs seen in the training data to the unseen ones present in the verification set,0
 the program terminates when seeing no numbers in the current column,1
" for bubble sort, the domain-specific encoder is
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1), q(1, i2), i3 == length, at(1), at(2), at(3)]),
where the environment q ∈ r1×n×k is a scratch-pad that contains 1 row, to represent the state of the array as sorting proceeds in-place, andn columns",0
" here as a concrete and general example, we consider a general neural programming architecture (npa), similar to neural programmer-interpreter (npi) in reed & de freitas (2016)",0
", the trace corresponding to the array [3,2])",0
" the core lstm module’s hidden state is preserved over all these add1 calls, and it is difficult to interpret with certainty what happens over longer timesteps without concretely evaluating the lstm with an input of that length",0
"
the training data for the neural programmer-interpreter consists of full execution traces for the program of interest",0
" fa8750-15-2-0104, and berkeley deep drive",0
g,1
"
bubble sort",0
"
base cases and reduction rules for quicksort",0
" for example, for the addition task, the model is trained on short inputs and then tested on its ability to sum inputs with much longer numbers of digits",1
" in fact, we argue that recursion is an essential element for neural programs to generalize, and makes it tractable to prove the generalization of neural programs",0
" the contents ofqresult andqstack are not exposed directly through the domain-specific encoder; rather, we define primitive functions which manipulate these scratch-pads",0
" at any given time, the npi has access to the values referred to by two pointers, represented by q(1, i1) and q(1, i2),",0
"
as in line 13 of the right-hand side of figure 1",0
"
quicksort",1
" if it is too large, then the semantics of p might not be well-defined on some elements in v , or the spurious step input sequences may not be reachable from any valid problem input (e",0
 we found that changing the npi training traces is a simple way to enable this,0
" the non-recursive and partially recursive versions are unable to sort long arrays, beyond length 8",0
"
to perform the verification as described here, it is critical to construct v correctly",0
 figure 2 shows examples of traces for the different versions of bubble sort,0
" as this complexity interfered with determining the base cases and reductions, we changed the algorithm to its current form",1
" in all experiments, α is set to 0",0
 the sequence of primitive operations (move and write operations) for the non-recursive and recursive versions are exactly the same,0
"
an alternative way of representing the environment slice is to expose the values of the absolute vertices to the model; however, this makes it difficult to scale the model to larger graphs, since large vertex values are not seen during training time",0
"
formally, verification consists of proving the following theorem:
∀i ∈ v,m(i) ⇓ p (i) where i denotes a sequence of step inputs (within one function call), v denotes the set of valid sequences of step inputs, m denotes the neural network model, p denotes the correct program, and p (i) denotes the next step output from the correct program",0
"
as a concrete instantiation, we show in this paper that we can enable recursive neural programs in the npi model, and thus enable perfectly generalizable neural programs for tasks such as sorting where the original, non-recursive npi program fails",0
 stack push call2 pushes pivot + 1 and hi to qstacklo and qstackhi,0
" without using a recursive program, such a proof is not possible, because the non-recursive program runs on an arbitrarily long addition problem that creates correspondingly long sequences of step inputs; in the non-recursive formulation of addition, add calls add1 a number of times that is dependent on the length of the input",0
"
the training set for quicksort contains 4 traces, synthesized from arrays of length 5",1
" , n , where the dag contains n vertices",0
" 2: initialize lo and hi to be 1 and n, where n is the length of a",0
" in this pair, the step input runs a subprogram add1 that has no arguments, and the step output contains a program write that has arguments of out and 1",0
" we performed experiments on the partially recursive version in order to examine what happens when only one recursive call is implemented, when in reality three are required for perfect generalization",0
 ,0
" however, models that make use of this strategy eventually fail after a certain level of complexity (e",0
" as this complexity interfered with determining the base cases and reductions, we changed the algorithm to its current form",0
 add1 is a subprogram that adds the current column (writing the appropriate digit to the output row and carrying a bit to the next column if needed),0
 b1b0 where no carry operations occur,1
g,1
" in the non-recursive trace, there are four functions that can be made recursive—toposort, check child, explore, and next start, and we add a tail recursive call to each of these functions in order to make the recursive trace",1
" for brevity, we refer the reader to the appendix for information about the program set and non-recursive and recursive trace-generating functions for quicksort",0
" to make an analogy to mdps, this procedure is analogous to how value iteration obtains the correct value for each state starting from any initialization",1
" in the non-recursive trace, there are four functions that can be made recursive—toposort, check child, explore, and next start, and we add a tail recursive call to each of these functions in order to make the recursive trace",0
"
the operations we need to be concerned most about are carry and lshift, which induce partial environment states spanning two columns",0
"
grade school addition",1
"
we also report on generalization results for the non-recursive and recursive versions of this variant of bubble sort",0
" any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of national science foundation and darpa",0
" without using a recursive program, such a proof is not possible, because the non-recursive program runs on an arbitrarily long addition problem that creates correspondingly long sequences of step inputs; in the non-recursive formulation of addition, add calls add1 a number of times that is dependent on the length of the input",1
" for brevity, we refer the reader to the appendix for further details on the program set and non-recursive and recursive trace-generating functions used for topological sort",0
 we created a program set that reflects the semantics of algorithm 2,0
" the add reduction rule ensures that each call to add only covers two adjacent columns, and so the lstm only ever runs for a fixed number of steps necessary to process these two columns",0
" in computer science, recursion (as opposed to iteration) involves solving a larger problem by combining solutions to smaller instances of the same problem",0
 this is due to the properties of the training set – we trained on 2 traces synthesized from arrays of length 7 and 1 trace synthesized from an array of length 6,0
"6) on the verification set, and ensure they match the traces produced by the true program p ",0
 add1 is a subprogram that adds the current column (writing the appropriate digit to the output row and carrying a bit to the next column if needed),0
" if the program is primitive, the next environmental state et+1 will be affected by pt and at, i",0
" recall that this verification procedure cannot be performed for the non-recursive versions, since the propagation of the hidden state in the core lstm module makes reasoning difficult and so we would need to check an unbounded number of examples",1
" we hypothesize that the reason for this is that the neural network learns to spuriously depend on specific characteristics of the training examples that are irrelevant to the true program semantics, such as length of the training inputs, and thus fails to generalize to more complex inputs",1
" in the non-recursive trace, there are four functions that can be made recursive—toposort, check child, explore, and next start, and we add a tail recursive call to each of these functions in order to make the recursive trace",0
"as discussed in section 2, the neural programmer-interpreter (npi) is an instance of a neural programmer architecture and hence it naturally supports recursion",0
" in our experiments, we only present dag’s as inputs and represent the vertices as values ranging from 1, ",0
" although the model is exposed to more and more data, it might learn spurious and overly complex representations of the program, as suggested in zaremba   (2016)",0
"
an alternative method is to run p on many different program inputs and then observe step input sequences which occur, to create v ",1
" consequently, it is infeasible to construct a verification set that covers all cases, and the verification procedure we have described is inadequate",1
 add1 is a subprogram that adds the current column (writing the appropriate digit to the output row and carrying a bit to the next column if needed),0
"
it is important to emphasize that at inference time in the npi, the hidden state of the lstm controller is reset (to zero) at each subprogram call, as in line 3 of algorithm 1 (h ← 0)",0
" twc-1409915, darpa under grant no",1
"
algorithm 2 shows the topological sort task of interest",0
"we describe how models trained with recursive traces can be proven to generalize, by using the verification procedure described in section 3",1
" for each function we use to implement the recursive version of topological sort, we need to consider the set of possible environment observation sequences we can create from all valid inputs and test that the learned program produces the correct behavior on each of these inputs",0
" for each task, given the verification set, we check the traces and results of the learned, to-be-verified neural program (described in section 4",1
"
arg 2 (increment or decrement): up, down
swap",1
 figure 2 shows examples of traces for the different versions of bubble sort,0
"
we refer the reader to the appendix for the non-recursive trace generating functions",0
" more specifically, when m is trained on simpler inputs, such as inputs of a small length, the generalization metric evaluates how well m will do on more complex inputs, such as inputs of much longer length",0
" full recursion, in addition to making the aforementioned tail recursive call, adds two additional recursive calls; bstep and lshift are made tail recursive",1
 the sequence of primitive operations (move and write operations) for the non-recursive and recursive versions are exactly the same,1
"
under the assumption that there are no leading 0’s (except in the case of single digits) and the two numbers to be added have the same number of digits, the verification set for addition contains 20,181 input problems",0
3,1
 this makes it difficult to reason about what the model will do when given complex inputs,1
" in fact, it is one of our future directions to explore new ways to train a recursive neural program without providing explicit training execution traces or with only partial or non-recursive traces",1
" the learned recursive program is different from neural programs learned in all previous work in an important aspect: previous approaches do not explicitly incorporate this abstraction, and hence generalize poorly, whereas our learned neural programs incorporate recursion and achieve perfect generalization",0
" we note that an earlier version of quicksort we tried lacked primitive operations to directly move a pointer to another, and therefore needed more functions and observations",0
" this functionality the original npi paper decodes to a program key embedding kt ∈ rk and then computes a program embedding pt+1, which we also did in our implementation, but we omit this for brevity",1
" the swap operation has the following arguments:
arg 1 (swap object 1): ppivot
arg 2 (swap object 2): phi, pj
write",1
" for instance, for recursive addition, consider the family f of addition problems anan−1 ",1
" we performed experiments on the partially recursive version in order to examine what happens when only one recursive call is implemented, when in reality three are required for perfect generalization",0
"
thus far, to evaluate the efficacy of neural models on programming tasks, the only metric that has been used is generalization of expected behavior to inputs of greater complexity (vinyals   (2015), kaiser & sutskever (2015), reed & de freitas (2016), graves   (2016), zaremba   (2016))",0
 a1a0 + bnbn−1 ,0
" the move operation has the following arguments:
arg 1 (pointer): presult, pstack, pstart, childlist[vactive], childlist[vsave]
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 5 values",1
" in this pair, the step input runs a subprogram add1 that has no arguments, and the step output contains a program write that has arguments of out and 1",0
" the space of possible programs is extremely large, and composing a program that performs robustly on the infinite space of possible inputs is difficult—in part because it is impractical to obtain enough training examples to easily disambiguate amongst all possible programs",1
" we further have qresult ∈ nu , a scratch-pad which contains the sorted list of vertices at the end of execution, and qstack ∈ nu , which serves the role of the stack s in algorithm 2",0
" the pointer at index i3 represents a counter internal to the environment that is incremented once after each pass of the algorithm (one cycle of bubble and reset); when incremented a number of times equal to the length of the array, the flag i3 == length becomes true and terminates the entire algorithm ",0
 this paper is the first (to our knowledge) to investigate the important problem of provable generalization properties of neural programs,1
we emphasize that the notion of a neural recursive program has not been presented in the literature before: this is our main contribution,0
", to change the value of phi) none described belowstack",1
" our implementation uses two stacks qstacklo and
qstackhi, each in ru , that store the arguments to the recursive quicksort calls in algorithm 3; before each recursive call, the appropriate arguments are popped off the stack and written to plo and phi",1
" observe that partially recursive does slightly better than non-recursive for the setting in which the length of the array is 3, and that the fully recursive version is able to sort every array given to it",1
" we performed experiments on the partially recursive version in order to examine what happens when only one recursive call is implemented, when in reality three are required for perfect generalization",1
"
as in line 13 of the right-hand side of figure 1",1
" if it is too small, then execution of the program on some input might require evaluation of m(i) on some i /∈ v , and so the behavior ofm(i) might deviate from p (i)",0
" the verification set can be very small, as we found a 10-element array ([8,2,1,2,0,8,5,8,3,7]) is sufficient to cover all of v ",0
" our implementation uses two stacks qstacklo and
qstackhi, each in ru , that store the arguments to the recursive quicksort calls in algorithm 3; before each recursive call, the appropriate arguments are popped off the stack and written to plo and phi",0
" for example, for the addition task, the model is trained on short inputs and then tested on its ability to sum inputs with much longer numbers of digits",0
 the first is the actual model architecture,0
" the pointer at index i3 represents a counter internal to the environment that is incremented once after each pass of the algorithm (one cycle of bubble and reset); when incremented a number of times equal to the length of the array, the flag i3 == length becomes true and terminates the entire algorithm ",1
" plo and phi point to the lo and hi indices of the array, as in algorithm 3",0
" for tasks other than addition, it is difficult to analytically determine the verification set, so instead, we randomly generate input candidates until they completely cover the base cases and reduction rules",0
"
note that the training set can often be considerably smaller than the verification set, and despite this, the learned model can still pass the entire verification set",0
" we can obtain this mapping by first considering the possible observations that can arise at the beginning of the entry function (add, bubblesort, toposort, and quicksort) for some valid program input, and iteratively applying the observation-to-observation mapping implied by the reference implementation’s step output at that point in the execution",1
"
in this paper, we propose that recursion is an important concept for neural programs as well",0
" an example of part of an addition task trace, written in shorthand, is given in figure 1",0
" however, the authors of npi did not consider explicitly the notion of recursion and as a consequence, did not learn recursive programs",1
 stack push call2 pushes pivot + 1 and hi to qstacklo and qstackhi,0
", pstart or childlist[vactive]) up or down none described belowwrite",0
"
for topological sort, the domain-specific encoder is
fenc(dag,qcolor, pstack, pstart, vactive, childlist, at)
=mlp ([qcolor(pstart), qcolor(dag[vactive][childlist[vactive]]), pstack == 1, at(1), at(2), at(3)]),
where qcolor ∈ ru×4 is a scratch-pad that contains u rows, each containing one of four colors (white, gray, black, invalid) with one-hot encoding",0
", the verification set",0
"
in this general neural programming architecture, we show it is easy to support recursion",0
 we find that recursion makes it easier for the network to learn the right program and generalize to unknown situations,0
" while this matches the description from reed & de freitas (2016), we found that this causes an unnecessary blowup in the size of v and makes it much more difficult to construct the verification set",0
 u varies with the number of vertices in the graph,1
" in contrast, each call to the recursive add always runs for a fixed number of steps, even on arbitrarily long problems in f , so we can test that it performs correctly on a small, fixed number of step input sequences",0
" for example, for the addition task, the model is trained on short inputs and then tested on its ability to sum inputs with much longer numbers of digits",1
6,0
" each step during an execution of the program does one of three things: (1) another subprogram along with associated arguments is called, as in line 10, (2) the program writes to the environment if it is primitive, as in line 8, or (3) the loop is terminated if the return probability exceeds a threshold α, after which the stack frame is popped and control is returned to the caller",0
"
grade school addition",0
" provably perfect generalization implies the model will behave correctly, given any valid input",0
"
algorithm 3 recursive quicksort 1: initialize an array a to sort",0
" in the non-recursive trace, there are four functions that can be made recursive—toposort, check child, explore, and next start, and we add a tail recursive call to each of these functions in order to make the recursive trace",1
g,1
" we further have qresult ∈ nu , a scratch-pad which contains the sorted list of vertices at the end of execution, and qstack ∈ nu , which serves the role of the stack s in algorithm 2",1
" our results show that for all learned, to-be-verified neural programs, they all produced the same traces as those produced by p on the verification set",0
" consequently, it is infeasible to construct a verification set that covers all cases, and the verification procedure we have described is inadequate",0
 u varies with the number of vertices in the graph,1
"
topological sort",1
3 to construct v and then empirically create a verification set which covers v ,1
" 9: else 10: function run(e, p2, a2)
a description of the inference procedure is given in algorithm 1",1
" we can obtain this mapping by first considering the possible observations that can arise at the beginning of the entry function (add, bubblesort, toposort, and quicksort) for some valid program input, and iteratively applying the observation-to-observation mapping implied by the reference implementation’s step output at that point in the execution",0
" this is possible if and only if the graph has no directed cycles; that is to say, it must be a directed acyclic graph (dag)",1
" lshift, used in reset, shifts the pointers left until reaching the start token",0
 pstacklo and pstackhi point to the top (empty) positions in qstacklo and qstackhi,0
 we then construct a verification set of size 73 by ensuring that randomly generated graphs cover the analytically derived v ,0
we now report on generalization for the varying tasks,1
"
in this paper, we propose to resolve these issues by explicitly incorporating recursion into neural architectures",1
"
by reasoning about the possible set of environment observations created by all valid inputs, we construct v using the procedure described in section 3",0
 ,0
 we leave this as future work to devise a verification procedure more appropriate to this setting,1
 we use the lomuto partition scheme; the logic for the recursive trace is shown in algorithm 3,0
" hence, if all subproblems in s are added correctly, we have proven that any member of f will be added correctly, thus eliminating an infinite family of inputs that need to be tested",0
", to color a vertex) or variable (e",1
g,0
"
we propose and describe our verification procedure",1
" empirically, we observe that the learned recursive programs solve all valid inputs with 100% accuracy after training on a very small number of examples, out-performing previous generalization results",0
" as an application, we incorporate recursion into the neural programmer-interpreter architecture and consider four sample tasks: grade-school addition, bubble sort, topological sort, and quicksort",1
" if no white vertex is found, the entire execution is terminated move none write write a value either to environment (e",1
" we can obtain this mapping by first considering the possible observations that can arise at the beginning of the entry function (add, bubblesort, toposort, and quicksort) for some valid program input, and iteratively applying the observation-to-observation mapping implied by the reference implementation’s step output at that point in the execution",1
" via the recursive call, we effectively forget that the column just added exists, since the recursive call to add starts with a new hidden state for the lstm controller",0
"
grade school addition",1
"
an alternative way of representing the environment slice is to expose the values of the absolute vertices to the model; however, this makes it difficult to scale the model to larger graphs, since large vertex values are not seen during training time",0
" to make an analogy to mdps, this procedure is analogous to how value iteration obtains the correct value for each state starting from any initialization",1
" the swap operation has the following arguments:
arg 1 (swap object 1): ppivot
arg 2 (swap object 2): phi, pj
write",0
 we leave this as future work to devise a verification procedure more appropriate to this setting,0
", to change the value of phi) none described belowstack",0
" the arrow in the theorem refers to evaluation, as in big-step semantics",1
"
topological sort",0
 table 4 demonstrates that the accuracy of the non-recursive program degrades sharply when moving from arrays of length 7 to arrays of length 8,0
 table 4 also demonstrates that the (verified) recursive program generalizes perfectly,0
" via the recursive call, we effectively forget that the column just added exists, since the recursive call to add starts with a new hidden state for the lstm controller",1
"
figure 1 shows examples of non-recursive and recursive addition traces",1
 18: push lo and p− 1 to slo and shi,0
" the core module of the npi is an lstm controller that takes as input a slice of the current external environment, via a set of pointers, and a program and arguments to execute",0
" given verification sets that cover all the base cases and reduction rules, we can provide proofs that these learned programs generalize perfectly",1
"
formally, verification consists of proving the following theorem:
∀i ∈ v,m(i) ⇓ p (i) where i denotes a sequence of step inputs (within one function call), v denotes the set of valid sequences of step inputs, m denotes the neural network model, p denotes the correct program, and p (i) denotes the next step output from the correct program",0
"
note that v should also contain the necessary reductions, which corresponds to making the recursive calls at the correct time, as indicated by p ",1
" the write operation has the following arguments:
arg 1 (object to write): env stack lo, env stack hi, phi, plo
env stack lo and env stack hi represent qstacklo(pstacklo) and qstackhi(pstackhi), respectively",0
" this makes it so that we only need to consider the vertices in the subgraph that are currently relevant for computing the sort, allowing simpler reasoning about behavior for large graphs",0
"
we also report on generalization results for the non-recursive and recursive versions of this variant of bubble sort",0
 we find that recursion makes it easier for the network to learn the right program and generalize to unknown situations,0
"
the training set for quicksort contains 4 traces, synthesized from arrays of length 5",0
"in this paper, we propose that the key abstraction of recursion is necessary for neural programs to generalize",0
3 to construct v and then empirically create a verification set which covers v ,0
 recursion enables provably perfect generalization,0
" bstep, used in bubble, compares pairs of numbers, continuously moving the bubble pointers once to the right each time until reaching the end of the array",1
"
figure 1 shows examples of non-recursive and recursive addition traces",0
 we find that recursion makes it easier for the network to learn the right program and generalize to unknown situations,0
" thus, we demonstrate that recursion enables provably perfect generalization for different tasks, including addition, topological sort, quicksort, and a variant of bubble sort",0
"
arg 2 (auxiliary variable): color grey, color black
color grey and color black color the given vertex grey and black, respectively",0
" our results show that for all learned, to-be-verified neural programs, they all produced the same traces as those produced by p on the verification set",0
"2 at any given time, the npi has access to values pointed to by four pointers in each of the four rows, represented by q(1, i1), q(2, i2), q(3, i3), and q(4, i4)",0
" our results show that for all learned, to-be-verified neural programs, they all produced the same traces as those produced by p on the verification set",1
 the environment and return probability are omitted for readability,0
 b1b0 where no carry operations occur,0
 the maximum problem length in this training set is 3 (e,1
 u varies with the number of vertices in the graph,1
" when the program is called, the current context including the caller’s memory state is stored on a stack; when the program returns, the stored context is popped off the stack to resume execution in the previous caller’s context",0
" although the model is exposed to more and more data, it might learn spurious and overly complex representations of the program, as suggested in zaremba   (2016)",0
" we call this set of inputs the verification set, sv ",1
g,1
" observe that partially recursive does slightly better than non-recursive for the setting in which the length of the array is 3, and that the fully recursive version is able to sort every array given to it",0
 npi then outputs the return probability and next program and arguments to execute,0
" k is set to 11, to denote the range of possible numbers (0 through 9), along with the start/end token (represented with the same encoding) which is observed when a pointer reaches beyond the bounds of the input",1
" in this version, there is a dependence on length: bstep and lshift are called a number of times equivalent to one less than the length of the input array, in bubble and reset respectively",0
" in fact, we argue that recursion is an essential element for neural programs to generalize, and makes it tractable to prove the generalization of neural programs",1
"1; and for bubble sort, appendix a",1
" naı̈vely, we might expect to synthesize and test an input for any sequence created by combining the four possible colors in two variables and another boolean variable for whether the stack is empty (so 32 possible observations at any point), but for various reasons, most of these combinations are impossible to occur at any given point in the execution trace",0
 table 4 also demonstrates that the (verified) recursive program generalizes perfectly,0
g,0
" for each graph size, we test the learned programs on 30 randomly generated dags",1
" for each graph size, we test the learned programs on 30 randomly generated dags",0
" in the non-recursive trace, there are four functions that can be made recursive—toposort, check child, explore, and next start, and we add a tail recursive call to each of these functions in order to make the recursive trace",0
" the write operation has the following arguments:
arg 1 (main action): color curr, color next, active start, active neighb, active stack, save, stack push, stack pop, result
color curr colors vactive, color next colors vertex dag[vactive][childlist[vactive]], active start writes pstart to vactive, active neighb writes dag[vactive][childlist[vactive]] to vactive, active stack writes qstack(pstack) to vactive, save writes vactive to vsave, stack push pushes vactive to the top of the stack, stack pop writes a null value to the top of the stack, and result writes vactive to qresult(presult)",1
" for tasks other than addition, it is difficult to analytically determine the verification set, so instead, we randomly generate input candidates until they completely cover the base cases and reduction rules",0
" that is to say, the network does not learn the true program semantics",1
" there are 3 pointers (presult, pstack, pstart), presult points to the next empty location in qresult,
pstack points to the top of the stack in qstack, and pstart points to the candidate starting node for a connected component",1
" the learned recursive program is different from neural programs learned in all previous work in an important aspect: previous approaches do not explicitly incorporate this abstraction, and hence generalize poorly, whereas our learned neural programs incorporate recursion and achieve perfect generalization",0
" the second is the training procedure, which consists of the form of the training data and the optimization process",0
" in this version of curriculum learning, even though the inputs are gradually becoming more complex, the semantics of the program is succinct and does not change",0
" we hypothesize that the reason for this is that the neural network learns to spuriously depend on specific characteristics of the training examples that are irrelevant to the true program semantics, such as length of the training inputs, and thus fails to generalize to more complex inputs",0
 reset represents a −∞ value,0
"
formally, verification consists of proving the following theorem:
∀i ∈ v,m(i) ⇓ p (i) where i denotes a sequence of step inputs (within one function call), v denotes the set of valid sequences of step inputs, m denotes the neural network model, p denotes the correct program, and p (i) denotes the next step output from the correct program",0
"program descriptions calls arguments toposort perform topological
sort on graph traverse, next start, write, move
none
traverse traverse graph until stack is empty check child, explore none check child check if a white child exists; if so, set childlist[vactive] to point to it move none explore repeatedly traverse subgraphs until stack is empty stack, check child, write, move none stack interact with stack, either pushing or popping write, move push, pop next start move pstart until reaching a white vertex",0
" ppivot and pj point to the pivot and j indices of the array, used in the partition function in algorithm 3",1
" for bubble sort, the domain-specific encoder is
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1), q(1, i2), i3 == length, at(1), at(2), at(3)]),
where the environment q ∈ r1×n×k is a scratch-pad that contains 1 row, to represent the state of the array as sorting proceeds in-place, andn columns",0
" we can obtain this mapping by first considering the possible observations that can arise at the beginning of the entry function (add, bubblesort, toposort, and quicksort) for some valid program input, and iteratively applying the observation-to-observation mapping implied by the reference implementation’s step output at that point in the execution",1
g,1
 we use the lomuto partition scheme; the logic for the recursive trace is shown in algorithm 3,0
" in this architecture, we consider a core controller, e",1
" we hypothesize that the reason for this is that the neural network learns to spuriously depend on specific characteristics of the training examples that are irrelevant to the true program semantics, such as length of the training inputs, and thus fails to generalize to more complex inputs",0
 the environment and return probability are omitted for readability,0
" in practice, we can additionally use tail recursion optimization to avoid problems with the call stack growing too deep",0
" nevertheless, we would like the model to quickly learn to represent the right semantics of the underlying program from a small number of training examples, not an exhaustive number of them",0
"
there are 6 pointers (plo, phi, pstacklo, pstackhi, ppivot, pj)",0
"
the three environment observations aid with control flow in algorithm 2",0
" any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of national science foundation and darpa",0
 this paper is the first (to our knowledge) to investigate the important problem of provable generalization properties of neural programs,0
" in particular, in the explore function, adding a tail recursive call resets and stores the hidden states associated with vertices in a stack-like fashion",0
" the space of possible programs is extremely large, and composing a program that performs robustly on the infinite space of possible inputs is difficult—in part because it is impractical to obtain enough training examples to easily disambiguate amongst all possible programs",0
"
is critical for implementing recursion, since it permits us to restrict our attention to the currently relevant recursive call, ignoring irrelevant details about other contexts",1
 our experiments use a small number of training examples,0
" in particular, this condition is satisfied by npi, and thus the npi model naturally supports recursion (even though the authors of npi did not consider this aspect explicitly)",1
" a single element of an execution trace consists of a step input-step output pair, which can be synthesized from algorithm 1: this corresponds to, for a given time-step, the step input tuple (e, p, a) and step output tuple (r, p2, a2)",0
" each time a subprogram is called, the stack depth increases",1
 indentation indicates the stack is one level deeper than before,1
"
in order to demonstrate that recursion can help learn and generalize better, for addition, we trained only on traces for 5 arbitrarily chosen 1-digit addition sum examples",0
" to the best of our knowledge, this is the first time verification has been applied to a neural program, providing provable guarantees about its behavior",0
"
the training set for bubble sort contains 100 traces, with maximum problem length of 2 (e",0
" in practice, we can additionally use tail recursion optimization to avoid problems with the call stack growing too deep",1
" twc-1409915, darpa under grant no",0
" while this matches the description from reed & de freitas (2016), we found that this causes an unnecessary blowup in the size of v and makes it much more difficult to construct the verification set",0
" the second is the training procedure, which consists of the form of the training data and the optimization process",0
 we find that recursion makes it easier for the network to learn the right program and generalize to unknown situations,0
" the stack operation has the following arguments:
arg 1 (operation): stack push call1, stack push call2, stack pop
stack push call1 pushes lo and pivot−1 toqstacklo andqstackhi",0
 b1b0 where no carry operations occur,1
 there is a (changing) list of neural programs used to accomplish a given task,0
" this is highly undesirable, since being able to provide the correct answer in all possible settings is one of the most important aspects of any learned neural program",1
" also, compared to the bubble pointers in bubble sort, the pointers that perform the comparison for quicksort (the compswap function) are usually not adjacent to each other, making quicksort less local than bubble sort",0
"
we propose and describe our verification procedure",0
" for each length, we test each program on 30 randomly generated problems",0
"in this paper, we propose that the key abstraction of recursion is necessary for neural programs to generalize",0
"
the training set for bubble sort contains 100 traces, with maximum problem length of 2 (e",1
" empirically, existing models suffer from a common limitation—generalization becomes poor beyond a threshold level of complexity",1
", pstart or childlist[vactive]) up or down none described belowwrite",0
" qcolor(dag[vactive][childlist[vactive]]) refers to the color of the next child of vactive, used in the evaluation of the condition in the if branch in line 9 of algorithm 2",1
"
for addition, we analytically determine the verification set",0
g,1
" both the non-recursive and recursive learned programs generalize on all graphs we tried, up to 120 vertices",0
"
to evaluate a neural network that learns a neural program to accomplish a certain task, one common evaluation metric is how well the learned model m generalizes",1
" if the step output specifies a primitive function call, we need to reason about how it can affect the environment so as to change the observation in the next step input",0
" for each task, given the verification set, we check the traces and results of the learned, to-be-verified neural program (described in section 4",0
" for non-primitive subprograms, we can update the observation-to-observation mapping currently associated with the subprogram and then apply that mapping to the current set",0
" in practice, we can additionally use tail recursion optimization to avoid problems with the call stack growing too deep",1
 table 4 demonstrates that the accuracy of the non-recursive program degrades sharply when moving from arrays of length 7 to arrays of length 8,1
" it is straightforward to deal with all other operations, which do not induce partial environment states",0
 2: initialize an empty stack s and a directed acyclic graph dag to traverse,0
" qcolor(pstart) contains the color of the current start vertex, used in the evaluation of the condition in the while loop in line 5 of algorithm 2",0
" for non-primitive subprograms, we can update the observation-to-observation mapping currently associated with the subprogram and then apply that mapping to the current set",1
" as with the original npi architecture, the experiments for this paper always used a 3-tuple of integers at = (at(1), at(2), at(3))",0
g,0
"
an alternative method is to run p on many different program inputs and then observe step input sequences which occur, to create v ",0
 table 4 demonstrates that the accuracy of the non-recursive program degrades sharply when moving from arrays of length 7 to arrays of length 8,0
" however, to be sure that the generated v is complete (covers all the cases needed), we need to check all pairs of observations seen in adjacent step inputs (in particular, those before and after a primitive function call), in a similar way as if we were constructing v from scratch",1
" finally, we implement recursive traces for our own topological sort and quicksort tasks",1
" in particular, given that a recursion is defined by two properties as mentioned before, the base cases and the set of reduction rules, we can prove a recursive neural program generalizes perfectly if we can prove that (1) it performs correctly on the base cases; (2) it learns the reduction rules correctly",1
 npi then outputs the return probability and next program and arguments to execute,1
"
we also report on generalization results for the non-recursive and recursive versions of this variant of bubble sort",0
" the general notion of recursion has been an important concept in many domains, including mathematics and computer science",0
" consequently, there is no concept of length relevant to the problem, which has traditionally been an important focus of length-based curriculum learning",0
" the arrow in the theorem refers to evaluation, as in big-step semantics",1
" in our experiments, we only present dag’s as inputs and represent the vertices as values ranging from 1, ",1
" the write operation has the following arguments:
arg 1 (object to write): env stack lo, env stack hi, phi, plo
env stack lo and env stack hi represent qstacklo(pstacklo) and qstackhi(pstackhi), respectively",0
" any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of national science foundation and darpa",0
"
given a verification set, we can then run the model on the verification set to check if the produced traces and results are correct",1
" the pointer at index i3 represents a counter internal to the environment that is incremented once after each pass of the algorithm (one cycle of bubble and reset); when incremented a number of times equal to the length of the array, the flag i3 == length becomes true and terminates the entire algorithm ",0
" the core module of the npi is an lstm controller that takes as input a slice of the current external environment, via a set of pointers, and a program and arguments to execute",0
 this modification also enables us to sort arrays containing arbitrary comparable elements,1
", an array for quicksort or a dag for topological sort)",0
" this agrees with the generalization of non-recursive addition in reed & de freitas (2016), where they reported generalization up to 3000 digits",0
" lshift, used in reset, shifts the pointers left until reaching the start token",0
"
through careful reasoning about the possible set of environment observations created by all valid inputs, and how each of the operations in the execution trace affects the environment, we can construct v using the procedure described in section 3",1
"1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 while (pstacklo 6= 1): 6 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 7 stack(stack_pop) 8 else: 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 stack(stack_pop) 12 partition() 13 stack(stack_push_call2) 14 stack(stack_push_call1) 15 } 16 17 partition() { 18 set_pivot_lo() 19 set_j_lo() 20 compswap_loop() 21 swap(ppivot, phi) 22 set_j_null() 23 } 24 25 compswap_loop() { 26 while (pj 6= phi): 27 compswap() 28 move(pj, up) 29 } 30 31 compswap() { 32 if (a[pj ] ≤ a[phi]): 33 swap(ppivot, pj) 34 move(ppivot, up) 35 } 36 37 stack(op) { 38 if (op == stack_push_call1): 39 write(env_stack_lo, plo) 40 write(env_stack_hi, ppivot − 1) 41 move(pstacklo, up) 42 move(pstackhi, up) 43 44 if (op == stack_push_call2): 45 write(env_stack_lo, ppivot + 1) 46 write(env_stack_hi, phi) 47 move(pstacklo, up) 48 move(pstackhi, up) 49 50 if (op == stack_pop): 51 write(env_stack_lo, reset) 52 write(env_stack_hi, reset) 53 move(pstacklo, down) 54 move(pstackhi, down) 55 }altered recursive functions 1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 6 partition() 7 stack(stack_push_call2) 8 stack(stack_push_call1) 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 quicksort() // recursive call 12 stack(stack_pop) 13 write(phi, env_stack_hi_peek) 14 write(plo, env_stack_lo_peek) 15 quicksort() // recursive call 16 stack(stack_pop) 17 } 18 19 compswap_loop() { 20 if (pj 6= phi): 21 compswap() 22 move(pj, up) 23 compswap_loop() // recursive call 24 }in this section, we describe the space of base cases and reduction rules that must be covered for each of the four sample tasks, in order to create the verification set",1
" for each function we use to implement the recursive version of topological sort, we need to consider the set of possible environment observation sequences we can create from all valid inputs and test that the learned program produces the correct behavior on each of these inputs",1
" as with the original npi architecture, the experiments for this paper always used a 3-tuple of integers at = (at(1), at(2), at(3))",1
"when constructing a neural network for the purpose of learning a program, there are two orthogonal aspects to consider",1
"
there are 6 pointers (plo, phi, pstacklo, pstackhi, ppivot, pj)",0
" note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values, and arg 2 can only take one of 7 values",0
" as described in the verification procedure, it is possible to prove our learned recursive program generalizes perfectly by testing on an appropriate set of problem inputs, i",0
" in future work, we would like to decrease supervision and construct models that are capable of coming up with recursive abstractions themselves",0
 ,0
 ,1
 we instantiated recursion for the neural programmerinterpreter by changing the training traces,0
 indentation indicates the stack is one level deeper than before,1
"when constructing a neural network for the purpose of learning a program, there are two orthogonal aspects to consider",1
" table 1 presents results on randomly generated arrays of varying length for the learned non-recursive, partially recursive, and full recursive programs",0
" qcolor(pstart) contains the color of the current start vertex, used in the evaluation of the condition in the while loop in line 5 of algorithm 2",0
" for these programs, we re-use the appropriate program sets (the associated subprograms), and we refer the reader to the appendix of reed & de freitas (2016) for further details on the subprograms used in addition and bubble sort",0
" in particular, this condition is satisfied by npi, and thus the npi model naturally supports recursion (even though the authors of npi did not consider this aspect explicitly)",0
" in future work, we seek to enable more tasks with recursive structure",0
"
bubble sort",0
", pstart or childlist[vactive]) up or down none described belowwrite",1
"
arg 2 (object to copy): env stack lo peek, env stack hi peek, phi, plo, ppivot − 1, ppivot + 1, reset
env stack lo peek and env stack hi peek represent qstacklo(pstacklo − 1) and qstackhi(pstackhi − 1), respectively",1
"
by reasoning about the possible set of environment observations created by all valid inputs, we construct v using the procedure described in section 3",0
" 9: else 10: function run(e, p2, a2)
a description of the inference procedure is given in algorithm 1",1
" for example, for the addition task, the model is trained on short inputs and then tested on its ability to sum inputs with much longer numbers of digits",1
 this modification also enables us to sort arrays containing arbitrary comparable elements,1
" provably perfect generalization implies the model will behave correctly, given any valid input",1
 the memory updates of these neural programs are so complex and interdependent that it is difficult to reason about the behaviors of the learned neural program under previously unseen situations (such as problems with longer inputs),0
 ,1
" ppivot and pj point to the pivot and j indices of the array, used in the partition function in algorithm 3",1
" as with the others, we apply the procedure described in section 3",1
" in computer science, recursion (as opposed to iteration) involves solving a larger problem by combining solutions to smaller instances of the same problem",0
" for many problems, the base cases and reduction rules usually consist of a finite (often small) number of cases",0
 table 3 presents results on randomly generated arrays of varying length for the learned non-recursive and recursive programs,0
"
we construct input problems by splitting into two cases: one case in which the left column contains a null value and another in which the left column does not contain any null values",0
"
we construct input problems by splitting into two cases: one case in which the left column contains a null value and another in which the left column does not contain any null values",0
" we make the trace recursive by adding a tail recursive call into the trace for the add program after calling add1 and lshift, the original paper uses k = 10, but we found it necessary to augment the range with an end token, in order to terminate properly",0
" m , as described in algorithm 1, processes the sequence of step inputs by using an lstm",0
" in order to compensate for this, ppivot and pj require special functions (move pivot lo and move j lo) to properly set them to lo in lines 11 and 12 of the partition function in algorithm 3",0
"
as a concrete instantiation, we show in this paper that we can enable recursive neural programs in the npi model, and thus enable perfectly generalizable neural programs for tasks such as sorting where the original, non-recursive npi program fails",1
 reset represents a −∞ value,1
" 3: 4: function quicksort(a, lo, hi) 5: if lo < hi: then 6: p = partition(a, lo, hi) 7: quicksort(a, lo, p− 1) 8: quicksort(a, p+ 1, hi) 9: 10: function partition(a, lo, hi) 11: pivot = lo 12: for j ∈ [lo, hi− 1] : do 13: if a[j] ≤ a[hi] then 14: swap a[pivot] with a[j] 15: pivot = pivot+ 1 16: swap a[pivot] with a[hi] 17: return pivot
for quicksort, the domain-specific encoder is
fenc(qarray, qstacklo, qstackhi, plo, phi, pstacklo, pstackhi, ppivot, pj , at) =
mlp ([qarray(pj) ≤ qarray(phi), pj == phi, qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1), pstacklo == 1, at(1), at(2), at(3)]),
whereqarray ∈ ru×11 is a scratch-pad that contains u rows, each containing one of 11 values (one of the numbers 0 through 9 or an invalid state)",1
" et+1 ∼ fenv(et, pt, at)",1
" provably perfect generalization implies the model will behave correctly, given any valid input",1
" we also hope to decrease supervision, for example by training with only partial or non-recursive traces, and to develop novel neural programming architectures integrated directly with a notion of recursion",1
 we instantiated recursion for the neural programmerinterpreter by changing the training traces,0
" if it is too small, then execution of the program on some input might require evaluation of m(i) on some i /∈ v , and so the behavior ofm(i) might deviate from p (i)",1
 the memory updates of these neural programs are so complex and interdependent that it is difficult to reason about the behaviors of the learned neural program under previously unseen situations (such as problems with longer inputs),1
"
the same set of problems was used to generate the training traces for all formulations of the task, for non-recursive and recursive versions",0
" the second is the training procedure, which consists of the form of the training data and the optimization process",1
" the arrow in the theorem refers to evaluation, as in big-step semantics",0
"
since npi naturally supports the notion of recursion, a key question is how to enable npi to learn recursive programs",0
" our results show that for all learned, to-be-verified neural programs, they all produced the same traces as those produced by p on the verification set",1
", to change the value of vactive) none described below move move a pointer (e",1
" while this matches the description from reed & de freitas (2016), we found that this causes an unnecessary blowup in the size of v and makes it much more difficult to construct the verification set",0
 the program terminates when seeing no numbers in the current column,1
" the move operation has the following arguments:
arg 1 (pointer): presult, pstack, pstart, childlist[vactive], childlist[vsave]
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 5 values",1
" this guarantees that the step input sequences considered during verification contain all step input sequences which arise during execution of an unseen problem in f , leading to generalization to any problem in f ",1
 figure 2 shows examples of traces for the different versions of bubble sort,1
"
in this paper, we propose that recursion is an important concept for neural programs as well",1
"
in this paper, we propose that recursion is an important concept for neural programs as well",0
" note that recursion helps make this process tractable, because we only need to test a finite number of inputs to show that the model will work correctly on inputs of unbounded complexity",0
"1, we hypothesize the non-recursive programs do not generalize well because they have learned spurious dependencies specific to the training set, such as length of the input problems",0
"
quicksort",1
" thus, any general neural programming architecture supporting such a call structure can be made to support recursion",1
" in contrast, each call to the recursive add always runs for a fixed number of steps, even on arbitrarily long problems in f , so we can test that it performs correctly on a small, fixed number of step input sequences",1
"
the training set for topological sort contains 6 traces, with one synthesized from a graph of size 5 and the rest synthesized from graphs of size 7",0
"
the training data for the neural programmer-interpreter consists of full execution traces for the program of interest",0
"
quicksort",0
" we call this set of inputs the verification set, sv ",1
"
the training set for topological sort contains 6 traces, with one synthesized from a graph of size 5 and the rest synthesized from graphs of size 7",0
" the pointers at index i1 and i2 are used to compare the pair of numbers considered during the bubble sweep, swapping them if the number at i1 is greater than that in i2",0
" the contents ofqresult andqstack are not exposed directly through the domain-specific encoder; rather, we define primitive functions which manipulate these scratch-pads",1
" , a1a0 + b1b0} are added properly",1
 almost all architectures train on program input/output pairs,1
" the general notion of recursion has been an important concept in many domains, including mathematics and computer science",1
 u varies with the number of vertices in the graph,0
" the architecture usually possesses some form of memory, which could be internal (such as the hidden state of a recurrent neural network) or external (such as a discrete “scratch pad” or a memory block with differentiable access)",0
"
training setup",0
this material is in part based upon work supported by the national science foundation under grant no,0
" the general notion of recursion has been an important concept in many domains, including mathematics and computer science",1
 we created a program set that reflects the semantics of algorithm 2,1
"
recursion drastically reduces the number of configurations we need to consider during the verification phase and makes the proof tractable, because it introduces structure that eliminates infinitely long sequences of step inputs that would otherwise need to be considered",0
g,1
", the trace corresponding to the problem “109 + 101”)",0
" the recursive version can generalize perfectly to long problems constructed from these components (such as the sum “822+233”, where “8+2” and “2+3” are in the training set), but the non-recursive version fails to sum these long problems properly",0
 there is a (changing) list of neural programs used to accomplish a given task,0
" the second is the training procedure, which consists of the form of the training data and the optimization process",0
 u varies with the number of vertices in the graph,0
"in this paper, we propose that the key abstraction of recursion is necessary for neural programs to generalize",0
g,0
", qstacklo or qstackhi) or to pointer (e",0
"
we experiment with two levels of recursion—partial and full",1
" k is set to 11, to denote the range of possible numbers (0 through 9), along with the start/end token (represented with the same encoding) which is observed when a pointer reaches beyond the bounds of the input",0
" note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values, and arg 2 can only take one of 7 values",0
"
the dag is represented as an adjacency list where dag[i][j] refers to the j-th child of vertex i",0
" , n , where the dag contains n vertices",1
 we created a program set that reflects the semantics of algorithm 2,1
 figure 2 shows examples of traces for the different versions of bubble sort,0
" any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of national science foundation and darpa",1
" we note that an earlier version of quicksort we tried lacked primitive operations to directly move a pointer to another, and therefore needed more functions and observations",0
we emphasize the overall goal of this work is to enable the learning of a recursive program,0
" we also hope to decrease supervision, for example by training with only partial or non-recursive traces, and to develop novel neural programming architectures integrated directly with a notion of recursion",1
"
we describe the details of the npi model relevant to our contributions",0
" thus, we demonstrate that recursion enables provably perfect generalization for different tasks, including addition, topological sort, quicksort, and a variant of bubble sort",0
g,0
"
given a verification set, we can then run the model on the verification set to check if the produced traces and results are correct",0
3,0
e,0
"
the dag is represented as an adjacency list where dag[i][j] refers to the j-th child of vertex i",0
"
we construct input problems by splitting into two cases: one case in which the left column contains a null value and another in which the left column does not contain any null values",0
" fa8750-15-2-0104, and berkeley deep drive",0
 this modification also enables us to sort arrays containing arbitrary comparable elements,0
 18: push lo and p− 1 to slo and shi,0
"
in this paper, we propose to resolve these issues by explicitly incorporating recursion into neural architectures",0
"
an alternative method is to run p on many different program inputs and then observe step input sequences which occur, to create v ",1
" recursion is an important concept in programming languages and a critical tool to
reduce the complexity of programs",0
" training on the full recursive trace leads to perfect generalization, as shown in section 4",0
"we train using the adam optimizer and use a 2-layer lstm and task-specific state encoders for the external environments, as described in reed & de freitas (2016)",1
" for each task, given the verification set, we check the traces and results of the learned, to-be-verified neural program (described in section 4",0
" the verification set can be very small, as we found a 10-element array ([8,2,1,2,0,8,5,8,3,7]) is sufficient to cover all of v ",0
"program descriptions calls arguments quicksort run the quicksort
routine in place for the array a, for indices from lo to hi non-recursive: partition, stack, write recursive: same as non-recursive version, along with quicksort implicitly: array a to sort, lo, hi
partition runs the partition function",0
" 9: else 10: function run(e, p2, a2)
a description of the inference procedure is given in algorithm 1",0
 table 3 presents results on randomly generated arrays of varying length for the learned non-recursive and recursive programs,1
" in particular, in the explore function, adding a tail recursive call resets and stores the hidden states associated with vertices in a stack-like fashion",0
" for each length, we test each program on 30 randomly generated problems",0
 recursion enables provable guarantees on neural programs’ behavior without needing to exhaustively enumerate all possible inputs to the programs,0
", an array for quicksort or a dag for topological sort)",0
 this modification also enables us to sort arrays containing arbitrary comparable elements,0
" formally, the npi is represented by the following set of equations:
st = fenc(et, at)
ht = flstm(st, pt, ht−1)
rt = fend(ht), pt+1 = fprog(ht), at+1 = farg(ht)
t is a subscript denoting the time-step; fenc is a domain-specific encoder (to be described later) that takes in the environment slice et and arguments at; flstm represents the core module, which takes in the state st generated by fenc, a program embedding pt ∈ rp , and hidden lstm state ht; fend decodes the return probability rt; fprog decodes a program key embedding pt+1;1 and farg decodes arguments at+1",0
" the core module of the npi is an lstm controller that takes as input a slice of the current external environment, via a set of pointers, and a program and arguments to execute",0
" for brevity, we refer the reader to the appendix for information about the program set and non-recursive and recursive trace-generating functions for quicksort",0
"
an alternative way of representing the environment slice is to expose the values of the absolute vertices to the model; however, this makes it difficult to scale the model to larger graphs, since large vertex values are not seen during training time",1
"
we also report on generalization results for the non-recursive and recursive versions of this variant of bubble sort",0
"
to perform the verification as described here, it is critical to construct v correctly",0
"
since npi naturally supports the notion of recursion, a key question is how to enable npi to learn recursive programs",0
"
as mentioned in section 1, all approaches to neural programming today fare poorly on this generalization issue",0
 the original version of the bubblesort implementation exposes the values within the array,0
 the model described in the training setup of section 4 (trained on 6 traces) was verified to be correct via the matching procedure described in section 4,0
"
after finding v , we construct a set of problem inputs which, when executed on p , create exactly the step input sequences which make up v ",0
", the trace corresponding to the problem “109 + 101”)",0
"
under the assumption that there are no leading 0’s (except in the case of single digits) and the two numbers to be added have the same number of digits, the verification set for addition contains 20,181 input problems",1
 this verification phase only needs to be performed once after training,1
"
base cases and reduction rules for topological sort",0
" provably perfect generalization implies the model will behave correctly, given any valid input",0
 stack push call2 pushes pivot + 1 and hi to qstacklo and qstackhi,1
" the architecture usually possesses some form of memory, which could be internal (such as the hidden state of a recurrent neural network) or external (such as a discrete “scratch pad” or a memory block with differentiable access)",0
" 16: p = partition(a, lo, hi) 17: push p+ 1 and hi to slo and shi",0
"in this paper, we propose that the key abstraction of recursion is necessary for neural programs to generalize",1
 table 4 demonstrates that the accuracy of the non-recursive program degrades sharply when moving from arrays of length 7 to arrays of length 8,0
" 3: 4: function quicksort(a, lo, hi) 5: if lo < hi: then 6: p = partition(a, lo, hi) 7: quicksort(a, lo, p− 1) 8: quicksort(a, p+ 1, hi) 9: 10: function partition(a, lo, hi) 11: pivot = lo 12: for j ∈ [lo, hi− 1] : do 13: if a[j] ≤ a[hi] then 14: swap a[pivot] with a[j] 15: pivot = pivot+ 1 16: swap a[pivot] with a[hi] 17: return pivot
for quicksort, the domain-specific encoder is
fenc(qarray, qstacklo, qstackhi, plo, phi, pstacklo, pstackhi, ppivot, pj , at) =
mlp ([qarray(pj) ≤ qarray(phi), pj == phi, qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1), pstacklo == 1, at(1), at(2), at(3)]),
whereqarray ∈ ru×11 is a scratch-pad that contains u rows, each containing one of 11 values (one of the numbers 0 through 9 or an invalid state)",1
"as there is no public implementation of npi, we implemented a version of it in keras that is as faithful to the paper as possible",0
" as with the original npi architecture, the experiments for this paper always used a 3-tuple of integers at = (at(1), at(2), at(3))",0
 b1b0 where no carry operations occur,1
 ,1
 this procedure verifies that all base cases and reductions are handled properly by the model via explicit tests,1
" because the context of the caller is stored on a stack when it calls another program and the callee starts in a fresh context, this enables recursion simply by allowing a program to call itself",0
g,0
"
since npi naturally supports the notion of recursion, a key question is how to enable npi to learn recursive programs",0
" for bubble sort, the domain-specific encoder is
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1), q(1, i2), i3 == length, at(1), at(2), at(3)]),
where the environment q ∈ r1×n×k is a scratch-pad that contains 1 row, to represent the state of the array as sorting proceeds in-place, andn columns",1
 stack pop pushes −∞ values to qstacklo and qstackhi,0
 the program terminates when seeing no numbers in the current column,0
"
after finding v , we construct a set of problem inputs which, when executed on p , create exactly the step input sequences which make up v ",0
"2 at any given time, the npi has access to values pointed to by four pointers in each of the four rows, represented by q(1, i1), q(2, i2), q(3, i3), and q(4, i4)",1
"
algorithm 3 recursive quicksort 1: initialize an array a to sort",1
"
there are 6 pointers (plo, phi, pstacklo, pstackhi, ppivot, pj)",0
" we implement a quicksort task, in order to demonstrate that recursion helps with learning divide-and-conquer algorithms",1
e,0
"
an alternative way of representing the environment slice is to expose the values of the absolute vertices to the model; however, this makes it difficult to scale the model to larger graphs, since large vertex values are not seen during training time",0
" for problems where the base cases may be extremely large or infinite, such as certain forms of motor control, recursion can still help reduce the problem of generalization to these two aspects and make the generalization problem significantly simpler to handle and reason about",0
"
in this paper, we propose to resolve these issues by explicitly incorporating recursion into neural architectures",1
" the write operation has the following arguments:
arg 1 (main action): color curr, color next, active start, active neighb, active stack, save, stack push, stack pop, result
color curr colors vactive, color next colors vertex dag[vactive][childlist[vactive]], active start writes pstart to vactive, active neighb writes dag[vactive][childlist[vactive]] to vactive, active stack writes qstack(pstack) to vactive, save writes vactive to vsave, stack push pushes vactive to the top of the stack, stack pop writes a null value to the top of the stack, and result writes vactive to qresult(presult)",0
"
directed acyclic graphs are structurally more diverse than inputs in the two tasks of grade-school addition and bubble sort",1
"
arg 2 (increment or decrement): up, down
swap",0
" the non-recursive and partially recursive versions are unable to sort long arrays, beyond length 8",0
", the verification set",0
" this makes it so that we only need to consider the vertices in the subgraph that are currently relevant for computing the sort, allowing simpler reasoning about behavior for large graphs",1
" 2: initialize lo and hi to be 1 and n, where n is the length of a",1
" this guarantees that the step input sequences considered during verification contain all step input sequences which arise during execution of an unseen problem in f , leading to generalization to any problem in f ",0
" it is straightforward to deal with all other operations, which do not induce partial environment states",0
" lshift moves the four pointers to the left, to move to the next column",0
g,0
" if a white vertex is found, set pstart to point to it; this signifies the start of a traversal of a new connected component",0
" in order to compensate for this, ppivot and pj require special functions (move pivot lo and move j lo) to properly set them to lo in lines 11 and 12 of the partition function in algorithm 3",1
 npi then outputs the return probability and next program and arguments to execute,0
" to the best of our knowledge, this is the first time verification has been applied to a neural program, providing provable guarantees about its behavior",0
g,0
" , n , where the dag contains n vertices",0
we now report on generalization for the varying tasks,1
" the write operation has the following arguments:
arg 1 (main action): color curr, color next, active start, active neighb, active stack, save, stack push, stack pop, result
color curr colors vactive, color next colors vertex dag[vactive][childlist[vactive]], active start writes pstart to vactive, active neighb writes dag[vactive][childlist[vactive]] to vactive, active stack writes qstack(pstack) to vactive, save writes vactive to vsave, stack push pushes vactive to the top of the stack, stack pop writes a null value to the top of the stack, and result writes vactive to qresult(presult)",0
 we found that changing the npi training traces is a simple way to enable this,1
" table 1 presents results on randomly generated arrays of varying length for the learned non-recursive, partially recursive, and full recursive programs",1
" the verification set can be very small, as we found a 10-element array ([8,2,1,2,0,8,5,8,3,7]) is sufficient to cover all of v ",0
" naı̈vely, we might expect to synthesize and test an input for any sequence created by combining the four possible colors in two variables and another boolean variable for whether the stack is empty (so 32 possible observations at any point), but for various reasons, most of these combinations are impossible to occur at any given point in the execution trace",0
" for brevity, we refer the reader to the appendix for information about the program set and non-recursive and recursive trace-generating functions for quicksort",1
" partial recursion only adds a tail recursive call to bubblesort after bubble and reset, similar to the tail recursive call described previously for addition",1
" note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values, and arg 2 can only take one of 7 values",1
 b1b0 where no carry operations occur,1
g,1
" consequently, it is infeasible to construct a verification set that covers all cases, and the verification procedure we have described is inadequate",1
"
base cases and reduction rules for quicksort",0
"
note that v should also contain the necessary reductions, which corresponds to making the recursive calls at the correct time, as indicated by p ",0
" using this modification, we constructed a verification set consisting of one array of size 10",1
" lshift moves the four pointers to the left, to move to the next column",1
3 to construct v and then empirically create a verification set which covers v ,1
"
grade school addition",0
"
note that v should also contain the necessary reductions, which corresponds to making the recursive calls at the correct time, as indicated by p ",0
 the program terminates when seeing no numbers in the current column,0
g,1
" the outputs rt, pt+1, at+1 are used to determine the next action, as described in algorithm 1",0
" lshift moves the four pointers to the left, to move to the next column",1
" table 1 presents results on randomly generated arrays of varying length for the learned non-recursive, partially recursive, and full recursive programs",0
 ,0
" using this modification, we constructed a verification set consisting of one array of size 10",0
3,0
" m , as described in algorithm 1, processes the sequence of step inputs by using an lstm",0
" we prove every member of f is added properly, given that subproblems s = {anan−1 + bnbn−1, an−1an−2 + bn−1bn−2, ",1
" in future work, we seek to enable more tasks with recursive structure",0
" this is possible if and only if the graph has no directed cycles; that is to say, it must be a directed acyclic graph (dag)",1
 we adapt machinery from the original paper slightly to fit our needs,0
"
training setup",1
" in particular, in the explore function, adding a tail recursive call resets and stores the hidden states associated with vertices in a stack-like fashion",0
"
move",0
"
in this general neural programming architecture, we show it is easy to support recursion",1
 the training set for addition contains 200 traces,1
"
arg 2 (object to copy): env stack lo peek, env stack hi peek, phi, plo, ppivot − 1, ppivot + 1, reset
env stack lo peek and env stack hi peek represent qstacklo(pstacklo − 1) and qstackhi(pstackhi − 1), respectively",1
" if it is too small, then execution of the program on some input might require evaluation of m(i) on some i /∈ v , and so the behavior ofm(i) might deviate from p (i)",1
" in particular, given that a recursion is defined by two properties as mentioned before, the base cases and the set of reduction rules, we can prove a recursive neural program generalizes perfectly if we can prove that (1) it performs correctly on the base cases; (2) it learns the reduction rules correctly",1
" note that recursion helps make this process tractable, because we only need to test a finite number of inputs to show that the model will work correctly on inputs of unbounded complexity",0
g,1
" for tasks other than addition, it is difficult to analytically determine the verification set, so instead, we randomly generate input candidates until they completely cover the base cases and reduction rules",1
" any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of national science foundation and darpa",1
as mentioned in section 2,0
we emphasize that the notion of a neural recursive program has not been presented in the literature before: this is our main contribution,1
"
the training set for bubble sort contains 100 traces, with maximum problem length of 2 (e",1
"
we also report on generalization results for the non-recursive and recursive versions of this variant of bubble sort",1
"
to evaluate a neural network that learns a neural program to accomplish a certain task, one common evaluation metric is how well the learned model m generalizes",1
 this procedure verifies that all base cases and reductions are handled properly by the model via explicit tests,0
"
training setup",1
g,0
"1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 while (pstacklo 6= 1): 6 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 7 stack(stack_pop) 8 else: 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 stack(stack_pop) 12 partition() 13 stack(stack_push_call2) 14 stack(stack_push_call1) 15 } 16 17 partition() { 18 set_pivot_lo() 19 set_j_lo() 20 compswap_loop() 21 swap(ppivot, phi) 22 set_j_null() 23 } 24 25 compswap_loop() { 26 while (pj 6= phi): 27 compswap() 28 move(pj, up) 29 } 30 31 compswap() { 32 if (a[pj ] ≤ a[phi]): 33 swap(ppivot, pj) 34 move(ppivot, up) 35 } 36 37 stack(op) { 38 if (op == stack_push_call1): 39 write(env_stack_lo, plo) 40 write(env_stack_hi, ppivot − 1) 41 move(pstacklo, up) 42 move(pstackhi, up) 43 44 if (op == stack_push_call2): 45 write(env_stack_lo, ppivot + 1) 46 write(env_stack_hi, phi) 47 move(pstacklo, up) 48 move(pstackhi, up) 49 50 if (op == stack_pop): 51 write(env_stack_lo, reset) 52 write(env_stack_hi, reset) 53 move(pstacklo, down) 54 move(pstackhi, down) 55 }altered recursive functions 1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 6 partition() 7 stack(stack_push_call2) 8 stack(stack_push_call1) 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 quicksort() // recursive call 12 stack(stack_pop) 13 write(phi, env_stack_hi_peek) 14 write(plo, env_stack_lo_peek) 15 quicksort() // recursive call 16 stack(stack_pop) 17 } 18 19 compswap_loop() { 20 if (pj 6= phi): 21 compswap() 22 move(pj, up) 23 compswap_loop() // recursive call 24 }in this section, we describe the space of base cases and reduction rules that must be covered for each of the four sample tasks, in order to create the verification set",1
" the add reduction rule ensures that each call to add only covers two adjacent columns, and so the lstm only ever runs for a fixed number of steps necessary to process these two columns",0
" twc-1409915, darpa under grant no",0
" as aforementioned, the npi model naturally supports recursion",0
" in this architecture, we consider a core controller, e",0
 ,0
"
arg 2 (auxiliary variable): color grey, color black
color grey and color black color the given vertex grey and black, respectively",0
"
thus far, to evaluate the efficacy of neural models on programming tasks, the only metric that has been used is generalization of expected behavior to inputs of greater complexity (vinyals   (2015), kaiser & sutskever (2015), reed & de freitas (2016), graves   (2016), zaremba   (2016))",0
"
after finding v , we construct a set of problem inputs which, when executed on p , create exactly the step input sequences which make up v ",1
 our experiments use a small number of training examples,1
", the trace corresponding to the array [3,2])",0
" finally, the boolean pstack == 1 is used to check whether the stack is empty in line 18 of algorithm 2",1
" qcolor(dag[vactive][childlist[vactive]]) refers to the color of the next child of vactive, used in the evaluation of the condition in the if branch in line 9 of algorithm 2",0
 the core controller acts as a dispatcher for the programs,0
 there is a (changing) list of neural programs used to accomplish a given task,1
g,1
 almost all architectures train on program input/output pairs,0
" both the non-recursive and recursive learned programs generalize on all graphs we tried, up to 120 vertices",0
 ,0
" in this version, there is a dependence on length: bstep and lshift are called a number of times equivalent to one less than the length of the input array, in bubble and reset respectively",1
"
move",1
" more specifically, when m is trained on simpler inputs, such as inputs of a small length, the generalization metric evaluates how well m will do on more complex inputs, such as inputs of much longer length",1
" there are 3 pointers (presult, pstack, pstart), presult points to the next empty location in qresult,
pstack points to the top of the stack in qstack, and pstart points to the candidate starting node for a connected component",1
" the write operation has the following arguments:
arg 1 (object to write): env stack lo, env stack hi, phi, plo
env stack lo and env stack hi represent qstacklo(pstacklo) and qstackhi(pstackhi), respectively",0
" k is set to 11, to denote the range of possible numbers (0 through 9), along with the start/end token (represented with the same encoding) which is observed when a pointer reaches beyond the bounds of the input",0
" in order to compensate for this, ppivot and pj require special functions (move pivot lo and move j lo) to properly set them to lo in lines 11 and 12 of the partition function in algorithm 3",1
"we show that if we incorporate recursion, the learned npi programs can achieve provably perfect generalization for different tasks",1
" any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of national science foundation and darpa",0
" the single-digit multiplication task in zaremba   (2016), the bubble sort task in reed & de freitas (2016), and the graph tasks in graves   (2016))",0
" twc-1409915, darpa under grant no",0
" the arrow in the theorem refers to evaluation, as in big-step semantics",0
" the recursive version of topological sort solves all graph instances we tried, from graphs of size 5 through 70",0
" the assumption of leading 0’s can be easily removed, at the cost of slightly increasing the size of the verification set",1
" however, to be sure that the generated v is complete (covers all the cases needed), we need to check all pairs of observations seen in adjacent step inputs (in particular, those before and after a primitive function call), in a similar way as if we were constructing v from scratch",1
"
the npi accesses an external environment, q, which varies according to the task",0
" ppivot and pj point to the pivot and j indices of the array, used in the partition function in algorithm 3",0
"
for topological sort, the domain-specific encoder is
fenc(dag,qcolor, pstack, pstart, vactive, childlist, at)
=mlp ([qcolor(pstart), qcolor(dag[vactive][childlist[vactive]]), pstack == 1, at(1), at(2), at(3)]),
where qcolor ∈ ru×4 is a scratch-pad that contains u rows, each containing one of four colors (white, gray, black, invalid) with one-hot encoding",0
" that is to say, the network does not learn the true program semantics",0
"program descriptions calls arguments quicksort run the quicksort
routine in place for the array a, for indices from lo to hi non-recursive: partition, stack, write recursive: same as non-recursive version, along with quicksort implicitly: array a to sort, lo, hi
partition runs the partition function",1
" in fact, it is one of our future directions to explore new ways to train a recursive neural program without providing explicit training execution traces or with only partial or non-recursive traces",0
" via the recursive call, we effectively forget that the column just added exists, since the recursive call to add starts with a new hidden state for the lstm controller",0
g,0
"we show that if we incorporate recursion, the learned npi programs can achieve provably perfect generalization for different tasks",1
" because the context of the caller is stored on a stack when it calls another program and the callee starts in a fresh context, this enables recursion simply by allowing a program to call itself",1
" in order to compensate for this, ppivot and pj require special functions (move pivot lo and move j lo) to properly set them to lo in lines 11 and 12 of the partition function in algorithm 3",0
", the trace corresponding to the array [3,2])",0
" for these programs, we re-use the appropriate program sets (the associated subprograms), and we refer the reader to the appendix of reed & de freitas (2016) for further details on the subprograms used in addition and bubble sort",1
" m is considered to have perfect generalization if m can give the right answer for any input, such as inputs of arbitrary length",0
" the arrow in the theorem refers to evaluation, as in big-step semantics",1
" the move operation has the following arguments:
arg 1 (pointer): pstacklo, pstackhi, pj , ppivot
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values",0
" however, models that make use of this strategy eventually fail after a certain level of complexity (e",1
" in particular, recursion can be implemented as a program calling itself",0
" as with the others, we apply the procedure described in section 3",0
 indentation indicates the stack is one level deeper than before,1
 this verification phase only needs to be performed once after training,0
"
we construct input problems by splitting into two cases: one case in which the left column contains a null value and another in which the left column does not contain any null values",0
" the pointer at index i3 represents a counter internal to the environment that is incremented once after each pass of the algorithm (one cycle of bubble and reset); when incremented a number of times equal to the length of the array, the flag i3 == length becomes true and terminates the entire algorithm ",1
", to color a vertex) or variable (e",1
", to change the value of vactive) none described below move move a pointer (e",1
"program descriptions calls arguments quicksort run the quicksort
routine in place for the array a, for indices from lo to hi non-recursive: partition, stack, write recursive: same as non-recursive version, along with quicksort implicitly: array a to sort, lo, hi
partition runs the partition function",1
 the environment and return probability are omitted for readability,0
" in this pair, the step input runs a subprogram add1 that has no arguments, and the step output contains a program write that has arguments of out and 1",0
" for the recursive formulation of addition, we analytically construct the set of input problems that cover all base cases and reduction rules",1
 the training set for addition contains 200 traces,0
" if no white vertex is found, the entire execution is terminated move none write write a value either to environment (e",0
", to change the value of phi) none described belowstack",1
" in particular, recursion can be implemented as a program calling itself",1
 u varies with the number of vertices in the graph,0
"
it is sufficient to construct problems where every transition between two adjacent columns is covered",1
" we can obtain this mapping by first considering the possible observations that can arise at the beginning of the entry function (add, bubblesort, toposort, and quicksort) for some valid program input, and iteratively applying the observation-to-observation mapping implied by the reference implementation’s step output at that point in the execution",1
", an array for quicksort or a dag for topological sort)",0
", to color a vertex) or variable (e",0
"
the npi accesses an external environment, q, which varies according to the task",0
 reset represents a −∞ value,0
" for non-primitive subprograms, we can update the observation-to-observation mapping currently associated with the subprogram and then apply that mapping to the current set",1
"
note that the recursion for quicksort is not purely tail recursive and therefore represents a more complex kind of recursion that is harder to learn than in the previous tasks",0
e,0
"
as a concrete instantiation, we show in this paper that we can enable recursive neural programs in the npi model, and thus enable perfectly generalizable neural programs for tasks such as sorting where the original, non-recursive npi program fails",0
" the single-digit multiplication task in zaremba   (2016), the bubble sort task in reed & de freitas (2016), and the graph tasks in graves   (2016))",0
"
to perform the verification as described here, it is critical to construct v correctly",1
" we hypothesize that the reason for this is that the neural network learns to spuriously depend on specific characteristics of the training examples that are irrelevant to the true program semantics, such as length of the training inputs, and thus fails to generalize to more complex inputs",0
" empirically, we observe that the learned recursive programs solve all valid inputs with 100% accuracy after training on a very small number of examples, out-performing previous generalization results",1
" the core lstm module’s hidden state is preserved over all these add1 calls, and it is difficult to interpret with certainty what happens over longer timesteps without concretely evaluating the lstm with an input of that length",0
" the recursive version can generalize perfectly to long problems constructed from these components (such as the sum “822+233”, where “8+2” and “2+3” are in the training set), but the non-recursive version fails to sum these long problems properly",0
" 9: else 10: function run(e, p2, a2)
a description of the inference procedure is given in algorithm 1",1
e,1
" this agrees with the generalization of non-recursive addition in reed & de freitas (2016), where they reported generalization up to 3000 digits",0
" this is highly undesirable, since being able to provide the correct answer in all possible settings is one of the most important aspects of any learned neural program",0
" for brevity, we refer the reader to the appendix for further details on the program set and non-recursive and recursive trace-generating functions used for topological sort",0
"
it is sufficient to construct problems where every transition between two adjacent columns is covered",1
" we made the assumption of equivalent lengths in order to parametrize the input format with respect to length, but this assumption can be removed as well",1
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,0
", the verification set",1
"
base cases and reduction rules for topological sort",1
"
topological sort",1
" also, compared to the bubble pointers in bubble sort, the pointers that perform the comparison for quicksort (the compswap function) are usually not adjacent to each other, making quicksort less local than bubble sort",0
 the model described in the training setup of section 4 (trained on 6 traces) was verified to be correct via the matching procedure described in section 4,1
" m is considered to have perfect generalization if m can give the right answer for any input, such as inputs of arbitrary length",0
"
we describe the details of the npi model relevant to our contributions",1
", to color a vertex) or variable (e",1
"
base cases and reduction rules for topological sort",1
"
as a concrete instantiation, we show in this paper that we can enable recursive neural programs in the npi model, and thus enable perfectly generalizable neural programs for tasks such as sorting where the original, non-recursive npi program fails",1
" 9: else 10: function run(e, p2, a2)
a description of the inference procedure is given in algorithm 1",0
"when constructing a neural network for the purpose of learning a program, there are two orthogonal aspects to consider",0
"
the training data for the neural programmer-interpreter consists of full execution traces for the program of interest",0
" in future work, we would like to decrease supervision and construct models that are capable of coming up with recursive abstractions themselves",0
 2: initialize an empty stack s and a directed acyclic graph dag to traverse,0
 u varies with the number of vertices in the graph,0
" in particular, recursion can be implemented as a program calling itself",1
" however, our concept of recursion for neural programs is general",1
" in this section, we give a brief review of the npi architecture from reed & de freitas (2016) as background",0
" the outputs rt, pt+1, at+1 are used to determine the next action, as described in algorithm 1",0
" in all experiments, α is set to 0",0
", pstart or childlist[vactive]) up or down none described belowwrite",0
"
the npi accesses an external environment, q, which varies according to the task",0
"
base cases and reduction rules for topological sort",1
"
through careful reasoning about the possible set of environment observations created by all valid inputs, and how each of the operations in the execution trace affects the environment, we can construct v using the procedure described in section 3",1
"
by reasoning about the possible set of environment observations created by all valid inputs, we construct v using the procedure described in section 3",1
"
by nature, recursion reduces the complexity of a problem to simpler instances",1
" ppivot and pj point to the pivot and j indices of the array, used in the partition function in algorithm 3",1
g,0
 b1b0 where no carry operations occur,0
" for purposes of verification, we replace the domain-specific encoder with the following:
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1) ≤ q(1, i2), 1 ≤ i1 ≤ length, 1 ≤ i2 ≤ length, i3 == length, at(1), at(2), at(3)]),which directly exposes which of the two values pointed to is larger",0
"
the three environment observations aid with control flow in algorithm 2",0
"
we construct input problems by splitting into two cases: one case in which the left column contains a null value and another in which the left column does not contain any null values",0
" we prove every member of f is added properly, given that subproblems s = {anan−1 + bnbn−1, an−1an−2 + bn−1bn−2, ",1
" that is to say, the network does not learn the true program semantics",0
 these pointers are referred to as bubble pointers,1
g,0
"
we also report on generalization results for the non-recursive and recursive versions of this variant of bubble sort",0
" , n , where the dag contains n vertices",0
"
arg 2 (object to copy): env stack lo peek, env stack hi peek, phi, plo, ppivot − 1, ppivot + 1, reset
env stack lo peek and env stack hi peek represent qstacklo(pstacklo − 1) and qstackhi(pstackhi − 1), respectively",1
" for each length, we test each program on 30 randomly generated problems",0
 we describe the recursive re-formulation of traces for two tasks from the original npi paper—grade-school addition and bubble sort,0
 we describe the recursive re-formulation of traces for two tasks from the original npi paper—grade-school addition and bubble sort,1
" we hypothesize that the reason for this is that the neural network learns to spuriously depend on specific characteristics of the training examples that are irrelevant to the true program semantics, such as length of the training inputs, and thus fails to generalize to more complex inputs",1
"
formally, verification consists of proving the following theorem:
∀i ∈ v,m(i) ⇓ p (i) where i denotes a sequence of step inputs (within one function call), v denotes the set of valid sequences of step inputs, m denotes the neural network model, p denotes the correct program, and p (i) denotes the next step output from the correct program",0
"
in this general neural programming architecture, we show it is easy to support recursion",0
"
in order to demonstrate that recursion can help learn and generalize better, for addition, we trained only on traces for 5 arbitrarily chosen 1-digit addition sum examples",0
"
since npi naturally supports the notion of recursion, a key question is how to enable npi to learn recursive programs",0
 ,0
" note that recursion helps make this process tractable, because we only need to test a finite number of inputs to show that the model will work correctly on inputs of unbounded complexity",0
" the write operation has the following arguments:
arg 1 (main action): color curr, color next, active start, active neighb, active stack, save, stack push, stack pop, result
color curr colors vactive, color next colors vertex dag[vactive][childlist[vactive]], active start writes pstart to vactive, active neighb writes dag[vactive][childlist[vactive]] to vactive, active stack writes qstack(pstack) to vactive, save writes vactive to vsave, stack push pushes vactive to the top of the stack, stack pop writes a null value to the top of the stack, and result writes vactive to qresult(presult)",1
 the training set for addition contains 200 traces,0
 the training set for addition contains 200 traces,0
" as before, the non-recursive learned program lacks a provable guarantee of generalization, whereas we show later that the recursive learned program has one",1
"
the training set for quicksort contains 4 traces, synthesized from arrays of length 5",0
"
we refer the reader to the appendix for the non-recursive trace generating functions",0
 npi then outputs the return probability and next program and arguments to execute,1
" for problems where the base cases may be extremely large or infinite, such as certain forms of motor control, recursion can still help reduce the problem of generalization to these two aspects and make the generalization problem significantly simpler to handle and reason about",0
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,0
3,1
" k is set to 11, to represent the range of 10 possible digits, along with a token representing the end of input",1
"
algorithm 2 depth first search topological sort 1: color all vertices white",0
 the original version of the bubblesort implementation exposes the values within the array,0
" however, our concept of recursion for neural programs is general",0
" the stack operation has the following arguments:
arg 1 (operation): stack push call1, stack push call2, stack pop
stack push call1 pushes lo and pivot−1 toqstacklo andqstackhi",0
" in the non-recursive trace, there are four functions that can be made recursive—toposort, check child, explore, and next start, and we add a tail recursive call to each of these functions in order to make the recursive trace",0
 the degree for any vertex in the dag is variable,0
"
we experiment with two levels of recursion—partial and full",1
 this paper is the first (to our knowledge) to investigate the important problem of provable generalization properties of neural programs,0
" ppivot and pj point to the pivot and j indices of the array, used in the partition function in algorithm 3",0
 the program terminates when seeing no numbers in the current column,1
" the verification set can be very small, as we found a 10-element array ([8,2,1,2,0,8,5,8,3,7]) is sufficient to cover all of v ",0
" k is set to 11, to denote the range of possible numbers (0 through 9), along with the start/end token (represented with the same encoding) which is observed when a pointer reaches beyond the bounds of the input",1
" qcolor(dag[vactive][childlist[vactive]]) refers to the color of the next child of vactive, used in the evaluation of the condition in the if branch in line 9 of algorithm 2",0
"
algorithm 2 depth first search topological sort 1: color all vertices white",0
 we describe the recursive re-formulation of traces for two tasks from the original npi paper—grade-school addition and bubble sort,1
"
recursion drastically reduces the number of configurations we need to consider during the verification phase and makes the proof tractable, because it introduces structure that eliminates infinitely long sequences of step inputs that would otherwise need to be considered",1
"
arg 2 (increment or decrement): up, down
swap",0
" the move operation has the following arguments:
arg 1 (pointer): presult, pstack, pstart, childlist[vactive], childlist[vsave]
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 5 values",1
"
directed acyclic graphs are structurally more diverse than inputs in the two tasks of grade-school addition and bubble sort",0
" the only model, to our knowledge, that does not train on input-output pairs is the neural programmer-interpreter (reed & de freitas, 2016), which trains on synthetic execution traces",1
 the logic for the non-recursive trace is shown in algorithm 4 in the appendix,0
" in order to claim a proof, we must verify the model produces correct behavior over all base cases and reductions, as described in section 2",0
" full recursion, in addition to making the aforementioned tail recursive call, adds two additional recursive calls; bstep and lshift are made tail recursive",0
" note that recursion helps make this process tractable, because we only need to test a finite number of inputs to show that the model will work correctly on inputs of unbounded complexity",0
 we choose to implement a topological sort task for graphs,0
" if no white vertex is found, the entire execution is terminated move none write write a value either to environment (e",1
"program descriptions calls arguments toposort perform topological
sort on graph traverse, next start, write, move
none
traverse traverse graph until stack is empty check child, explore none check child check if a white child exists; if so, set childlist[vactive] to point to it move none explore repeatedly traverse subgraphs until stack is empty stack, check child, write, move none stack interact with stack, either pushing or popping write, move push, pop next start move pstart until reaching a white vertex",0
g,0
" for each length, we test each program on 30 randomly generated problems",0
" twc-1409915, darpa under grant no",1
"
base cases and reduction rules for quicksort",1
 recursion enables provable guarantees on neural programs’ behavior without needing to exhaustively enumerate all possible inputs to the programs,0
" we implement a quicksort task, in order to demonstrate that recursion helps with learning divide-and-conquer algorithms",1
" however, the authors of npi did not consider explicitly the notion of recursion and as a consequence, did not learn recursive programs",0
" the non-recursive and partially recursive versions are unable to sort long arrays, beyond length 8",0
"
to construct this set, by using the reference implementation of each subprogram, we construct a mapping between two sets of environment observations: the first set consists of all observations that can occur at the beginning of a particular subprogram’s invocation, and the second set contains the observations at the end of that subprogram",1
 the original version of the bubblesort implementation exposes the values within the array,1
" for brevity, we refer the reader to the appendix for further details on the program set and non-recursive and recursive trace-generating functions used for topological sort",0
"
the training set for topological sort contains 6 traces, with one synthesized from a graph of size 5 and the rest synthesized from graphs of size 7",1
" thus, recursion helps decompose a problem and makes it easier to reason about a program’s behavior for previously unseen situations such as longer inputs",1
 recursion enables provable guarantees on neural programs’ behavior without needing to exhaustively enumerate all possible inputs to the programs,1
" given verification sets that cover all the base cases and reduction rules, we can provide proofs that these learned programs generalize perfectly",0
g,0
" we note that an earlier version of quicksort we tried lacked primitive operations to directly move a pointer to another, and therefore needed more functions and observations",1
" in particular, recursion can be implemented as a program calling itself",1
 u varies with the number of vertices in the graph,0
" consequently, it is infeasible to construct a verification set that covers all cases, and the verification procedure we have described is inadequate",1
" note that recursion helps make this process tractable, because we only need to test a finite number of inputs to show that the model will work correctly on inputs of unbounded complexity",1
" k is set to 11, to denote the range of possible numbers (0 through 9), along with the start/end token (represented with the same encoding) which is observed when a pointer reaches beyond the bounds of the input",0
" , a1a0 + b1b0} are added properly",1
 recursion can be implemented differently for different neural programming models,1
 table 3 presents results on randomly generated arrays of varying length for the learned non-recursive and recursive programs,1
 ,1
" hence, if all subproblems in s are added correctly, we have proven that any member of f will be added correctly, thus eliminating an infinite family of inputs that need to be tested",0
" at any given time, the npi has access to the values referred to by two pointers, represented by q(1, i1) and q(1, i2),",0
 we created a program set that reflects the semantics of algorithm 2,1
" in future work, we seek to enable more tasks with recursive structure",0
 reset represents a −∞ value,0
" formally, the npi is represented by the following set of equations:
st = fenc(et, at)
ht = flstm(st, pt, ht−1)
rt = fend(ht), pt+1 = fprog(ht), at+1 = farg(ht)
t is a subscript denoting the time-step; fenc is a domain-specific encoder (to be described later) that takes in the environment slice et and arguments at; flstm represents the core module, which takes in the state st generated by fenc, a program embedding pt ∈ rp , and hidden lstm state ht; fend decodes the return probability rt; fprog decodes a program key embedding pt+1;1 and farg decodes arguments at+1",0
"
it is important to emphasize that at inference time in the npi, the hidden state of the lstm controller is reset (to zero) at each subprogram call, as in line 3 of algorithm 1 (h ← 0)",1
" 16: p = partition(a, lo, hi) 17: push p+ 1 and hi to slo and shi",0
 these pointers are referred to as bubble pointers,0
" full recursion, in addition to making the aforementioned tail recursive call, adds two additional recursive calls; bstep and lshift are made tail recursive",1
"
bubble sort",1
 this algorithm is a variant of depth first search,0
" for each length, we test each program on 30 randomly generated problems",0
" fa8750-15-2-0104, and berkeley deep drive",1
" 4: function toposort(dag) 5: while there is still a white vertex u: do 6: color[u] = grey 7: vactive = u 8: do 9: if vactive has a white child v then 10: color[v] = grey 11: push vactive onto s 12: vactive = v 13: else 14: color[vactive] = black 15: write vactive to result 16: if s is empty then pass 17: else pop the top vertex off s and set it to vactive 18: while s is not empty
topological sort",0
" for purposes of verification, we replace the domain-specific encoder with the following:
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1) ≤ q(1, i2), 1 ≤ i1 ≤ length, 1 ≤ i2 ≤ length, i3 == length, at(1), at(2), at(3)]),which directly exposes which of the two values pointed to is larger",0
" the outputs rt, pt+1, at+1 are used to determine the next action, as described in algorithm 1",0
 stack pop pushes −∞ values to qstacklo and qstackhi,1
" m , as described in algorithm 1, processes the sequence of step inputs by using an lstm",0
"
figure 1 shows examples of non-recursive and recursive addition traces",0
 these pointers are referred to as bubble pointers,0
", qstacklo or qstackhi) or to pointer (e",1
" both the non-recursive and recursive learned programs generalize on all graphs we tried, up to 120 vertices",0
 the logic for the non-recursive trace is shown in algorithm 4 in the appendix,0
5,0
g,0
" the assumption of leading 0’s can be easily removed, at the cost of slightly increasing the size of the verification set",1
 the memory updates of these neural programs are so complex and interdependent that it is difficult to reason about the behaviors of the learned neural program under previously unseen situations (such as problems with longer inputs),0
"
note that the training set can often be considerably smaller than the verification set, and despite this, the learned model can still pass the entire verification set",1
"
as a concrete instantiation, we show in this paper that we can enable recursive neural programs in the npi model, and thus enable perfectly generalizable neural programs for tasks such as sorting where the original, non-recursive npi program fails",0
 this verification phase only needs to be performed once after training,1
" as with the others, we apply the procedure described in section 3",0
" by iterating with this procedure, and then running p on the input observation set that we obtain for the entry point function, we can obtain v precisely",1
" if yes, then this indicates that the learned neural program achieves provably perfect generalization",1
3 to construct v and then empirically create a verification set which covers v ,0
" 2: initialize lo and hi to be 1 and n, where n is the length of a",1
"
the training set for bubble sort contains 100 traces, with maximum problem length of 2 (e",0
" in particular, recursion can be implemented as a program calling itself",1
" we also hope to decrease supervision, for example by training with only partial or non-recursive traces, and to develop novel neural programming architectures integrated directly with a notion of recursion",0
" in particular, this condition is satisfied by npi, and thus the npi model naturally supports recursion (even though the authors of npi did not consider this aspect explicitly)",1
 the degree for any vertex in the dag is variable,1
"we show that if we incorporate recursion, the learned npi programs can achieve provably perfect generalization for different tasks",0
g,0
"
base cases and reduction rules for addition",0
" provably perfect generalization implies the model will behave correctly, given any valid input",0
"
we experiment with two levels of recursion—partial and full",0
 this procedure verifies that all base cases and reductions are handled properly by the model via explicit tests,0
", pstart or childlist[vactive]) up or down none described belowwrite",1
" thus, we demonstrate that recursion enables provably perfect generalization for different tasks, including addition, topological sort, quicksort, and a variant of bubble sort",1
" if it is too large, then the semantics of p might not be well-defined on some elements in v , or the spurious step input sequences may not be reachable from any valid problem input (e",1
" as with the original npi architecture, the experiments for this paper always used a 3-tuple of integers at = (at(1), at(2), at(3))",0
" both the non-recursive and recursive learned programs generalize on all input lengths we tried, up to 5000 digits",1
we emphasize that the notion of a neural recursive program has not been presented in the literature before: this is our main contribution,1
" to the best of our knowledge, this is the first time verification has been applied to a neural program, providing provable guarantees about its behavior",1
"
we describe the details of the npi model relevant to our contributions",0
"
it is important to emphasize that at inference time in the npi, the hidden state of the lstm controller is reset (to zero) at each subprogram call, as in line 3 of algorithm 1 (h ← 0)",0
" observe that the non-recursive program’s correctness degrades for length 11 and beyond, while the recursive program can sort any given array",0
" for each function we use to implement the recursive version of topological sort, we need to consider the set of possible environment observation sequences we can create from all valid inputs and test that the learned program produces the correct behavior on each of these inputs",1
" as this complexity interfered with determining the base cases and reductions, we changed the algorithm to its current form",1
" numerous models have been proposed for learning programs; to name a few, this includes the differentiable neural computer (graves  , 2016), neural turing machine (graves  , 2014), neural gpu (kaiser & sutskever, 2015), neural programmer (neelakantan  , 2015), pointer network (vinyals  , 2015), hierarchical attentive memory (andrychowicz & kurach, 2016), and neural random access machine (kurach  , 2016)",0
" because the context of the caller is stored on a stack when it calls another program and the callee starts in a fresh context, this enables recursion simply by allowing a program to call itself",0
" note that recursion helps make this process tractable, because we only need to test a finite number of inputs to show that the model will work correctly on inputs of unbounded complexity",0
"
through careful reasoning about the possible set of environment observations created by all valid inputs, and how each of the operations in the execution trace affects the environment, we can construct v using the procedure described in section 3",1
 recursion enables provable guarantees on neural programs’ behavior without needing to exhaustively enumerate all possible inputs to the programs,0
 this procedure verifies that all base cases and reductions are handled properly by the model via explicit tests,0
"we train using the adam optimizer and use a 2-layer lstm and task-specific state encoders for the external environments, as described in reed & de freitas (2016)",0
"
the training set for bubble sort contains 100 traces, with maximum problem length of 2 (e",1
"
is critical for implementing recursion, since it permits us to restrict our attention to the currently relevant recursive call, ignoring irrelevant details about other contexts",0
 there are 2 variables (vactive and vsave); vactive holds the active vertex (as in algorithm 2) and vsave holds the value of vactive before executing line 12 of algorithm 2,0
" a topological sort is a linear ordering of vertices such that for every directed edge (u, v) from u to v, u comes before v in the ordering",0
" the second is the training procedure, which consists of the form of the training data and the optimization process",1
" when the program is called, the current context including the caller’s memory state is stored on a stack; when the program returns, the stored context is popped off the stack to resume execution in the previous caller’s context",1
" for many problems, the base cases and reduction rules usually consist of a finite (often small) number of cases",1
"
by reasoning about the possible set of environment observations created by all valid inputs, we construct v using the procedure described in section 3",0
" consequently, there is no concept of length relevant to the problem, which has traditionally been an important focus of length-based curriculum learning",1
" in particular, this condition is satisfied by npi, and thus the npi model naturally supports recursion (even though the authors of npi did not consider this aspect explicitly)",1
"
we also report on generalization results for the non-recursive and recursive versions of this variant of bubble sort",0
", the trace corresponding to the problem “109 + 101”)",0
 2: initialize an empty stack s and a directed acyclic graph dag to traverse,1
" for each length, we test each program on 30 randomly generated problems",0
"
quicksort",0
" without using a recursive program, such a proof is not possible, because the non-recursive program runs on an arbitrarily long addition problem that creates correspondingly long sequences of step inputs; in the non-recursive formulation of addition, add calls add1 a number of times that is dependent on the length of the input",0
 ,0
" for problems where the base cases may be extremely large or infinite, such as certain forms of motor control, recursion can still help reduce the problem of generalization to these two aspects and make the generalization problem significantly simpler to handle and reason about",0
"
as in line 13 of the right-hand side of figure 1",1
" we show that by modifying the training procedure, we enable the npi model to learn recursive neural programs",1
"
in this paper, we propose to resolve these issues by explicitly incorporating recursion into neural architectures",0
 our experiments use a small number of training examples,1
 this paper is the first (to our knowledge) to investigate the important problem of provable generalization properties of neural programs,1
"
in this general neural programming architecture, we show it is easy to support recursion",0
" the move operation has the following arguments:
arg 1 (pointer): pstacklo, pstackhi, pj , ppivot
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values",1
" for example, a step input-step output pair in lines 2 and 3 of the left-hand side of figure 1 is (add1, write out 1)",0
" for each length, we test each program on 30 randomly generated problems",0
" this is highly undesirable, since being able to provide the correct answer in all possible settings is one of the most important aspects of any learned neural program",0
" we prove every member of f is added properly, given that subproblems s = {anan−1 + bnbn−1, an−1an−2 + bn−1bn−2, ",0
" given a precise definition of p , it may be possible to automate the generation of v from p in future work",0
" 4: function toposort(dag) 5: while there is still a white vertex u: do 6: color[u] = grey 7: vactive = u 8: do 9: if vactive has a white child v then 10: color[v] = grey 11: push vactive onto s 12: vactive = v 13: else 14: color[vactive] = black 15: write vactive to result 16: if s is empty then pass 17: else pop the top vertex off s and set it to vactive 18: while s is not empty
topological sort",0
" the single-digit multiplication task in zaremba   (2016), the bubble sort task in reed & de freitas (2016), and the graph tasks in graves   (2016))",0
"
algorithm 3 recursive quicksort 1: initialize an array a to sort",0
" , n , where the dag contains n vertices",0
" this agrees with the generalization of non-recursive addition in reed & de freitas (2016), where they reported generalization up to 3000 digits",1
"
there are 6 pointers (plo, phi, pstacklo, pstackhi, ppivot, pj)",1
 table 4 demonstrates that the accuracy of the non-recursive program degrades sharply when moving from arrays of length 7 to arrays of length 8,1
"
we describe the details of the npi model relevant to our contributions",0
"
arg 2 (increment or decrement): up, down1 // top level topological sort call 2 toposort() { 3 while (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 } 0 11 traverse() { 12 check_child() 13 explore() 14 } 15 16 check_child() { 17 while (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 18 move(childlist[vactive], up) 19 } 20 21 explore() { 22 do 23 if (qcolor(dag[vactive][childlist[vactive]]) is white): 24 write(color_next, color_grey) 25 stack(push) 26 write(save) 27 write(active_neighb) 28 move(childlist[vsave], up) 29 else: 30 write(color_curr, color_black) 31 write(result) 32 move(presult, up) 33 if(pstack == 1): 34 break 35 else: 36 stack(pop) 37 check_child() 38 while (true) 39 } 40 41 stack(op) { 42 if (op == push): 43 write(stack_push) 44 move(pstack, up) 45 46 if (op == pop): 47 write(active_stack) 48 write(stack_pop) 49 move(pstack, down) 50 } 51 52 next_start() { 53 while(qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 54 move(pstart, up) 55 }altered recursive functions  // top level topological sort call 2 toposort() { 3 if (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 toposort() // recursive call 0 } 11 12 check_child() { 13 if (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 14 move(childlist[vactive], up) 15 check_child() // recursive call 16 } 17 18 explore() { 19 if (qcolor(dag[vactive][childlist[vactive]]) is white): 20 write(color_next, color_grey) 21 stack(push) 22 write(save) 23 write(active_neighb) 24 move(childlist[vsave], up) 25 else: 26 write(color_curr, color_black) 27 write(result) 28 move(presult, up) 29 if(pstack == 1): 30 return 31 else: 32 stack(pop) 33 check_child() 34 explore() // recursive call 35 } 36 37 next_start() { 38 if (qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 39 move(pstart, up) 40 next_start() // recursive call 41 }algorithm 4 iterative quicksort 1: initialize an array a to sort and two empty stacks slo and shi",1
" more specifically, when m is trained on simpler inputs, such as inputs of a small length, the generalization metric evaluates how well m will do on more complex inputs, such as inputs of much longer length",0
"
as mentioned in section 1, all approaches to neural programming today fare poorly on this generalization issue",0
g,1
" 3: 4: function partition(a, lo, hi) 5: pivot = lo 6: for j ∈ [lo, hi− 1] : do 7: if a[j] ≤ a[hi] then 8: swap a[pivot] with a[j] 9: pivot = pivot+ 1 0: swap a[pivot] with a[hi] 11: return pivot 12: 13: function quicksort(a, lo, hi) 14: while slo and shi are not empty: do 15: pop states off slo and shi, writing them to lo and hi",0
"
arg 2 (object to copy): env stack lo peek, env stack hi peek, phi, plo, ppivot − 1, ppivot + 1, reset
env stack lo peek and env stack hi peek represent qstacklo(pstacklo − 1) and qstackhi(pstackhi − 1), respectively",1
" 2: initialize lo and hi to be 1 and n, where n is the length of a",1
"
base cases and reduction rules for bubble sort",1
", the verification set",1
6,1
 the maximum problem length in this training set is 3 (e,1
 there are 2 variables (vactive and vsave); vactive holds the active vertex (as in algorithm 2) and vsave holds the value of vactive before executing line 12 of algorithm 2,0
 the sequence of primitive operations (move and write operations) for the non-recursive and recursive versions are exactly the same,1
 our result shows that the training procedure and the npi architecture is capable of generalizing from the step input-output pairs seen in the training data to the unseen ones present in the verification set,1
 ,0
" for brevity, we refer the reader to the appendix for information about the program set and non-recursive and recursive trace-generating functions for quicksort",0
" to the best of our knowledge, this is the first time verification has been applied to a neural program, providing provable guarantees about its behavior",1
"
we propose and describe our verification procedure",0
" furthermore, given a verification input set that covers all base cases and reduction rules, we can formally prove that the learned neural programs achieve perfect generalization after verifying its behavior on the verification input set",1
" the core lstm module’s hidden state is preserved over all these add1 calls, and it is difficult to interpret with certainty what happens over longer timesteps without concretely evaluating the lstm with an input of that length",0
 some functional programming languages go so far as not to define any looping constructs but rely solely on recursion to enable repeated execution of the same code,0
" we can obtain this mapping by first considering the possible observations that can arise at the beginning of the entry function (add, bubblesort, toposort, and quicksort) for some valid program input, and iteratively applying the observation-to-observation mapping implied by the reference implementation’s step output at that point in the execution",1
 table 3 presents results on randomly generated arrays of varying length for the learned non-recursive and recursive programs,0
"program descriptions calls arguments toposort perform topological
sort on graph traverse, next start, write, move
none
traverse traverse graph until stack is empty check child, explore none check child check if a white child exists; if so, set childlist[vactive] to point to it move none explore repeatedly traverse subgraphs until stack is empty stack, check child, write, move none stack interact with stack, either pushing or popping write, move push, pop next start move pstart until reaching a white vertex",1
" even though the earlier version also generalized just as well in practice, relatively small differences in the formulation of the traces and the environment observations can drastically change the difficulty of verification",0
" while this matches the description from reed & de freitas (2016), we found that this causes an unnecessary blowup in the size of v and makes it much more difficult to construct the verification set",0
"
for topological sort, the domain-specific encoder is
fenc(dag,qcolor, pstack, pstart, vactive, childlist, at)
=mlp ([qcolor(pstart), qcolor(dag[vactive][childlist[vactive]]), pstack == 1, at(1), at(2), at(3)]),
where qcolor ∈ ru×4 is a scratch-pad that contains u rows, each containing one of four colors (white, gray, black, invalid) with one-hot encoding",0
", an lstm in npi’s case, but possibly other networks in different cases",0
" 16: p = partition(a, lo, hi) 17: push p+ 1 and hi to slo and shi",0
" we implement a quicksort task, in order to demonstrate that recursion helps with learning divide-and-conquer algorithms",0
 the sequence of primitive operations (move and write operations) for the non-recursive and recursive versions are exactly the same,0
"
base cases and reduction rules for bubble sort",1
" as this complexity interfered with determining the base cases and reductions, we changed the algorithm to its current form",0
" thus, any general neural programming architecture supporting such a call structure can be made to support recursion",1
" 16: p = partition(a, lo, hi) 17: push p+ 1 and hi to slo and shi",1
" in particular, we construct new training traces which explicitly contain recursive elements and show that with this type of trace, npi easily learns recursive programs",0
this material is in part based upon work supported by the national science foundation under grant no,0
"
in order to demonstrate that recursion can help learn and generalize better, for addition, we trained only on traces for 5 arbitrarily chosen 1-digit addition sum examples",0
" as an application, we incorporate recursion into the neural programmer-interpreter architecture and consider four sample tasks: grade-school addition, bubble sort, topological sort, and quicksort",0
"
as a concrete instantiation, we show in this paper that we can enable recursive neural programs in the npi model, and thus enable perfectly generalizable neural programs for tasks such as sorting where the original, non-recursive npi program fails",0
"
to evaluate a neural network that learns a neural program to accomplish a certain task, one common evaluation metric is how well the learned model m generalizes",0
" qcolor(pstart) contains the color of the current start vertex, used in the evaluation of the condition in the while loop in line 5 of algorithm 2",1
"
the npi accesses an external environment, q, which varies according to the task",0
"
note that the training set can often be considerably smaller than the verification set, and despite this, the learned model can still pass the entire verification set",0
 this verification phase only needs to be performed once after training,0
" in fact, we argue that recursion is an essential element for neural programs to generalize, and makes it tractable to prove the generalization of neural programs",1
"
there are 6 pointers (plo, phi, pstacklo, pstackhi, ppivot, pj)",0
 3: begin traversing from vertex 1 in the dag,0
 recursion enables provably perfect generalization,0
"
the non-recursive trace loops on cycles of add1 and lshift",1
" the add reduction rule ensures that each call to add only covers two adjacent columns, and so the lstm only ever runs for a fixed number of steps necessary to process these two columns",0
 ,0
"
algorithm 1 neural programming inference 1: inputs: environment observation e, program p, arguments a, stop threshold α 2: function run(e, p, a) 3: h← 0, r ← 0 4: while r < α do 5: s← fenc(e, a), h← flstm(s, p, h) 6: r ← fend(h), p2 ← fprog(h), a2 ← farg(h) 7: if p is a primitive function then 8: e← fenv(e, p, a)",1
" we further have qresult ∈ nu , a scratch-pad which contains the sorted list of vertices at the end of execution, and qstack ∈ nu , which serves the role of the stack s in algorithm 2",0
", an lstm in npi’s case, but possibly other networks in different cases",1
g,0
" note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values, and arg 2 can only take one of 7 values",0
" this agrees with the generalization of non-recursive addition in reed & de freitas (2016), where they reported generalization up to 3000 digits",1
", qstacklo or qstackhi) or to pointer (e",1
" for each length, we test each program on 30 randomly generated problems",0
g,1
" without using a recursive program, such a proof is not possible, because the non-recursive program runs on an arbitrarily long addition problem that creates correspondingly long sequences of step inputs; in the non-recursive formulation of addition, add calls add1 a number of times that is dependent on the length of the input",0
"
note that v should also contain the necessary reductions, which corresponds to making the recursive calls at the correct time, as indicated by p ",1
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,0
" the partial recursion is not enough for perfect generalization, as will be presented later in section 4",1
 table 4 also demonstrates that the (verified) recursive program generalizes perfectly,0
" however, to be sure that the generated v is complete (covers all the cases needed), we need to check all pairs of observations seen in adjacent step inputs (in particular, those before and after a primitive function call), in a similar way as if we were constructing v from scratch",0
" training on the full recursive trace leads to perfect generalization, as shown in section 4",1
 almost all architectures train on program input/output pairs,1
" as with the original npi architecture, the experiments for this paper always used a 3-tuple of integers at = (at(1), at(2), at(3))",0
" the core module of the npi is an lstm controller that takes as input a slice of the current external environment, via a set of pointers, and a program and arguments to execute",0
"
arg 2 (increment or decrement): up, down
swap",1
" both the non-recursive and recursive learned programs generalize on all input lengths we tried, up to 5000 digits",0
 recursion enables provably perfect generalization,1
 stack push call2 pushes pivot + 1 and hi to qstacklo and qstackhi,1
" although the model is exposed to more and more data, it might learn spurious and overly complex representations of the program, as suggested in zaremba   (2016)",1
" in particular, in the explore function, adding a tail recursive call resets and stores the hidden states associated with vertices in a stack-like fashion",1
" 3: 4: function partition(a, lo, hi) 5: pivot = lo 6: for j ∈ [lo, hi− 1] : do 7: if a[j] ≤ a[hi] then 8: swap a[pivot] with a[j] 9: pivot = pivot+ 1 0: swap a[pivot] with a[hi] 11: return pivot 12: 13: function quicksort(a, lo, hi) 14: while slo and shi are not empty: do 15: pop states off slo and shi, writing them to lo and hi",1
" there are 3 pointers (presult, pstack, pstart), presult points to the next empty location in qresult,
pstack points to the top of the stack in qstack, and pstart points to the candidate starting node for a connected component",1
 recursion can be implemented differently for different neural programming models,0
"
we describe the details of the npi model relevant to our contributions",1
" finally, we implement recursive traces for our own topological sort and quicksort tasks",1
" in particular, this condition is satisfied by npi, and thus the npi model naturally supports recursion (even though the authors of npi did not consider this aspect explicitly)",0
" we note that an earlier version of quicksort we tried lacked primitive operations to directly move a pointer to another, and therefore needed more functions and observations",0
"6) on the verification set, and ensure they match the traces produced by the true program p ",0
"
algorithm 2 shows the topological sort task of interest",0
" via the recursive call, we effectively forget that the column just added exists, since the recursive call to add starts with a new hidden state for the lstm controller",1
"
we would also like to point out that in this paper, we provide as an example one way to train a recursive neural program, by providing a certain training execution trace to the npi model",0
", the trace corresponding to the array [3,2])",0
 3: begin traversing from vertex 1 in the dag,0
" at end, pointer ppivot is moved to the pivot compswap loop, move pivot lo, move j lo, swap
none
compswap loop runs the for loop inside the partition function compswap, move none compswap compares a[pivot] ≤ a[j]; if so, perform a swap and increment ppivot swap, move none set pivot lo sets ppivot to lo index none none set j lo sets pj to lo index none none set j null sets pj to −∞ none none stack pushes lo/hi states
onto stacks slo and shi according to argument (described below)
write, move described below
move moves pointer one unit up or down none described below swap swaps elements at given array indices none described below write write a value either to stack (e",1
"
recursion drastically reduces the number of configurations we need to consider during the verification phase and makes the proof tractable, because it introduces structure that eliminates infinitely long sequences of step inputs that would otherwise need to be considered",1
"
we note that for tasks with very large input domains, such as ones involving mnist digits or speech samples, the state space of base cases and reduction rules could be prohibitively large, possibly infinite",0
3,0
", the verification set",0
" the write operation has the following arguments:
arg 1 (main action): color curr, color next, active start, active neighb, active stack, save, stack push, stack pop, result
color curr colors vactive, color next colors vertex dag[vactive][childlist[vactive]], active start writes pstart to vactive, active neighb writes dag[vactive][childlist[vactive]] to vactive, active stack writes qstack(pstack) to vactive, save writes vactive to vsave, stack push pushes vactive to the top of the stack, stack pop writes a null value to the top of the stack, and result writes vactive to qresult(presult)",1
 recursion enables provably perfect generalization,0
 a1a0 + bnbn−1 ,0
"in this paper, we propose that the key abstraction of recursion is necessary for neural programs to generalize",0
" the contents ofqresult andqstack are not exposed directly through the domain-specific encoder; rather, we define primitive functions which manipulate these scratch-pads",0
" lshift moves the four pointers to the left, to move to the next column",0
" we call this set of inputs the verification set, sv ",0
"we show that if we incorporate recursion, the learned npi programs can achieve provably perfect generalization for different tasks",0
" observe that the non-recursive program’s correctness degrades for length 11 and beyond, while the recursive program can sort any given array",1
" if the step output specifies a primitive function call, we need to reason about how it can affect the environment so as to change the observation in the next step input",1
", an array for quicksort or a dag for topological sort)",1
 3: begin traversing from vertex 1 in the dag,1
"2 at any given time, the npi has access to values pointed to by four pointers in each of the four rows, represented by q(1, i1), q(2, i2), q(3, i3), and q(4, i4)",0
" as aforementioned, the npi model naturally supports recursion",0
" childlist ∈ nu is a vector of pointers, where childlist[i] points to the next child under consideration for vertex i",1
 some functional programming languages go so far as not to define any looping constructs but rely solely on recursion to enable repeated execution of the same code,0
" 2: initialize lo and hi to be 1 and n, where n is the length of a",0
", to color a vertex) or variable (e",0
"
the dag is represented as an adjacency list where dag[i][j] refers to the j-th child of vertex i",0
"
since npi naturally supports the notion of recursion, a key question is how to enable npi to learn recursive programs",1
" we made the assumption of equivalent lengths in order to parametrize the input format with respect to length, but this assumption can be removed as well",1
", to color a vertex) or variable (e",0
" at end, pointer ppivot is moved to the pivot compswap loop, move pivot lo, move j lo, swap
none
compswap loop runs the for loop inside the partition function compswap, move none compswap compares a[pivot] ≤ a[j]; if so, perform a swap and increment ppivot swap, move none set pivot lo sets ppivot to lo index none none set j lo sets pj to lo index none none set j null sets pj to −∞ none none stack pushes lo/hi states
onto stacks slo and shi according to argument (described below)
write, move described below
move moves pointer one unit up or down none described below swap swaps elements at given array indices none described below write write a value either to stack (e",1
"
bubble sort",1
this material is in part based upon work supported by the national science foundation under grant no,0
" thus, recursion helps decompose a problem and makes it easier to reason about a program’s behavior for previously unseen situations such as longer inputs",1
 ,0
" even though the earlier version also generalized just as well in practice, relatively small differences in the formulation of the traces and the environment observations can drastically change the difficulty of verification",1
" for the recursive formulation of addition, we analytically construct the set of input problems that cover all base cases and reduction rules",1
" et+1 ∼ fenv(et, pt, at)",1
" for the recursive formulation of addition, we analytically construct the set of input problems that cover all base cases and reduction rules",0
" 3: 4: function quicksort(a, lo, hi) 5: if lo < hi: then 6: p = partition(a, lo, hi) 7: quicksort(a, lo, p− 1) 8: quicksort(a, p+ 1, hi) 9: 10: function partition(a, lo, hi) 11: pivot = lo 12: for j ∈ [lo, hi− 1] : do 13: if a[j] ≤ a[hi] then 14: swap a[pivot] with a[j] 15: pivot = pivot+ 1 16: swap a[pivot] with a[hi] 17: return pivot
for quicksort, the domain-specific encoder is
fenc(qarray, qstacklo, qstackhi, plo, phi, pstacklo, pstackhi, ppivot, pj , at) =
mlp ([qarray(pj) ≤ qarray(phi), pj == phi, qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1), pstacklo == 1, at(1), at(2), at(3)]),
whereqarray ∈ ru×11 is a scratch-pad that contains u rows, each containing one of 11 values (one of the numbers 0 through 9 or an invalid state)",0
"
algorithm 3 recursive quicksort 1: initialize an array a to sort",1
"in this paper, we propose that the key abstraction of recursion is necessary for neural programs to generalize",0
"
recursion drastically reduces the number of configurations we need to consider during the verification phase and makes the proof tractable, because it introduces structure that eliminates infinitely long sequences of step inputs that would otherwise need to be considered",0
"in this paper, we propose that the key abstraction of recursion is necessary for neural programs to generalize",0
" et+1 ∼ fenv(et, pt, at)",0
" by iterating with this procedure, and then running p on the input observation set that we obtain for the entry point function, we can obtain v precisely",1
"
we describe the base cases, reduction rules, and the verification set for each task in appendix a",0
 we found that changing the npi training traces is a simple way to enable this,0
" for each task, given the verification set, we check the traces and results of the learned, to-be-verified neural program (described in section 4",0
" the recursive version can generalize perfectly to long problems constructed from these components (such as the sum “822+233”, where “8+2” and “2+3” are in the training set), but the non-recursive version fails to sum these long problems properly",0
 this algorithm is a variant of depth first search,0
" for these programs, we re-use the appropriate program sets (the associated subprograms), and we refer the reader to the appendix of reed & de freitas (2016) for further details on the subprograms used in addition and bubble sort",1
" for each function we use to implement the recursive version of topological sort, we need to consider the set of possible environment observation sequences we can create from all valid inputs and test that the learned program produces the correct behavior on each of these inputs",0
 ,0
 we find that recursion makes it easier for the network to learn the right program and generalize to unknown situations,0
" 3: 4: function quicksort(a, lo, hi) 5: if lo < hi: then 6: p = partition(a, lo, hi) 7: quicksort(a, lo, p− 1) 8: quicksort(a, p+ 1, hi) 9: 10: function partition(a, lo, hi) 11: pivot = lo 12: for j ∈ [lo, hi− 1] : do 13: if a[j] ≤ a[hi] then 14: swap a[pivot] with a[j] 15: pivot = pivot+ 1 16: swap a[pivot] with a[hi] 17: return pivot
for quicksort, the domain-specific encoder is
fenc(qarray, qstacklo, qstackhi, plo, phi, pstacklo, pstackhi, ppivot, pj , at) =
mlp ([qarray(pj) ≤ qarray(phi), pj == phi, qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1), pstacklo == 1, at(1), at(2), at(3)]),
whereqarray ∈ ru×11 is a scratch-pad that contains u rows, each containing one of 11 values (one of the numbers 0 through 9 or an invalid state)",1
2,0
" in this version, there is a dependence on length: bstep and lshift are called a number of times equivalent to one less than the length of the input array, in bubble and reset respectively",0
"
note that the recursion for quicksort is not purely tail recursive and therefore represents a more complex kind of recursion that is harder to learn than in the previous tasks",1
"as there is no public implementation of npi, we implemented a version of it in keras that is as faithful to the paper as possible",1
" in particular, recursion can be implemented as a program calling itself",1
"
as a concrete instantiation, we show in this paper that we can enable recursive neural programs in the npi model, and thus enable perfectly generalizable neural programs for tasks such as sorting where the original, non-recursive npi program fails",1
" for example, for the addition task, the model is trained on short inputs and then tested on its ability to sum inputs with much longer numbers of digits",0
" without using a recursive program, such a proof is not possible, because the non-recursive program runs on an arbitrarily long addition problem that creates correspondingly long sequences of step inputs; in the non-recursive formulation of addition, add calls add1 a number of times that is dependent on the length of the input",1
"2 at any given time, the npi has access to values pointed to by four pointers in each of the four rows, represented by q(1, i1), q(2, i2), q(3, i3), and q(4, i4)",1
" 3: 4: function quicksort(a, lo, hi) 5: if lo < hi: then 6: p = partition(a, lo, hi) 7: quicksort(a, lo, p− 1) 8: quicksort(a, p+ 1, hi) 9: 10: function partition(a, lo, hi) 11: pivot = lo 12: for j ∈ [lo, hi− 1] : do 13: if a[j] ≤ a[hi] then 14: swap a[pivot] with a[j] 15: pivot = pivot+ 1 16: swap a[pivot] with a[hi] 17: return pivot
for quicksort, the domain-specific encoder is
fenc(qarray, qstacklo, qstackhi, plo, phi, pstacklo, pstackhi, ppivot, pj , at) =
mlp ([qarray(pj) ≤ qarray(phi), pj == phi, qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1), pstacklo == 1, at(1), at(2), at(3)]),
whereqarray ∈ ru×11 is a scratch-pad that contains u rows, each containing one of 11 values (one of the numbers 0 through 9 or an invalid state)",0
" m is considered to have perfect generalization if m can give the right answer for any input, such as inputs of arbitrary length",0
"
to evaluate a neural network that learns a neural program to accomplish a certain task, one common evaluation metric is how well the learned model m generalizes",1
 the degree for any vertex in the dag is variable,1
" bstep, used in bubble, compares pairs of numbers, continuously moving the bubble pointers once to the right each time until reaching the end of the array",0
"
the training set for quicksort contains 4 traces, synthesized from arrays of length 5",1
"
in what follows, we describe the way in which we constructed npi training traces so as to make them contain recursive elements and thus enable npi to learn recursive programs",0
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,1
"
quicksort",0
" even though the earlier version also generalized just as well in practice, relatively small differences in the formulation of the traces and the environment observations can drastically change the difficulty of verification",1
" table 1 presents results on randomly generated arrays of varying length for the learned non-recursive, partially recursive, and full recursive programs",1
 our result shows that the training procedure and the npi architecture is capable of generalizing from the step input-output pairs seen in the training data to the unseen ones present in the verification set,0
" childlist ∈ nu is a vector of pointers, where childlist[i] points to the next child under consideration for vertex i",1
" this functionality the original npi paper decodes to a program key embedding kt ∈ rk and then computes a program embedding pt+1, which we also did in our implementation, but we omit this for brevity",1
"
the npi accesses an external environment, q, which varies according to the task",1
" for these models, table 2 presents results on randomly generated dags of varying graph sizes (varying in the number of vertices)",0
 recursion enables provably perfect generalization,0
" full recursion, in addition to making the aforementioned tail recursive call, adds two additional recursive calls; bstep and lshift are made tail recursive",1
", to change the value of vactive) none described below move move a pointer (e",1
" numerous models have been proposed for learning programs; to name a few, this includes the differentiable neural computer (graves  , 2016), neural turing machine (graves  , 2014), neural gpu (kaiser & sutskever, 2015), neural programmer (neelakantan  , 2015), pointer network (vinyals  , 2015), hierarchical attentive memory (andrychowicz & kurach, 2016), and neural random access machine (kurach  , 2016)",0
" as aforementioned, the npi model naturally supports recursion",0
 the first is the actual model architecture,0
 we leave this as future work to devise a verification procedure more appropriate to this setting,1
" ppivot and pj point to the pivot and j indices of the array, used in the partition function in algorithm 3",0
"
algorithm 1 neural programming inference 1: inputs: environment observation e, program p, arguments a, stop threshold α 2: function run(e, p, a) 3: h← 0, r ← 0 4: while r < α do 5: s← fenc(e, a), h← flstm(s, p, h) 6: r ← fend(h), p2 ← fprog(h), a2 ← farg(h) 7: if p is a primitive function then 8: e← fenv(e, p, a)",0
e,1
" in future work, we would like to decrease supervision and construct models that are capable of coming up with recursive abstractions themselves",1
" if it is too large, then the semantics of p might not be well-defined on some elements in v , or the spurious step input sequences may not be reachable from any valid problem input (e",0
" if the program is primitive, the next environmental state et+1 will be affected by pt and at, i",0
" recall that this verification procedure cannot be performed for the non-recursive versions, since the propagation of the hidden state in the core lstm module makes reasoning difficult and so we would need to check an unbounded number of examples",0
" at each time step, the core controller can decide to select one of the programs to call with certain arguments",0
" in order to compensate for this, ppivot and pj require special functions (move pivot lo and move j lo) to properly set them to lo in lines 11 and 12 of the partition function in algorithm 3",1
 the first is the actual model architecture,0
" training on the full recursive trace leads to perfect generalization, as shown in section 4",0
" for these programs, we re-use the appropriate program sets (the associated subprograms), and we refer the reader to the appendix of reed & de freitas (2016) for further details on the subprograms used in addition and bubble sort",0
"
arg 2 (object to copy): env stack lo peek, env stack hi peek, phi, plo, ppivot − 1, ppivot + 1, reset
env stack lo peek and env stack hi peek represent qstacklo(pstacklo − 1) and qstackhi(pstackhi − 1), respectively",1
 we outline how to construct this set,1
" the npi model has three learnable components: a task-agnostic core, a program-key embedding, and domain-specific encoders that allow the npi to operate in diverse environments",1
" for purposes of verification, we replace the domain-specific encoder with the following:
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1) ≤ q(1, i2), 1 ≤ i1 ≤ length, 1 ≤ i2 ≤ length, i3 == length, at(1), at(2), at(3)]),which directly exposes which of the two values pointed to is larger",0
g,1
"
base cases and reduction rules for bubble sort",1
" more specifically, when m is trained on simpler inputs, such as inputs of a small length, the generalization metric evaluates how well m will do on more complex inputs, such as inputs of much longer length",0
"
note that the recursion for quicksort is not purely tail recursive and therefore represents a more complex kind of recursion that is harder to learn than in the previous tasks",0
"
the training set for quicksort contains 4 traces, synthesized from arrays of length 5",0
 indentation indicates the stack is one level deeper than before,0
" we can obtain this mapping by first considering the possible observations that can arise at the beginning of the entry function (add, bubblesort, toposort, and quicksort) for some valid program input, and iteratively applying the observation-to-observation mapping implied by the reference implementation’s step output at that point in the execution",0
 the core controller acts as a dispatcher for the programs,0
" in this section, we give a brief review of the npi architecture from reed & de freitas (2016) as background",1
"
base cases and reduction rules for quicksort",1
"we show that if we incorporate recursion, the learned npi programs can achieve provably perfect generalization for different tasks",0
we emphasize that the notion of a neural recursive program has not been presented in the literature before: this is our main contribution,0
this material is in part based upon work supported by the national science foundation under grant no,0
"
note that the recursion for quicksort is not purely tail recursive and therefore represents a more complex kind of recursion that is harder to learn than in the previous tasks",1
"
we note that for tasks with very large input domains, such as ones involving mnist digits or speech samples, the state space of base cases and reduction rules could be prohibitively large, possibly infinite",0
" for example, for the addition task, the model is trained on short inputs and then tested on its ability to sum inputs with much longer numbers of digits",0
" because the context of the caller is stored on a stack when it calls another program and the callee starts in a fresh context, this enables recursion simply by allowing a program to call itself",1
 pstacklo and pstackhi point to the top (empty) positions in qstacklo and qstackhi,0
" k is set to 11, to denote the range of possible numbers (0 through 9), along with the start/end token (represented with the same encoding) which is observed when a pointer reaches beyond the bounds of the input",0
 the original version of the bubblesort implementation exposes the values within the array,0
" we can obtain this mapping by first considering the possible observations that can arise at the beginning of the entry function (add, bubblesort, toposort, and quicksort) for some valid program input, and iteratively applying the observation-to-observation mapping implied by the reference implementation’s step output at that point in the execution",0
 almost all architectures train on program input/output pairs,1
"
we note that for tasks with very large input domains, such as ones involving mnist digits or speech samples, the state space of base cases and reduction rules could be prohibitively large, possibly infinite",0
we emphasize that the notion of a neural recursive program has not been presented in the literature before: this is our main contribution,0
 stack pop pushes −∞ values to qstacklo and qstackhi,0
 we outline how to construct this set,1
" given verification sets that cover all the base cases and reduction rules, we can provide proofs that these learned programs generalize perfectly",0
" we made the assumption of equivalent lengths in order to parametrize the input format with respect to length, but this assumption can be removed as well",1
 there is a (changing) list of neural programs used to accomplish a given task,0
g,1
g,1
" on the other hand, the recursive programs have learned the true program semantics",0
 add1 is a subprogram that adds the current column (writing the appropriate digit to the output row and carrying a bit to the next column if needed),1
" the 4 environment observations aid with control flow; qstacklo(pstacklo− 1) < qstackhi(pstackhi− 1) implements the lo < hi comparison in line 5 of algorithm 3, pstacklo == 1 checks if the stacks are empty in line 18 of algorithm 4, and the other observations (all involving ppivot or pj) deal with logic in the partition function",1
" m is considered to have perfect generalization if m can give the right answer for any input, such as inputs of arbitrary length",1
 ,1
"
an alternative way of representing the environment slice is to expose the values of the absolute vertices to the model; however, this makes it difficult to scale the model to larger graphs, since large vertex values are not seen during training time",0
" thus, recursion helps decompose a problem and makes it easier to reason about a program’s behavior for previously unseen situations such as longer inputs",1
"
in addition, none of the current approaches to neural programming provide a method or even aim to enable provable guarantees about generalization",0
" 9: else 10: function run(e, p2, a2)
a description of the inference procedure is given in algorithm 1",1
"
we describe the base cases, reduction rules, and the verification set for each task in appendix a",1
" for problems where the base cases may be extremely large or infinite, such as certain forms of motor control, recursion can still help reduce the problem of generalization to these two aspects and make the generalization problem significantly simpler to handle and reason about",0
 this makes it difficult to reason about what the model will do when given complex inputs,1
"
grade-school addition",0
" for many problems, the base cases and reduction rules usually consist of a finite (often small) number of cases",0
" finally, the boolean pstack == 1 is used to check whether the stack is empty in line 18 of algorithm 2",1
"1; and for bubble sort, appendix a",0
" in our experiments, we only present dag’s as inputs and represent the vertices as values ranging from 1, ",0
" for each length, we test each program on 30 randomly generated problems",0
 npi then outputs the return probability and next program and arguments to execute,0
 the logic for the non-recursive trace is shown in algorithm 4 in the appendix,0
" on the other hand, the recursive programs have learned the true program semantics",0
" a topological sort is a linear ordering of vertices such that for every directed edge (u, v) from u to v, u comes before v in the ordering",1
g,1
" the swap operation has the following arguments:
arg 1 (swap object 1): ppivot
arg 2 (swap object 2): phi, pj
write",0
 this procedure verifies that all base cases and reductions are handled properly by the model via explicit tests,0
e,0
"
grade-school addition",1
" if the program is primitive, the next environmental state et+1 will be affected by pt and at, i",0
" to the best of our knowledge, this is the first time verification has been applied to a neural program, providing provable guarantees about its behavior",0
" consequently, it is infeasible to construct a verification set that covers all cases, and the verification procedure we have described is inadequate",0
" in fact, it is one of our future directions to explore new ways to train a recursive neural program without providing explicit training execution traces or with only partial or non-recursive traces",0
"6) on the verification set, and ensure they match the traces produced by the true program p ",0
" this makes it so that we only need to consider the vertices in the subgraph that are currently relevant for computing the sort, allowing simpler reasoning about behavior for large graphs",0
" however, models that make use of this strategy eventually fail after a certain level of complexity (e",0
" for each length, we test each program on 30 randomly generated problems",0
" furthermore, given a verification input set that covers all base cases and reduction rules, we can formally prove that the learned neural programs achieve perfect generalization after verifying its behavior on the verification input set",1
" at each time step, the core controller can decide to select one of the programs to call with certain arguments",0
 add1 is a subprogram that adds the current column (writing the appropriate digit to the output row and carrying a bit to the next column if needed),1
"in this paper, we propose that the key abstraction of recursion is necessary for neural programs to generalize",0
"
arg 2 (object to copy): env stack lo peek, env stack hi peek, phi, plo, ppivot − 1, ppivot + 1, reset
env stack lo peek and env stack hi peek represent qstacklo(pstacklo − 1) and qstackhi(pstackhi − 1), respectively",1
" in this section, we give a brief review of the npi architecture from reed & de freitas (2016) as background",0
" for grade-school addition, the domain-specific encoder is
fenc(q, i1, i2, i3, i4, at) =mlp ([q(1, i1), q(2, i2), q(3, i3), q(4, i4), at(1), at(2), at(3)]),
where the environment q ∈ r4×n×k is a scratch-pad that contains four rows (the first input number, the second input number, the carry bits, and the output) and n columns",1
" for each length, we test each program on 30 randomly generated problems",1
we emphasize the overall goal of this work is to enable the learning of a recursive program,1
" the core lstm module’s hidden state is preserved over all these add1 calls, and it is difficult to interpret with certainty what happens over longer timesteps without concretely evaluating the lstm with an input of that length",0
", to change the value of vactive) none described below move move a pointer (e",0
"
is critical for implementing recursion, since it permits us to restrict our attention to the currently relevant recursive call, ignoring irrelevant details about other contexts",1
"
in this general neural programming architecture, we show it is easy to support recursion",0
" however, our concept of recursion for neural programs is general",0
6,0
"
training setup",0
" for brevity, we refer the reader to the appendix for information about the program set and non-recursive and recursive trace-generating functions for quicksort",1
" for purposes of verification, we replace the domain-specific encoder with the following:
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1) ≤ q(1, i2), 1 ≤ i1 ≤ length, 1 ≤ i2 ≤ length, i3 == length, at(1), at(2), at(3)]),which directly exposes which of the two values pointed to is larger",1
" in practice, we can additionally use tail recursion optimization to avoid problems with the call stack growing too deep",1
" a topological sort is a linear ordering of vertices such that for every directed edge (u, v) from u to v, u comes before v in the ordering",0
 stack push call2 pushes pivot + 1 and hi to qstacklo and qstackhi,0
" the single-digit multiplication task in zaremba   (2016), the bubble sort task in reed & de freitas (2016), and the graph tasks in graves   (2016))",0
 almost all architectures train on program input/output pairs,1
" empirically, we observe that the learned recursive programs solve all valid inputs with 100% accuracy after training on a very small number of examples, out-performing previous generalization results",1
" our results show that for all learned, to-be-verified neural programs, they all produced the same traces as those produced by p on the verification set",1
" there are 3 pointers (presult, pstack, pstart), presult points to the next empty location in qresult,
pstack points to the top of the stack in qstack, and pstart points to the candidate starting node for a connected component",0
 this algorithm is a variant of depth first search,1
 this makes it difficult to reason about what the model will do when given complex inputs,1
" if the program is primitive, the next environmental state et+1 will be affected by pt and at, i",1
"
directed acyclic graphs are structurally more diverse than inputs in the two tasks of grade-school addition and bubble sort",1
"
we also report on generalization results for the non-recursive and recursive versions of this variant of bubble sort",0
 we instantiated recursion for the neural programmerinterpreter by changing the training traces,0
" nevertheless, we would like the model to quickly learn to represent the right semantics of the underlying program from a small number of training examples, not an exhaustive number of them",0
" however, models that make use of this strategy eventually fail after a certain level of complexity (e",1
" we then construct problem configurations that span all possible valid environment states (for instance, in order to force the carry bit in a column to be 1, one can add the sum “1+9” in the column to the right)",1
as mentioned in section 2,0
" if it is too small, then execution of the program on some input might require evaluation of m(i) on some i /∈ v , and so the behavior ofm(i) might deviate from p (i)",0
"
for addition, we analytically determine the verification set",1
" although the model is exposed to more and more data, it might learn spurious and overly complex representations of the program, as suggested in zaremba   (2016)",0
 this algorithm is a variant of depth first search,0
" for example, a step input-step output pair in lines 2 and 3 of the left-hand side of figure 1 is (add1, write out 1)",1
", the trace corresponding to the array [3,2])",0
"
we refer the reader to the appendix for the non-recursive trace generating functions",1
" each step during an execution of the program does one of three things: (1) another subprogram along with associated arguments is called, as in line 10, (2) the program writes to the environment if it is primitive, as in line 8, or (3) the loop is terminated if the return probability exceeds a threshold α, after which the stack frame is popped and control is returned to the caller",0
"
since npi naturally supports the notion of recursion, a key question is how to enable npi to learn recursive programs",1
" although the model is exposed to more and more data, it might learn spurious and overly complex representations of the program, as suggested in zaremba   (2016)",0
"
the same set of problems was used to generate the training traces for all formulations of the task, for non-recursive and recursive versions",0
 the degree for any vertex in the dag is variable,0
" as aforementioned, the npi model naturally supports recursion",0
" the write operation has the following arguments:
arg 1 (main action): color curr, color next, active start, active neighb, active stack, save, stack push, stack pop, result
color curr colors vactive, color next colors vertex dag[vactive][childlist[vactive]], active start writes pstart to vactive, active neighb writes dag[vactive][childlist[vactive]] to vactive, active stack writes qstack(pstack) to vactive, save writes vactive to vsave, stack push pushes vactive to the top of the stack, stack pop writes a null value to the top of the stack, and result writes vactive to qresult(presult)",1
" even though the earlier version also generalized just as well in practice, relatively small differences in the formulation of the traces and the environment observations can drastically change the difficulty of verification",0
"
to evaluate a neural network that learns a neural program to accomplish a certain task, one common evaluation metric is how well the learned model m generalizes",1
" in this version of curriculum learning, even though the inputs are gradually becoming more complex, the semantics of the program is succinct and does not change",0
 the original version of the bubblesort implementation exposes the values within the array,0
"
note that the training set can often be considerably smaller than the verification set, and despite this, the learned model can still pass the entire verification set",0
 ,1
 ,0
"
bubble sort",0
" the write operation has the following arguments:
arg 1 (main action): color curr, color next, active start, active neighb, active stack, save, stack push, stack pop, result
color curr colors vactive, color next colors vertex dag[vactive][childlist[vactive]], active start writes pstart to vactive, active neighb writes dag[vactive][childlist[vactive]] to vactive, active stack writes qstack(pstack) to vactive, save writes vactive to vsave, stack push pushes vactive to the top of the stack, stack pop writes a null value to the top of the stack, and result writes vactive to qresult(presult)",0
" this is possible if and only if the graph has no directed cycles; that is to say, it must be a directed acyclic graph (dag)",0
"
for topological sort, the domain-specific encoder is
fenc(dag,qcolor, pstack, pstart, vactive, childlist, at)
=mlp ([qcolor(pstart), qcolor(dag[vactive][childlist[vactive]]), pstack == 1, at(1), at(2), at(3)]),
where qcolor ∈ ru×4 is a scratch-pad that contains u rows, each containing one of four colors (white, gray, black, invalid) with one-hot encoding",1
" for purposes of verification, we replace the domain-specific encoder with the following:
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1) ≤ q(1, i2), 1 ≤ i1 ≤ length, 1 ≤ i2 ≤ length, i3 == length, at(1), at(2), at(3)]),which directly exposes which of the two values pointed to is larger",0
e,1
g,1
"
the training data for the neural programmer-interpreter consists of full execution traces for the program of interest",0
" the learned recursive program is different from neural programs learned in all previous work in an important aspect: previous approaches do not explicitly incorporate this abstraction, and hence generalize poorly, whereas our learned neural programs incorporate recursion and achieve perfect generalization",0
" without using a recursive program, such a proof is not possible, because the non-recursive program runs on an arbitrarily long addition problem that creates correspondingly long sequences of step inputs; in the non-recursive formulation of addition, add calls add1 a number of times that is dependent on the length of the input",0
"
algorithm 2 depth first search topological sort 1: color all vertices white",1
" given verification sets that cover all the base cases and reduction rules, we can provide proofs that these learned programs generalize perfectly",0
"
by reasoning about the possible set of environment observations created by all valid inputs, we construct v using the procedure described in section 3",0
 we leave this as future work to devise a verification procedure more appropriate to this setting,0
" note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values, and arg 2 can only take one of 7 values",0
" while this matches the description from reed & de freitas (2016), we found that this causes an unnecessary blowup in the size of v and makes it much more difficult to construct the verification set",1
" the write operation has the following arguments:
arg 1 (object to write): env stack lo, env stack hi, phi, plo
env stack lo and env stack hi represent qstacklo(pstacklo) and qstackhi(pstackhi), respectively",1
"
note that the recursion for quicksort is not purely tail recursive and therefore represents a more complex kind of recursion that is harder to learn than in the previous tasks",1
", an array for quicksort or a dag for topological sort)",0
" we note that an earlier version of quicksort we tried lacked primitive operations to directly move a pointer to another, and therefore needed more functions and observations",0
", the trace corresponding to the problem “109 + 101”)",0
" finally, we implement recursive traces for our own topological sort and quicksort tasks",0
 ,1
"
for addition, we analytically determine the verification set",1
" our implementation uses two stacks qstacklo and
qstackhi, each in ru , that store the arguments to the recursive quicksort calls in algorithm 3; before each recursive call, the appropriate arguments are popped off the stack and written to plo and phi",0
this material is in part based upon work supported by the national science foundation under grant no,0
" given a precise definition of p , it may be possible to automate the generation of v from p in future work",0
" , a1a0 + b1b0} are added properly",0
3 to construct v and then empirically create a verification set which covers v ,0
" it is straightforward to deal with all other operations, which do not induce partial environment states",0
" in all experiments, α is set to 0",0
"
inside bubble and reset, there are two operations that can be made recursive",1
"
arg 2 (auxiliary variable): color grey, color black
color grey and color black color the given vertex grey and black, respectively",0
"
we construct input problems by splitting into two cases: one case in which the left column contains a null value and another in which the left column does not contain any null values",0
"
it is important to emphasize that at inference time in the npi, the hidden state of the lstm controller is reset (to zero) at each subprogram call, as in line 3 of algorithm 1 (h ← 0)",0
" observe that the non-recursive program’s correctness degrades for length 11 and beyond, while the recursive program can sort any given array",0
 almost all architectures train on program input/output pairs,1
 recursion can be implemented differently for different neural programming models,0
training neural networks to synthesize robust programs from a small number of examples is a challenging task,0
 3: begin traversing from vertex 1 in the dag,1
" the architecture usually possesses some form of memory, which could be internal (such as the hidden state of a recurrent neural network) or external (such as a discrete “scratch pad” or a memory block with differentiable access)",1
" this makes it so that we only need to consider the vertices in the subgraph that are currently relevant for computing the sort, allowing simpler reasoning about behavior for large graphs",1
" in order to compensate for this, ppivot and pj require special functions (move pivot lo and move j lo) to properly set them to lo in lines 11 and 12 of the partition function in algorithm 3",0
"
grade-school addition",1
" fa8750-15-2-0104, and berkeley deep drive",1
" the architecture usually possesses some form of memory, which could be internal (such as the hidden state of a recurrent neural network) or external (such as a discrete “scratch pad” or a memory block with differentiable access)",1
 there are 2 variables (vactive and vsave); vactive holds the active vertex (as in algorithm 2) and vsave holds the value of vactive before executing line 12 of algorithm 2,0
 ,1
 ,1
" finally, the boolean pstack == 1 is used to check whether the stack is empty in line 18 of algorithm 2",0
" however, models that make use of this strategy eventually fail after a certain level of complexity (e",0
" in particular, given that a recursion is defined by two properties as mentioned before, the base cases and the set of reduction rules, we can prove a recursive neural program generalizes perfectly if we can prove that (1) it performs correctly on the base cases; (2) it learns the reduction rules correctly",1
" in particular, recursion can be implemented as a program calling itself",0
" full recursion, in addition to making the aforementioned tail recursive call, adds two additional recursive calls; bstep and lshift are made tail recursive",0
" 9: else 10: function run(e, p2, a2)
a description of the inference procedure is given in algorithm 1",1
" the move operation has the following arguments:
arg 1 (pointer): pstacklo, pstackhi, pj , ppivot
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values",0
"
as mentioned in section 1, all approaches to neural programming today fare poorly on this generalization issue",0
" as a consequence, our learned neural programs empirically achieve perfect generalization from a very small number of training examples",0
"
topological sort",0
 there is a (changing) list of neural programs used to accomplish a given task,1
" for non-primitive subprograms, we can update the observation-to-observation mapping currently associated with the subprogram and then apply that mapping to the current set",1
"in this paper, we propose that the key abstraction of recursion is necessary for neural programs to generalize",0
" however, note that there is no provable guarantee that the non-recursive learned program will generalize to all inputs, whereas we show later that the recursive learned program has a provable guarantee of perfect generalization",1
 table 3 presents results on randomly generated arrays of varying length for the learned non-recursive and recursive programs,0
" as a consequence, our learned neural programs empirically achieve perfect generalization from a very small number of training examples",1
" for these programs, we re-use the appropriate program sets (the associated subprograms), and we refer the reader to the appendix of reed & de freitas (2016) for further details on the subprograms used in addition and bubble sort",0
g,0
 u varies with the number of vertices in the graph,1
we emphasize that the notion of a neural recursive program has not been presented in the literature before: this is our main contribution,0
" 2: initialize lo and hi to be 1 and n, where n is the length of a",1
g,1
" for these models, table 2 presents results on randomly generated dags of varying graph sizes (varying in the number of vertices)",0
" an example of part of an addition task trace, written in shorthand, is given in figure 1",0
" as before, the non-recursive learned program lacks a provable guarantee of generalization, whereas we show later that the recursive learned program has one",1
this material is in part based upon work supported by the national science foundation under grant no,0
 the model described in the training setup of section 4 (trained on 6 traces) was verified to be correct via the matching procedure described in section 4,1
"
to evaluate a neural network that learns a neural program to accomplish a certain task, one common evaluation metric is how well the learned model m generalizes",0
" we hypothesize that the reason for this is that the neural network learns to spuriously depend on specific characteristics of the training examples that are irrelevant to the true program semantics, such as length of the training inputs, and thus fails to generalize to more complex inputs",0
" the space of possible programs is extremely large, and composing a program that performs robustly on the infinite space of possible inputs is difficult—in part because it is impractical to obtain enough training examples to easily disambiguate amongst all possible programs",0
g,0
" for tasks other than addition, it is difficult to analytically determine the verification set, so instead, we randomly generate input candidates until they completely cover the base cases and reduction rules",1
" our implementation uses two stacks qstacklo and
qstackhi, each in ru , that store the arguments to the recursive quicksort calls in algorithm 3; before each recursive call, the appropriate arguments are popped off the stack and written to plo and phi",1
" if it is too small, then execution of the program on some input might require evaluation of m(i) on some i /∈ v , and so the behavior ofm(i) might deviate from p (i)",1
" our results show that for all learned, to-be-verified neural programs, they all produced the same traces as those produced by p on the verification set",0
" 4: function toposort(dag) 5: while there is still a white vertex u: do 6: color[u] = grey 7: vactive = u 8: do 9: if vactive has a white child v then 10: color[v] = grey 11: push vactive onto s 12: vactive = v 13: else 14: color[vactive] = black 15: write vactive to result 16: if s is empty then pass 17: else pop the top vertex off s and set it to vactive 18: while s is not empty
topological sort",1
"
formally, verification consists of proving the following theorem:
∀i ∈ v,m(i) ⇓ p (i) where i denotes a sequence of step inputs (within one function call), v denotes the set of valid sequences of step inputs, m denotes the neural network model, p denotes the correct program, and p (i) denotes the next step output from the correct program",0
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,0
" this guarantees that the step input sequences considered during verification contain all step input sequences which arise during execution of an unseen problem in f , leading to generalization to any problem in f ",1
" in fact, it is one of our future directions to explore new ways to train a recursive neural program without providing explicit training execution traces or with only partial or non-recursive traces",1
" in particular, given that a recursion is defined by two properties as mentioned before, the base cases and the set of reduction rules, we can prove a recursive neural program generalizes perfectly if we can prove that (1) it performs correctly on the base cases; (2) it learns the reduction rules correctly",0
" note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values, and arg 2 can only take one of 7 values",0
"when constructing a neural network for the purpose of learning a program, there are two orthogonal aspects to consider",1
"6) on the verification set, and ensure they match the traces produced by the true program p ",0
" both the non-recursive and recursive learned programs generalize on all graphs we tried, up to 120 vertices",0
"
arg 2 (object to copy): env stack lo peek, env stack hi peek, phi, plo, ppivot − 1, ppivot + 1, reset
env stack lo peek and env stack hi peek represent qstacklo(pstacklo − 1) and qstackhi(pstackhi − 1), respectively",1
 we use the lomuto partition scheme; the logic for the recursive trace is shown in algorithm 3,0
 reset represents a −∞ value,1
 this procedure verifies that all base cases and reductions are handled properly by the model via explicit tests,1
" for each graph size, we test the learned programs on 30 randomly generated dags",0
"
the npi accesses an external environment, q, which varies according to the task",1
" for purposes of verification, we replace the domain-specific encoder with the following:
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1) ≤ q(1, i2), 1 ≤ i1 ≤ length, 1 ≤ i2 ≤ length, i3 == length, at(1), at(2), at(3)]),which directly exposes which of the two values pointed to is larger",0
g,1
" 2: initialize lo and hi to be 1 and n, where n is the length of a",1
"
the non-recursive trace loops on cycles of bubble and reset, which logically represents one bubble sweep through the array and reset of the two bubble pointers to the very beginning of the array, respectively",0
" provably perfect generalization implies the model will behave correctly, given any valid input",1
", the trace corresponding to the array [3,2])",0
" provably perfect generalization implies the model will behave correctly, given any valid input",1
"
grade-school addition",0
" in computer science, recursion (as opposed to iteration) involves solving a larger problem by combining solutions to smaller instances of the same problem",0
" for example, a step input-step output pair in lines 2 and 3 of the left-hand side of figure 1 is (add1, write out 1)",0
"in this paper, we propose that the key abstraction of recursion is necessary for neural programs to generalize",0
"
in order to demonstrate that recursion can help learn and generalize better, we trained a non-recursive and recursive model on just a single execution trace generated from a graph containing 5 nodes3 for the topological sort task",0
" we note that an earlier version of quicksort we tried lacked primitive operations to directly move a pointer to another, and therefore needed more functions and observations",0
" the pointer at index i3 represents a counter internal to the environment that is incremented once after each pass of the algorithm (one cycle of bubble and reset); when incremented a number of times equal to the length of the array, the flag i3 == length becomes true and terminates the entire algorithm ",0
"
base cases and reduction rules for quicksort",1
" this makes it so that we only need to consider the vertices in the subgraph that are currently relevant for computing the sort, allowing simpler reasoning about behavior for large graphs",0
 stack pop pushes −∞ values to qstacklo and qstackhi,0
 some functional programming languages go so far as not to define any looping constructs but rely solely on recursion to enable repeated execution of the same code,0
"
we construct input problems by splitting into two cases: one case in which the left column contains a null value and another in which the left column does not contain any null values",1
" k is set to 11, to denote the range of possible numbers (0 through 9), along with the start/end token (represented with the same encoding) which is observed when a pointer reaches beyond the bounds of the input",0
 ,0
" the contents ofqresult andqstack are not exposed directly through the domain-specific encoder; rather, we define primitive functions which manipulate these scratch-pads",1
"
as a concrete instantiation, we show in this paper that we can enable recursive neural programs in the npi model, and thus enable perfectly generalizable neural programs for tasks such as sorting where the original, non-recursive npi program fails",1
" as an application, we incorporate recursion into the neural programmer-interpreter architecture and consider four sample tasks: grade-school addition, bubble sort, topological sort, and quicksort",0
 table 4 also demonstrates that the (verified) recursive program generalizes perfectly,0
"
the non-recursive trace loops on cycles of bubble and reset, which logically represents one bubble sweep through the array and reset of the two bubble pointers to the very beginning of the array, respectively",1
"
base cases and reduction rules for bubble sort",0
"
we note that for tasks with very large input domains, such as ones involving mnist digits or speech samples, the state space of base cases and reduction rules could be prohibitively large, possibly infinite",0
"
the operations we need to be concerned most about are carry and lshift, which induce partial environment states spanning two columns",1
"as discussed in section 2, the neural programmer-interpreter (npi) is an instance of a neural programmer architecture and hence it naturally supports recursion",0
 the maximum problem length in this training set is 3 (e,0
" thus, any general neural programming architecture supporting such a call structure can be made to support recursion",0
 ,1
" lshift moves the four pointers to the left, to move to the next column",1
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,0
" the assumption of leading 0’s can be easily removed, at the cost of slightly increasing the size of the verification set",0
", the trace corresponding to the problem “109 + 101”)",0
 ,1
" recall that this verification procedure cannot be performed for the non-recursive versions, since the propagation of the hidden state in the core lstm module makes reasoning difficult and so we would need to check an unbounded number of examples",0
this material is in part based upon work supported by the national science foundation under grant no,0
" for each length, we test each program on 30 randomly generated problems",1
 ,0
3 to construct v and then empirically create a verification set which covers v ,0
training neural networks to synthesize robust programs from a small number of examples is a challenging task,1
"
in this paper, we propose that recursion is an important concept for neural programs as well",0
" the write operation has the following arguments:
arg 1 (main action): color curr, color next, active start, active neighb, active stack, save, stack push, stack pop, result
color curr colors vactive, color next colors vertex dag[vactive][childlist[vactive]], active start writes pstart to vactive, active neighb writes dag[vactive][childlist[vactive]] to vactive, active stack writes qstack(pstack) to vactive, save writes vactive to vsave, stack push pushes vactive to the top of the stack, stack pop writes a null value to the top of the stack, and result writes vactive to qresult(presult)",1
" we prove every member of f is added properly, given that subproblems s = {anan−1 + bnbn−1, an−1an−2 + bn−1bn−2, ",0
"
we would also like to point out that in this paper, we provide as an example one way to train a recursive neural program, by providing a certain training execution trace to the npi model",1
" in future work, we would like to decrease supervision and construct models that are capable of coming up with recursive abstractions themselves",1
" in particular, recursion can be implemented as a program calling itself",1
"
in this general neural programming architecture, we show it is easy to support recursion",0
" the write operation has the following arguments:
arg 1 (object to write): env stack lo, env stack hi, phi, plo
env stack lo and env stack hi represent qstacklo(pstacklo) and qstackhi(pstackhi), respectively",1
"
one common strategy to improve generalization is to use curriculum learning, where the model is trained on inputs of gradually increasing complexity",0
 the training set for addition contains 200 traces,0
" in particular, recursion can be implemented as a program calling itself",0
"
in what follows, we describe the way in which we constructed npi training traces so as to make them contain recursive elements and thus enable npi to learn recursive programs",0
 the training set for addition contains 200 traces,0
" we hypothesize that the reason for this is that the neural network learns to spuriously depend on specific characteristics of the training examples that are irrelevant to the true program semantics, such as length of the training inputs, and thus fails to generalize to more complex inputs",0
" both the non-recursive and recursive learned programs generalize on all graphs we tried, up to 120 vertices",0
"
algorithm 2 depth first search topological sort 1: color all vertices white",1
 add1 is a subprogram that adds the current column (writing the appropriate digit to the output row and carrying a bit to the next column if needed),0
 ,0
g,0
"
inside bubble and reset, there are two operations that can be made recursive",1
"
is critical for implementing recursion, since it permits us to restrict our attention to the currently relevant recursive call, ignoring irrelevant details about other contexts",0
" as described in the verification procedure, it is possible to prove our learned recursive program generalizes perfectly by testing on an appropriate set of problem inputs, i",0
"
note that the recursion for quicksort is not purely tail recursive and therefore represents a more complex kind of recursion that is harder to learn than in the previous tasks",1
"
arg 2 (increment or decrement): up, down
swap",0
" as a consequence, our learned neural programs empirically achieve perfect generalization from a very small number of training examples",0
" for example, a step input-step output pair in lines 2 and 3 of the left-hand side of figure 1 is (add1, write out 1)",1
" nevertheless, we would like the model to quickly learn to represent the right semantics of the underlying program from a small number of training examples, not an exhaustive number of them",0
" as described in the verification procedure, it is possible to prove our learned recursive program generalizes perfectly by testing on an appropriate set of problem inputs, i",0
"
inside bubble and reset, there are two operations that can be made recursive",0
"
the dag is represented as an adjacency list where dag[i][j] refers to the j-th child of vertex i",1
"2 at any given time, the npi has access to values pointed to by four pointers in each of the four rows, represented by q(1, i1), q(2, i2), q(3, i3), and q(4, i4)",0
" for bubble sort, the domain-specific encoder is
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1), q(1, i2), i3 == length, at(1), at(2), at(3)]),
where the environment q ∈ r1×n×k is a scratch-pad that contains 1 row, to represent the state of the array as sorting proceeds in-place, andn columns",1
" in fact, it is one of our future directions to explore new ways to train a recursive neural program without providing explicit training execution traces or with only partial or non-recursive traces",1
" the non-recursive and partially recursive versions are unable to sort long arrays, beyond length 8",0
 ,0
" formally, a function exhibits recursive behavior when it possesses two properties: (1) base cases— terminating scenarios that do not use recursion to produce answers; (2) a set of rules that reduces all other problems toward the base cases",1
"
there are 6 pointers (plo, phi, pstacklo, pstackhi, ppivot, pj)",1
"
an alternative way of representing the environment slice is to expose the values of the absolute vertices to the model; however, this makes it difficult to scale the model to larger graphs, since large vertex values are not seen during training time",1
" however, note that there is no provable guarantee that the non-recursive learned program will generalize to all inputs, whereas we show later that the recursive learned program has a provable guarantee of perfect generalization",0
" the arrow in the theorem refers to evaluation, as in big-step semantics",0
"
the operations we need to be concerned most about are carry and lshift, which induce partial environment states spanning two columns",1
6,1
" for example, a step input-step output pair in lines 2 and 3 of the left-hand side of figure 1 is (add1, write out 1)",0
" hence, if all subproblems in s are added correctly, we have proven that any member of f will be added correctly, thus eliminating an infinite family of inputs that need to be tested",0
3,1
" as this complexity interfered with determining the base cases and reductions, we changed the algorithm to its current form",0
" 4: function toposort(dag) 5: while there is still a white vertex u: do 6: color[u] = grey 7: vactive = u 8: do 9: if vactive has a white child v then 10: color[v] = grey 11: push vactive onto s 12: vactive = v 13: else 14: color[vactive] = black 15: write vactive to result 16: if s is empty then pass 17: else pop the top vertex off s and set it to vactive 18: while s is not empty
topological sort",0
" the write operation has the following arguments:
arg 1 (object to write): env stack lo, env stack hi, phi, plo
env stack lo and env stack hi represent qstacklo(pstacklo) and qstackhi(pstackhi), respectively",1
 this is due to the properties of the training set – we trained on 2 traces synthesized from arrays of length 7 and 1 trace synthesized from an array of length 6,0
" given verification sets that cover all the base cases and reduction rules, we can provide proofs that these learned programs generalize perfectly",0
" more specifically, when m is trained on simpler inputs, such as inputs of a small length, the generalization metric evaluates how well m will do on more complex inputs, such as inputs of much longer length",0
" we call this set of inputs the verification set, sv ",0
"as there is no public implementation of npi, we implemented a version of it in keras that is as faithful to the paper as possible",1
 npi then outputs the return probability and next program and arguments to execute,1
"
in this general neural programming architecture, we show it is easy to support recursion",0
"
an alternative method is to run p on many different program inputs and then observe step input sequences which occur, to create v ",0
" 16: p = partition(a, lo, hi) 17: push p+ 1 and hi to slo and shi",1
g,0
" we note that an earlier version of quicksort we tried lacked primitive operations to directly move a pointer to another, and therefore needed more functions and observations",0
" formally, the npi is represented by the following set of equations:
st = fenc(et, at)
ht = flstm(st, pt, ht−1)
rt = fend(ht), pt+1 = fprog(ht), at+1 = farg(ht)
t is a subscript denoting the time-step; fenc is a domain-specific encoder (to be described later) that takes in the environment slice et and arguments at; flstm represents the core module, which takes in the state st generated by fenc, a program embedding pt ∈ rp , and hidden lstm state ht; fend decodes the return probability rt; fprog decodes a program key embedding pt+1;1 and farg decodes arguments at+1",1
" in order to compensate for this, ppivot and pj require special functions (move pivot lo and move j lo) to properly set them to lo in lines 11 and 12 of the partition function in algorithm 3",0
" the corresponding edge list is [(1, 2), (1, 5), (2, 4), (2, 5), (3, 5)]",0
" given verification sets that cover all the base cases and reduction rules, we can provide proofs that these learned programs generalize perfectly",0
" for each function we use to implement the recursive version of topological sort, we need to consider the set of possible environment observation sequences we can create from all valid inputs and test that the learned program produces the correct behavior on each of these inputs",0
" at any given time, the npi has access to the values referred to by two pointers, represented by q(1, i1) and q(1, i2),",0
"
quicksort",1
" for each task, given the verification set, we check the traces and results of the learned, to-be-verified neural program (described in section 4",1
"
to construct this set, by using the reference implementation of each subprogram, we construct a mapping between two sets of environment observations: the first set consists of all observations that can occur at the beginning of a particular subprogram’s invocation, and the second set contains the observations at the end of that subprogram",1
" in this pair, the step input runs a subprogram add1 that has no arguments, and the step output contains a program write that has arguments of out and 1",1
"
in order to demonstrate that recursion can help learn and generalize better, for addition, we trained only on traces for 5 arbitrarily chosen 1-digit addition sum examples",1
g,0
 we then construct a verification set of size 73 by ensuring that randomly generated graphs cover the analytically derived v ,0
" we performed experiments on the partially recursive version in order to examine what happens when only one recursive call is implemented, when in reality three are required for perfect generalization",0
" a single element of an execution trace consists of a step input-step output pair, which can be synthesized from algorithm 1: this corresponds to, for a given time-step, the step input tuple (e, p, a) and step output tuple (r, p2, a2)",1
" this guarantees that the step input sequences considered during verification contain all step input sequences which arise during execution of an unseen problem in f , leading to generalization to any problem in f ",1
" for problems where the base cases may be extremely large or infinite, such as certain forms of motor control, recursion can still help reduce the problem of generalization to these two aspects and make the generalization problem significantly simpler to handle and reason about",0
 this paper is the first (to our knowledge) to investigate the important problem of provable generalization properties of neural programs,0
 our result shows that the training procedure and the npi architecture is capable of generalizing from the step input-output pairs seen in the training data to the unseen ones present in the verification set,1
" here as a concrete and general example, we consider a general neural programming architecture (npa), similar to neural programmer-interpreter (npi) in reed & de freitas (2016)",0
 errors arise due to undesirable and uninterpretable dependencies and associations the architecture learns to store in some high-dimensional hidden state,0
" for each graph size, we test the learned programs on 30 randomly generated dags",0
" in this version, there is a dependence on length: bstep and lshift are called a number of times equivalent to one less than the length of the input array, in bubble and reset respectively",0
"
the training set for bubble sort contains 100 traces, with maximum problem length of 2 (e",1
" at any given time, the npi has access to the values referred to by two pointers, represented by q(1, i1) and q(1, i2),",0
" in order to compensate for this, ppivot and pj require special functions (move pivot lo and move j lo) to properly set them to lo in lines 11 and 12 of the partition function in algorithm 3",1
" we performed experiments on the partially recursive version in order to examine what happens when only one recursive call is implemented, when in reality three are required for perfect generalization",1
"
by nature, recursion reduces the complexity of a problem to simpler instances",0
" for many problems, the base cases and reduction rules usually consist of a finite (often small) number of cases",0
" thus, any general neural programming architecture supporting such a call structure can be made to support recursion",1
"
inside bubble and reset, there are two operations that can be made recursive",0
 indentation indicates the stack is one level deeper than before,0
 ,0
"
directed acyclic graphs are structurally more diverse than inputs in the two tasks of grade-school addition and bubble sort",1
"
since npi naturally supports the notion of recursion, a key question is how to enable npi to learn recursive programs",0
" as this complexity interfered with determining the base cases and reductions, we changed the algorithm to its current form",1
"
the same set of problems was used to generate the training traces for all formulations of the task, for non-recursive and recursive versions",1
" at any given time, the npi has access to the values referred to by two pointers, represented by q(1, i1) and q(1, i2),",0
" if the step output specifies a primitive function call, we need to reason about how it can affect the environment so as to change the observation in the next step input",0
" , n , where the dag contains n vertices",1
" m is considered to have perfect generalization if m can give the right answer for any input, such as inputs of arbitrary length",0
" formally, a function exhibits recursive behavior when it possesses two properties: (1) base cases— terminating scenarios that do not use recursion to produce answers; (2) a set of rules that reduces all other problems toward the base cases",1
" the write operation has the following arguments:
arg 1 (main action): color curr, color next, active start, active neighb, active stack, save, stack push, stack pop, result
color curr colors vactive, color next colors vertex dag[vactive][childlist[vactive]], active start writes pstart to vactive, active neighb writes dag[vactive][childlist[vactive]] to vactive, active stack writes qstack(pstack) to vactive, save writes vactive to vsave, stack push pushes vactive to the top of the stack, stack pop writes a null value to the top of the stack, and result writes vactive to qresult(presult)",1
"
algorithm 1 neural programming inference 1: inputs: environment observation e, program p, arguments a, stop threshold α 2: function run(e, p, a) 3: h← 0, r ← 0 4: while r < α do 5: s← fenc(e, a), h← flstm(s, p, h) 6: r ← fend(h), p2 ← fprog(h), a2 ← farg(h) 7: if p is a primitive function then 8: e← fenv(e, p, a)",0
 we then construct a verification set of size 73 by ensuring that randomly generated graphs cover the analytically derived v ,1
" if no white vertex is found, the entire execution is terminated move none write write a value either to environment (e",0
" the move operation has the following arguments:
arg 1 (pointer): pstacklo, pstackhi, pj , ppivot
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values",0
", to change the value of vactive) none described below move move a pointer (e",0
" , a1a0 + b1b0} are added properly",0
" consequently, it is infeasible to construct a verification set that covers all cases, and the verification procedure we have described is inadequate",1
" the npi model has three learnable components: a task-agnostic core, a program-key embedding, and domain-specific encoders that allow the npi to operate in diverse environments",0
g,0
" the 4 environment observations aid with control flow; qstacklo(pstacklo− 1) < qstackhi(pstackhi− 1) implements the lo < hi comparison in line 5 of algorithm 3, pstacklo == 1 checks if the stacks are empty in line 18 of algorithm 4, and the other observations (all involving ppivot or pj) deal with logic in the partition function",1
" for bubble sort, the domain-specific encoder is
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1), q(1, i2), i3 == length, at(1), at(2), at(3)]),
where the environment q ∈ r1×n×k is a scratch-pad that contains 1 row, to represent the state of the array as sorting proceeds in-place, andn columns",1
" 3: 4: function partition(a, lo, hi) 5: pivot = lo 6: for j ∈ [lo, hi− 1] : do 7: if a[j] ≤ a[hi] then 8: swap a[pivot] with a[j] 9: pivot = pivot+ 1 0: swap a[pivot] with a[hi] 11: return pivot 12: 13: function quicksort(a, lo, hi) 14: while slo and shi are not empty: do 15: pop states off slo and shi, writing them to lo and hi",1
" partial recursion only adds a tail recursive call to bubblesort after bubble and reset, similar to the tail recursive call described previously for addition",0
"
base cases and reduction rules for topological sort",0
" the partial recursion is not enough for perfect generalization, as will be presented later in section 4",0
 ,0
" in this pair, the step input runs a subprogram add1 that has no arguments, and the step output contains a program write that has arguments of out and 1",0
"
grade school addition",0
" in computer science, recursion (as opposed to iteration) involves solving a larger problem by combining solutions to smaller instances of the same problem",1
 a1a0 + bnbn−1 ,1
" at each time step, the core controller can decide to select one of the programs to call with certain arguments",1
" using this modification, we constructed a verification set consisting of one array of size 10",1
" the pointers at index i1 and i2 are used to compare the pair of numbers considered during the bubble sweep, swapping them if the number at i1 is greater than that in i2",0
"
through careful reasoning about the possible set of environment observations created by all valid inputs, and how each of the operations in the execution trace affects the environment, we can construct v using the procedure described in section 3",0
3,1
" childlist ∈ nu is a vector of pointers, where childlist[i] points to the next child under consideration for vertex i",1
g,1
" we further have qresult ∈ nu , a scratch-pad which contains the sorted list of vertices at the end of execution, and qstack ∈ nu , which serves the role of the stack s in algorithm 2",0
" we then construct problem configurations that span all possible valid environment states (for instance, in order to force the carry bit in a column to be 1, one can add the sum “1+9” in the column to the right)",1
" naı̈vely, we might expect to synthesize and test an input for any sequence created by combining the four possible colors in two variables and another boolean variable for whether the stack is empty (so 32 possible observations at any point), but for various reasons, most of these combinations are impossible to occur at any given point in the execution trace",1
" if a white vertex is found, set pstart to point to it; this signifies the start of a traversal of a new connected component",0
" however, models that make use of this strategy eventually fail after a certain level of complexity (e",1
"we describe how models trained with recursive traces can be proven to generalize, by using the verification procedure described in section 3",0
" consequently, there is no concept of length relevant to the problem, which has traditionally been an important focus of length-based curriculum learning",1
 ,0
"
the non-recursive trace loops on cycles of bubble and reset, which logically represents one bubble sweep through the array and reset of the two bubble pointers to the very beginning of the array, respectively",1
" if the program is primitive, the next environmental state et+1 will be affected by pt and at, i",1
" both the non-recursive and recursive learned programs generalize on all graphs we tried, up to 120 vertices",1
" we performed experiments on the partially recursive version in order to examine what happens when only one recursive call is implemented, when in reality three are required for perfect generalization",0
" this makes it so that we only need to consider the vertices in the subgraph that are currently relevant for computing the sort, allowing simpler reasoning about behavior for large graphs",1
" finally, the boolean pstack == 1 is used to check whether the stack is empty in line 18 of algorithm 2",0
 we leave this as future work to devise a verification procedure more appropriate to this setting,0
"
quicksort",0
"6) on the verification set, and ensure they match the traces produced by the true program p ",1
" for problems where the base cases may be extremely large or infinite, such as certain forms of motor control, recursion can still help reduce the problem of generalization to these two aspects and make the generalization problem significantly simpler to handle and reason about",1
"
as in line 13 of the right-hand side of figure 1",0
as mentioned in section 2,0
" for these models, table 2 presents results on randomly generated dags of varying graph sizes (varying in the number of vertices)",0
"
under the assumption that there are no leading 0’s (except in the case of single digits) and the two numbers to be added have the same number of digits, the verification set for addition contains 20,181 input problems",1
", qstacklo or qstackhi) or to pointer (e",0
 we adapt machinery from the original paper slightly to fit our needs,0
"1, we hypothesize the non-recursive programs do not generalize well because they have learned spurious dependencies specific to the training set, such as length of the input problems",0
" for the recursive formulation of addition, we analytically construct the set of input problems that cover all base cases and reduction rules",0
6,0
" any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of national science foundation and darpa",1
" in computer science, recursion (as opposed to iteration) involves solving a larger problem by combining solutions to smaller instances of the same problem",1
", to color a vertex) or variable (e",1
" more specifically, when m is trained on simpler inputs, such as inputs of a small length, the generalization metric evaluates how well m will do on more complex inputs, such as inputs of much longer length",0
 the core controller acts as a dispatcher for the programs,0
"we train using the adam optimizer and use a 2-layer lstm and task-specific state encoders for the external environments, as described in reed & de freitas (2016)",1
" any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of national science foundation and darpa",1
" 16: p = partition(a, lo, hi) 17: push p+ 1 and hi to slo and shi",1
" because the context of the caller is stored on a stack when it calls another program and the callee starts in a fresh context, this enables recursion simply by allowing a program to call itself",0
" at each time step, the core controller can decide to select one of the programs to call with certain arguments",0
"
through careful reasoning about the possible set of environment observations created by all valid inputs, and how each of the operations in the execution trace affects the environment, we can construct v using the procedure described in section 3",0
"
the npi accesses an external environment, q, which varies according to the task",0
"
in this paper, we propose to resolve these issues by explicitly incorporating recursion into neural architectures",1
" at any given time, the npi has access to the values referred to by two pointers, represented by q(1, i1) and q(1, i2),",0
" however, the authors of npi did not consider explicitly the notion of recursion and as a consequence, did not learn recursive programs",1
" k is set to 11, to represent the range of 10 possible digits, along with a token representing the end of input",1
 we choose to implement a topological sort task for graphs,1
"
by reasoning about the possible set of environment observations created by all valid inputs, we construct v using the procedure described in section 3",1
" when the program is called, the current context including the caller’s memory state is stored on a stack; when the program returns, the stored context is popped off the stack to resume execution in the previous caller’s context",1
 ,0
"
note that the training set can often be considerably smaller than the verification set, and despite this, the learned model can still pass the entire verification set",0
" the contents ofqresult andqstack are not exposed directly through the domain-specific encoder; rather, we define primitive functions which manipulate these scratch-pads",1
" this agrees with the generalization of non-recursive addition in reed & de freitas (2016), where they reported generalization up to 3000 digits",0
", to change the value of vactive) none described below move move a pointer (e",0
"
for topological sort, the domain-specific encoder is
fenc(dag,qcolor, pstack, pstart, vactive, childlist, at)
=mlp ([qcolor(pstart), qcolor(dag[vactive][childlist[vactive]]), pstack == 1, at(1), at(2), at(3)]),
where qcolor ∈ ru×4 is a scratch-pad that contains u rows, each containing one of four colors (white, gray, black, invalid) with one-hot encoding",0
" the only model, to our knowledge, that does not train on input-output pairs is the neural programmer-interpreter (reed & de freitas, 2016), which trains on synthetic execution traces",1
"
algorithm 3 recursive quicksort 1: initialize an array a to sort",0
 table 3 presents results on randomly generated arrays of varying length for the learned non-recursive and recursive programs,1
" without using a recursive program, such a proof is not possible, because the non-recursive program runs on an arbitrarily long addition problem that creates correspondingly long sequences of step inputs; in the non-recursive formulation of addition, add calls add1 a number of times that is dependent on the length of the input",1
" in this section, we give a brief review of the npi architecture from reed & de freitas (2016) as background",0
 indentation indicates the stack is one level deeper than before,0
 stack pop pushes −∞ values to qstacklo and qstackhi,0
" the assumption of leading 0’s can be easily removed, at the cost of slightly increasing the size of the verification set",0
"
grade school addition",0
" hence, if all subproblems in s are added correctly, we have proven that any member of f will be added correctly, thus eliminating an infinite family of inputs that need to be tested",1
" plo and phi point to the lo and hi indices of the array, as in algorithm 3",0
"
in addition, none of the current approaches to neural programming provide a method or even aim to enable provable guarantees about generalization",0
" for each length, we test each program on 30 randomly generated problems",0
"
an alternative way of representing the environment slice is to expose the values of the absolute vertices to the model; however, this makes it difficult to scale the model to larger graphs, since large vertex values are not seen during training time",0
" without using a recursive program, such a proof is not possible, because the non-recursive program runs on an arbitrarily long addition problem that creates correspondingly long sequences of step inputs; in the non-recursive formulation of addition, add calls add1 a number of times that is dependent on the length of the input",0
" fa8750-15-2-0104, and berkeley deep drive",0
" the move operation has the following arguments:
arg 1 (pointer): presult, pstack, pstart, childlist[vactive], childlist[vsave]
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 5 values",1
 the logic for the non-recursive trace is shown in algorithm 4 in the appendix,1
" the 4 environment observations aid with control flow; qstacklo(pstacklo− 1) < qstackhi(pstackhi− 1) implements the lo < hi comparison in line 5 of algorithm 3, pstacklo == 1 checks if the stacks are empty in line 18 of algorithm 4, and the other observations (all involving ppivot or pj) deal with logic in the partition function",0
" however, note that there is no provable guarantee that the non-recursive learned program will generalize to all inputs, whereas we show later that the recursive learned program has a provable guarantee of perfect generalization",1
" thus, recursion helps decompose a problem and makes it easier to reason about a program’s behavior for previously unseen situations such as longer inputs",0
g,0
"
the training set for bubble sort contains 100 traces, with maximum problem length of 2 (e",0
 npi then outputs the return probability and next program and arguments to execute,1
" this is possible if and only if the graph has no directed cycles; that is to say, it must be a directed acyclic graph (dag)",0
2,0
 we instantiated recursion for the neural programmerinterpreter by changing the training traces,1
" for each graph size, we test the learned programs on 30 randomly generated dags",0
" in particular, this condition is satisfied by npi, and thus the npi model naturally supports recursion (even though the authors of npi did not consider this aspect explicitly)",0
" even though the earlier version also generalized just as well in practice, relatively small differences in the formulation of the traces and the environment observations can drastically change the difficulty of verification",1
we now report on generalization for the varying tasks,1
" when the program is called, the current context including the caller’s memory state is stored on a stack; when the program returns, the stored context is popped off the stack to resume execution in the previous caller’s context",1
 stack push call2 pushes pivot + 1 and hi to qstacklo and qstackhi,1
" even though the earlier version also generalized just as well in practice, relatively small differences in the formulation of the traces and the environment observations can drastically change the difficulty of verification",0
"we describe how models trained with recursive traces can be proven to generalize, by using the verification procedure described in section 3",0
 this makes it difficult to reason about what the model will do when given complex inputs,1
" as before, the non-recursive learned program lacks a provable guarantee of generalization, whereas we show later that the recursive learned program has one",0
" in future work, we would like to decrease supervision and construct models that are capable of coming up with recursive abstractions themselves",1
"
base cases and reduction rules for topological sort",0
" recursion is an important concept in programming languages and a critical tool to
reduce the complexity of programs",0
 we outline how to construct this set,1
3,0
 the first is the actual model architecture,1
3,0
" we hypothesize that the reason for this is that the neural network learns to spuriously depend on specific characteristics of the training examples that are irrelevant to the true program semantics, such as length of the training inputs, and thus fails to generalize to more complex inputs",1
" in our experiments, we only present dag’s as inputs and represent the vertices as values ranging from 1, ",1
" in this version of curriculum learning, even though the inputs are gradually becoming more complex, the semantics of the program is succinct and does not change",1
" finally, the boolean pstack == 1 is used to check whether the stack is empty in line 18 of algorithm 2",0
" if it is too small, then execution of the program on some input might require evaluation of m(i) on some i /∈ v , and so the behavior ofm(i) might deviate from p (i)",1
" the pointers at index i1 and i2 are used to compare the pair of numbers considered during the bubble sweep, swapping them if the number at i1 is greater than that in i2",0
" to the best of our knowledge, this is the first time verification has been applied to a neural program, providing provable guarantees about its behavior",0
"
is critical for implementing recursion, since it permits us to restrict our attention to the currently relevant recursive call, ignoring irrelevant details about other contexts",0
" finally, the boolean pstack == 1 is used to check whether the stack is empty in line 18 of algorithm 2",1
" in particular, given that a recursion is defined by two properties as mentioned before, the base cases and the set of reduction rules, we can prove a recursive neural program generalizes perfectly if we can prove that (1) it performs correctly on the base cases; (2) it learns the reduction rules correctly",1
 this paper is the first (to our knowledge) to investigate the important problem of provable generalization properties of neural programs,0
" note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values, and arg 2 can only take one of 7 values",0
" that is to say, the network does not learn the true program semantics",0
" the outputs rt, pt+1, at+1 are used to determine the next action, as described in algorithm 1",0
" in all experiments, α is set to 0",0
"
we also report on generalization results for the non-recursive and recursive versions of this variant of bubble sort",0
" table 1 presents results on randomly generated arrays of varying length for the learned non-recursive, partially recursive, and full recursive programs",1
"
in addition, none of the current approaches to neural programming provide a method or even aim to enable provable guarantees about generalization",0
" in future work, we seek to enable more tasks with recursive structure",1
 we describe the recursive re-formulation of traces for two tasks from the original npi paper—grade-school addition and bubble sort,0
"as discussed in section 2, the neural programmer-interpreter (npi) is an instance of a neural programmer architecture and hence it naturally supports recursion",1
" that is to say, the network does not learn the true program semantics",1
" the partial recursion is not enough for perfect generalization, as will be presented later in section 4",0
" although the model is exposed to more and more data, it might learn spurious and overly complex representations of the program, as suggested in zaremba   (2016)",1
" our results show that for all learned, to-be-verified neural programs, they all produced the same traces as those produced by p on the verification set",1
" if the step output specifies a primitive function call, we need to reason about how it can affect the environment so as to change the observation in the next step input",0
" as with the others, we apply the procedure described in section 3",0
"
for addition, we analytically determine the verification set",1
" our implementation uses two stacks qstacklo and
qstackhi, each in ru , that store the arguments to the recursive quicksort calls in algorithm 3; before each recursive call, the appropriate arguments are popped off the stack and written to plo and phi",1
"
the three environment observations aid with control flow in algorithm 2",0
 the program terminates when seeing no numbers in the current column,0
"
formally, verification consists of proving the following theorem:
∀i ∈ v,m(i) ⇓ p (i) where i denotes a sequence of step inputs (within one function call), v denotes the set of valid sequences of step inputs, m denotes the neural network model, p denotes the correct program, and p (i) denotes the next step output from the correct program",0
 the original version of the bubblesort implementation exposes the values within the array,1
" k is set to 11, to represent the range of 10 possible digits, along with a token representing the end of input",0
 we instantiated recursion for the neural programmerinterpreter by changing the training traces,0
 the logic for the non-recursive trace is shown in algorithm 4 in the appendix,1
" the write operation has the following arguments:
arg 1 (object to write): env stack lo, env stack hi, phi, plo
env stack lo and env stack hi represent qstacklo(pstacklo) and qstackhi(pstackhi), respectively",0
" also the dag can have potentially more than one connected component, meaning it is necessary to transition between these components appropriately",1
" provably perfect generalization implies the model will behave correctly, given any valid input",0
" empirically, we observe that the learned recursive programs solve all valid inputs with 100% accuracy after training on a very small number of examples, out-performing previous generalization results",0
"as discussed in section 2, the neural programmer-interpreter (npi) is an instance of a neural programmer architecture and hence it naturally supports recursion",0
" thus, recursion helps decompose a problem and makes it easier to reason about a program’s behavior for previously unseen situations such as longer inputs",0
g,1
" in future work, we would like to decrease supervision and construct models that are capable of coming up with recursive abstractions themselves",1
" in fact, it is one of our future directions to explore new ways to train a recursive neural program without providing explicit training execution traces or with only partial or non-recursive traces",1
" qcolor(dag[vactive][childlist[vactive]]) refers to the color of the next child of vactive, used in the evaluation of the condition in the if branch in line 9 of algorithm 2",0
"
quicksort",0
" m is considered to have perfect generalization if m can give the right answer for any input, such as inputs of arbitrary length",1
", pstart or childlist[vactive]) up or down none described belowwrite",1
 we adapt machinery from the original paper slightly to fit our needs,0
 recursion can be implemented differently for different neural programming models,0
 recursion enables provable guarantees on neural programs’ behavior without needing to exhaustively enumerate all possible inputs to the programs,0
" for these models, table 2 presents results on randomly generated dags of varying graph sizes (varying in the number of vertices)",0
 figure 2 shows examples of traces for the different versions of bubble sort,0
" k is set to 11, to represent the range of 10 possible digits, along with a token representing the end of input",0
"
in addition, none of the current approaches to neural programming provide a method or even aim to enable provable guarantees about generalization",1
"
as in line 13 of the right-hand side of figure 1",1
" we make the trace recursive by adding a tail recursive call into the trace for the add program after calling add1 and lshift, the original paper uses k = 10, but we found it necessary to augment the range with an end token, in order to terminate properly",0
"
the operations we need to be concerned most about are carry and lshift, which induce partial environment states spanning two columns",1
"
grade school addition",1
g,0
 almost all architectures train on program input/output pairs,1
" a single element of an execution trace consists of a step input-step output pair, which can be synthesized from algorithm 1: this corresponds to, for a given time-step, the step input tuple (e, p, a) and step output tuple (r, p2, a2)",0
" numerous models have been proposed for learning programs; to name a few, this includes the differentiable neural computer (graves  , 2016), neural turing machine (graves  , 2014), neural gpu (kaiser & sutskever, 2015), neural programmer (neelakantan  , 2015), pointer network (vinyals  , 2015), hierarchical attentive memory (andrychowicz & kurach, 2016), and neural random access machine (kurach  , 2016)",0
 we leave this as future work to devise a verification procedure more appropriate to this setting,1
" this makes it so that we only need to consider the vertices in the subgraph that are currently relevant for computing the sort, allowing simpler reasoning about behavior for large graphs",0
training neural networks to synthesize robust programs from a small number of examples is a challenging task,0
"
we propose and describe our verification procedure",1
 this paper is the first (to our knowledge) to investigate the important problem of provable generalization properties of neural programs,1
"we show that if we incorporate recursion, the learned npi programs can achieve provably perfect generalization for different tasks",0
 indentation indicates the stack is one level deeper than before,0
"
we note that for tasks with very large input domains, such as ones involving mnist digits or speech samples, the state space of base cases and reduction rules could be prohibitively large, possibly infinite",0
" the core module of the npi is an lstm controller that takes as input a slice of the current external environment, via a set of pointers, and a program and arguments to execute",0
 the sequence of primitive operations (move and write operations) for the non-recursive and recursive versions are exactly the same,1
" the only model, to our knowledge, that does not train on input-output pairs is the neural programmer-interpreter (reed & de freitas, 2016), which trains on synthetic execution traces",0
" formally, the npi is represented by the following set of equations:
st = fenc(et, at)
ht = flstm(st, pt, ht−1)
rt = fend(ht), pt+1 = fprog(ht), at+1 = farg(ht)
t is a subscript denoting the time-step; fenc is a domain-specific encoder (to be described later) that takes in the environment slice et and arguments at; flstm represents the core module, which takes in the state st generated by fenc, a program embedding pt ∈ rp , and hidden lstm state ht; fend decodes the return probability rt; fprog decodes a program key embedding pt+1;1 and farg decodes arguments at+1",0
" we call this set of inputs the verification set, sv ",0
"
one common strategy to improve generalization is to use curriculum learning, where the model is trained on inputs of gradually increasing complexity",1
" childlist ∈ nu is a vector of pointers, where childlist[i] points to the next child under consideration for vertex i",0
"
is critical for implementing recursion, since it permits us to restrict our attention to the currently relevant recursive call, ignoring irrelevant details about other contexts",0
" as described in the verification procedure, it is possible to prove our learned recursive program generalizes perfectly by testing on an appropriate set of problem inputs, i",0
" we note that an earlier version of quicksort we tried lacked primitive operations to directly move a pointer to another, and therefore needed more functions and observations",1
" m , as described in algorithm 1, processes the sequence of step inputs by using an lstm",1
" for each function we use to implement the recursive version of topological sort, we need to consider the set of possible environment observation sequences we can create from all valid inputs and test that the learned program produces the correct behavior on each of these inputs",0
" for bubble sort, the domain-specific encoder is
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1), q(1, i2), i3 == length, at(1), at(2), at(3)]),
where the environment q ∈ r1×n×k is a scratch-pad that contains 1 row, to represent the state of the array as sorting proceeds in-place, andn columns",0
 we use the lomuto partition scheme; the logic for the recursive trace is shown in algorithm 3,0
" as a consequence, our learned neural programs empirically achieve perfect generalization from a very small number of training examples",1
" the non-recursive and partially recursive versions are unable to sort long arrays, beyond length 8",0
"
the training set for quicksort contains 4 traces, synthesized from arrays of length 5",0
"
quicksort",1
 pstacklo and pstackhi point to the top (empty) positions in qstacklo and qstackhi,0
"
note that the recursion for quicksort is not purely tail recursive and therefore represents a more complex kind of recursion that is harder to learn than in the previous tasks",0
 pstacklo and pstackhi point to the top (empty) positions in qstacklo and qstackhi,0
e,0
 ,1
" a single element of an execution trace consists of a step input-step output pair, which can be synthesized from algorithm 1: this corresponds to, for a given time-step, the step input tuple (e, p, a) and step output tuple (r, p2, a2)",1
" the space of possible programs is extremely large, and composing a program that performs robustly on the infinite space of possible inputs is difficult—in part because it is impractical to obtain enough training examples to easily disambiguate amongst all possible programs",1
" hence, if all subproblems in s are added correctly, we have proven that any member of f will be added correctly, thus eliminating an infinite family of inputs that need to be tested",1
" we made the assumption of equivalent lengths in order to parametrize the input format with respect to length, but this assumption can be removed as well",1
" when the program is called, the current context including the caller’s memory state is stored on a stack; when the program returns, the stored context is popped off the stack to resume execution in the previous caller’s context",0
3 to construct v and then empirically create a verification set which covers v ,0
3,1
"as there is no public implementation of npi, we implemented a version of it in keras that is as faithful to the paper as possible",1
" we call this set of inputs the verification set, sv ",0
 table 4 demonstrates that the accuracy of the non-recursive program degrades sharply when moving from arrays of length 7 to arrays of length 8,1
"
as mentioned in section 1, all approaches to neural programming today fare poorly on this generalization issue",1
" to make an analogy to mdps, this procedure is analogous to how value iteration obtains the correct value for each state starting from any initialization",0
" for each length, we test each program on 30 randomly generated problems",0
 we instantiated recursion for the neural programmerinterpreter by changing the training traces,1
" both the non-recursive and recursive learned programs generalize on all graphs we tried, up to 120 vertices",0
 table 3 presents results on randomly generated arrays of varying length for the learned non-recursive and recursive programs,0
"as discussed in section 2, the neural programmer-interpreter (npi) is an instance of a neural programmer architecture and hence it naturally supports recursion",0
" the recursive version of topological sort solves all graph instances we tried, from graphs of size 5 through 70",0
" the outputs rt, pt+1, at+1 are used to determine the next action, as described in algorithm 1",1
"
in order to demonstrate that recursion can help learn and generalize better, for addition, we trained only on traces for 5 arbitrarily chosen 1-digit addition sum examples",0
 ,1
"
to evaluate a neural network that learns a neural program to accomplish a certain task, one common evaluation metric is how well the learned model m generalizes",0
 we choose to implement a topological sort task for graphs,0
"
quicksort",0
" however, the authors of npi did not consider explicitly the notion of recursion and as a consequence, did not learn recursive programs",0
" empirically, we observe that the learned recursive programs solve all valid inputs with 100% accuracy after training on a very small number of examples, out-performing previous generalization results",0
 we find that recursion makes it easier for the network to learn the right program and generalize to unknown situations,1
" 16: p = partition(a, lo, hi) 17: push p+ 1 and hi to slo and shi",1
"1, we hypothesize the non-recursive programs do not generalize well because they have learned spurious dependencies specific to the training set, such as length of the input problems",1
 the maximum problem length in this training set is 3 (e,0
" in this version, there is a dependence on length: bstep and lshift are called a number of times equivalent to one less than the length of the input array, in bubble and reset respectively",1
" the recursive version can generalize perfectly to long problems constructed from these components (such as the sum “822+233”, where “8+2” and “2+3” are in the training set), but the non-recursive version fails to sum these long problems properly",1
" however, models that make use of this strategy eventually fail after a certain level of complexity (e",1
 the logic for the non-recursive trace is shown in algorithm 4 in the appendix,1
" the recursive version can generalize perfectly to long problems constructed from these components (such as the sum “822+233”, where “8+2” and “2+3” are in the training set), but the non-recursive version fails to sum these long problems properly",1
"
we would also like to point out that in this paper, we provide as an example one way to train a recursive neural program, by providing a certain training execution trace to the npi model",0
" for instance, for recursive addition, consider the family f of addition problems anan−1 ",1
"
as mentioned in section 1, all approaches to neural programming today fare poorly on this generalization issue",0
"
directed acyclic graphs are structurally more diverse than inputs in the two tasks of grade-school addition and bubble sort",1
"
through careful reasoning about the possible set of environment observations created by all valid inputs, and how each of the operations in the execution trace affects the environment, we can construct v using the procedure described in section 3",0
" this agrees with the generalization of non-recursive addition in reed & de freitas (2016), where they reported generalization up to 3000 digits",0
e,1
 reset represents a −∞ value,1
 ,0
" in this architecture, we consider a core controller, e",1
" to the best of our knowledge, this is the first time verification has been applied to a neural program, providing provable guarantees about its behavior",0
 the memory updates of these neural programs are so complex and interdependent that it is difficult to reason about the behaviors of the learned neural program under previously unseen situations (such as problems with longer inputs),0
 the degree for any vertex in the dag is variable,0
3,0
" the verification set can be very small, as we found a 10-element array ([8,2,1,2,0,8,5,8,3,7]) is sufficient to cover all of v ",1
" this is highly undesirable, since being able to provide the correct answer in all possible settings is one of the most important aspects of any learned neural program",0
" in this section, we give a brief review of the npi architecture from reed & de freitas (2016) as background",1
 this procedure verifies that all base cases and reductions are handled properly by the model via explicit tests,0
training neural networks to synthesize robust programs from a small number of examples is a challenging task,0
 add1 is a subprogram that adds the current column (writing the appropriate digit to the output row and carrying a bit to the next column if needed),0
"in this paper, we propose that the key abstraction of recursion is necessary for neural programs to generalize",1
" while this matches the description from reed & de freitas (2016), we found that this causes an unnecessary blowup in the size of v and makes it much more difficult to construct the verification set",0
"
the same set of problems was used to generate the training traces for all formulations of the task, for non-recursive and recursive versions",1
"program descriptions calls arguments toposort perform topological
sort on graph traverse, next start, write, move
none
traverse traverse graph until stack is empty check child, explore none check child check if a white child exists; if so, set childlist[vactive] to point to it move none explore repeatedly traverse subgraphs until stack is empty stack, check child, write, move none stack interact with stack, either pushing or popping write, move push, pop next start move pstart until reaching a white vertex",1
we now report on generalization for the varying tasks,0
 the first is the actual model architecture,0
"
arg 2 (increment or decrement): up, down1 // top level topological sort call 2 toposort() { 3 while (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 } 0 11 traverse() { 12 check_child() 13 explore() 14 } 15 16 check_child() { 17 while (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 18 move(childlist[vactive], up) 19 } 20 21 explore() { 22 do 23 if (qcolor(dag[vactive][childlist[vactive]]) is white): 24 write(color_next, color_grey) 25 stack(push) 26 write(save) 27 write(active_neighb) 28 move(childlist[vsave], up) 29 else: 30 write(color_curr, color_black) 31 write(result) 32 move(presult, up) 33 if(pstack == 1): 34 break 35 else: 36 stack(pop) 37 check_child() 38 while (true) 39 } 40 41 stack(op) { 42 if (op == push): 43 write(stack_push) 44 move(pstack, up) 45 46 if (op == pop): 47 write(active_stack) 48 write(stack_pop) 49 move(pstack, down) 50 } 51 52 next_start() { 53 while(qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 54 move(pstart, up) 55 }altered recursive functions  // top level topological sort call 2 toposort() { 3 if (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 toposort() // recursive call 0 } 11 12 check_child() { 13 if (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 14 move(childlist[vactive], up) 15 check_child() // recursive call 16 } 17 18 explore() { 19 if (qcolor(dag[vactive][childlist[vactive]]) is white): 20 write(color_next, color_grey) 21 stack(push) 22 write(save) 23 write(active_neighb) 24 move(childlist[vsave], up) 25 else: 26 write(color_curr, color_black) 27 write(result) 28 move(presult, up) 29 if(pstack == 1): 30 return 31 else: 32 stack(pop) 33 check_child() 34 explore() // recursive call 35 } 36 37 next_start() { 38 if (qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 39 move(pstart, up) 40 next_start() // recursive call 41 }algorithm 4 iterative quicksort 1: initialize an array a to sort and two empty stacks slo and shi",0
" for grade-school addition, the domain-specific encoder is
fenc(q, i1, i2, i3, i4, at) =mlp ([q(1, i1), q(2, i2), q(3, i3), q(4, i4), at(1), at(2), at(3)]),
where the environment q ∈ r4×n×k is a scratch-pad that contains four rows (the first input number, the second input number, the carry bits, and the output) and n columns",0
" also the dag can have potentially more than one connected component, meaning it is necessary to transition between these components appropriately",0
" a topological sort is a linear ordering of vertices such that for every directed edge (u, v) from u to v, u comes before v in the ordering",0
" in all experiments, α is set to 0",1
" in order to compensate for this, ppivot and pj require special functions (move pivot lo and move j lo) to properly set them to lo in lines 11 and 12 of the partition function in algorithm 3",0
 3: begin traversing from vertex 1 in the dag,0
" each time a subprogram is called, the stack depth increases",0
" the contents ofqresult andqstack are not exposed directly through the domain-specific encoder; rather, we define primitive functions which manipulate these scratch-pads",0
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,0
"
since npi naturally supports the notion of recursion, a key question is how to enable npi to learn recursive programs",0
", the verification set",0
 ,0
" this functionality the original npi paper decodes to a program key embedding kt ∈ rk and then computes a program embedding pt+1, which we also did in our implementation, but we omit this for brevity",0
" for each length, we test each program on 30 randomly generated problems",1
" for many problems, the base cases and reduction rules usually consist of a finite (often small) number of cases",1
" we prove every member of f is added properly, given that subproblems s = {anan−1 + bnbn−1, an−1an−2 + bn−1bn−2, ",0
" as this complexity interfered with determining the base cases and reductions, we changed the algorithm to its current form",0
" as an application, we incorporate recursion into the neural programmer-interpreter architecture and consider four sample tasks: grade-school addition, bubble sort, topological sort, and quicksort",0
" observe that partially recursive does slightly better than non-recursive for the setting in which the length of the array is 3, and that the fully recursive version is able to sort every array given to it",0
"
by reasoning about the possible set of environment observations created by all valid inputs, we construct v using the procedure described in section 3",0
" as with the others, we apply the procedure described in section 3",0
" on the other hand, the non-recursive version has low accuracy, beginning from size 5, and fails completely for graphs of size 8 and beyond",1
" childlist ∈ nu is a vector of pointers, where childlist[i] points to the next child under consideration for vertex i",1
" 3: 4: function quicksort(a, lo, hi) 5: if lo < hi: then 6: p = partition(a, lo, hi) 7: quicksort(a, lo, p− 1) 8: quicksort(a, p+ 1, hi) 9: 10: function partition(a, lo, hi) 11: pivot = lo 12: for j ∈ [lo, hi− 1] : do 13: if a[j] ≤ a[hi] then 14: swap a[pivot] with a[j] 15: pivot = pivot+ 1 16: swap a[pivot] with a[hi] 17: return pivot
for quicksort, the domain-specific encoder is
fenc(qarray, qstacklo, qstackhi, plo, phi, pstacklo, pstackhi, ppivot, pj , at) =
mlp ([qarray(pj) ≤ qarray(phi), pj == phi, qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1), pstacklo == 1, at(1), at(2), at(3)]),
whereqarray ∈ ru×11 is a scratch-pad that contains u rows, each containing one of 11 values (one of the numbers 0 through 9 or an invalid state)",0
" partial recursion only adds a tail recursive call to bubblesort after bubble and reset, similar to the tail recursive call described previously for addition",0
"
an alternative method is to run p on many different program inputs and then observe step input sequences which occur, to create v ",0
" bstep, used in bubble, compares pairs of numbers, continuously moving the bubble pointers once to the right each time until reaching the end of the array",0
" 2: initialize lo and hi to be 1 and n, where n is the length of a",1
"
for addition, we analytically determine the verification set",0
e,1
"
in what follows, we describe the way in which we constructed npi training traces so as to make them contain recursive elements and thus enable npi to learn recursive programs",0
" for each graph size, we test the learned programs on 30 randomly generated dags",0
" although the model is exposed to more and more data, it might learn spurious and overly complex representations of the program, as suggested in zaremba   (2016)",0
" twc-1409915, darpa under grant no",0
" in future work, we seek to enable more tasks with recursive structure",0
" if the program is primitive, the next environmental state et+1 will be affected by pt and at, i",0
" k is set to 11, to denote the range of possible numbers (0 through 9), along with the start/end token (represented with the same encoding) which is observed when a pointer reaches beyond the bounds of the input",0
" for each function we use to implement the recursive version of topological sort, we need to consider the set of possible environment observation sequences we can create from all valid inputs and test that the learned program produces the correct behavior on each of these inputs",0
" in all experiments, α is set to 0",0
" also the dag can have potentially more than one connected component, meaning it is necessary to transition between these components appropriately",0
 we find that recursion makes it easier for the network to learn the right program and generalize to unknown situations,1
" the space of possible programs is extremely large, and composing a program that performs robustly on the infinite space of possible inputs is difficult—in part because it is impractical to obtain enough training examples to easily disambiguate amongst all possible programs",0
" we make the trace recursive by adding a tail recursive call into the trace for the add program after calling add1 and lshift, the original paper uses k = 10, but we found it necessary to augment the range with an end token, in order to terminate properly",1
"
we propose and describe our verification procedure",1
" in particular, we construct new training traces which explicitly contain recursive elements and show that with this type of trace, npi easily learns recursive programs",1
"
the same set of problems was used to generate the training traces for all formulations of the task, for non-recursive and recursive versions",0
"
is critical for implementing recursion, since it permits us to restrict our attention to the currently relevant recursive call, ignoring irrelevant details about other contexts",0
 we created a program set that reflects the semantics of algorithm 2,0
" empirically, we observe that the learned recursive programs solve all valid inputs with 100% accuracy after training on a very small number of examples, out-performing previous generalization results",0
"
for topological sort, the domain-specific encoder is
fenc(dag,qcolor, pstack, pstart, vactive, childlist, at)
=mlp ([qcolor(pstart), qcolor(dag[vactive][childlist[vactive]]), pstack == 1, at(1), at(2), at(3)]),
where qcolor ∈ ru×4 is a scratch-pad that contains u rows, each containing one of four colors (white, gray, black, invalid) with one-hot encoding",1
"
after finding v , we construct a set of problem inputs which, when executed on p , create exactly the step input sequences which make up v ",0
"we describe how models trained with recursive traces can be proven to generalize, by using the verification procedure described in section 3",0
", an array for quicksort or a dag for topological sort)",0
 ,0
" however, to be sure that the generated v is complete (covers all the cases needed), we need to check all pairs of observations seen in adjacent step inputs (in particular, those before and after a primitive function call), in a similar way as if we were constructing v from scratch",0
" if it is too large, then the semantics of p might not be well-defined on some elements in v , or the spurious step input sequences may not be reachable from any valid problem input (e",0
 this paper is the first (to our knowledge) to investigate the important problem of provable generalization properties of neural programs,1
" numerous models have been proposed for learning programs; to name a few, this includes the differentiable neural computer (graves  , 2016), neural turing machine (graves  , 2014), neural gpu (kaiser & sutskever, 2015), neural programmer (neelakantan  , 2015), pointer network (vinyals  , 2015), hierarchical attentive memory (andrychowicz & kurach, 2016), and neural random access machine (kurach  , 2016)",0
" the core module of the npi is an lstm controller that takes as input a slice of the current external environment, via a set of pointers, and a program and arguments to execute",0
" however, our concept of recursion for neural programs is general",0
" 3: 4: function quicksort(a, lo, hi) 5: if lo < hi: then 6: p = partition(a, lo, hi) 7: quicksort(a, lo, p− 1) 8: quicksort(a, p+ 1, hi) 9: 10: function partition(a, lo, hi) 11: pivot = lo 12: for j ∈ [lo, hi− 1] : do 13: if a[j] ≤ a[hi] then 14: swap a[pivot] with a[j] 15: pivot = pivot+ 1 16: swap a[pivot] with a[hi] 17: return pivot
for quicksort, the domain-specific encoder is
fenc(qarray, qstacklo, qstackhi, plo, phi, pstacklo, pstackhi, ppivot, pj , at) =
mlp ([qarray(pj) ≤ qarray(phi), pj == phi, qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1), pstacklo == 1, at(1), at(2), at(3)]),
whereqarray ∈ ru×11 is a scratch-pad that contains u rows, each containing one of 11 values (one of the numbers 0 through 9 or an invalid state)",1
" fa8750-15-2-0104, and berkeley deep drive",0
"
arg 2 (object to copy): env stack lo peek, env stack hi peek, phi, plo, ppivot − 1, ppivot + 1, reset
env stack lo peek and env stack hi peek represent qstacklo(pstacklo − 1) and qstackhi(pstackhi − 1), respectively",0
"
we propose and describe our verification procedure",0
 ,0
"
one common strategy to improve generalization is to use curriculum learning, where the model is trained on inputs of gradually increasing complexity",1
"we show that if we incorporate recursion, the learned npi programs can achieve provably perfect generalization for different tasks",0
" as a consequence, our learned neural programs empirically achieve perfect generalization from a very small number of training examples",1
" in particular, given that a recursion is defined by two properties as mentioned before, the base cases and the set of reduction rules, we can prove a recursive neural program generalizes perfectly if we can prove that (1) it performs correctly on the base cases; (2) it learns the reduction rules correctly",0
"
we describe the base cases, reduction rules, and the verification set for each task in appendix a",1
g,0
 we created a program set that reflects the semantics of algorithm 2,0
" we call this set of inputs the verification set, sv ",1
 add1 is a subprogram that adds the current column (writing the appropriate digit to the output row and carrying a bit to the next column if needed),0
"
arg 2 (increment or decrement): up, down
swap",1
" as before, the non-recursive learned program lacks a provable guarantee of generalization, whereas we show later that the recursive learned program has one",0
" for bubble sort, the domain-specific encoder is
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1), q(1, i2), i3 == length, at(1), at(2), at(3)]),
where the environment q ∈ r1×n×k is a scratch-pad that contains 1 row, to represent the state of the array as sorting proceeds in-place, andn columns",0
"
directed acyclic graphs are structurally more diverse than inputs in the two tasks of grade-school addition and bubble sort",1
" even though the earlier version also generalized just as well in practice, relatively small differences in the formulation of the traces and the environment observations can drastically change the difficulty of verification",1
" recall that this verification procedure cannot be performed for the non-recursive versions, since the propagation of the hidden state in the core lstm module makes reasoning difficult and so we would need to check an unbounded number of examples",0
" the corresponding edge list is [(1, 2), (1, 5), (2, 4), (2, 5), (3, 5)]",0
" each time a subprogram is called, the stack depth increases",0
" the single-digit multiplication task in zaremba   (2016), the bubble sort task in reed & de freitas (2016), and the graph tasks in graves   (2016))",1
g,0
 we describe the recursive re-formulation of traces for two tasks from the original npi paper—grade-school addition and bubble sort,0
" in fact, we argue that recursion is an essential element for neural programs to generalize, and makes it tractable to prove the generalization of neural programs",0
 recursion can be implemented differently for different neural programming models,0
"
we experiment with two levels of recursion—partial and full",0
" in this section, we give a brief review of the npi architecture from reed & de freitas (2016) as background",0
"
we would also like to point out that in this paper, we provide as an example one way to train a recursive neural program, by providing a certain training execution trace to the npi model",0
" for bubble sort, the domain-specific encoder is
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1), q(1, i2), i3 == length, at(1), at(2), at(3)]),
where the environment q ∈ r1×n×k is a scratch-pad that contains 1 row, to represent the state of the array as sorting proceeds in-place, andn columns",0
" here as a concrete and general example, we consider a general neural programming architecture (npa), similar to neural programmer-interpreter (npi) in reed & de freitas (2016)",1
" for these models, table 2 presents results on randomly generated dags of varying graph sizes (varying in the number of vertices)",0
 recursion enables provably perfect generalization,0
" there are 3 pointers (presult, pstack, pstart), presult points to the next empty location in qresult,
pstack points to the top of the stack in qstack, and pstart points to the candidate starting node for a connected component",1
"
since npi naturally supports the notion of recursion, a key question is how to enable npi to learn recursive programs",0
"
an alternative method is to run p on many different program inputs and then observe step input sequences which occur, to create v ",0
training neural networks to synthesize robust programs from a small number of examples is a challenging task,0
" as described in the verification procedure, it is possible to prove our learned recursive program generalizes perfectly by testing on an appropriate set of problem inputs, i",1
" it is straightforward to deal with all other operations, which do not induce partial environment states",0
 we choose to implement a topological sort task for graphs,1
"
the same set of problems was used to generate the training traces for all formulations of the task, for non-recursive and recursive versions",0
 npi then outputs the return probability and next program and arguments to execute,0
 the program terminates when seeing no numbers in the current column,1
 these pointers are referred to as bubble pointers,0
" formally, a function exhibits recursive behavior when it possesses two properties: (1) base cases— terminating scenarios that do not use recursion to produce answers; (2) a set of rules that reduces all other problems toward the base cases",0
" if no white vertex is found, the entire execution is terminated move none write write a value either to environment (e",1
" in fact, it is one of our future directions to explore new ways to train a recursive neural program without providing explicit training execution traces or with only partial or non-recursive traces",0
" in particular, in the explore function, adding a tail recursive call resets and stores the hidden states associated with vertices in a stack-like fashion",1
" given verification sets that cover all the base cases and reduction rules, we can provide proofs that these learned programs generalize perfectly",0
" for these programs, we re-use the appropriate program sets (the associated subprograms), and we refer the reader to the appendix of reed & de freitas (2016) for further details on the subprograms used in addition and bubble sort",0
" more specifically, when m is trained on simpler inputs, such as inputs of a small length, the generalization metric evaluates how well m will do on more complex inputs, such as inputs of much longer length",1
"as discussed in section 2, the neural programmer-interpreter (npi) is an instance of a neural programmer architecture and hence it naturally supports recursion",0
g,0
 the core controller acts as a dispatcher for the programs,0
" the core lstm module’s hidden state is preserved over all these add1 calls, and it is difficult to interpret with certainty what happens over longer timesteps without concretely evaluating the lstm with an input of that length",1
" the stack operation has the following arguments:
arg 1 (operation): stack push call1, stack push call2, stack pop
stack push call1 pushes lo and pivot−1 toqstacklo andqstackhi",0
", to change the value of phi) none described belowstack",0
"
as mentioned in section 1, all approaches to neural programming today fare poorly on this generalization issue",1
" in this version, there is a dependence on length: bstep and lshift are called a number of times equivalent to one less than the length of the input array, in bubble and reset respectively",0
"
by reasoning about the possible set of environment observations created by all valid inputs, we construct v using the procedure described in section 3",1
"
in this general neural programming architecture, we show it is easy to support recursion",1
"1, we hypothesize the non-recursive programs do not generalize well because they have learned spurious dependencies specific to the training set, such as length of the input problems",0
"
by nature, recursion reduces the complexity of a problem to simpler instances",1
" in particular, this condition is satisfied by npi, and thus the npi model naturally supports recursion (even though the authors of npi did not consider this aspect explicitly)",1
 figure 2 shows examples of traces for the different versions of bubble sort,0
"
formally, verification consists of proving the following theorem:
∀i ∈ v,m(i) ⇓ p (i) where i denotes a sequence of step inputs (within one function call), v denotes the set of valid sequences of step inputs, m denotes the neural network model, p denotes the correct program, and p (i) denotes the next step output from the correct program",0
 this is due to the properties of the training set – we trained on 2 traces synthesized from arrays of length 7 and 1 trace synthesized from an array of length 6,0
" the 4 environment observations aid with control flow; qstacklo(pstacklo− 1) < qstackhi(pstackhi− 1) implements the lo < hi comparison in line 5 of algorithm 3, pstacklo == 1 checks if the stacks are empty in line 18 of algorithm 4, and the other observations (all involving ppivot or pj) deal with logic in the partition function",1
"
the training data for the neural programmer-interpreter consists of full execution traces for the program of interest",0
" if a white vertex is found, set pstart to point to it; this signifies the start of a traversal of a new connected component",0
"
in this paper, we propose that recursion is an important concept for neural programs as well",0
" formally, a function exhibits recursive behavior when it possesses two properties: (1) base cases— terminating scenarios that do not use recursion to produce answers; (2) a set of rules that reduces all other problems toward the base cases",1
" for the recursive formulation of addition, we analytically construct the set of input problems that cover all base cases and reduction rules",0
g,1
"
quicksort",0
" in order to claim a proof, we must verify the model produces correct behavior over all base cases and reductions, as described in section 2",0
 table 4 demonstrates that the accuracy of the non-recursive program degrades sharply when moving from arrays of length 7 to arrays of length 8,0
" 4: function toposort(dag) 5: while there is still a white vertex u: do 6: color[u] = grey 7: vactive = u 8: do 9: if vactive has a white child v then 10: color[v] = grey 11: push vactive onto s 12: vactive = v 13: else 14: color[vactive] = black 15: write vactive to result 16: if s is empty then pass 17: else pop the top vertex off s and set it to vactive 18: while s is not empty
topological sort",0
" the write operation has the following arguments:
arg 1 (main action): color curr, color next, active start, active neighb, active stack, save, stack push, stack pop, result
color curr colors vactive, color next colors vertex dag[vactive][childlist[vactive]], active start writes pstart to vactive, active neighb writes dag[vactive][childlist[vactive]] to vactive, active stack writes qstack(pstack) to vactive, save writes vactive to vsave, stack push pushes vactive to the top of the stack, stack pop writes a null value to the top of the stack, and result writes vactive to qresult(presult)",1
 this verification phase only needs to be performed once after training,0
"2 at any given time, the npi has access to values pointed to by four pointers in each of the four rows, represented by q(1, i1), q(2, i2), q(3, i3), and q(4, i4)",0
" fa8750-15-2-0104, and berkeley deep drive",0
" we can obtain this mapping by first considering the possible observations that can arise at the beginning of the entry function (add, bubblesort, toposort, and quicksort) for some valid program input, and iteratively applying the observation-to-observation mapping implied by the reference implementation’s step output at that point in the execution",1
 we choose to implement a topological sort task for graphs,0
", an lstm in npi’s case, but possibly other networks in different cases",0
 ,0
" however, our concept of recursion for neural programs is general",1
this material is in part based upon work supported by the national science foundation under grant no,1
" as before, the non-recursive learned program lacks a provable guarantee of generalization, whereas we show later that the recursive learned program has one",1
" in future work, we would like to decrease supervision and construct models that are capable of coming up with recursive abstractions themselves",1
"
inside bubble and reset, there are two operations that can be made recursive",0
"we show that if we incorporate recursion, the learned npi programs can achieve provably perfect generalization for different tasks",0
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,0
" provably perfect generalization implies the model will behave correctly, given any valid input",0
" for each length, we test each program on 30 randomly generated problems",0
" if no white vertex is found, the entire execution is terminated move none write write a value either to environment (e",1
" in fact, we argue that recursion is an essential element for neural programs to generalize, and makes it tractable to prove the generalization of neural programs",1
 this procedure verifies that all base cases and reductions are handled properly by the model via explicit tests,0
" for example, a step input-step output pair in lines 2 and 3 of the left-hand side of figure 1 is (add1, write out 1)",1
e,0
"
quicksort",1
g,1
"
we note that for tasks with very large input domains, such as ones involving mnist digits or speech samples, the state space of base cases and reduction rules could be prohibitively large, possibly infinite",0
" for grade-school addition, the domain-specific encoder is
fenc(q, i1, i2, i3, i4, at) =mlp ([q(1, i1), q(2, i2), q(3, i3), q(4, i4), at(1), at(2), at(3)]),
where the environment q ∈ r4×n×k is a scratch-pad that contains four rows (the first input number, the second input number, the carry bits, and the output) and n columns",0
"
figure 1 shows examples of non-recursive and recursive addition traces",0
" the verification set can be very small, as we found a 10-element array ([8,2,1,2,0,8,5,8,3,7]) is sufficient to cover all of v ",1
" however, note that there is no provable guarantee that the non-recursive learned program will generalize to all inputs, whereas we show later that the recursive learned program has a provable guarantee of perfect generalization",1
 a1a0 + bnbn−1 ,0
" in this version of curriculum learning, even though the inputs are gradually becoming more complex, the semantics of the program is succinct and does not change",0
g,1
"
in this paper, we propose to resolve these issues by explicitly incorporating recursion into neural architectures",0
" the theorem states that for the same sequence of step inputs, the model produces the exact same step output as the target program it aims to learn",0
", the verification set",1
"
we also report on generalization results for the non-recursive and recursive versions of this variant of bubble sort",1
" on the other hand, the recursive programs have learned the true program semantics",0
 add1 is a subprogram that adds the current column (writing the appropriate digit to the output row and carrying a bit to the next column if needed),1
"
arg 2 (auxiliary variable): color grey, color black
color grey and color black color the given vertex grey and black, respectively",0
 npi then outputs the return probability and next program and arguments to execute,1
"
it is sufficient to construct problems where every transition between two adjacent columns is covered",0
", an array for quicksort or a dag for topological sort)",1
"program descriptions calls arguments quicksort run the quicksort
routine in place for the array a, for indices from lo to hi non-recursive: partition, stack, write recursive: same as non-recursive version, along with quicksort implicitly: array a to sort, lo, hi
partition runs the partition function",1
" if it is too large, then the semantics of p might not be well-defined on some elements in v , or the spurious step input sequences may not be reachable from any valid problem input (e",1
" observe that partially recursive does slightly better than non-recursive for the setting in which the length of the array is 3, and that the fully recursive version is able to sort every array given to it",1
" that is to say, the network does not learn the true program semantics",0
" the recursive version of topological sort solves all graph instances we tried, from graphs of size 5 through 70",1
 there is a (changing) list of neural programs used to accomplish a given task,0
"
the npi accesses an external environment, q, which varies according to the task",0
 figure 2 shows examples of traces for the different versions of bubble sort,0
" as aforementioned, the npi model naturally supports recursion",1
"
arg 2 (increment or decrement): up, down1 // top level topological sort call 2 toposort() { 3 while (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 } 0 11 traverse() { 12 check_child() 13 explore() 14 } 15 16 check_child() { 17 while (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 18 move(childlist[vactive], up) 19 } 20 21 explore() { 22 do 23 if (qcolor(dag[vactive][childlist[vactive]]) is white): 24 write(color_next, color_grey) 25 stack(push) 26 write(save) 27 write(active_neighb) 28 move(childlist[vsave], up) 29 else: 30 write(color_curr, color_black) 31 write(result) 32 move(presult, up) 33 if(pstack == 1): 34 break 35 else: 36 stack(pop) 37 check_child() 38 while (true) 39 } 40 41 stack(op) { 42 if (op == push): 43 write(stack_push) 44 move(pstack, up) 45 46 if (op == pop): 47 write(active_stack) 48 write(stack_pop) 49 move(pstack, down) 50 } 51 52 next_start() { 53 while(qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 54 move(pstart, up) 55 }altered recursive functions  // top level topological sort call 2 toposort() { 3 if (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 toposort() // recursive call 0 } 11 12 check_child() { 13 if (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 14 move(childlist[vactive], up) 15 check_child() // recursive call 16 } 17 18 explore() { 19 if (qcolor(dag[vactive][childlist[vactive]]) is white): 20 write(color_next, color_grey) 21 stack(push) 22 write(save) 23 write(active_neighb) 24 move(childlist[vsave], up) 25 else: 26 write(color_curr, color_black) 27 write(result) 28 move(presult, up) 29 if(pstack == 1): 30 return 31 else: 32 stack(pop) 33 check_child() 34 explore() // recursive call 35 } 36 37 next_start() { 38 if (qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 39 move(pstart, up) 40 next_start() // recursive call 41 }algorithm 4 iterative quicksort 1: initialize an array a to sort and two empty stacks slo and shi",0
3,1
" in fact, we argue that recursion is an essential element for neural programs to generalize, and makes it tractable to prove the generalization of neural programs",0
 pstacklo and pstackhi point to the top (empty) positions in qstacklo and qstackhi,0
" more specifically, when m is trained on simpler inputs, such as inputs of a small length, the generalization metric evaluates how well m will do on more complex inputs, such as inputs of much longer length",1
"we train using the adam optimizer and use a 2-layer lstm and task-specific state encoders for the external environments, as described in reed & de freitas (2016)",1
" the swap operation has the following arguments:
arg 1 (swap object 1): ppivot
arg 2 (swap object 2): phi, pj
write",0
" at each time step, the core controller can decide to select one of the programs to call with certain arguments",0
 this verification phase only needs to be performed once after training,0
 figure 2 shows examples of traces for the different versions of bubble sort,1
5,0
 we choose to implement a topological sort task for graphs,1
" the core module of the npi is an lstm controller that takes as input a slice of the current external environment, via a set of pointers, and a program and arguments to execute",1
" thus, any general neural programming architecture supporting such a call structure can be made to support recursion",0
 this modification also enables us to sort arrays containing arbitrary comparable elements,0
"
base cases and reduction rules for bubble sort",1
"
formally, verification consists of proving the following theorem:
∀i ∈ v,m(i) ⇓ p (i) where i denotes a sequence of step inputs (within one function call), v denotes the set of valid sequences of step inputs, m denotes the neural network model, p denotes the correct program, and p (i) denotes the next step output from the correct program",1
" the second is the training procedure, which consists of the form of the training data and the optimization process",0
" in particular, this condition is satisfied by npi, and thus the npi model naturally supports recursion (even though the authors of npi did not consider this aspect explicitly)",0
"
bubble sort",1
"
topological sort",1
" there are 3 pointers (presult, pstack, pstart), presult points to the next empty location in qresult,
pstack points to the top of the stack in qstack, and pstart points to the candidate starting node for a connected component",0
" observe that the non-recursive program’s correctness degrades for length 11 and beyond, while the recursive program can sort any given array",1
" an example of part of an addition task trace, written in shorthand, is given in figure 1",1
e,1
" when the program is called, the current context including the caller’s memory state is stored on a stack; when the program returns, the stored context is popped off the stack to resume execution in the previous caller’s context",1
 the original version of the bubblesort implementation exposes the values within the array,0
" ppivot and pj point to the pivot and j indices of the array, used in the partition function in algorithm 3",0
"
one common strategy to improve generalization is to use curriculum learning, where the model is trained on inputs of gradually increasing complexity",0
" any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of national science foundation and darpa",0
" lshift moves the four pointers to the left, to move to the next column",0
" lshift moves the four pointers to the left, to move to the next column",1
"
the training set for topological sort contains 6 traces, with one synthesized from a graph of size 5 and the rest synthesized from graphs of size 7",1
"
to evaluate a neural network that learns a neural program to accomplish a certain task, one common evaluation metric is how well the learned model m generalizes",1
" , n , where the dag contains n vertices",1
" in fact, we argue that recursion is an essential element for neural programs to generalize, and makes it tractable to prove the generalization of neural programs",0
"
the npi accesses an external environment, q, which varies according to the task",0
"
recursion drastically reduces the number of configurations we need to consider during the verification phase and makes the proof tractable, because it introduces structure that eliminates infinitely long sequences of step inputs that would otherwise need to be considered",0
"
we refer the reader to the appendix for the non-recursive trace generating functions",0
" for purposes of verification, we replace the domain-specific encoder with the following:
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1) ≤ q(1, i2), 1 ≤ i1 ≤ length, 1 ≤ i2 ≤ length, i3 == length, at(1), at(2), at(3)]),which directly exposes which of the two values pointed to is larger",0
" if it is too small, then execution of the program on some input might require evaluation of m(i) on some i /∈ v , and so the behavior ofm(i) might deviate from p (i)",0
g,1
"
through careful reasoning about the possible set of environment observations created by all valid inputs, and how each of the operations in the execution trace affects the environment, we can construct v using the procedure described in section 3",0
 ,0
" given verification sets that cover all the base cases and reduction rules, we can provide proofs that these learned programs generalize perfectly",0
 recursion can be implemented differently for different neural programming models,1
" in this architecture, we consider a core controller, e",0
" qcolor(pstart) contains the color of the current start vertex, used in the evaluation of the condition in the while loop in line 5 of algorithm 2",1
"
bubble sort",0
" for purposes of verification, we replace the domain-specific encoder with the following:
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1) ≤ q(1, i2), 1 ≤ i1 ≤ length, 1 ≤ i2 ≤ length, i3 == length, at(1), at(2), at(3)]),which directly exposes which of the two values pointed to is larger",0
" if the program is primitive, the next environmental state et+1 will be affected by pt and at, i",0
"program descriptions calls arguments toposort perform topological
sort on graph traverse, next start, write, move
none
traverse traverse graph until stack is empty check child, explore none check child check if a white child exists; if so, set childlist[vactive] to point to it move none explore repeatedly traverse subgraphs until stack is empty stack, check child, write, move none stack interact with stack, either pushing or popping write, move push, pop next start move pstart until reaching a white vertex",0
"
given a verification set, we can then run the model on the verification set to check if the produced traces and results are correct",0
"2 at any given time, the npi has access to values pointed to by four pointers in each of the four rows, represented by q(1, i1), q(2, i2), q(3, i3), and q(4, i4)",1
", pstart or childlist[vactive]) up or down none described belowwrite",1
" more specifically, when m is trained on simpler inputs, such as inputs of a small length, the generalization metric evaluates how well m will do on more complex inputs, such as inputs of much longer length",0
" in particular, we construct new training traces which explicitly contain recursive elements and show that with this type of trace, npi easily learns recursive programs",1
" at each time step, the core controller can decide to select one of the programs to call with certain arguments",0
"
is critical for implementing recursion, since it permits us to restrict our attention to the currently relevant recursive call, ignoring irrelevant details about other contexts",1
"
we propose and describe our verification procedure",0
" we note that an earlier version of quicksort we tried lacked primitive operations to directly move a pointer to another, and therefore needed more functions and observations",1
"
algorithm 2 depth first search topological sort 1: color all vertices white",0
" we prove every member of f is added properly, given that subproblems s = {anan−1 + bnbn−1, an−1an−2 + bn−1bn−2, ",1
" we made the assumption of equivalent lengths in order to parametrize the input format with respect to length, but this assumption can be removed as well",1
 a1a0 + bnbn−1 ,0
" bstep, used in bubble, compares pairs of numbers, continuously moving the bubble pointers once to the right each time until reaching the end of the array",0
" note that recursion helps make this process tractable, because we only need to test a finite number of inputs to show that the model will work correctly on inputs of unbounded complexity",1
"
we also report on generalization results for the non-recursive and recursive versions of this variant of bubble sort",0
" consequently, there is no concept of length relevant to the problem, which has traditionally been an important focus of length-based curriculum learning",0
"
arg 2 (object to copy): env stack lo peek, env stack hi peek, phi, plo, ppivot − 1, ppivot + 1, reset
env stack lo peek and env stack hi peek represent qstacklo(pstacklo − 1) and qstackhi(pstackhi − 1), respectively",0
"
inside bubble and reset, there are two operations that can be made recursive",1
" for each function we use to implement the recursive version of topological sort, we need to consider the set of possible environment observation sequences we can create from all valid inputs and test that the learned program produces the correct behavior on each of these inputs",1
" the write operation has the following arguments:
arg 1 (object to write): env stack lo, env stack hi, phi, plo
env stack lo and env stack hi represent qstacklo(pstacklo) and qstackhi(pstackhi), respectively",1
"
base cases and reduction rules for topological sort",1
 the sequence of primitive operations (move and write operations) for the non-recursive and recursive versions are exactly the same,0
 we then construct a verification set of size 73 by ensuring that randomly generated graphs cover the analytically derived v ,0
"as there is no public implementation of npi, we implemented a version of it in keras that is as faithful to the paper as possible",1
 we instantiated recursion for the neural programmerinterpreter by changing the training traces,0
" the single-digit multiplication task in zaremba   (2016), the bubble sort task in reed & de freitas (2016), and the graph tasks in graves   (2016))",0
" the partial recursion is not enough for perfect generalization, as will be presented later in section 4",0
" while this matches the description from reed & de freitas (2016), we found that this causes an unnecessary blowup in the size of v and makes it much more difficult to construct the verification set",0
" as a consequence, our learned neural programs empirically achieve perfect generalization from a very small number of training examples",0
"1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 while (pstacklo 6= 1): 6 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 7 stack(stack_pop) 8 else: 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 stack(stack_pop) 12 partition() 13 stack(stack_push_call2) 14 stack(stack_push_call1) 15 } 16 17 partition() { 18 set_pivot_lo() 19 set_j_lo() 20 compswap_loop() 21 swap(ppivot, phi) 22 set_j_null() 23 } 24 25 compswap_loop() { 26 while (pj 6= phi): 27 compswap() 28 move(pj, up) 29 } 30 31 compswap() { 32 if (a[pj ] ≤ a[phi]): 33 swap(ppivot, pj) 34 move(ppivot, up) 35 } 36 37 stack(op) { 38 if (op == stack_push_call1): 39 write(env_stack_lo, plo) 40 write(env_stack_hi, ppivot − 1) 41 move(pstacklo, up) 42 move(pstackhi, up) 43 44 if (op == stack_push_call2): 45 write(env_stack_lo, ppivot + 1) 46 write(env_stack_hi, phi) 47 move(pstacklo, up) 48 move(pstackhi, up) 49 50 if (op == stack_pop): 51 write(env_stack_lo, reset) 52 write(env_stack_hi, reset) 53 move(pstacklo, down) 54 move(pstackhi, down) 55 }altered recursive functions 1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 6 partition() 7 stack(stack_push_call2) 8 stack(stack_push_call1) 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 quicksort() // recursive call 12 stack(stack_pop) 13 write(phi, env_stack_hi_peek) 14 write(plo, env_stack_lo_peek) 15 quicksort() // recursive call 16 stack(stack_pop) 17 } 18 19 compswap_loop() { 20 if (pj 6= phi): 21 compswap() 22 move(pj, up) 23 compswap_loop() // recursive call 24 }in this section, we describe the space of base cases and reduction rules that must be covered for each of the four sample tasks, in order to create the verification set",0
" we implement a quicksort task, in order to demonstrate that recursion helps with learning divide-and-conquer algorithms",0
 pstacklo and pstackhi point to the top (empty) positions in qstacklo and qstackhi,1
" however, models that make use of this strategy eventually fail after a certain level of complexity (e",0
"
figure 1 shows examples of non-recursive and recursive addition traces",0
"
it is sufficient to construct problems where every transition between two adjacent columns is covered",0
" it is straightforward to deal with all other operations, which do not induce partial environment states",0
" the single-digit multiplication task in zaremba   (2016), the bubble sort task in reed & de freitas (2016), and the graph tasks in graves   (2016))",1
" consequently, it is infeasible to construct a verification set that covers all cases, and the verification procedure we have described is inadequate",0
" for purposes of verification, we replace the domain-specific encoder with the following:
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1) ≤ q(1, i2), 1 ≤ i1 ≤ length, 1 ≤ i2 ≤ length, i3 == length, at(1), at(2), at(3)]),which directly exposes which of the two values pointed to is larger",0
 ,1
"in this paper, we propose that the key abstraction of recursion is necessary for neural programs to generalize",1
" the corresponding edge list is [(1, 2), (1, 5), (2, 4), (2, 5), (3, 5)]",0
"
after finding v , we construct a set of problem inputs which, when executed on p , create exactly the step input sequences which make up v ",0
3 to construct v and then empirically create a verification set which covers v ,0
" for these models, table 2 presents results on randomly generated dags of varying graph sizes (varying in the number of vertices)",1
 stack pop pushes −∞ values to qstacklo and qstackhi,1
"
by nature, recursion reduces the complexity of a problem to simpler instances",0
" plo and phi point to the lo and hi indices of the array, as in algorithm 3",0
" the second is the training procedure, which consists of the form of the training data and the optimization process",1
 ,0
" in particular, in the explore function, adding a tail recursive call resets and stores the hidden states associated with vertices in a stack-like fashion",0
 there is a (changing) list of neural programs used to accomplish a given task,1
" however, our concept of recursion for neural programs is general",1
"
base cases and reduction rules for quicksort",1
" we further have qresult ∈ nu , a scratch-pad which contains the sorted list of vertices at the end of execution, and qstack ∈ nu , which serves the role of the stack s in algorithm 2",0
g,0
"in this paper, we propose that the key abstraction of recursion is necessary for neural programs to generalize",0
 ,1
 2: initialize an empty stack s and a directed acyclic graph dag to traverse,0
" k is set to 11, to represent the range of 10 possible digits, along with a token representing the end of input",0
" recursion is an important concept in programming languages and a critical tool to
reduce the complexity of programs",1
 ,0
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,0
" as before, the non-recursive learned program lacks a provable guarantee of generalization, whereas we show later that the recursive learned program has one",1
" if the program is primitive, the next environmental state et+1 will be affected by pt and at, i",1
" 2: initialize lo and hi to be 1 and n, where n is the length of a",1
" 3: 4: function partition(a, lo, hi) 5: pivot = lo 6: for j ∈ [lo, hi− 1] : do 7: if a[j] ≤ a[hi] then 8: swap a[pivot] with a[j] 9: pivot = pivot+ 1 0: swap a[pivot] with a[hi] 11: return pivot 12: 13: function quicksort(a, lo, hi) 14: while slo and shi are not empty: do 15: pop states off slo and shi, writing them to lo and hi",0
" lshift moves the four pointers to the left, to move to the next column",0
"
since npi naturally supports the notion of recursion, a key question is how to enable npi to learn recursive programs",1
"
directed acyclic graphs are structurally more diverse than inputs in the two tasks of grade-school addition and bubble sort",0
" in this section, we give a brief review of the npi architecture from reed & de freitas (2016) as background",0
 this makes it difficult to reason about what the model will do when given complex inputs,1
" for each graph size, we test the learned programs on 30 randomly generated dags",0
 reset represents a −∞ value,1
 the maximum problem length in this training set is 3 (e,1
"
given a verification set, we can then run the model on the verification set to check if the produced traces and results are correct",0
" recursion is an important concept in programming languages and a critical tool to
reduce the complexity of programs",1
"
the npi accesses an external environment, q, which varies according to the task",0
" 2: initialize lo and hi to be 1 and n, where n is the length of a",1
"
under the assumption that there are no leading 0’s (except in the case of single digits) and the two numbers to be added have the same number of digits, the verification set for addition contains 20,181 input problems",0
6,0
 indentation indicates the stack is one level deeper than before,0
" while this matches the description from reed & de freitas (2016), we found that this causes an unnecessary blowup in the size of v and makes it much more difficult to construct the verification set",0
"
we construct input problems by splitting into two cases: one case in which the left column contains a null value and another in which the left column does not contain any null values",1
" each step during an execution of the program does one of three things: (1) another subprogram along with associated arguments is called, as in line 10, (2) the program writes to the environment if it is primitive, as in line 8, or (3) the loop is terminated if the return probability exceeds a threshold α, after which the stack frame is popped and control is returned to the caller",0
" if no white vertex is found, the entire execution is terminated move none write write a value either to environment (e",0
" both the non-recursive and recursive learned programs generalize on all input lengths we tried, up to 5000 digits",1
" that is to say, the network does not learn the true program semantics",1
 the logic for the non-recursive trace is shown in algorithm 4 in the appendix,1
" ppivot and pj point to the pivot and j indices of the array, used in the partition function in algorithm 3",1
" any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of national science foundation and darpa",0
" there are 3 pointers (presult, pstack, pstart), presult points to the next empty location in qresult,
pstack points to the top of the stack in qstack, and pstart points to the candidate starting node for a connected component",0
" we note that an earlier version of quicksort we tried lacked primitive operations to directly move a pointer to another, and therefore needed more functions and observations",1
 we instantiated recursion for the neural programmerinterpreter by changing the training traces,0
" this is possible if and only if the graph has no directed cycles; that is to say, it must be a directed acyclic graph (dag)",0
 this is due to the properties of the training set – we trained on 2 traces synthesized from arrays of length 7 and 1 trace synthesized from an array of length 6,0
" the npi model has three learnable components: a task-agnostic core, a program-key embedding, and domain-specific encoders that allow the npi to operate in diverse environments",0
"
the training set for quicksort contains 4 traces, synthesized from arrays of length 5",1
" when the program is called, the current context including the caller’s memory state is stored on a stack; when the program returns, the stored context is popped off the stack to resume execution in the previous caller’s context",1
g,0
"
we refer the reader to the appendix for the non-recursive trace generating functions",1
"
training setup",1
training neural networks to synthesize robust programs from a small number of examples is a challenging task,0
 indentation indicates the stack is one level deeper than before,1
" plo and phi point to the lo and hi indices of the array, as in algorithm 3",1
" as a consequence, our learned neural programs empirically achieve perfect generalization from a very small number of training examples",0
" the outputs rt, pt+1, at+1 are used to determine the next action, as described in algorithm 1",1
 add1 is a subprogram that adds the current column (writing the appropriate digit to the output row and carrying a bit to the next column if needed),0
" in order to claim a proof, we must verify the model produces correct behavior over all base cases and reductions, as described in section 2",0
"2 at any given time, the npi has access to values pointed to by four pointers in each of the four rows, represented by q(1, i1), q(2, i2), q(3, i3), and q(4, i4)",1
" however, the authors of npi did not consider explicitly the notion of recursion and as a consequence, did not learn recursive programs",1
" however, note that there is no provable guarantee that the non-recursive learned program will generalize to all inputs, whereas we show later that the recursive learned program has a provable guarantee of perfect generalization",0
3,1
 this modification also enables us to sort arrays containing arbitrary comparable elements,0
"
we refer the reader to the appendix for the non-recursive trace generating functions",1
" consequently, there is no concept of length relevant to the problem, which has traditionally been an important focus of length-based curriculum learning",0
" the partial recursion is not enough for perfect generalization, as will be presented later in section 4",0
 this algorithm is a variant of depth first search,0
"
through careful reasoning about the possible set of environment observations created by all valid inputs, and how each of the operations in the execution trace affects the environment, we can construct v using the procedure described in section 3",0
"
as mentioned in section 1, all approaches to neural programming today fare poorly on this generalization issue",0
" plo and phi point to the lo and hi indices of the array, as in algorithm 3",1
"
one common strategy to improve generalization is to use curriculum learning, where the model is trained on inputs of gradually increasing complexity",0
3 to construct v and then empirically create a verification set which covers v ,0
" for each length, we test each program on 30 randomly generated problems",1
" finally, we implement recursive traces for our own topological sort and quicksort tasks",0
we now report on generalization for the varying tasks,0
" as a consequence, our learned neural programs empirically achieve perfect generalization from a very small number of training examples",0
" lshift moves the four pointers to the left, to move to the next column",0
" on the other hand, the recursive programs have learned the true program semantics",1
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,1
"
the dag is represented as an adjacency list where dag[i][j] refers to the j-th child of vertex i",1
"
base cases and reduction rules for quicksort",0
" on the other hand, the recursive programs have learned the true program semantics",0
" the general notion of recursion has been an important concept in many domains, including mathematics and computer science",0
" the assumption of leading 0’s can be easily removed, at the cost of slightly increasing the size of the verification set",0
" in all experiments, α is set to 0",1
"
the training data for the neural programmer-interpreter consists of full execution traces for the program of interest",1
" for these models, table 2 presents results on randomly generated dags of varying graph sizes (varying in the number of vertices)",1
" at each time step, the core controller can decide to select one of the programs to call with certain arguments",1
" in particular, given that a recursion is defined by two properties as mentioned before, the base cases and the set of reduction rules, we can prove a recursive neural program generalizes perfectly if we can prove that (1) it performs correctly on the base cases; (2) it learns the reduction rules correctly",0
" the stack operation has the following arguments:
arg 1 (operation): stack push call1, stack push call2, stack pop
stack push call1 pushes lo and pivot−1 toqstacklo andqstackhi",0
", the trace corresponding to the array [3,2])",1
"
arg 2 (increment or decrement): up, down
swap",0
 the degree for any vertex in the dag is variable,1
" the non-recursive and partially recursive versions are unable to sort long arrays, beyond length 8",0
" if it is too large, then the semantics of p might not be well-defined on some elements in v , or the spurious step input sequences may not be reachable from any valid problem input (e",0
 we outline how to construct this set,1
 recursion can be implemented differently for different neural programming models,1
" , n , where the dag contains n vertices",0
"
as in line 13 of the right-hand side of figure 1",0
", qstacklo or qstackhi) or to pointer (e",0
" as an application, we incorporate recursion into the neural programmer-interpreter architecture and consider four sample tasks: grade-school addition, bubble sort, topological sort, and quicksort",0
" however, our concept of recursion for neural programs is general",1
" as with the original npi architecture, the experiments for this paper always used a 3-tuple of integers at = (at(1), at(2), at(3))",0
" partial recursion only adds a tail recursive call to bubblesort after bubble and reset, similar to the tail recursive call described previously for addition",0
 ,1
"1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 while (pstacklo 6= 1): 6 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 7 stack(stack_pop) 8 else: 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 stack(stack_pop) 12 partition() 13 stack(stack_push_call2) 14 stack(stack_push_call1) 15 } 16 17 partition() { 18 set_pivot_lo() 19 set_j_lo() 20 compswap_loop() 21 swap(ppivot, phi) 22 set_j_null() 23 } 24 25 compswap_loop() { 26 while (pj 6= phi): 27 compswap() 28 move(pj, up) 29 } 30 31 compswap() { 32 if (a[pj ] ≤ a[phi]): 33 swap(ppivot, pj) 34 move(ppivot, up) 35 } 36 37 stack(op) { 38 if (op == stack_push_call1): 39 write(env_stack_lo, plo) 40 write(env_stack_hi, ppivot − 1) 41 move(pstacklo, up) 42 move(pstackhi, up) 43 44 if (op == stack_push_call2): 45 write(env_stack_lo, ppivot + 1) 46 write(env_stack_hi, phi) 47 move(pstacklo, up) 48 move(pstackhi, up) 49 50 if (op == stack_pop): 51 write(env_stack_lo, reset) 52 write(env_stack_hi, reset) 53 move(pstacklo, down) 54 move(pstackhi, down) 55 }altered recursive functions 1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 6 partition() 7 stack(stack_push_call2) 8 stack(stack_push_call1) 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 quicksort() // recursive call 12 stack(stack_pop) 13 write(phi, env_stack_hi_peek) 14 write(plo, env_stack_lo_peek) 15 quicksort() // recursive call 16 stack(stack_pop) 17 } 18 19 compswap_loop() { 20 if (pj 6= phi): 21 compswap() 22 move(pj, up) 23 compswap_loop() // recursive call 24 }in this section, we describe the space of base cases and reduction rules that must be covered for each of the four sample tasks, in order to create the verification set",1
" the outputs rt, pt+1, at+1 are used to determine the next action, as described in algorithm 1",0
" in order to claim a proof, we must verify the model produces correct behavior over all base cases and reductions, as described in section 2",1
"
algorithm 2 depth first search topological sort 1: color all vertices white",0
"
the three environment observations aid with control flow in algorithm 2",0
"
the operations we need to be concerned most about are carry and lshift, which induce partial environment states spanning two columns",0
" however, to be sure that the generated v is complete (covers all the cases needed), we need to check all pairs of observations seen in adjacent step inputs (in particular, those before and after a primitive function call), in a similar way as if we were constructing v from scratch",0
" qcolor(dag[vactive][childlist[vactive]]) refers to the color of the next child of vactive, used in the evaluation of the condition in the if branch in line 9 of algorithm 2",0
 figure 2 shows examples of traces for the different versions of bubble sort,1
"
in addition, none of the current approaches to neural programming provide a method or even aim to enable provable guarantees about generalization",1
" on the other hand, the non-recursive version has low accuracy, beginning from size 5, and fails completely for graphs of size 8 and beyond",0
" by iterating with this procedure, and then running p on the input observation set that we obtain for the entry point function, we can obtain v precisely",1
"
since npi naturally supports the notion of recursion, a key question is how to enable npi to learn recursive programs",0
"
base cases and reduction rules for addition",1
" also the dag can have potentially more than one connected component, meaning it is necessary to transition between these components appropriately",1
 this paper is the first (to our knowledge) to investigate the important problem of provable generalization properties of neural programs,1
" we make the trace recursive by adding a tail recursive call into the trace for the add program after calling add1 and lshift, the original paper uses k = 10, but we found it necessary to augment the range with an end token, in order to terminate properly",0
 the program terminates when seeing no numbers in the current column,0
g,1
" qcolor(pstart) contains the color of the current start vertex, used in the evaluation of the condition in the while loop in line 5 of algorithm 2",0
"
the training set for quicksort contains 4 traces, synthesized from arrays of length 5",0
" our implementation uses two stacks qstacklo and
qstackhi, each in ru , that store the arguments to the recursive quicksort calls in algorithm 3; before each recursive call, the appropriate arguments are popped off the stack and written to plo and phi",0
"
in order to demonstrate that recursion can help learn and generalize better, we trained a non-recursive and recursive model on just a single execution trace generated from a graph containing 5 nodes3 for the topological sort task",0
" although the model is exposed to more and more data, it might learn spurious and overly complex representations of the program, as suggested in zaremba   (2016)",1
g,1
" for the recursive formulation of addition, we analytically construct the set of input problems that cover all base cases and reduction rules",0
" we call this set of inputs the verification set, sv ",0
"
arg 2 (auxiliary variable): color grey, color black
color grey and color black color the given vertex grey and black, respectively",1
 there are 2 variables (vactive and vsave); vactive holds the active vertex (as in algorithm 2) and vsave holds the value of vactive before executing line 12 of algorithm 2,0
" the contents ofqresult andqstack are not exposed directly through the domain-specific encoder; rather, we define primitive functions which manipulate these scratch-pads",0
"
in addition, none of the current approaches to neural programming provide a method or even aim to enable provable guarantees about generalization",0
" when the program is called, the current context including the caller’s memory state is stored on a stack; when the program returns, the stored context is popped off the stack to resume execution in the previous caller’s context",0
"
there are 6 pointers (plo, phi, pstacklo, pstackhi, ppivot, pj)",0
"
thus far, to evaluate the efficacy of neural models on programming tasks, the only metric that has been used is generalization of expected behavior to inputs of greater complexity (vinyals   (2015), kaiser & sutskever (2015), reed & de freitas (2016), graves   (2016), zaremba   (2016))",0
" if no white vertex is found, the entire execution is terminated move none write write a value either to environment (e",0
" the arrow in the theorem refers to evaluation, as in big-step semantics",0
" in particular, we construct new training traces which explicitly contain recursive elements and show that with this type of trace, npi easily learns recursive programs",0
 we then construct a verification set of size 73 by ensuring that randomly generated graphs cover the analytically derived v ,1
"
in order to demonstrate that recursion can help learn and generalize better, we trained a non-recursive and recursive model on just a single execution trace generated from a graph containing 5 nodes3 for the topological sort task",1
" in this version, there is a dependence on length: bstep and lshift are called a number of times equivalent to one less than the length of the input array, in bubble and reset respectively",0
" the second is the training procedure, which consists of the form of the training data and the optimization process",0
 the original version of the bubblesort implementation exposes the values within the array,0
" the recursive version of topological sort solves all graph instances we tried, from graphs of size 5 through 70",0
 we outline how to construct this set,0
" we call this set of inputs the verification set, sv ",1
we emphasize the overall goal of this work is to enable the learning of a recursive program,0
" qcolor(pstart) contains the color of the current start vertex, used in the evaluation of the condition in the while loop in line 5 of algorithm 2",0
" the only model, to our knowledge, that does not train on input-output pairs is the neural programmer-interpreter (reed & de freitas, 2016), which trains on synthetic execution traces",1
" for brevity, we refer the reader to the appendix for further details on the program set and non-recursive and recursive trace-generating functions used for topological sort",1
"
given a verification set, we can then run the model on the verification set to check if the produced traces and results are correct",0
" the recursive version can generalize perfectly to long problems constructed from these components (such as the sum “822+233”, where “8+2” and “2+3” are in the training set), but the non-recursive version fails to sum these long problems properly",0
 we then construct a verification set of size 73 by ensuring that randomly generated graphs cover the analytically derived v ,1
"
the three environment observations aid with control flow in algorithm 2",1
"
thus far, to evaluate the efficacy of neural models on programming tasks, the only metric that has been used is generalization of expected behavior to inputs of greater complexity (vinyals   (2015), kaiser & sutskever (2015), reed & de freitas (2016), graves   (2016), zaremba   (2016))",0
" by iterating with this procedure, and then running p on the input observation set that we obtain for the entry point function, we can obtain v precisely",0
" that is to say, the network does not learn the true program semantics",0
" if the step output specifies a primitive function call, we need to reason about how it can affect the environment so as to change the observation in the next step input",0
" we make the trace recursive by adding a tail recursive call into the trace for the add program after calling add1 and lshift, the original paper uses k = 10, but we found it necessary to augment the range with an end token, in order to terminate properly",0
 we choose to implement a topological sort task for graphs,0
" the stack operation has the following arguments:
arg 1 (operation): stack push call1, stack push call2, stack pop
stack push call1 pushes lo and pivot−1 toqstacklo andqstackhi",1
 some functional programming languages go so far as not to define any looping constructs but rely solely on recursion to enable repeated execution of the same code,1
"as discussed in section 2, the neural programmer-interpreter (npi) is an instance of a neural programmer architecture and hence it naturally supports recursion",1
" in fact, we argue that recursion is an essential element for neural programs to generalize, and makes it tractable to prove the generalization of neural programs",0
 ,1
"
topological sort",0
 recursion enables provably perfect generalization,0
" for grade-school addition, the domain-specific encoder is
fenc(q, i1, i2, i3, i4, at) =mlp ([q(1, i1), q(2, i2), q(3, i3), q(4, i4), at(1), at(2), at(3)]),
where the environment q ∈ r4×n×k is a scratch-pad that contains four rows (the first input number, the second input number, the carry bits, and the output) and n columns",0
"
base cases and reduction rules for topological sort",1
" if the step output specifies a primitive function call, we need to reason about how it can affect the environment so as to change the observation in the next step input",1
" this makes it so that we only need to consider the vertices in the subgraph that are currently relevant for computing the sort, allowing simpler reasoning about behavior for large graphs",0
" at any given time, the npi has access to the values referred to by two pointers, represented by q(1, i1) and q(1, i2),",1
" in contrast, each call to the recursive add always runs for a fixed number of steps, even on arbitrarily long problems in f , so we can test that it performs correctly on a small, fixed number of step input sequences",0
we now report on generalization for the varying tasks,0
"
we note that for tasks with very large input domains, such as ones involving mnist digits or speech samples, the state space of base cases and reduction rules could be prohibitively large, possibly infinite",0
" in our experiments, we only present dag’s as inputs and represent the vertices as values ranging from 1, ",0
" we performed experiments on the partially recursive version in order to examine what happens when only one recursive call is implemented, when in reality three are required for perfect generalization",0
" for these models, table 2 presents results on randomly generated dags of varying graph sizes (varying in the number of vertices)",1
 the first is the actual model architecture,1
" the 4 environment observations aid with control flow; qstacklo(pstacklo− 1) < qstackhi(pstackhi− 1) implements the lo < hi comparison in line 5 of algorithm 3, pstacklo == 1 checks if the stacks are empty in line 18 of algorithm 4, and the other observations (all involving ppivot or pj) deal with logic in the partition function",0
" the general notion of recursion has been an important concept in many domains, including mathematics and computer science",0
" on the other hand, the recursive programs have learned the true program semantics",0
"
it is important to emphasize that at inference time in the npi, the hidden state of the lstm controller is reset (to zero) at each subprogram call, as in line 3 of algorithm 1 (h ← 0)",1
 2: initialize an empty stack s and a directed acyclic graph dag to traverse,0
" a topological sort is a linear ordering of vertices such that for every directed edge (u, v) from u to v, u comes before v in the ordering",1
"
as mentioned in section 1, all approaches to neural programming today fare poorly on this generalization issue",1
" as described in the verification procedure, it is possible to prove our learned recursive program generalizes perfectly by testing on an appropriate set of problem inputs, i",0
" however, our concept of recursion for neural programs is general",1
 the degree for any vertex in the dag is variable,0
"program descriptions calls arguments quicksort run the quicksort
routine in place for the array a, for indices from lo to hi non-recursive: partition, stack, write recursive: same as non-recursive version, along with quicksort implicitly: array a to sort, lo, hi
partition runs the partition function",0
" any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of national science foundation and darpa",0
" 16: p = partition(a, lo, hi) 17: push p+ 1 and hi to slo and shi",0
" our implementation uses two stacks qstacklo and
qstackhi, each in ru , that store the arguments to the recursive quicksort calls in algorithm 3; before each recursive call, the appropriate arguments are popped off the stack and written to plo and phi",1
" if it is too small, then execution of the program on some input might require evaluation of m(i) on some i /∈ v , and so the behavior ofm(i) might deviate from p (i)",0
" if it is too small, then execution of the program on some input might require evaluation of m(i) on some i /∈ v , and so the behavior ofm(i) might deviate from p (i)",0
 errors arise due to undesirable and uninterpretable dependencies and associations the architecture learns to store in some high-dimensional hidden state,0
" we performed experiments on the partially recursive version in order to examine what happens when only one recursive call is implemented, when in reality three are required for perfect generalization",1
 table 4 also demonstrates that the (verified) recursive program generalizes perfectly,0
g,0
"
quicksort",0
"
the training set for quicksort contains 4 traces, synthesized from arrays of length 5",0
" finally, we implement recursive traces for our own topological sort and quicksort tasks",0
 we found that changing the npi training traces is a simple way to enable this,0
" for each task, given the verification set, we check the traces and results of the learned, to-be-verified neural program (described in section 4",1
"as there is no public implementation of npi, we implemented a version of it in keras that is as faithful to the paper as possible",0
", an array for quicksort or a dag for topological sort)",0
"
by reasoning about the possible set of environment observations created by all valid inputs, we construct v using the procedure described in section 3",1
" this is highly undesirable, since being able to provide the correct answer in all possible settings is one of the most important aspects of any learned neural program",1
 the environment and return probability are omitted for readability,0
"
an alternative method is to run p on many different program inputs and then observe step input sequences which occur, to create v ",0
"
move",0
" using this modification, we constructed a verification set consisting of one array of size 10",1
"
under the assumption that there are no leading 0’s (except in the case of single digits) and the two numbers to be added have the same number of digits, the verification set for addition contains 20,181 input problems",0
" twc-1409915, darpa under grant no",1
 errors arise due to undesirable and uninterpretable dependencies and associations the architecture learns to store in some high-dimensional hidden state,0
" however, models that make use of this strategy eventually fail after a certain level of complexity (e",0
"
note that the training set can often be considerably smaller than the verification set, and despite this, the learned model can still pass the entire verification set",0
" we hypothesize that the reason for this is that the neural network learns to spuriously depend on specific characteristics of the training examples that are irrelevant to the true program semantics, such as length of the training inputs, and thus fails to generalize to more complex inputs",0
" the pointers at index i1 and i2 are used to compare the pair of numbers considered during the bubble sweep, swapping them if the number at i1 is greater than that in i2",1
"
in this general neural programming architecture, we show it is easy to support recursion",1
"in this paper, we propose that the key abstraction of recursion is necessary for neural programs to generalize",1
"
the npi accesses an external environment, q, which varies according to the task",0
" note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values, and arg 2 can only take one of 7 values",0
we now report on generalization for the varying tasks,0
we now report on generalization for the varying tasks,1
" we implement a quicksort task, in order to demonstrate that recursion helps with learning divide-and-conquer algorithms",1
" in future work, we seek to enable more tasks with recursive structure",0
" childlist ∈ nu is a vector of pointers, where childlist[i] points to the next child under consideration for vertex i",1
 u varies with the number of vertices in the graph,1
 the original version of the bubblesort implementation exposes the values within the array,1
" on the other hand, the non-recursive version has low accuracy, beginning from size 5, and fails completely for graphs of size 8 and beyond",1
" in this version of curriculum learning, even though the inputs are gradually becoming more complex, the semantics of the program is succinct and does not change",1
" as this complexity interfered with determining the base cases and reductions, we changed the algorithm to its current form",1
3,0
" in order to compensate for this, ppivot and pj require special functions (move pivot lo and move j lo) to properly set them to lo in lines 11 and 12 of the partition function in algorithm 3",0
"
to construct this set, by using the reference implementation of each subprogram, we construct a mapping between two sets of environment observations: the first set consists of all observations that can occur at the beginning of a particular subprogram’s invocation, and the second set contains the observations at the end of that subprogram",1
" furthermore, given a verification input set that covers all base cases and reduction rules, we can formally prove that the learned neural programs achieve perfect generalization after verifying its behavior on the verification input set",0
"
to construct this set, by using the reference implementation of each subprogram, we construct a mapping between two sets of environment observations: the first set consists of all observations that can occur at the beginning of a particular subprogram’s invocation, and the second set contains the observations at the end of that subprogram",0
 table 4 also demonstrates that the (verified) recursive program generalizes perfectly,0
" for tasks other than addition, it is difficult to analytically determine the verification set, so instead, we randomly generate input candidates until they completely cover the base cases and reduction rules",0
"
by nature, recursion reduces the complexity of a problem to simpler instances",1
" lshift, used in reset, shifts the pointers left until reaching the start token",1
"
as a concrete instantiation, we show in this paper that we can enable recursive neural programs in the npi model, and thus enable perfectly generalizable neural programs for tasks such as sorting where the original, non-recursive npi program fails",0
"program descriptions calls arguments toposort perform topological
sort on graph traverse, next start, write, move
none
traverse traverse graph until stack is empty check child, explore none check child check if a white child exists; if so, set childlist[vactive] to point to it move none explore repeatedly traverse subgraphs until stack is empty stack, check child, write, move none stack interact with stack, either pushing or popping write, move push, pop next start move pstart until reaching a white vertex",0
" we have three observations: the color of the start node, the color of the active node’s next child to be considered, and whether the stack is empty",0
" nevertheless, we would like the model to quickly learn to represent the right semantics of the underlying program from a small number of training examples, not an exhaustive number of them",1
" however, our concept of recursion for neural programs is general",0
 the model described in the training setup of section 4 (trained on 6 traces) was verified to be correct via the matching procedure described in section 4,0
 there are 2 variables (vactive and vsave); vactive holds the active vertex (as in algorithm 2) and vsave holds the value of vactive before executing line 12 of algorithm 2,0
"
for topological sort, the domain-specific encoder is
fenc(dag,qcolor, pstack, pstart, vactive, childlist, at)
=mlp ([qcolor(pstart), qcolor(dag[vactive][childlist[vactive]]), pstack == 1, at(1), at(2), at(3)]),
where qcolor ∈ ru×4 is a scratch-pad that contains u rows, each containing one of four colors (white, gray, black, invalid) with one-hot encoding",0
"
through careful reasoning about the possible set of environment observations created by all valid inputs, and how each of the operations in the execution trace affects the environment, we can construct v using the procedure described in section 3",0
" in this architecture, we consider a core controller, e",1
"
by reasoning about the possible set of environment observations created by all valid inputs, we construct v using the procedure described in section 3",0
" to the best of our knowledge, this is the first time verification has been applied to a neural program, providing provable guarantees about its behavior",0
"6) on the verification set, and ensure they match the traces produced by the true program p ",1
" the add reduction rule ensures that each call to add only covers two adjacent columns, and so the lstm only ever runs for a fixed number of steps necessary to process these two columns",0
g,0
" 9: else 10: function run(e, p2, a2)
a description of the inference procedure is given in algorithm 1",0
" 4: function toposort(dag) 5: while there is still a white vertex u: do 6: color[u] = grey 7: vactive = u 8: do 9: if vactive has a white child v then 10: color[v] = grey 11: push vactive onto s 12: vactive = v 13: else 14: color[vactive] = black 15: write vactive to result 16: if s is empty then pass 17: else pop the top vertex off s and set it to vactive 18: while s is not empty
topological sort",1
 we adapt machinery from the original paper slightly to fit our needs,0
"6) on the verification set, and ensure they match the traces produced by the true program p ",1
 these pointers are referred to as bubble pointers,1
" m , as described in algorithm 1, processes the sequence of step inputs by using an lstm",1
" m , as described in algorithm 1, processes the sequence of step inputs by using an lstm",0
" in particular, recursion can be implemented as a program calling itself",0
"
base cases and reduction rules for addition",1
" table 1 presents results on randomly generated arrays of varying length for the learned non-recursive, partially recursive, and full recursive programs",1
 there are 2 variables (vactive and vsave); vactive holds the active vertex (as in algorithm 2) and vsave holds the value of vactive before executing line 12 of algorithm 2,1
" a single element of an execution trace consists of a step input-step output pair, which can be synthesized from algorithm 1: this corresponds to, for a given time-step, the step input tuple (e, p, a) and step output tuple (r, p2, a2)",1
" the recursive version can generalize perfectly to long problems constructed from these components (such as the sum “822+233”, where “8+2” and “2+3” are in the training set), but the non-recursive version fails to sum these long problems properly",0
"
arg 2 (increment or decrement): up, down
swap",0
" the npi model has three learnable components: a task-agnostic core, a program-key embedding, and domain-specific encoders that allow the npi to operate in diverse environments",1
"
one common strategy to improve generalization is to use curriculum learning, where the model is trained on inputs of gradually increasing complexity",1
 figure 2 shows examples of traces for the different versions of bubble sort,1
 ,0
" for problems where the base cases may be extremely large or infinite, such as certain forms of motor control, recursion can still help reduce the problem of generalization to these two aspects and make the generalization problem significantly simpler to handle and reason about",0
" the move operation has the following arguments:
arg 1 (pointer): pstacklo, pstackhi, pj , ppivot
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values",0
" for each function we use to implement the recursive version of topological sort, we need to consider the set of possible environment observation sequences we can create from all valid inputs and test that the learned program produces the correct behavior on each of these inputs",0
"
one common strategy to improve generalization is to use curriculum learning, where the model is trained on inputs of gradually increasing complexity",1
" the move operation has the following arguments:
arg 1 (pointer): pstacklo, pstackhi, pj , ppivot
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values",1
" thus, any general neural programming architecture supporting such a call structure can be made to support recursion",0
" at end, pointer ppivot is moved to the pivot compswap loop, move pivot lo, move j lo, swap
none
compswap loop runs the for loop inside the partition function compswap, move none compswap compares a[pivot] ≤ a[j]; if so, perform a swap and increment ppivot swap, move none set pivot lo sets ppivot to lo index none none set j lo sets pj to lo index none none set j null sets pj to −∞ none none stack pushes lo/hi states
onto stacks slo and shi according to argument (described below)
write, move described below
move moves pointer one unit up or down none described below swap swaps elements at given array indices none described below write write a value either to stack (e",1
"2 at any given time, the npi has access to values pointed to by four pointers in each of the four rows, represented by q(1, i1), q(2, i2), q(3, i3), and q(4, i4)",1
"
thus far, to evaluate the efficacy of neural models on programming tasks, the only metric that has been used is generalization of expected behavior to inputs of greater complexity (vinyals   (2015), kaiser & sutskever (2015), reed & de freitas (2016), graves   (2016), zaremba   (2016))",1
", pstart or childlist[vactive]) up or down none described belowwrite",1
" ppivot and pj point to the pivot and j indices of the array, used in the partition function in algorithm 3",1
" we also hope to decrease supervision, for example by training with only partial or non-recursive traces, and to develop novel neural programming architectures integrated directly with a notion of recursion",0
"
recursion drastically reduces the number of configurations we need to consider during the verification phase and makes the proof tractable, because it introduces structure that eliminates infinitely long sequences of step inputs that would otherwise need to be considered",0
"
in order to demonstrate that recursion can help learn and generalize better, for addition, we trained only on traces for 5 arbitrarily chosen 1-digit addition sum examples",1
"
arg 2 (object to copy): env stack lo peek, env stack hi peek, phi, plo, ppivot − 1, ppivot + 1, reset
env stack lo peek and env stack hi peek represent qstacklo(pstacklo − 1) and qstackhi(pstackhi − 1), respectively",0
"
for addition, we analytically determine the verification set",1
"
the training set for quicksort contains 4 traces, synthesized from arrays of length 5",0
 our result shows that the training procedure and the npi architecture is capable of generalizing from the step input-output pairs seen in the training data to the unseen ones present in the verification set,0
" this functionality the original npi paper decodes to a program key embedding kt ∈ rk and then computes a program embedding pt+1, which we also did in our implementation, but we omit this for brevity",0
" 2: initialize lo and hi to be 1 and n, where n is the length of a",1
5,1
" the verification set can be very small, as we found a 10-element array ([8,2,1,2,0,8,5,8,3,7]) is sufficient to cover all of v ",1
"we describe how models trained with recursive traces can be proven to generalize, by using the verification procedure described in section 3",1
"
arg 2 (increment or decrement): up, down
swap",0
"
the training data for the neural programmer-interpreter consists of full execution traces for the program of interest",0
" the assumption of leading 0’s can be easily removed, at the cost of slightly increasing the size of the verification set",1
" both the non-recursive and recursive learned programs generalize on all input lengths we tried, up to 5000 digits",0
" as an application, we incorporate recursion into the neural programmer-interpreter architecture and consider four sample tasks: grade-school addition, bubble sort, topological sort, and quicksort",0
"
for topological sort, the domain-specific encoder is
fenc(dag,qcolor, pstack, pstart, vactive, childlist, at)
=mlp ([qcolor(pstart), qcolor(dag[vactive][childlist[vactive]]), pstack == 1, at(1), at(2), at(3)]),
where qcolor ∈ ru×4 is a scratch-pad that contains u rows, each containing one of four colors (white, gray, black, invalid) with one-hot encoding",0
 ,0
" as aforementioned, the npi model naturally supports recursion",0
" if the step output specifies a primitive function call, we need to reason about how it can affect the environment so as to change the observation in the next step input",0
" the core module of the npi is an lstm controller that takes as input a slice of the current external environment, via a set of pointers, and a program and arguments to execute",1
" we hypothesize that the reason for this is that the neural network learns to spuriously depend on specific characteristics of the training examples that are irrelevant to the true program semantics, such as length of the training inputs, and thus fails to generalize to more complex inputs",0
 we outline how to construct this set,0
" we further have qresult ∈ nu , a scratch-pad which contains the sorted list of vertices at the end of execution, and qstack ∈ nu , which serves the role of the stack s in algorithm 2",0
" as before, the non-recursive learned program lacks a provable guarantee of generalization, whereas we show later that the recursive learned program has one",1
 the first is the actual model architecture,0
" in future work, we seek to enable more tasks with recursive structure",1
" to make an analogy to mdps, this procedure is analogous to how value iteration obtains the correct value for each state starting from any initialization",0
"
note that the training set can often be considerably smaller than the verification set, and despite this, the learned model can still pass the entire verification set",0
" k is set to 11, to represent the range of 10 possible digits, along with a token representing the end of input",0
" for tasks other than addition, it is difficult to analytically determine the verification set, so instead, we randomly generate input candidates until they completely cover the base cases and reduction rules",1
"
grade-school addition",0
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,1
" the architecture usually possesses some form of memory, which could be internal (such as the hidden state of a recurrent neural network) or external (such as a discrete “scratch pad” or a memory block with differentiable access)",0
" the move operation has the following arguments:
arg 1 (pointer): presult, pstack, pstart, childlist[vactive], childlist[vsave]
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 5 values",0
", the trace corresponding to the array [3,2])",0
" for brevity, we refer the reader to the appendix for further details on the program set and non-recursive and recursive trace-generating functions used for topological sort",0
" in this version of curriculum learning, even though the inputs are gradually becoming more complex, the semantics of the program is succinct and does not change",1
" to make an analogy to mdps, this procedure is analogous to how value iteration obtains the correct value for each state starting from any initialization",0
" however, our concept of recursion for neural programs is general",0
" partial recursion only adds a tail recursive call to bubblesort after bubble and reset, similar to the tail recursive call described previously for addition",0
 ,0
g,0
"when constructing a neural network for the purpose of learning a program, there are two orthogonal aspects to consider",1
" on the other hand, the non-recursive version has low accuracy, beginning from size 5, and fails completely for graphs of size 8 and beyond",1
" for these models, table 2 presents results on randomly generated dags of varying graph sizes (varying in the number of vertices)",0
" however, note that there is no provable guarantee that the non-recursive learned program will generalize to all inputs, whereas we show later that the recursive learned program has a provable guarantee of perfect generalization",0
" formally, a function exhibits recursive behavior when it possesses two properties: (1) base cases— terminating scenarios that do not use recursion to produce answers; (2) a set of rules that reduces all other problems toward the base cases",0
" nevertheless, we would like the model to quickly learn to represent the right semantics of the underlying program from a small number of training examples, not an exhaustive number of them",0
" by iterating with this procedure, and then running p on the input observation set that we obtain for the entry point function, we can obtain v precisely",0
 3: begin traversing from vertex 1 in the dag,1
 stack push call2 pushes pivot + 1 and hi to qstacklo and qstackhi,0
" provably perfect generalization implies the model will behave correctly, given any valid input",0
" in future work, we would like to decrease supervision and construct models that are capable of coming up with recursive abstractions themselves",0
" this functionality the original npi paper decodes to a program key embedding kt ∈ rk and then computes a program embedding pt+1, which we also did in our implementation, but we omit this for brevity",1
 3: begin traversing from vertex 1 in the dag,0
" 16: p = partition(a, lo, hi) 17: push p+ 1 and hi to slo and shi",1
", an array for quicksort or a dag for topological sort)",1
"
grade school addition",1
"
in what follows, we describe the way in which we constructed npi training traces so as to make them contain recursive elements and thus enable npi to learn recursive programs",1
" we call this set of inputs the verification set, sv ",0
 b1b0 where no carry operations occur,1
" in particular, this condition is satisfied by npi, and thus the npi model naturally supports recursion (even though the authors of npi did not consider this aspect explicitly)",1
g,1
" this makes it so that we only need to consider the vertices in the subgraph that are currently relevant for computing the sort, allowing simpler reasoning about behavior for large graphs",0
"
training setup",1
" 4: function toposort(dag) 5: while there is still a white vertex u: do 6: color[u] = grey 7: vactive = u 8: do 9: if vactive has a white child v then 10: color[v] = grey 11: push vactive onto s 12: vactive = v 13: else 14: color[vactive] = black 15: write vactive to result 16: if s is empty then pass 17: else pop the top vertex off s and set it to vactive 18: while s is not empty
topological sort",0
" for purposes of verification, we replace the domain-specific encoder with the following:
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1) ≤ q(1, i2), 1 ≤ i1 ≤ length, 1 ≤ i2 ≤ length, i3 == length, at(1), at(2), at(3)]),which directly exposes which of the two values pointed to is larger",1
" provably perfect generalization implies the model will behave correctly, given any valid input",1
" the arrow in the theorem refers to evaluation, as in big-step semantics",1
 this procedure verifies that all base cases and reductions are handled properly by the model via explicit tests,1
" we made the assumption of equivalent lengths in order to parametrize the input format with respect to length, but this assumption can be removed as well",0
" we prove every member of f is added properly, given that subproblems s = {anan−1 + bnbn−1, an−1an−2 + bn−1bn−2, ",0
"
note that the training set can often be considerably smaller than the verification set, and despite this, the learned model can still pass the entire verification set",1
"
figure 1 shows examples of non-recursive and recursive addition traces",1
" formally, the npi is represented by the following set of equations:
st = fenc(et, at)
ht = flstm(st, pt, ht−1)
rt = fend(ht), pt+1 = fprog(ht), at+1 = farg(ht)
t is a subscript denoting the time-step; fenc is a domain-specific encoder (to be described later) that takes in the environment slice et and arguments at; flstm represents the core module, which takes in the state st generated by fenc, a program embedding pt ∈ rp , and hidden lstm state ht; fend decodes the return probability rt; fprog decodes a program key embedding pt+1;1 and farg decodes arguments at+1",0
"
algorithm 2 depth first search topological sort 1: color all vertices white",0
" in all experiments, α is set to 0",0
"
the three environment observations aid with control flow in algorithm 2",0
" naı̈vely, we might expect to synthesize and test an input for any sequence created by combining the four possible colors in two variables and another boolean variable for whether the stack is empty (so 32 possible observations at any point), but for various reasons, most of these combinations are impossible to occur at any given point in the execution trace",0
" as with the original npi architecture, the experiments for this paper always used a 3-tuple of integers at = (at(1), at(2), at(3))",0
"
the training set for topological sort contains 6 traces, with one synthesized from a graph of size 5 and the rest synthesized from graphs of size 7",1
" this functionality the original npi paper decodes to a program key embedding kt ∈ rk and then computes a program embedding pt+1, which we also did in our implementation, but we omit this for brevity",0
", an array for quicksort or a dag for topological sort)",0
 this verification phase only needs to be performed once after training,0
"
there are 6 pointers (plo, phi, pstacklo, pstackhi, ppivot, pj)",1
g,0
 18: push lo and p− 1 to slo and shi,1
" in this section, we give a brief review of the npi architecture from reed & de freitas (2016) as background",1
" to make an analogy to mdps, this procedure is analogous to how value iteration obtains the correct value for each state starting from any initialization",1
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,0
" 9: else 10: function run(e, p2, a2)
a description of the inference procedure is given in algorithm 1",1
 a1a0 + bnbn−1 ,0
" the non-recursive and partially recursive versions are unable to sort long arrays, beyond length 8",0
" we further have qresult ∈ nu , a scratch-pad which contains the sorted list of vertices at the end of execution, and qstack ∈ nu , which serves the role of the stack s in algorithm 2",1
"
there are 6 pointers (plo, phi, pstacklo, pstackhi, ppivot, pj)",0
 we use the lomuto partition scheme; the logic for the recursive trace is shown in algorithm 3,1
" the assumption of leading 0’s can be easily removed, at the cost of slightly increasing the size of the verification set",0
 npi then outputs the return probability and next program and arguments to execute,0
 ,0
" in this version, there is a dependence on length: bstep and lshift are called a number of times equivalent to one less than the length of the input array, in bubble and reset respectively",1
 we instantiated recursion for the neural programmerinterpreter by changing the training traces,0
" fa8750-15-2-0104, and berkeley deep drive",1
"1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 while (pstacklo 6= 1): 6 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 7 stack(stack_pop) 8 else: 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 stack(stack_pop) 12 partition() 13 stack(stack_push_call2) 14 stack(stack_push_call1) 15 } 16 17 partition() { 18 set_pivot_lo() 19 set_j_lo() 20 compswap_loop() 21 swap(ppivot, phi) 22 set_j_null() 23 } 24 25 compswap_loop() { 26 while (pj 6= phi): 27 compswap() 28 move(pj, up) 29 } 30 31 compswap() { 32 if (a[pj ] ≤ a[phi]): 33 swap(ppivot, pj) 34 move(ppivot, up) 35 } 36 37 stack(op) { 38 if (op == stack_push_call1): 39 write(env_stack_lo, plo) 40 write(env_stack_hi, ppivot − 1) 41 move(pstacklo, up) 42 move(pstackhi, up) 43 44 if (op == stack_push_call2): 45 write(env_stack_lo, ppivot + 1) 46 write(env_stack_hi, phi) 47 move(pstacklo, up) 48 move(pstackhi, up) 49 50 if (op == stack_pop): 51 write(env_stack_lo, reset) 52 write(env_stack_hi, reset) 53 move(pstacklo, down) 54 move(pstackhi, down) 55 }altered recursive functions 1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 6 partition() 7 stack(stack_push_call2) 8 stack(stack_push_call1) 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 quicksort() // recursive call 12 stack(stack_pop) 13 write(phi, env_stack_hi_peek) 14 write(plo, env_stack_lo_peek) 15 quicksort() // recursive call 16 stack(stack_pop) 17 } 18 19 compswap_loop() { 20 if (pj 6= phi): 21 compswap() 22 move(pj, up) 23 compswap_loop() // recursive call 24 }in this section, we describe the space of base cases and reduction rules that must be covered for each of the four sample tasks, in order to create the verification set",0
"
there are 6 pointers (plo, phi, pstacklo, pstackhi, ppivot, pj)",1
"
algorithm 2 depth first search topological sort 1: color all vertices white",1
" in this pair, the step input runs a subprogram add1 that has no arguments, and the step output contains a program write that has arguments of out and 1",0
" however, our concept of recursion for neural programs is general",1
"
in order to demonstrate that recursion can help learn and generalize better, for addition, we trained only on traces for 5 arbitrarily chosen 1-digit addition sum examples",0
 we instantiated recursion for the neural programmerinterpreter by changing the training traces,1
 stack pop pushes −∞ values to qstacklo and qstackhi,0
 we leave this as future work to devise a verification procedure more appropriate to this setting,1
g,0
"
the training set for topological sort contains 6 traces, with one synthesized from a graph of size 5 and the rest synthesized from graphs of size 7",1
"
the same set of problems was used to generate the training traces for all formulations of the task, for non-recursive and recursive versions",1
" both the non-recursive and recursive learned programs generalize on all graphs we tried, up to 120 vertices",0
" furthermore, given a verification input set that covers all base cases and reduction rules, we can formally prove that the learned neural programs achieve perfect generalization after verifying its behavior on the verification input set",0
" recursion is an important concept in programming languages and a critical tool to
reduce the complexity of programs",0
" for purposes of verification, we replace the domain-specific encoder with the following:
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1) ≤ q(1, i2), 1 ≤ i1 ≤ length, 1 ≤ i2 ≤ length, i3 == length, at(1), at(2), at(3)]),which directly exposes which of the two values pointed to is larger",1
"
in order to demonstrate that recursion can help learn and generalize better, we trained a non-recursive and recursive model on just a single execution trace generated from a graph containing 5 nodes3 for the topological sort task",1
" the write operation has the following arguments:
arg 1 (object to write): env stack lo, env stack hi, phi, plo
env stack lo and env stack hi represent qstacklo(pstacklo) and qstackhi(pstackhi), respectively",0
" if it is too large, then the semantics of p might not be well-defined on some elements in v , or the spurious step input sequences may not be reachable from any valid problem input (e",1
" formally, a function exhibits recursive behavior when it possesses two properties: (1) base cases— terminating scenarios that do not use recursion to produce answers; (2) a set of rules that reduces all other problems toward the base cases",0
" if no white vertex is found, the entire execution is terminated move none write write a value either to environment (e",1
" the core module of the npi is an lstm controller that takes as input a slice of the current external environment, via a set of pointers, and a program and arguments to execute",0
3,0
 this is due to the properties of the training set – we trained on 2 traces synthesized from arrays of length 7 and 1 trace synthesized from an array of length 6,1
", the trace corresponding to the array [3,2])",1
g,0
 recursion enables provably perfect generalization,1
" there are 3 pointers (presult, pstack, pstart), presult points to the next empty location in qresult,
pstack points to the top of the stack in qstack, and pstart points to the candidate starting node for a connected component",0
" a topological sort is a linear ordering of vertices such that for every directed edge (u, v) from u to v, u comes before v in the ordering",0
" for tasks other than addition, it is difficult to analytically determine the verification set, so instead, we randomly generate input candidates until they completely cover the base cases and reduction rules",0
 this is due to the properties of the training set – we trained on 2 traces synthesized from arrays of length 7 and 1 trace synthesized from an array of length 6,1
" we further have qresult ∈ nu , a scratch-pad which contains the sorted list of vertices at the end of execution, and qstack ∈ nu , which serves the role of the stack s in algorithm 2",1
 the environment and return probability are omitted for readability,0
" k is set to 11, to represent the range of 10 possible digits, along with a token representing the end of input",1
" in this version, there is a dependence on length: bstep and lshift are called a number of times equivalent to one less than the length of the input array, in bubble and reset respectively",1
" by iterating with this procedure, and then running p on the input observation set that we obtain for the entry point function, we can obtain v precisely",0
 there are 2 variables (vactive and vsave); vactive holds the active vertex (as in algorithm 2) and vsave holds the value of vactive before executing line 12 of algorithm 2,0
" we hypothesize that the reason for this is that the neural network learns to spuriously depend on specific characteristics of the training examples that are irrelevant to the true program semantics, such as length of the training inputs, and thus fails to generalize to more complex inputs",1
" we further have qresult ∈ nu , a scratch-pad which contains the sorted list of vertices at the end of execution, and qstack ∈ nu , which serves the role of the stack s in algorithm 2",0
" to make an analogy to mdps, this procedure is analogous to how value iteration obtains the correct value for each state starting from any initialization",0
" recall that this verification procedure cannot be performed for the non-recursive versions, since the propagation of the hidden state in the core lstm module makes reasoning difficult and so we would need to check an unbounded number of examples",0
" , a1a0 + b1b0} are added properly",0
" as an application, we incorporate recursion into the neural programmer-interpreter architecture and consider four sample tasks: grade-school addition, bubble sort, topological sort, and quicksort",1
"
move",0
"
by reasoning about the possible set of environment observations created by all valid inputs, we construct v using the procedure described in section 3",0
" if yes, then this indicates that the learned neural program achieves provably perfect generalization",0
"6) on the verification set, and ensure they match the traces produced by the true program p ",1
 this paper is the first (to our knowledge) to investigate the important problem of provable generalization properties of neural programs,1
 there is a (changing) list of neural programs used to accomplish a given task,0
"
the dag is represented as an adjacency list where dag[i][j] refers to the j-th child of vertex i",0
"when constructing a neural network for the purpose of learning a program, there are two orthogonal aspects to consider",0
" we have three observations: the color of the start node, the color of the active node’s next child to be considered, and whether the stack is empty",1
"
in this general neural programming architecture, we show it is easy to support recursion",0
" there are 3 pointers (presult, pstack, pstart), presult points to the next empty location in qresult,
pstack points to the top of the stack in qstack, and pstart points to the candidate starting node for a connected component",0
" as an application, we incorporate recursion into the neural programmer-interpreter architecture and consider four sample tasks: grade-school addition, bubble sort, topological sort, and quicksort",0
"when constructing a neural network for the purpose of learning a program, there are two orthogonal aspects to consider",0
" via the recursive call, we effectively forget that the column just added exists, since the recursive call to add starts with a new hidden state for the lstm controller",0
"
we would also like to point out that in this paper, we provide as an example one way to train a recursive neural program, by providing a certain training execution trace to the npi model",1
" as before, the non-recursive learned program lacks a provable guarantee of generalization, whereas we show later that the recursive learned program has one",0
", the trace corresponding to the problem “109 + 101”)",0
" the single-digit multiplication task in zaremba   (2016), the bubble sort task in reed & de freitas (2016), and the graph tasks in graves   (2016))",1
" the theorem states that for the same sequence of step inputs, the model produces the exact same step output as the target program it aims to learn",0
" at end, pointer ppivot is moved to the pivot compswap loop, move pivot lo, move j lo, swap
none
compswap loop runs the for loop inside the partition function compswap, move none compswap compares a[pivot] ≤ a[j]; if so, perform a swap and increment ppivot swap, move none set pivot lo sets ppivot to lo index none none set j lo sets pj to lo index none none set j null sets pj to −∞ none none stack pushes lo/hi states
onto stacks slo and shi according to argument (described below)
write, move described below
move moves pointer one unit up or down none described below swap swaps elements at given array indices none described below write write a value either to stack (e",0
" we made the assumption of equivalent lengths in order to parametrize the input format with respect to length, but this assumption can be removed as well",0
" , a1a0 + b1b0} are added properly",0
" fa8750-15-2-0104, and berkeley deep drive",1
"
directed acyclic graphs are structurally more diverse than inputs in the two tasks of grade-school addition and bubble sort",0
" in particular, in the explore function, adding a tail recursive call resets and stores the hidden states associated with vertices in a stack-like fashion",0
" for each task, given the verification set, we check the traces and results of the learned, to-be-verified neural program (described in section 4",1
" as an application, we incorporate recursion into the neural programmer-interpreter architecture and consider four sample tasks: grade-school addition, bubble sort, topological sort, and quicksort",0
2,0
" the pointers at index i1 and i2 are used to compare the pair of numbers considered during the bubble sweep, swapping them if the number at i1 is greater than that in i2",0
" the verification set can be very small, as we found a 10-element array ([8,2,1,2,0,8,5,8,3,7]) is sufficient to cover all of v ",0
"
base cases and reduction rules for quicksort",0
" as an application, we incorporate recursion into the neural programmer-interpreter architecture and consider four sample tasks: grade-school addition, bubble sort, topological sort, and quicksort",1
"
given a verification set, we can then run the model on the verification set to check if the produced traces and results are correct",1
"
base cases and reduction rules for quicksort",0
" qcolor(dag[vactive][childlist[vactive]]) refers to the color of the next child of vactive, used in the evaluation of the condition in the if branch in line 9 of algorithm 2",0
" an example of part of an addition task trace, written in shorthand, is given in figure 1",1
"as discussed in section 2, the neural programmer-interpreter (npi) is an instance of a neural programmer architecture and hence it naturally supports recursion",1
" ppivot and pj point to the pivot and j indices of the array, used in the partition function in algorithm 3",1
" as aforementioned, the npi model naturally supports recursion",0
" the pointer at index i3 represents a counter internal to the environment that is incremented once after each pass of the algorithm (one cycle of bubble and reset); when incremented a number of times equal to the length of the array, the flag i3 == length becomes true and terminates the entire algorithm ",0
 these pointers are referred to as bubble pointers,1
" we have three observations: the color of the start node, the color of the active node’s next child to be considered, and whether the stack is empty",0
2,0
" any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of national science foundation and darpa",1
" nevertheless, we would like the model to quickly learn to represent the right semantics of the underlying program from a small number of training examples, not an exhaustive number of them",1
" thus, recursion helps decompose a problem and makes it easier to reason about a program’s behavior for previously unseen situations such as longer inputs",1
 we leave this as future work to devise a verification procedure more appropriate to this setting,0
" observe that the non-recursive program’s correctness degrades for length 11 and beyond, while the recursive program can sort any given array",1
"
for topological sort, the domain-specific encoder is
fenc(dag,qcolor, pstack, pstart, vactive, childlist, at)
=mlp ([qcolor(pstart), qcolor(dag[vactive][childlist[vactive]]), pstack == 1, at(1), at(2), at(3)]),
where qcolor ∈ ru×4 is a scratch-pad that contains u rows, each containing one of four colors (white, gray, black, invalid) with one-hot encoding",0
" for non-primitive subprograms, we can update the observation-to-observation mapping currently associated with the subprogram and then apply that mapping to the current set",1
g,1
" m , as described in algorithm 1, processes the sequence of step inputs by using an lstm",0
" given verification sets that cover all the base cases and reduction rules, we can provide proofs that these learned programs generalize perfectly",0
" by iterating with this procedure, and then running p on the input observation set that we obtain for the entry point function, we can obtain v precisely",1
"
the operations we need to be concerned most about are carry and lshift, which induce partial environment states spanning two columns",1
" this is highly undesirable, since being able to provide the correct answer in all possible settings is one of the most important aspects of any learned neural program",1
 the memory updates of these neural programs are so complex and interdependent that it is difficult to reason about the behaviors of the learned neural program under previously unseen situations (such as problems with longer inputs),0
 this procedure verifies that all base cases and reductions are handled properly by the model via explicit tests,1
" the 4 environment observations aid with control flow; qstacklo(pstacklo− 1) < qstackhi(pstackhi− 1) implements the lo < hi comparison in line 5 of algorithm 3, pstacklo == 1 checks if the stacks are empty in line 18 of algorithm 4, and the other observations (all involving ppivot or pj) deal with logic in the partition function",1
"
to construct this set, by using the reference implementation of each subprogram, we construct a mapping between two sets of environment observations: the first set consists of all observations that can occur at the beginning of a particular subprogram’s invocation, and the second set contains the observations at the end of that subprogram",0
"
note that the training set can often be considerably smaller than the verification set, and despite this, the learned model can still pass the entire verification set",0
"
to construct this set, by using the reference implementation of each subprogram, we construct a mapping between two sets of environment observations: the first set consists of all observations that can occur at the beginning of a particular subprogram’s invocation, and the second set contains the observations at the end of that subprogram",0
 b1b0 where no carry operations occur,0
g,0
" the theorem states that for the same sequence of step inputs, the model produces the exact same step output as the target program it aims to learn",1
g,0
as mentioned in section 2,0
" because the context of the caller is stored on a stack when it calls another program and the callee starts in a fresh context, this enables recursion simply by allowing a program to call itself",0
" consequently, it is infeasible to construct a verification set that covers all cases, and the verification procedure we have described is inadequate",0
" also the dag can have potentially more than one connected component, meaning it is necessary to transition between these components appropriately",1
 we found that changing the npi training traces is a simple way to enable this,0
" thus, we demonstrate that recursion enables provably perfect generalization for different tasks, including addition, topological sort, quicksort, and a variant of bubble sort",0
" the verification set can be very small, as we found a 10-element array ([8,2,1,2,0,8,5,8,3,7]) is sufficient to cover all of v ",1
", the verification set",1
" thus, recursion helps decompose a problem and makes it easier to reason about a program’s behavior for previously unseen situations such as longer inputs",1
"
we construct input problems by splitting into two cases: one case in which the left column contains a null value and another in which the left column does not contain any null values",1
" this makes it so that we only need to consider the vertices in the subgraph that are currently relevant for computing the sort, allowing simpler reasoning about behavior for large graphs",1
" the pointers at index i1 and i2 are used to compare the pair of numbers considered during the bubble sweep, swapping them if the number at i1 is greater than that in i2",0
 reset represents a −∞ value,0
" naı̈vely, we might expect to synthesize and test an input for any sequence created by combining the four possible colors in two variables and another boolean variable for whether the stack is empty (so 32 possible observations at any point), but for various reasons, most of these combinations are impossible to occur at any given point in the execution trace",0
" we prove every member of f is added properly, given that subproblems s = {anan−1 + bnbn−1, an−1an−2 + bn−1bn−2, ",0
"
in this paper, we propose to resolve these issues by explicitly incorporating recursion into neural architectures",0
" although the model is exposed to more and more data, it might learn spurious and overly complex representations of the program, as suggested in zaremba   (2016)",1
 the logic for the non-recursive trace is shown in algorithm 4 in the appendix,1
" table 1 presents results on randomly generated arrays of varying length for the learned non-recursive, partially recursive, and full recursive programs",0
" the core module of the npi is an lstm controller that takes as input a slice of the current external environment, via a set of pointers, and a program and arguments to execute",0
as mentioned in section 2,0
" for each length, we test each program on 30 randomly generated problems",1
"
for addition, we analytically determine the verification set",0
g,0
3 to construct v and then empirically create a verification set which covers v ,1
" in particular, this condition is satisfied by npi, and thus the npi model naturally supports recursion (even though the authors of npi did not consider this aspect explicitly)",0
"
under the assumption that there are no leading 0’s (except in the case of single digits) and the two numbers to be added have the same number of digits, the verification set for addition contains 20,181 input problems",1
"
move",0
"
algorithm 2 depth first search topological sort 1: color all vertices white",1
" lshift moves the four pointers to the left, to move to the next column",0
"
we refer the reader to the appendix for the non-recursive trace generating functions",0
e,0
" this agrees with the generalization of non-recursive addition in reed & de freitas (2016), where they reported generalization up to 3000 digits",1
" the recursive version of topological sort solves all graph instances we tried, from graphs of size 5 through 70",0
" this is possible if and only if the graph has no directed cycles; that is to say, it must be a directed acyclic graph (dag)",1
 ,0
" we call this set of inputs the verification set, sv ",0
"
in order to demonstrate that recursion can help learn and generalize better, for addition, we trained only on traces for 5 arbitrarily chosen 1-digit addition sum examples",1
" 16: p = partition(a, lo, hi) 17: push p+ 1 and hi to slo and shi",1
 the model described in the training setup of section 4 (trained on 6 traces) was verified to be correct via the matching procedure described in section 4,1
"
arg 2 (increment or decrement): up, down1 // top level topological sort call 2 toposort() { 3 while (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 } 0 11 traverse() { 12 check_child() 13 explore() 14 } 15 16 check_child() { 17 while (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 18 move(childlist[vactive], up) 19 } 20 21 explore() { 22 do 23 if (qcolor(dag[vactive][childlist[vactive]]) is white): 24 write(color_next, color_grey) 25 stack(push) 26 write(save) 27 write(active_neighb) 28 move(childlist[vsave], up) 29 else: 30 write(color_curr, color_black) 31 write(result) 32 move(presult, up) 33 if(pstack == 1): 34 break 35 else: 36 stack(pop) 37 check_child() 38 while (true) 39 } 40 41 stack(op) { 42 if (op == push): 43 write(stack_push) 44 move(pstack, up) 45 46 if (op == pop): 47 write(active_stack) 48 write(stack_pop) 49 move(pstack, down) 50 } 51 52 next_start() { 53 while(qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 54 move(pstart, up) 55 }altered recursive functions  // top level topological sort call 2 toposort() { 3 if (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 toposort() // recursive call 0 } 11 12 check_child() { 13 if (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 14 move(childlist[vactive], up) 15 check_child() // recursive call 16 } 17 18 explore() { 19 if (qcolor(dag[vactive][childlist[vactive]]) is white): 20 write(color_next, color_grey) 21 stack(push) 22 write(save) 23 write(active_neighb) 24 move(childlist[vsave], up) 25 else: 26 write(color_curr, color_black) 27 write(result) 28 move(presult, up) 29 if(pstack == 1): 30 return 31 else: 32 stack(pop) 33 check_child() 34 explore() // recursive call 35 } 36 37 next_start() { 38 if (qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 39 move(pstart, up) 40 next_start() // recursive call 41 }algorithm 4 iterative quicksort 1: initialize an array a to sort and two empty stacks slo and shi",0
"
we refer the reader to the appendix for the non-recursive trace generating functions",1
" thus, we demonstrate that recursion enables provably perfect generalization for different tasks, including addition, topological sort, quicksort, and a variant of bubble sort",0
"
the operations we need to be concerned most about are carry and lshift, which induce partial environment states spanning two columns",0
" for grade-school addition, the domain-specific encoder is
fenc(q, i1, i2, i3, i4, at) =mlp ([q(1, i1), q(2, i2), q(3, i3), q(4, i4), at(1), at(2), at(3)]),
where the environment q ∈ r4×n×k is a scratch-pad that contains four rows (the first input number, the second input number, the carry bits, and the output) and n columns",1
" the single-digit multiplication task in zaremba   (2016), the bubble sort task in reed & de freitas (2016), and the graph tasks in graves   (2016))",1
"
through careful reasoning about the possible set of environment observations created by all valid inputs, and how each of the operations in the execution trace affects the environment, we can construct v using the procedure described in section 3",1
" if the step output specifies a primitive function call, we need to reason about how it can affect the environment so as to change the observation in the next step input",0
" for each function we use to implement the recursive version of topological sort, we need to consider the set of possible environment observation sequences we can create from all valid inputs and test that the learned program produces the correct behavior on each of these inputs",1
" as a consequence, our learned neural programs empirically achieve perfect generalization from a very small number of training examples",0
g,1
" , n , where the dag contains n vertices",0
"
topological sort",0
 errors arise due to undesirable and uninterpretable dependencies and associations the architecture learns to store in some high-dimensional hidden state,0
" consequently, there is no concept of length relevant to the problem, which has traditionally been an important focus of length-based curriculum learning",1
" for instance, for recursive addition, consider the family f of addition problems anan−1 ",0
"we show that if we incorporate recursion, the learned npi programs can achieve provably perfect generalization for different tasks",1
as mentioned in section 2,0
 ,0
" the stack operation has the following arguments:
arg 1 (operation): stack push call1, stack push call2, stack pop
stack push call1 pushes lo and pivot−1 toqstacklo andqstackhi",0
 the core controller acts as a dispatcher for the programs,1
"
we would also like to point out that in this paper, we provide as an example one way to train a recursive neural program, by providing a certain training execution trace to the npi model",0
" the only model, to our knowledge, that does not train on input-output pairs is the neural programmer-interpreter (reed & de freitas, 2016), which trains on synthetic execution traces",0
3,1
6,0
" on the other hand, the non-recursive version has low accuracy, beginning from size 5, and fails completely for graphs of size 8 and beyond",0
"
we describe the details of the npi model relevant to our contributions",0
" for many problems, the base cases and reduction rules usually consist of a finite (often small) number of cases",0
 the degree for any vertex in the dag is variable,0
 we find that recursion makes it easier for the network to learn the right program and generalize to unknown situations,1
", the trace corresponding to the problem “109 + 101”)",1
" if the step output specifies a primitive function call, we need to reason about how it can affect the environment so as to change the observation in the next step input",1
"
the three environment observations aid with control flow in algorithm 2",1
 recursion enables provably perfect generalization,1
" for each length, we test each program on 30 randomly generated problems",1
 reset represents a −∞ value,0
" the partial recursion is not enough for perfect generalization, as will be presented later in section 4",1
"
to evaluate a neural network that learns a neural program to accomplish a certain task, one common evaluation metric is how well the learned model m generalizes",0
", the trace corresponding to the array [3,2])",0
" this is possible if and only if the graph has no directed cycles; that is to say, it must be a directed acyclic graph (dag)",1
" as with the others, we apply the procedure described in section 3",0
" for non-primitive subprograms, we can update the observation-to-observation mapping currently associated with the subprogram and then apply that mapping to the current set",0
 the original version of the bubblesort implementation exposes the values within the array,1
" we have three observations: the color of the start node, the color of the active node’s next child to be considered, and whether the stack is empty",1
" we also hope to decrease supervision, for example by training with only partial or non-recursive traces, and to develop novel neural programming architectures integrated directly with a notion of recursion",0
" observe that partially recursive does slightly better than non-recursive for the setting in which the length of the array is 3, and that the fully recursive version is able to sort every array given to it",1
", the verification set",0
" in fact, it is one of our future directions to explore new ways to train a recursive neural program without providing explicit training execution traces or with only partial or non-recursive traces",1
" thus, any general neural programming architecture supporting such a call structure can be made to support recursion",0
"
in this paper, we propose that recursion is an important concept for neural programs as well",1
"
arg 2 (increment or decrement): up, down1 // top level topological sort call 2 toposort() { 3 while (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 } 0 11 traverse() { 12 check_child() 13 explore() 14 } 15 16 check_child() { 17 while (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 18 move(childlist[vactive], up) 19 } 20 21 explore() { 22 do 23 if (qcolor(dag[vactive][childlist[vactive]]) is white): 24 write(color_next, color_grey) 25 stack(push) 26 write(save) 27 write(active_neighb) 28 move(childlist[vsave], up) 29 else: 30 write(color_curr, color_black) 31 write(result) 32 move(presult, up) 33 if(pstack == 1): 34 break 35 else: 36 stack(pop) 37 check_child() 38 while (true) 39 } 40 41 stack(op) { 42 if (op == push): 43 write(stack_push) 44 move(pstack, up) 45 46 if (op == pop): 47 write(active_stack) 48 write(stack_pop) 49 move(pstack, down) 50 } 51 52 next_start() { 53 while(qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 54 move(pstart, up) 55 }altered recursive functions  // top level topological sort call 2 toposort() { 3 if (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 toposort() // recursive call 0 } 11 12 check_child() { 13 if (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 14 move(childlist[vactive], up) 15 check_child() // recursive call 16 } 17 18 explore() { 19 if (qcolor(dag[vactive][childlist[vactive]]) is white): 20 write(color_next, color_grey) 21 stack(push) 22 write(save) 23 write(active_neighb) 24 move(childlist[vsave], up) 25 else: 26 write(color_curr, color_black) 27 write(result) 28 move(presult, up) 29 if(pstack == 1): 30 return 31 else: 32 stack(pop) 33 check_child() 34 explore() // recursive call 35 } 36 37 next_start() { 38 if (qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 39 move(pstart, up) 40 next_start() // recursive call 41 }algorithm 4 iterative quicksort 1: initialize an array a to sort and two empty stacks slo and shi",1
 table 3 presents results on randomly generated arrays of varying length for the learned non-recursive and recursive programs,0
"
through careful reasoning about the possible set of environment observations created by all valid inputs, and how each of the operations in the execution trace affects the environment, we can construct v using the procedure described in section 3",1
", pstart or childlist[vactive]) up or down none described belowwrite",0
"
algorithm 3 recursive quicksort 1: initialize an array a to sort",0
 the training set for addition contains 200 traces,0
" lshift, used in reset, shifts the pointers left until reaching the start token",1
 reset represents a −∞ value,0
" for each graph size, we test the learned programs on 30 randomly generated dags",1
" provably perfect generalization implies the model will behave correctly, given any valid input",0
 npi then outputs the return probability and next program and arguments to execute,0
" provably perfect generalization implies the model will behave correctly, given any valid input",1
" childlist ∈ nu is a vector of pointers, where childlist[i] points to the next child under consideration for vertex i",1
 the core controller acts as a dispatcher for the programs,0
" this functionality the original npi paper decodes to a program key embedding kt ∈ rk and then computes a program embedding pt+1, which we also did in our implementation, but we omit this for brevity",0
 there are 2 variables (vactive and vsave); vactive holds the active vertex (as in algorithm 2) and vsave holds the value of vactive before executing line 12 of algorithm 2,0
"
arg 2 (object to copy): env stack lo peek, env stack hi peek, phi, plo, ppivot − 1, ppivot + 1, reset
env stack lo peek and env stack hi peek represent qstacklo(pstacklo − 1) and qstackhi(pstackhi − 1), respectively",1
" in future work, we would like to decrease supervision and construct models that are capable of coming up with recursive abstractions themselves",0
 we found that changing the npi training traces is a simple way to enable this,1
" however, models that make use of this strategy eventually fail after a certain level of complexity (e",1
 the maximum problem length in this training set is 3 (e,0
"program descriptions calls arguments quicksort run the quicksort
routine in place for the array a, for indices from lo to hi non-recursive: partition, stack, write recursive: same as non-recursive version, along with quicksort implicitly: array a to sort, lo, hi
partition runs the partition function",0
" although the model is exposed to more and more data, it might learn spurious and overly complex representations of the program, as suggested in zaremba   (2016)",0
", an lstm in npi’s case, but possibly other networks in different cases",1
"
the same set of problems was used to generate the training traces for all formulations of the task, for non-recursive and recursive versions",1
 some functional programming languages go so far as not to define any looping constructs but rely solely on recursion to enable repeated execution of the same code,0
" 3: 4: function partition(a, lo, hi) 5: pivot = lo 6: for j ∈ [lo, hi− 1] : do 7: if a[j] ≤ a[hi] then 8: swap a[pivot] with a[j] 9: pivot = pivot+ 1 0: swap a[pivot] with a[hi] 11: return pivot 12: 13: function quicksort(a, lo, hi) 14: while slo and shi are not empty: do 15: pop states off slo and shi, writing them to lo and hi",0
"we describe how models trained with recursive traces can be proven to generalize, by using the verification procedure described in section 3",1
" thus, recursion helps decompose a problem and makes it easier to reason about a program’s behavior for previously unseen situations such as longer inputs",0
"
figure 1 shows examples of non-recursive and recursive addition traces",0
 almost all architectures train on program input/output pairs,0
 the first is the actual model architecture,0
" in this version, there is a dependence on length: bstep and lshift are called a number of times equivalent to one less than the length of the input array, in bubble and reset respectively",0
", pstart or childlist[vactive]) up or down none described belowwrite",1
" without using a recursive program, such a proof is not possible, because the non-recursive program runs on an arbitrarily long addition problem that creates correspondingly long sequences of step inputs; in the non-recursive formulation of addition, add calls add1 a number of times that is dependent on the length of the input",1
" if yes, then this indicates that the learned neural program achieves provably perfect generalization",0
" the space of possible programs is extremely large, and composing a program that performs robustly on the infinite space of possible inputs is difficult—in part because it is impractical to obtain enough training examples to easily disambiguate amongst all possible programs",0
"
for topological sort, the domain-specific encoder is
fenc(dag,qcolor, pstack, pstart, vactive, childlist, at)
=mlp ([qcolor(pstart), qcolor(dag[vactive][childlist[vactive]]), pstack == 1, at(1), at(2), at(3)]),
where qcolor ∈ ru×4 is a scratch-pad that contains u rows, each containing one of four colors (white, gray, black, invalid) with one-hot encoding",1
" in fact, it is one of our future directions to explore new ways to train a recursive neural program without providing explicit training execution traces or with only partial or non-recursive traces",0
" the recursive version can generalize perfectly to long problems constructed from these components (such as the sum “822+233”, where “8+2” and “2+3” are in the training set), but the non-recursive version fails to sum these long problems properly",0
"
bubble sort",0
" in computer science, recursion (as opposed to iteration) involves solving a larger problem by combining solutions to smaller instances of the same problem",0
 npi then outputs the return probability and next program and arguments to execute,0
" the core lstm module’s hidden state is preserved over all these add1 calls, and it is difficult to interpret with certainty what happens over longer timesteps without concretely evaluating the lstm with an input of that length",0
"
it is important to emphasize that at inference time in the npi, the hidden state of the lstm controller is reset (to zero) at each subprogram call, as in line 3 of algorithm 1 (h ← 0)",0
"we train using the adam optimizer and use a 2-layer lstm and task-specific state encoders for the external environments, as described in reed & de freitas (2016)",0
" also the dag can have potentially more than one connected component, meaning it is necessary to transition between these components appropriately",0
" consequently, there is no concept of length relevant to the problem, which has traditionally been an important focus of length-based curriculum learning",0
" the theorem states that for the same sequence of step inputs, the model produces the exact same step output as the target program it aims to learn",0
" we also hope to decrease supervision, for example by training with only partial or non-recursive traces, and to develop novel neural programming architectures integrated directly with a notion of recursion",1
 the core controller acts as a dispatcher for the programs,0
" note that recursion helps make this process tractable, because we only need to test a finite number of inputs to show that the model will work correctly on inputs of unbounded complexity",1
" the stack operation has the following arguments:
arg 1 (operation): stack push call1, stack push call2, stack pop
stack push call1 pushes lo and pivot−1 toqstacklo andqstackhi",1
" lshift, used in reset, shifts the pointers left until reaching the start token",0
training neural networks to synthesize robust programs from a small number of examples is a challenging task,1
" the outputs rt, pt+1, at+1 are used to determine the next action, as described in algorithm 1",0
" qcolor(pstart) contains the color of the current start vertex, used in the evaluation of the condition in the while loop in line 5 of algorithm 2",1
", the verification set",0
"
we propose and describe our verification procedure",0
" given verification sets that cover all the base cases and reduction rules, we can provide proofs that these learned programs generalize perfectly",1
 we find that recursion makes it easier for the network to learn the right program and generalize to unknown situations,1
" by iterating with this procedure, and then running p on the input observation set that we obtain for the entry point function, we can obtain v precisely",0
" plo and phi point to the lo and hi indices of the array, as in algorithm 3",0
"
by reasoning about the possible set of environment observations created by all valid inputs, we construct v using the procedure described in section 3",1
"
directed acyclic graphs are structurally more diverse than inputs in the two tasks of grade-school addition and bubble sort",0
" the partial recursion is not enough for perfect generalization, as will be presented later in section 4",0
" we have three observations: the color of the start node, the color of the active node’s next child to be considered, and whether the stack is empty",1
" as with the original npi architecture, the experiments for this paper always used a 3-tuple of integers at = (at(1), at(2), at(3))",1
" the contents ofqresult andqstack are not exposed directly through the domain-specific encoder; rather, we define primitive functions which manipulate these scratch-pads",0
" in our experiments, we only present dag’s as inputs and represent the vertices as values ranging from 1, ",0
" also, compared to the bubble pointers in bubble sort, the pointers that perform the comparison for quicksort (the compswap function) are usually not adjacent to each other, making quicksort less local than bubble sort",0
" we prove every member of f is added properly, given that subproblems s = {anan−1 + bnbn−1, an−1an−2 + bn−1bn−2, ",0
 recursion can be implemented differently for different neural programming models,0
" we have three observations: the color of the start node, the color of the active node’s next child to be considered, and whether the stack is empty",1
2,0
" although the model is exposed to more and more data, it might learn spurious and overly complex representations of the program, as suggested in zaremba   (2016)",0
we now report on generalization for the varying tasks,0
e,1
 table 3 presents results on randomly generated arrays of varying length for the learned non-recursive and recursive programs,1
" even though the earlier version also generalized just as well in practice, relatively small differences in the formulation of the traces and the environment observations can drastically change the difficulty of verification",1
 the model described in the training setup of section 4 (trained on 6 traces) was verified to be correct via the matching procedure described in section 4,0
 we leave this as future work to devise a verification procedure more appropriate to this setting,1
" we note that an earlier version of quicksort we tried lacked primitive operations to directly move a pointer to another, and therefore needed more functions and observations",1
3,0
" the learned recursive program is different from neural programs learned in all previous work in an important aspect: previous approaches do not explicitly incorporate this abstraction, and hence generalize poorly, whereas our learned neural programs incorporate recursion and achieve perfect generalization",1
" in particular, we construct new training traces which explicitly contain recursive elements and show that with this type of trace, npi easily learns recursive programs",1
", to change the value of phi) none described belowstack",0
 the environment and return probability are omitted for readability,1
" if the program is primitive, the next environmental state et+1 will be affected by pt and at, i",0
"
base cases and reduction rules for topological sort",1
" numerous models have been proposed for learning programs; to name a few, this includes the differentiable neural computer (graves  , 2016), neural turing machine (graves  , 2014), neural gpu (kaiser & sutskever, 2015), neural programmer (neelakantan  , 2015), pointer network (vinyals  , 2015), hierarchical attentive memory (andrychowicz & kurach, 2016), and neural random access machine (kurach  , 2016)",1
" if it is too large, then the semantics of p might not be well-defined on some elements in v , or the spurious step input sequences may not be reachable from any valid problem input (e",1
"
quicksort",0
" for brevity, we refer the reader to the appendix for further details on the program set and non-recursive and recursive trace-generating functions used for topological sort",0
g,1
 b1b0 where no carry operations occur,1
" as described in the verification procedure, it is possible to prove our learned recursive program generalizes perfectly by testing on an appropriate set of problem inputs, i",0
" note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values, and arg 2 can only take one of 7 values",1
" in contrast, each call to the recursive add always runs for a fixed number of steps, even on arbitrarily long problems in f , so we can test that it performs correctly on a small, fixed number of step input sequences",1
" as described in the verification procedure, it is possible to prove our learned recursive program generalizes perfectly by testing on an appropriate set of problem inputs, i",0
as mentioned in section 2,0
" nevertheless, we would like the model to quickly learn to represent the right semantics of the underlying program from a small number of training examples, not an exhaustive number of them",0
", pstart or childlist[vactive]) up or down none described belowwrite",0
" both the non-recursive and recursive learned programs generalize on all graphs we tried, up to 120 vertices",0
" we also hope to decrease supervision, for example by training with only partial or non-recursive traces, and to develop novel neural programming architectures integrated directly with a notion of recursion",1
 the training set for addition contains 200 traces,0
"
for addition, we analytically determine the verification set",0
 we found that changing the npi training traces is a simple way to enable this,0
" in this version of curriculum learning, even though the inputs are gradually becoming more complex, the semantics of the program is succinct and does not change",0
" for non-primitive subprograms, we can update the observation-to-observation mapping currently associated with the subprogram and then apply that mapping to the current set",1
" plo and phi point to the lo and hi indices of the array, as in algorithm 3",0
"
we also report on generalization results for the non-recursive and recursive versions of this variant of bubble sort",0
" 2: initialize lo and hi to be 1 and n, where n is the length of a",0
" while this matches the description from reed & de freitas (2016), we found that this causes an unnecessary blowup in the size of v and makes it much more difficult to construct the verification set",0
" qcolor(dag[vactive][childlist[vactive]]) refers to the color of the next child of vactive, used in the evaluation of the condition in the if branch in line 9 of algorithm 2",0
" bstep, used in bubble, compares pairs of numbers, continuously moving the bubble pointers once to the right each time until reaching the end of the array",0
 the core controller acts as a dispatcher for the programs,1
"
arg 2 (increment or decrement): up, down1 // top level topological sort call 2 toposort() { 3 while (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 } 0 11 traverse() { 12 check_child() 13 explore() 14 } 15 16 check_child() { 17 while (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 18 move(childlist[vactive], up) 19 } 20 21 explore() { 22 do 23 if (qcolor(dag[vactive][childlist[vactive]]) is white): 24 write(color_next, color_grey) 25 stack(push) 26 write(save) 27 write(active_neighb) 28 move(childlist[vsave], up) 29 else: 30 write(color_curr, color_black) 31 write(result) 32 move(presult, up) 33 if(pstack == 1): 34 break 35 else: 36 stack(pop) 37 check_child() 38 while (true) 39 } 40 41 stack(op) { 42 if (op == push): 43 write(stack_push) 44 move(pstack, up) 45 46 if (op == pop): 47 write(active_stack) 48 write(stack_pop) 49 move(pstack, down) 50 } 51 52 next_start() { 53 while(qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 54 move(pstart, up) 55 }altered recursive functions  // top level topological sort call 2 toposort() { 3 if (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 toposort() // recursive call 0 } 11 12 check_child() { 13 if (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 14 move(childlist[vactive], up) 15 check_child() // recursive call 16 } 17 18 explore() { 19 if (qcolor(dag[vactive][childlist[vactive]]) is white): 20 write(color_next, color_grey) 21 stack(push) 22 write(save) 23 write(active_neighb) 24 move(childlist[vsave], up) 25 else: 26 write(color_curr, color_black) 27 write(result) 28 move(presult, up) 29 if(pstack == 1): 30 return 31 else: 32 stack(pop) 33 check_child() 34 explore() // recursive call 35 } 36 37 next_start() { 38 if (qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 39 move(pstart, up) 40 next_start() // recursive call 41 }algorithm 4 iterative quicksort 1: initialize an array a to sort and two empty stacks slo and shi",0
" our results show that for all learned, to-be-verified neural programs, they all produced the same traces as those produced by p on the verification set",0
" as with the others, we apply the procedure described in section 3",0
" consequently, there is no concept of length relevant to the problem, which has traditionally been an important focus of length-based curriculum learning",0
" if the program is primitive, the next environmental state et+1 will be affected by pt and at, i",1
"
as a concrete instantiation, we show in this paper that we can enable recursive neural programs in the npi model, and thus enable perfectly generalizable neural programs for tasks such as sorting where the original, non-recursive npi program fails",0
" as before, the non-recursive learned program lacks a provable guarantee of generalization, whereas we show later that the recursive learned program has one",1
" if the program is primitive, the next environmental state et+1 will be affected by pt and at, i",1
"
in what follows, we describe the way in which we constructed npi training traces so as to make them contain recursive elements and thus enable npi to learn recursive programs",1
" the npi model has three learnable components: a task-agnostic core, a program-key embedding, and domain-specific encoders that allow the npi to operate in diverse environments",1
" in this section, we give a brief review of the npi architecture from reed & de freitas (2016) as background",1
", pstart or childlist[vactive]) up or down none described belowwrite",0
e,0
"
it is important to emphasize that at inference time in the npi, the hidden state of the lstm controller is reset (to zero) at each subprogram call, as in line 3 of algorithm 1 (h ← 0)",1
"6) on the verification set, and ensure they match the traces produced by the true program p ",0
training neural networks to synthesize robust programs from a small number of examples is a challenging task,0
 the environment and return probability are omitted for readability,0
"
we construct input problems by splitting into two cases: one case in which the left column contains a null value and another in which the left column does not contain any null values",1
" although the model is exposed to more and more data, it might learn spurious and overly complex representations of the program, as suggested in zaremba   (2016)",1
" m is considered to have perfect generalization if m can give the right answer for any input, such as inputs of arbitrary length",1
"
bubble sort",1
 indentation indicates the stack is one level deeper than before,0
e,1
", the trace corresponding to the problem “109 + 101”)",0
"we describe how models trained with recursive traces can be proven to generalize, by using the verification procedure described in section 3",0
" in computer science, recursion (as opposed to iteration) involves solving a larger problem by combining solutions to smaller instances of the same problem",0
"6) on the verification set, and ensure they match the traces produced by the true program p ",0
"in this paper, we propose that the key abstraction of recursion is necessary for neural programs to generalize",0
" given a precise definition of p , it may be possible to automate the generation of v from p in future work",0
" as aforementioned, the npi model naturally supports recursion",1
" in particular, given that a recursion is defined by two properties as mentioned before, the base cases and the set of reduction rules, we can prove a recursive neural program generalizes perfectly if we can prove that (1) it performs correctly on the base cases; (2) it learns the reduction rules correctly",0
" partial recursion only adds a tail recursive call to bubblesort after bubble and reset, similar to the tail recursive call described previously for addition",1
 we found that changing the npi training traces is a simple way to enable this,0
" in this version of curriculum learning, even though the inputs are gradually becoming more complex, the semantics of the program is succinct and does not change",1
" that is to say, the network does not learn the true program semantics",0
"
figure 1 shows examples of non-recursive and recursive addition traces",0
 the model described in the training setup of section 4 (trained on 6 traces) was verified to be correct via the matching procedure described in section 4,0
"
algorithm 2 shows the topological sort task of interest",1
" here as a concrete and general example, we consider a general neural programming architecture (npa), similar to neural programmer-interpreter (npi) in reed & de freitas (2016)",1
g,1
", the trace corresponding to the array [3,2])",1
" the npi model has three learnable components: a task-agnostic core, a program-key embedding, and domain-specific encoders that allow the npi to operate in diverse environments",0
 recursion can be implemented differently for different neural programming models,1
 table 3 presents results on randomly generated arrays of varying length for the learned non-recursive and recursive programs,0
"
to evaluate a neural network that learns a neural program to accomplish a certain task, one common evaluation metric is how well the learned model m generalizes",0
" consequently, there is no concept of length relevant to the problem, which has traditionally been an important focus of length-based curriculum learning",1
" finally, we implement recursive traces for our own topological sort and quicksort tasks",1
" thus, any general neural programming architecture supporting such a call structure can be made to support recursion",0
" as aforementioned, the npi model naturally supports recursion",1
" the recursive version of topological sort solves all graph instances we tried, from graphs of size 5 through 70",0
"
since npi naturally supports the notion of recursion, a key question is how to enable npi to learn recursive programs",0
"we show that if we incorporate recursion, the learned npi programs can achieve provably perfect generalization for different tasks",1
"6) on the verification set, and ensure they match the traces produced by the true program p ",0
" if a white vertex is found, set pstart to point to it; this signifies the start of a traversal of a new connected component",0
" the theorem states that for the same sequence of step inputs, the model produces the exact same step output as the target program it aims to learn",0
" k is set to 11, to denote the range of possible numbers (0 through 9), along with the start/end token (represented with the same encoding) which is observed when a pointer reaches beyond the bounds of the input",1
" while this matches the description from reed & de freitas (2016), we found that this causes an unnecessary blowup in the size of v and makes it much more difficult to construct the verification set",0
" however, models that make use of this strategy eventually fail after a certain level of complexity (e",1
"
base cases and reduction rules for addition",1
" , a1a0 + b1b0} are added properly",0
we emphasize the overall goal of this work is to enable the learning of a recursive program,0
"
as in line 13 of the right-hand side of figure 1",1
" in practice, we can additionally use tail recursion optimization to avoid problems with the call stack growing too deep",1
" if the step output specifies a primitive function call, we need to reason about how it can affect the environment so as to change the observation in the next step input",0
" plo and phi point to the lo and hi indices of the array, as in algorithm 3",1
"
figure 1 shows examples of non-recursive and recursive addition traces",1
3,0
g,1
" nevertheless, we would like the model to quickly learn to represent the right semantics of the underlying program from a small number of training examples, not an exhaustive number of them",1
" as described in the verification procedure, it is possible to prove our learned recursive program generalizes perfectly by testing on an appropriate set of problem inputs, i",1
"
the same set of problems was used to generate the training traces for all formulations of the task, for non-recursive and recursive versions",1
 the degree for any vertex in the dag is variable,0
" the recursive version can generalize perfectly to long problems constructed from these components (such as the sum “822+233”, where “8+2” and “2+3” are in the training set), but the non-recursive version fails to sum these long problems properly",0
" , a1a0 + b1b0} are added properly",0
"
algorithm 2 depth first search topological sort 1: color all vertices white",1
"
formally, verification consists of proving the following theorem:
∀i ∈ v,m(i) ⇓ p (i) where i denotes a sequence of step inputs (within one function call), v denotes the set of valid sequences of step inputs, m denotes the neural network model, p denotes the correct program, and p (i) denotes the next step output from the correct program",0
" for bubble sort, the domain-specific encoder is
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1), q(1, i2), i3 == length, at(1), at(2), at(3)]),
where the environment q ∈ r1×n×k is a scratch-pad that contains 1 row, to represent the state of the array as sorting proceeds in-place, andn columns",0
 the original version of the bubblesort implementation exposes the values within the array,1
" for brevity, we refer the reader to the appendix for information about the program set and non-recursive and recursive trace-generating functions for quicksort",1
"
as a concrete instantiation, we show in this paper that we can enable recursive neural programs in the npi model, and thus enable perfectly generalizable neural programs for tasks such as sorting where the original, non-recursive npi program fails",0
g,0
 we find that recursion makes it easier for the network to learn the right program and generalize to unknown situations,1
"1, we hypothesize the non-recursive programs do not generalize well because they have learned spurious dependencies specific to the training set, such as length of the input problems",0
" for each graph size, we test the learned programs on 30 randomly generated dags",0
" in the non-recursive trace, there are four functions that can be made recursive—toposort, check child, explore, and next start, and we add a tail recursive call to each of these functions in order to make the recursive trace",0
" for these programs, we re-use the appropriate program sets (the associated subprograms), and we refer the reader to the appendix of reed & de freitas (2016) for further details on the subprograms used in addition and bubble sort",0
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,0
" as a consequence, our learned neural programs empirically achieve perfect generalization from a very small number of training examples",0
 3: begin traversing from vertex 1 in the dag,0
" here as a concrete and general example, we consider a general neural programming architecture (npa), similar to neural programmer-interpreter (npi) in reed & de freitas (2016)",0
" in this version of curriculum learning, even though the inputs are gradually becoming more complex, the semantics of the program is succinct and does not change",0
"
we describe the details of the npi model relevant to our contributions",1
 npi then outputs the return probability and next program and arguments to execute,1
"
under the assumption that there are no leading 0’s (except in the case of single digits) and the two numbers to be added have the same number of digits, the verification set for addition contains 20,181 input problems",0
" naı̈vely, we might expect to synthesize and test an input for any sequence created by combining the four possible colors in two variables and another boolean variable for whether the stack is empty (so 32 possible observations at any point), but for various reasons, most of these combinations are impossible to occur at any given point in the execution trace",0
3,0
 some functional programming languages go so far as not to define any looping constructs but rely solely on recursion to enable repeated execution of the same code,1
" finally, we implement recursive traces for our own topological sort and quicksort tasks",1
" for non-primitive subprograms, we can update the observation-to-observation mapping currently associated with the subprogram and then apply that mapping to the current set",0
"
note that v should also contain the necessary reductions, which corresponds to making the recursive calls at the correct time, as indicated by p ",0
"
in what follows, we describe the way in which we constructed npi training traces so as to make them contain recursive elements and thus enable npi to learn recursive programs",1
" table 1 presents results on randomly generated arrays of varying length for the learned non-recursive, partially recursive, and full recursive programs",1
"
to evaluate a neural network that learns a neural program to accomplish a certain task, one common evaluation metric is how well the learned model m generalizes",1
"
after finding v , we construct a set of problem inputs which, when executed on p , create exactly the step input sequences which make up v ",0
" however, our concept of recursion for neural programs is general",0
 the environment and return probability are omitted for readability,1
" we also hope to decrease supervision, for example by training with only partial or non-recursive traces, and to develop novel neural programming architectures integrated directly with a notion of recursion",1
"
the non-recursive trace loops on cycles of bubble and reset, which logically represents one bubble sweep through the array and reset of the two bubble pointers to the very beginning of the array, respectively",1
"
the training set for bubble sort contains 100 traces, with maximum problem length of 2 (e",1
 recursion enables provable guarantees on neural programs’ behavior without needing to exhaustively enumerate all possible inputs to the programs,0
 the original version of the bubblesort implementation exposes the values within the array,1
" the arrow in the theorem refers to evaluation, as in big-step semantics",0
" in future work, we seek to enable more tasks with recursive structure",0
" the corresponding edge list is [(1, 2), (1, 5), (2, 4), (2, 5), (3, 5)]",1
" empirically, existing models suffer from a common limitation—generalization becomes poor beyond a threshold level of complexity",0
 ,0
g,0
"
arg 2 (object to copy): env stack lo peek, env stack hi peek, phi, plo, ppivot − 1, ppivot + 1, reset
env stack lo peek and env stack hi peek represent qstacklo(pstacklo − 1) and qstackhi(pstackhi − 1), respectively",0
" we can obtain this mapping by first considering the possible observations that can arise at the beginning of the entry function (add, bubblesort, toposort, and quicksort) for some valid program input, and iteratively applying the observation-to-observation mapping implied by the reference implementation’s step output at that point in the execution",1
" if yes, then this indicates that the learned neural program achieves provably perfect generalization",0
" in this version, there is a dependence on length: bstep and lshift are called a number of times equivalent to one less than the length of the input array, in bubble and reset respectively",0
 almost all architectures train on program input/output pairs,1
" 3: 4: function partition(a, lo, hi) 5: pivot = lo 6: for j ∈ [lo, hi− 1] : do 7: if a[j] ≤ a[hi] then 8: swap a[pivot] with a[j] 9: pivot = pivot+ 1 0: swap a[pivot] with a[hi] 11: return pivot 12: 13: function quicksort(a, lo, hi) 14: while slo and shi are not empty: do 15: pop states off slo and shi, writing them to lo and hi",0
" finally, we implement recursive traces for our own topological sort and quicksort tasks",0
" without using a recursive program, such a proof is not possible, because the non-recursive program runs on an arbitrarily long addition problem that creates correspondingly long sequences of step inputs; in the non-recursive formulation of addition, add calls add1 a number of times that is dependent on the length of the input",0
" if the program is primitive, the next environmental state et+1 will be affected by pt and at, i",1
" given a precise definition of p , it may be possible to automate the generation of v from p in future work",1
 we leave this as future work to devise a verification procedure more appropriate to this setting,0
" note that recursion helps make this process tractable, because we only need to test a finite number of inputs to show that the model will work correctly on inputs of unbounded complexity",0
" qcolor(pstart) contains the color of the current start vertex, used in the evaluation of the condition in the while loop in line 5 of algorithm 2",0
 we found that changing the npi training traces is a simple way to enable this,0
" for many problems, the base cases and reduction rules usually consist of a finite (often small) number of cases",0
 the core controller acts as a dispatcher for the programs,0
" if it is too large, then the semantics of p might not be well-defined on some elements in v , or the spurious step input sequences may not be reachable from any valid problem input (e",1
 this algorithm is a variant of depth first search,0
 we instantiated recursion for the neural programmerinterpreter by changing the training traces,0
 ,1
"
grade-school addition",0
"
we also report on generalization results for the non-recursive and recursive versions of this variant of bubble sort",1
"
algorithm 1 neural programming inference 1: inputs: environment observation e, program p, arguments a, stop threshold α 2: function run(e, p, a) 3: h← 0, r ← 0 4: while r < α do 5: s← fenc(e, a), h← flstm(s, p, h) 6: r ← fend(h), p2 ← fprog(h), a2 ← farg(h) 7: if p is a primitive function then 8: e← fenv(e, p, a)",1
"
note that the recursion for quicksort is not purely tail recursive and therefore represents a more complex kind of recursion that is harder to learn than in the previous tasks",0
 the logic for the non-recursive trace is shown in algorithm 4 in the appendix,0
"
given a verification set, we can then run the model on the verification set to check if the produced traces and results are correct",0
" for each length, we test each program on 30 randomly generated problems",0
" for brevity, we refer the reader to the appendix for further details on the program set and non-recursive and recursive trace-generating functions used for topological sort",0
" as with the others, we apply the procedure described in section 3",1
"as discussed in section 2, the neural programmer-interpreter (npi) is an instance of a neural programmer architecture and hence it naturally supports recursion",1
"when constructing a neural network for the purpose of learning a program, there are two orthogonal aspects to consider",0
 ,0
" given a precise definition of p , it may be possible to automate the generation of v from p in future work",1
 we outline how to construct this set,0
" recursion is an important concept in programming languages and a critical tool to
reduce the complexity of programs",0
" the core lstm module’s hidden state is preserved over all these add1 calls, and it is difficult to interpret with certainty what happens over longer timesteps without concretely evaluating the lstm with an input of that length",0
" also the dag can have potentially more than one connected component, meaning it is necessary to transition between these components appropriately",0
" while this matches the description from reed & de freitas (2016), we found that this causes an unnecessary blowup in the size of v and makes it much more difficult to construct the verification set",1
" also, compared to the bubble pointers in bubble sort, the pointers that perform the comparison for quicksort (the compswap function) are usually not adjacent to each other, making quicksort less local than bubble sort",1
" each time a subprogram is called, the stack depth increases",1
" as a consequence, our learned neural programs empirically achieve perfect generalization from a very small number of training examples",1
" for problems where the base cases may be extremely large or infinite, such as certain forms of motor control, recursion can still help reduce the problem of generalization to these two aspects and make the generalization problem significantly simpler to handle and reason about",1
" note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values, and arg 2 can only take one of 7 values",0
"in this paper, we propose that the key abstraction of recursion is necessary for neural programs to generalize",0
"
we describe the base cases, reduction rules, and the verification set for each task in appendix a",0
" recall that this verification procedure cannot be performed for the non-recursive versions, since the propagation of the hidden state in the core lstm module makes reasoning difficult and so we would need to check an unbounded number of examples",1
"
through careful reasoning about the possible set of environment observations created by all valid inputs, and how each of the operations in the execution trace affects the environment, we can construct v using the procedure described in section 3",0
" m is considered to have perfect generalization if m can give the right answer for any input, such as inputs of arbitrary length",1
" if the program is primitive, the next environmental state et+1 will be affected by pt and at, i",0
 recursion enables provably perfect generalization,1
 b1b0 where no carry operations occur,1
training neural networks to synthesize robust programs from a small number of examples is a challenging task,1
" for problems where the base cases may be extremely large or infinite, such as certain forms of motor control, recursion can still help reduce the problem of generalization to these two aspects and make the generalization problem significantly simpler to handle and reason about",0
"
quicksort",1
" for each graph size, we test the learned programs on 30 randomly generated dags",1
" on the other hand, the non-recursive version has low accuracy, beginning from size 5, and fails completely for graphs of size 8 and beyond",0
" in this pair, the step input runs a subprogram add1 that has no arguments, and the step output contains a program write that has arguments of out and 1",0
 figure 2 shows examples of traces for the different versions of bubble sort,1
" we make the trace recursive by adding a tail recursive call into the trace for the add program after calling add1 and lshift, the original paper uses k = 10, but we found it necessary to augment the range with an end token, in order to terminate properly",1
e,0
" on the other hand, the non-recursive version has low accuracy, beginning from size 5, and fails completely for graphs of size 8 and beyond",0
 this makes it difficult to reason about what the model will do when given complex inputs,1
" in practice, we can additionally use tail recursion optimization to avoid problems with the call stack growing too deep",0
" if yes, then this indicates that the learned neural program achieves provably perfect generalization",0
 ,0
"
algorithm 2 shows the topological sort task of interest",1
" for brevity, we refer the reader to the appendix for information about the program set and non-recursive and recursive trace-generating functions for quicksort",0
" in this architecture, we consider a core controller, e",1
"
for addition, we analytically determine the verification set",0
" m is considered to have perfect generalization if m can give the right answer for any input, such as inputs of arbitrary length",0
"
formally, verification consists of proving the following theorem:
∀i ∈ v,m(i) ⇓ p (i) where i denotes a sequence of step inputs (within one function call), v denotes the set of valid sequences of step inputs, m denotes the neural network model, p denotes the correct program, and p (i) denotes the next step output from the correct program",0
 a1a0 + bnbn−1 ,1
" each step during an execution of the program does one of three things: (1) another subprogram along with associated arguments is called, as in line 10, (2) the program writes to the environment if it is primitive, as in line 8, or (3) the loop is terminated if the return probability exceeds a threshold α, after which the stack frame is popped and control is returned to the caller",0
 we then construct a verification set of size 73 by ensuring that randomly generated graphs cover the analytically derived v ,1
" for example, for the addition task, the model is trained on short inputs and then tested on its ability to sum inputs with much longer numbers of digits",1
" if yes, then this indicates that the learned neural program achieves provably perfect generalization",1
"
we describe the details of the npi model relevant to our contributions",1
" we performed experiments on the partially recursive version in order to examine what happens when only one recursive call is implemented, when in reality three are required for perfect generalization",1
" if yes, then this indicates that the learned neural program achieves provably perfect generalization",1
"as there is no public implementation of npi, we implemented a version of it in keras that is as faithful to the paper as possible",0
"
thus far, to evaluate the efficacy of neural models on programming tasks, the only metric that has been used is generalization of expected behavior to inputs of greater complexity (vinyals   (2015), kaiser & sutskever (2015), reed & de freitas (2016), graves   (2016), zaremba   (2016))",1
" in our experiments, we only present dag’s as inputs and represent the vertices as values ranging from 1, ",0
 ,0
"
recursion drastically reduces the number of configurations we need to consider during the verification phase and makes the proof tractable, because it introduces structure that eliminates infinitely long sequences of step inputs that would otherwise need to be considered",1
"1, we hypothesize the non-recursive programs do not generalize well because they have learned spurious dependencies specific to the training set, such as length of the input problems",1
 we adapt machinery from the original paper slightly to fit our needs,1
"
base cases and reduction rules for addition",0
" we show that by modifying the training procedure, we enable the npi model to learn recursive neural programs",0
" for brevity, we refer the reader to the appendix for information about the program set and non-recursive and recursive trace-generating functions for quicksort",1
" without using a recursive program, such a proof is not possible, because the non-recursive program runs on an arbitrarily long addition problem that creates correspondingly long sequences of step inputs; in the non-recursive formulation of addition, add calls add1 a number of times that is dependent on the length of the input",0
" the pointer at index i3 represents a counter internal to the environment that is incremented once after each pass of the algorithm (one cycle of bubble and reset); when incremented a number of times equal to the length of the array, the flag i3 == length becomes true and terminates the entire algorithm ",1
" formally, the npi is represented by the following set of equations:
st = fenc(et, at)
ht = flstm(st, pt, ht−1)
rt = fend(ht), pt+1 = fprog(ht), at+1 = farg(ht)
t is a subscript denoting the time-step; fenc is a domain-specific encoder (to be described later) that takes in the environment slice et and arguments at; flstm represents the core module, which takes in the state st generated by fenc, a program embedding pt ∈ rp , and hidden lstm state ht; fend decodes the return probability rt; fprog decodes a program key embedding pt+1;1 and farg decodes arguments at+1",0
 indentation indicates the stack is one level deeper than before,1
" there are 3 pointers (presult, pstack, pstart), presult points to the next empty location in qresult,
pstack points to the top of the stack in qstack, and pstart points to the candidate starting node for a connected component",1
" furthermore, given a verification input set that covers all base cases and reduction rules, we can formally prove that the learned neural programs achieve perfect generalization after verifying its behavior on the verification input set",1
"
in this paper, we propose that recursion is an important concept for neural programs as well",0
 we found that changing the npi training traces is a simple way to enable this,1
" because the context of the caller is stored on a stack when it calls another program and the callee starts in a fresh context, this enables recursion simply by allowing a program to call itself",1
 we created a program set that reflects the semantics of algorithm 2,0
" the assumption of leading 0’s can be easily removed, at the cost of slightly increasing the size of the verification set",0
g,0
" full recursion, in addition to making the aforementioned tail recursive call, adds two additional recursive calls; bstep and lshift are made tail recursive",0
 add1 is a subprogram that adds the current column (writing the appropriate digit to the output row and carrying a bit to the next column if needed),1
" although the model is exposed to more and more data, it might learn spurious and overly complex representations of the program, as suggested in zaremba   (2016)",0
"
the same set of problems was used to generate the training traces for all formulations of the task, for non-recursive and recursive versions",0
" the non-recursive and partially recursive versions are unable to sort long arrays, beyond length 8",1
 recursion can be implemented differently for different neural programming models,1
" qcolor(dag[vactive][childlist[vactive]]) refers to the color of the next child of vactive, used in the evaluation of the condition in the if branch in line 9 of algorithm 2",0
"
one common strategy to improve generalization is to use curriculum learning, where the model is trained on inputs of gradually increasing complexity",0
"
we also report on generalization results for the non-recursive and recursive versions of this variant of bubble sort",0
"
arg 2 (auxiliary variable): color grey, color black
color grey and color black color the given vertex grey and black, respectively",0
" observe that the non-recursive program’s correctness degrades for length 11 and beyond, while the recursive program can sort any given array",0
", the trace corresponding to the array [3,2])",0
 we then construct a verification set of size 73 by ensuring that randomly generated graphs cover the analytically derived v ,0
 we outline how to construct this set,1
" at end, pointer ppivot is moved to the pivot compswap loop, move pivot lo, move j lo, swap
none
compswap loop runs the for loop inside the partition function compswap, move none compswap compares a[pivot] ≤ a[j]; if so, perform a swap and increment ppivot swap, move none set pivot lo sets ppivot to lo index none none set j lo sets pj to lo index none none set j null sets pj to −∞ none none stack pushes lo/hi states
onto stacks slo and shi according to argument (described below)
write, move described below
move moves pointer one unit up or down none described below swap swaps elements at given array indices none described below write write a value either to stack (e",1
" each step during an execution of the program does one of three things: (1) another subprogram along with associated arguments is called, as in line 10, (2) the program writes to the environment if it is primitive, as in line 8, or (3) the loop is terminated if the return probability exceeds a threshold α, after which the stack frame is popped and control is returned to the caller",1
 almost all architectures train on program input/output pairs,0
" if it is too large, then the semantics of p might not be well-defined on some elements in v , or the spurious step input sequences may not be reachable from any valid problem input (e",0
3,1
" via the recursive call, we effectively forget that the column just added exists, since the recursive call to add starts with a new hidden state for the lstm controller",0
"
the non-recursive trace loops on cycles of add1 and lshift",1
" twc-1409915, darpa under grant no",0
"
algorithm 2 shows the topological sort task of interest",0
"1, we hypothesize the non-recursive programs do not generalize well because they have learned spurious dependencies specific to the training set, such as length of the input problems",0
 ,1
"
arg 2 (object to copy): env stack lo peek, env stack hi peek, phi, plo, ppivot − 1, ppivot + 1, reset
env stack lo peek and env stack hi peek represent qstacklo(pstacklo − 1) and qstackhi(pstackhi − 1), respectively",0
 18: push lo and p− 1 to slo and shi,1
" , a1a0 + b1b0} are added properly",0
"
arg 2 (increment or decrement): up, down
swap",0
", the trace corresponding to the problem “109 + 101”)",1
 ,0
" to make an analogy to mdps, this procedure is analogous to how value iteration obtains the correct value for each state starting from any initialization",1
" both the non-recursive and recursive learned programs generalize on all graphs we tried, up to 120 vertices",0
 recursion enables provable guarantees on neural programs’ behavior without needing to exhaustively enumerate all possible inputs to the programs,1
" as an application, we incorporate recursion into the neural programmer-interpreter architecture and consider four sample tasks: grade-school addition, bubble sort, topological sort, and quicksort",1
"
move",1
"
base cases and reduction rules for addition",0
" in particular, we construct new training traces which explicitly contain recursive elements and show that with this type of trace, npi easily learns recursive programs",0
g,0
"
bubble sort",0
" m , as described in algorithm 1, processes the sequence of step inputs by using an lstm",1
" as a consequence, our learned neural programs empirically achieve perfect generalization from a very small number of training examples",1
" the move operation has the following arguments:
arg 1 (pointer): presult, pstack, pstart, childlist[vactive], childlist[vsave]
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 5 values",1
" the swap operation has the following arguments:
arg 1 (swap object 1): ppivot
arg 2 (swap object 2): phi, pj
write",0
" empirically, existing models suffer from a common limitation—generalization becomes poor beyond a threshold level of complexity",0
" as this complexity interfered with determining the base cases and reductions, we changed the algorithm to its current form",0
 recursion enables provable guarantees on neural programs’ behavior without needing to exhaustively enumerate all possible inputs to the programs,0
"
bubble sort",1
" hence, if all subproblems in s are added correctly, we have proven that any member of f will be added correctly, thus eliminating an infinite family of inputs that need to be tested",1
"
topological sort",1
" if it is too large, then the semantics of p might not be well-defined on some elements in v , or the spurious step input sequences may not be reachable from any valid problem input (e",1
"
the operations we need to be concerned most about are carry and lshift, which induce partial environment states spanning two columns",0
"
formally, verification consists of proving the following theorem:
∀i ∈ v,m(i) ⇓ p (i) where i denotes a sequence of step inputs (within one function call), v denotes the set of valid sequences of step inputs, m denotes the neural network model, p denotes the correct program, and p (i) denotes the next step output from the correct program",0
"
figure 1 shows examples of non-recursive and recursive addition traces",0
" we performed experiments on the partially recursive version in order to examine what happens when only one recursive call is implemented, when in reality three are required for perfect generalization",1
3 to construct v and then empirically create a verification set which covers v ,0
"
we describe the base cases, reduction rules, and the verification set for each task in appendix a",1
" we further have qresult ∈ nu , a scratch-pad which contains the sorted list of vertices at the end of execution, and qstack ∈ nu , which serves the role of the stack s in algorithm 2",0
"
algorithm 1 neural programming inference 1: inputs: environment observation e, program p, arguments a, stop threshold α 2: function run(e, p, a) 3: h← 0, r ← 0 4: while r < α do 5: s← fenc(e, a), h← flstm(s, p, h) 6: r ← fend(h), p2 ← fprog(h), a2 ← farg(h) 7: if p is a primitive function then 8: e← fenv(e, p, a)",0
"
the npi accesses an external environment, q, which varies according to the task",0
" if yes, then this indicates that the learned neural program achieves provably perfect generalization",0
"
quicksort",1
" the recursive version can generalize perfectly to long problems constructed from these components (such as the sum “822+233”, where “8+2” and “2+3” are in the training set), but the non-recursive version fails to sum these long problems properly",0
", the trace corresponding to the array [3,2])",0
" hence, if all subproblems in s are added correctly, we have proven that any member of f will be added correctly, thus eliminating an infinite family of inputs that need to be tested",1
" while this matches the description from reed & de freitas (2016), we found that this causes an unnecessary blowup in the size of v and makes it much more difficult to construct the verification set",0
" we have three observations: the color of the start node, the color of the active node’s next child to be considered, and whether the stack is empty",0
" in order to compensate for this, ppivot and pj require special functions (move pivot lo and move j lo) to properly set them to lo in lines 11 and 12 of the partition function in algorithm 3",0
" in particular, we construct new training traces which explicitly contain recursive elements and show that with this type of trace, npi easily learns recursive programs",1
" for non-primitive subprograms, we can update the observation-to-observation mapping currently associated with the subprogram and then apply that mapping to the current set",0
" at any given time, the npi has access to the values referred to by two pointers, represented by q(1, i1) and q(1, i2),",1
" for example, for the addition task, the model is trained on short inputs and then tested on its ability to sum inputs with much longer numbers of digits",0
 table 3 presents results on randomly generated arrays of varying length for the learned non-recursive and recursive programs,1
g,1
" the only model, to our knowledge, that does not train on input-output pairs is the neural programmer-interpreter (reed & de freitas, 2016), which trains on synthetic execution traces",0
" m , as described in algorithm 1, processes the sequence of step inputs by using an lstm",0
"
the training set for topological sort contains 6 traces, with one synthesized from a graph of size 5 and the rest synthesized from graphs of size 7",1
 the logic for the non-recursive trace is shown in algorithm 4 in the appendix,0
"
in addition, none of the current approaches to neural programming provide a method or even aim to enable provable guarantees about generalization",1
"as discussed in section 2, the neural programmer-interpreter (npi) is an instance of a neural programmer architecture and hence it naturally supports recursion",0
" both the non-recursive and recursive learned programs generalize on all input lengths we tried, up to 5000 digits",0
 2: initialize an empty stack s and a directed acyclic graph dag to traverse,0
"
note that the training set can often be considerably smaller than the verification set, and despite this, the learned model can still pass the entire verification set",0
" there are 3 pointers (presult, pstack, pstart), presult points to the next empty location in qresult,
pstack points to the top of the stack in qstack, and pstart points to the candidate starting node for a connected component",0
 this verification phase only needs to be performed once after training,0
 there are 2 variables (vactive and vsave); vactive holds the active vertex (as in algorithm 2) and vsave holds the value of vactive before executing line 12 of algorithm 2,1
"
to perform the verification as described here, it is critical to construct v correctly",1
"
we describe the details of the npi model relevant to our contributions",0
" numerous models have been proposed for learning programs; to name a few, this includes the differentiable neural computer (graves  , 2016), neural turing machine (graves  , 2014), neural gpu (kaiser & sutskever, 2015), neural programmer (neelakantan  , 2015), pointer network (vinyals  , 2015), hierarchical attentive memory (andrychowicz & kurach, 2016), and neural random access machine (kurach  , 2016)",0
"
through careful reasoning about the possible set of environment observations created by all valid inputs, and how each of the operations in the execution trace affects the environment, we can construct v using the procedure described in section 3",1
", to change the value of vactive) none described below move move a pointer (e",0
 this procedure verifies that all base cases and reductions are handled properly by the model via explicit tests,0
"as discussed in section 2, the neural programmer-interpreter (npi) is an instance of a neural programmer architecture and hence it naturally supports recursion",0
" when the program is called, the current context including the caller’s memory state is stored on a stack; when the program returns, the stored context is popped off the stack to resume execution in the previous caller’s context",1
"
by nature, recursion reduces the complexity of a problem to simpler instances",1
6,0
" if no white vertex is found, the entire execution is terminated move none write write a value either to environment (e",1
g,0
 we then construct a verification set of size 73 by ensuring that randomly generated graphs cover the analytically derived v ,1
"1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 while (pstacklo 6= 1): 6 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 7 stack(stack_pop) 8 else: 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 stack(stack_pop) 12 partition() 13 stack(stack_push_call2) 14 stack(stack_push_call1) 15 } 16 17 partition() { 18 set_pivot_lo() 19 set_j_lo() 20 compswap_loop() 21 swap(ppivot, phi) 22 set_j_null() 23 } 24 25 compswap_loop() { 26 while (pj 6= phi): 27 compswap() 28 move(pj, up) 29 } 30 31 compswap() { 32 if (a[pj ] ≤ a[phi]): 33 swap(ppivot, pj) 34 move(ppivot, up) 35 } 36 37 stack(op) { 38 if (op == stack_push_call1): 39 write(env_stack_lo, plo) 40 write(env_stack_hi, ppivot − 1) 41 move(pstacklo, up) 42 move(pstackhi, up) 43 44 if (op == stack_push_call2): 45 write(env_stack_lo, ppivot + 1) 46 write(env_stack_hi, phi) 47 move(pstacklo, up) 48 move(pstackhi, up) 49 50 if (op == stack_pop): 51 write(env_stack_lo, reset) 52 write(env_stack_hi, reset) 53 move(pstacklo, down) 54 move(pstackhi, down) 55 }altered recursive functions 1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 6 partition() 7 stack(stack_push_call2) 8 stack(stack_push_call1) 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 quicksort() // recursive call 12 stack(stack_pop) 13 write(phi, env_stack_hi_peek) 14 write(plo, env_stack_lo_peek) 15 quicksort() // recursive call 16 stack(stack_pop) 17 } 18 19 compswap_loop() { 20 if (pj 6= phi): 21 compswap() 22 move(pj, up) 23 compswap_loop() // recursive call 24 }in this section, we describe the space of base cases and reduction rules that must be covered for each of the four sample tasks, in order to create the verification set",1
 3: begin traversing from vertex 1 in the dag,1
" observe that partially recursive does slightly better than non-recursive for the setting in which the length of the array is 3, and that the fully recursive version is able to sort every array given to it",1
 recursion enables provable guarantees on neural programs’ behavior without needing to exhaustively enumerate all possible inputs to the programs,1
" the single-digit multiplication task in zaremba   (2016), the bubble sort task in reed & de freitas (2016), and the graph tasks in graves   (2016))",1
"
in addition, none of the current approaches to neural programming provide a method or even aim to enable provable guarantees about generalization",0
"
the non-recursive trace loops on cycles of bubble and reset, which logically represents one bubble sweep through the array and reset of the two bubble pointers to the very beginning of the array, respectively",1
"
we construct input problems by splitting into two cases: one case in which the left column contains a null value and another in which the left column does not contain any null values",1
 ,0
", pstart or childlist[vactive]) up or down none described belowwrite",0
"
in this paper, we propose that recursion is an important concept for neural programs as well",0
" we call this set of inputs the verification set, sv ",0
"
topological sort",0
"
the non-recursive trace loops on cycles of add1 and lshift",0
"
in addition, none of the current approaches to neural programming provide a method or even aim to enable provable guarantees about generalization",0
" if no white vertex is found, the entire execution is terminated move none write write a value either to environment (e",0
" the write operation has the following arguments:
arg 1 (main action): color curr, color next, active start, active neighb, active stack, save, stack push, stack pop, result
color curr colors vactive, color next colors vertex dag[vactive][childlist[vactive]], active start writes pstart to vactive, active neighb writes dag[vactive][childlist[vactive]] to vactive, active stack writes qstack(pstack) to vactive, save writes vactive to vsave, stack push pushes vactive to the top of the stack, stack pop writes a null value to the top of the stack, and result writes vactive to qresult(presult)",0
" we implement a quicksort task, in order to demonstrate that recursion helps with learning divide-and-conquer algorithms",0
" also, compared to the bubble pointers in bubble sort, the pointers that perform the comparison for quicksort (the compswap function) are usually not adjacent to each other, making quicksort less local than bubble sort",0
 the original version of the bubblesort implementation exposes the values within the array,0
" via the recursive call, we effectively forget that the column just added exists, since the recursive call to add starts with a new hidden state for the lstm controller",0
"
arg 2 (auxiliary variable): color grey, color black
color grey and color black color the given vertex grey and black, respectively",1
" furthermore, given a verification input set that covers all base cases and reduction rules, we can formally prove that the learned neural programs achieve perfect generalization after verifying its behavior on the verification input set",0
" numerous models have been proposed for learning programs; to name a few, this includes the differentiable neural computer (graves  , 2016), neural turing machine (graves  , 2014), neural gpu (kaiser & sutskever, 2015), neural programmer (neelakantan  , 2015), pointer network (vinyals  , 2015), hierarchical attentive memory (andrychowicz & kurach, 2016), and neural random access machine (kurach  , 2016)",0
" the space of possible programs is extremely large, and composing a program that performs robustly on the infinite space of possible inputs is difficult—in part because it is impractical to obtain enough training examples to easily disambiguate amongst all possible programs",1
" we hypothesize that the reason for this is that the neural network learns to spuriously depend on specific characteristics of the training examples that are irrelevant to the true program semantics, such as length of the training inputs, and thus fails to generalize to more complex inputs",1
" more specifically, when m is trained on simpler inputs, such as inputs of a small length, the generalization metric evaluates how well m will do on more complex inputs, such as inputs of much longer length",1
" for each function we use to implement the recursive version of topological sort, we need to consider the set of possible environment observation sequences we can create from all valid inputs and test that the learned program produces the correct behavior on each of these inputs",0
" for purposes of verification, we replace the domain-specific encoder with the following:
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1) ≤ q(1, i2), 1 ≤ i1 ≤ length, 1 ≤ i2 ≤ length, i3 == length, at(1), at(2), at(3)]),which directly exposes which of the two values pointed to is larger",0
 ,1
" hence, if all subproblems in s are added correctly, we have proven that any member of f will be added correctly, thus eliminating an infinite family of inputs that need to be tested",1
 this algorithm is a variant of depth first search,1
" for example, for the addition task, the model is trained on short inputs and then tested on its ability to sum inputs with much longer numbers of digits",0
 recursion enables provable guarantees on neural programs’ behavior without needing to exhaustively enumerate all possible inputs to the programs,0
" at any given time, the npi has access to the values referred to by two pointers, represented by q(1, i1) and q(1, i2),",1
" 16: p = partition(a, lo, hi) 17: push p+ 1 and hi to slo and shi",1
"
the training set for quicksort contains 4 traces, synthesized from arrays of length 5",1
"
it is important to emphasize that at inference time in the npi, the hidden state of the lstm controller is reset (to zero) at each subprogram call, as in line 3 of algorithm 1 (h ← 0)",1
" in this section, we give a brief review of the npi architecture from reed & de freitas (2016) as background",0
 18: push lo and p− 1 to slo and shi,1
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,1
"
recursion drastically reduces the number of configurations we need to consider during the verification phase and makes the proof tractable, because it introduces structure that eliminates infinitely long sequences of step inputs that would otherwise need to be considered",0
"
bubble sort",0
" in order to claim a proof, we must verify the model produces correct behavior over all base cases and reductions, as described in section 2",0
 the first is the actual model architecture,0
 there are 2 variables (vactive and vsave); vactive holds the active vertex (as in algorithm 2) and vsave holds the value of vactive before executing line 12 of algorithm 2,1
" note that recursion helps make this process tractable, because we only need to test a finite number of inputs to show that the model will work correctly on inputs of unbounded complexity",1
" m is considered to have perfect generalization if m can give the right answer for any input, such as inputs of arbitrary length",1
" the outputs rt, pt+1, at+1 are used to determine the next action, as described in algorithm 1",0
" the npi model has three learnable components: a task-agnostic core, a program-key embedding, and domain-specific encoders that allow the npi to operate in diverse environments",0
"
algorithm 3 recursive quicksort 1: initialize an array a to sort",0
"
in this paper, we propose that recursion is an important concept for neural programs as well",1
"1, we hypothesize the non-recursive programs do not generalize well because they have learned spurious dependencies specific to the training set, such as length of the input problems",1
" bstep, used in bubble, compares pairs of numbers, continuously moving the bubble pointers once to the right each time until reaching the end of the array",1
"
quicksort",0
" on the other hand, the recursive programs have learned the true program semantics",0
" in fact, it is one of our future directions to explore new ways to train a recursive neural program without providing explicit training execution traces or with only partial or non-recursive traces",0
 this algorithm is a variant of depth first search,1
" table 1 presents results on randomly generated arrays of varying length for the learned non-recursive, partially recursive, and full recursive programs",0
" for purposes of verification, we replace the domain-specific encoder with the following:
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1) ≤ q(1, i2), 1 ≤ i1 ≤ length, 1 ≤ i2 ≤ length, i3 == length, at(1), at(2), at(3)]),which directly exposes which of the two values pointed to is larger",1
" if it is too small, then execution of the program on some input might require evaluation of m(i) on some i /∈ v , and so the behavior ofm(i) might deviate from p (i)",0
 ,1
" it is straightforward to deal with all other operations, which do not induce partial environment states",1
g,0
" a single element of an execution trace consists of a step input-step output pair, which can be synthesized from algorithm 1: this corresponds to, for a given time-step, the step input tuple (e, p, a) and step output tuple (r, p2, a2)",0
" lshift, used in reset, shifts the pointers left until reaching the start token",0
", an array for quicksort or a dag for topological sort)",1
" we implement a quicksort task, in order to demonstrate that recursion helps with learning divide-and-conquer algorithms",0
"
we describe the base cases, reduction rules, and the verification set for each task in appendix a",1
 we outline how to construct this set,1
 the logic for the non-recursive trace is shown in algorithm 4 in the appendix,1
" we also hope to decrease supervision, for example by training with only partial or non-recursive traces, and to develop novel neural programming architectures integrated directly with a notion of recursion",0
" the write operation has the following arguments:
arg 1 (main action): color curr, color next, active start, active neighb, active stack, save, stack push, stack pop, result
color curr colors vactive, color next colors vertex dag[vactive][childlist[vactive]], active start writes pstart to vactive, active neighb writes dag[vactive][childlist[vactive]] to vactive, active stack writes qstack(pstack) to vactive, save writes vactive to vsave, stack push pushes vactive to the top of the stack, stack pop writes a null value to the top of the stack, and result writes vactive to qresult(presult)",1
" 2: initialize lo and hi to be 1 and n, where n is the length of a",0
3,0
"
to construct this set, by using the reference implementation of each subprogram, we construct a mapping between two sets of environment observations: the first set consists of all observations that can occur at the beginning of a particular subprogram’s invocation, and the second set contains the observations at the end of that subprogram",0
"1; and for bubble sort, appendix a",0
"
for topological sort, the domain-specific encoder is
fenc(dag,qcolor, pstack, pstart, vactive, childlist, at)
=mlp ([qcolor(pstart), qcolor(dag[vactive][childlist[vactive]]), pstack == 1, at(1), at(2), at(3)]),
where qcolor ∈ ru×4 is a scratch-pad that contains u rows, each containing one of four colors (white, gray, black, invalid) with one-hot encoding",0
" this is highly undesirable, since being able to provide the correct answer in all possible settings is one of the most important aspects of any learned neural program",0
" in particular, this condition is satisfied by npi, and thus the npi model naturally supports recursion (even though the authors of npi did not consider this aspect explicitly)",1
this material is in part based upon work supported by the national science foundation under grant no,1
 we choose to implement a topological sort task for graphs,1
" hence, if all subproblems in s are added correctly, we have proven that any member of f will be added correctly, thus eliminating an infinite family of inputs that need to be tested",0
 3: begin traversing from vertex 1 in the dag,0
"
the npi accesses an external environment, q, which varies according to the task",1
"in this paper, we propose that the key abstraction of recursion is necessary for neural programs to generalize",1
 recursion can be implemented differently for different neural programming models,0
" the contents ofqresult andqstack are not exposed directly through the domain-specific encoder; rather, we define primitive functions which manipulate these scratch-pads",0
"
given a verification set, we can then run the model on the verification set to check if the produced traces and results are correct",0
" both the non-recursive and recursive learned programs generalize on all input lengths we tried, up to 5000 digits",0
" ppivot and pj point to the pivot and j indices of the array, used in the partition function in algorithm 3",0
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,0
" this is possible if and only if the graph has no directed cycles; that is to say, it must be a directed acyclic graph (dag)",0
"
grade school addition",1
" thus, any general neural programming architecture supporting such a call structure can be made to support recursion",0
 b1b0 where no carry operations occur,0
" in this pair, the step input runs a subprogram add1 that has no arguments, and the step output contains a program write that has arguments of out and 1",1
" for each function we use to implement the recursive version of topological sort, we need to consider the set of possible environment observation sequences we can create from all valid inputs and test that the learned program produces the correct behavior on each of these inputs",0
" for each function we use to implement the recursive version of topological sort, we need to consider the set of possible environment observation sequences we can create from all valid inputs and test that the learned program produces the correct behavior on each of these inputs",1
"
note that v should also contain the necessary reductions, which corresponds to making the recursive calls at the correct time, as indicated by p ",0
", an array for quicksort or a dag for topological sort)",1
" if the program is primitive, the next environmental state et+1 will be affected by pt and at, i",1
" that is to say, the network does not learn the true program semantics",0
"
arg 2 (increment or decrement): up, down
swap",0
"6) on the verification set, and ensure they match the traces produced by the true program p ",1
"
we note that for tasks with very large input domains, such as ones involving mnist digits or speech samples, the state space of base cases and reduction rules could be prohibitively large, possibly infinite",0
 the memory updates of these neural programs are so complex and interdependent that it is difficult to reason about the behaviors of the learned neural program under previously unseen situations (such as problems with longer inputs),0
" the move operation has the following arguments:
arg 1 (pointer): pstacklo, pstackhi, pj , ppivot
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values",0
" et+1 ∼ fenv(et, pt, at)",1
"
is critical for implementing recursion, since it permits us to restrict our attention to the currently relevant recursive call, ignoring irrelevant details about other contexts",0
" thus, we demonstrate that recursion enables provably perfect generalization for different tasks, including addition, topological sort, quicksort, and a variant of bubble sort",0
" our implementation uses two stacks qstacklo and
qstackhi, each in ru , that store the arguments to the recursive quicksort calls in algorithm 3; before each recursive call, the appropriate arguments are popped off the stack and written to plo and phi",0
" the core lstm module’s hidden state is preserved over all these add1 calls, and it is difficult to interpret with certainty what happens over longer timesteps without concretely evaluating the lstm with an input of that length",0
" the only model, to our knowledge, that does not train on input-output pairs is the neural programmer-interpreter (reed & de freitas, 2016), which trains on synthetic execution traces",1
" our implementation uses two stacks qstacklo and
qstackhi, each in ru , that store the arguments to the recursive quicksort calls in algorithm 3; before each recursive call, the appropriate arguments are popped off the stack and written to plo and phi",0
" in this pair, the step input runs a subprogram add1 that has no arguments, and the step output contains a program write that has arguments of out and 1",0
"
formally, verification consists of proving the following theorem:
∀i ∈ v,m(i) ⇓ p (i) where i denotes a sequence of step inputs (within one function call), v denotes the set of valid sequences of step inputs, m denotes the neural network model, p denotes the correct program, and p (i) denotes the next step output from the correct program",1
", the trace corresponding to the problem “109 + 101”)",1
" for these programs, we re-use the appropriate program sets (the associated subprograms), and we refer the reader to the appendix of reed & de freitas (2016) for further details on the subprograms used in addition and bubble sort",1
" at end, pointer ppivot is moved to the pivot compswap loop, move pivot lo, move j lo, swap
none
compswap loop runs the for loop inside the partition function compswap, move none compswap compares a[pivot] ≤ a[j]; if so, perform a swap and increment ppivot swap, move none set pivot lo sets ppivot to lo index none none set j lo sets pj to lo index none none set j null sets pj to −∞ none none stack pushes lo/hi states
onto stacks slo and shi according to argument (described below)
write, move described below
move moves pointer one unit up or down none described below swap swaps elements at given array indices none described below write write a value either to stack (e",0
" for each length, we test each program on 30 randomly generated problems",0
 the first is the actual model architecture,0
" bstep, used in bubble, compares pairs of numbers, continuously moving the bubble pointers once to the right each time until reaching the end of the array",1
" the arrow in the theorem refers to evaluation, as in big-step semantics",0
" as described in the verification procedure, it is possible to prove our learned recursive program generalizes perfectly by testing on an appropriate set of problem inputs, i",1
"
an alternative method is to run p on many different program inputs and then observe step input sequences which occur, to create v ",0
" this guarantees that the step input sequences considered during verification contain all step input sequences which arise during execution of an unseen problem in f , leading to generalization to any problem in f ",1
" naı̈vely, we might expect to synthesize and test an input for any sequence created by combining the four possible colors in two variables and another boolean variable for whether the stack is empty (so 32 possible observations at any point), but for various reasons, most of these combinations are impossible to occur at any given point in the execution trace",0
" finally, the boolean pstack == 1 is used to check whether the stack is empty in line 18 of algorithm 2",0
" as a consequence, our learned neural programs empirically achieve perfect generalization from a very small number of training examples",0
" the general notion of recursion has been an important concept in many domains, including mathematics and computer science",0
 this is due to the properties of the training set – we trained on 2 traces synthesized from arrays of length 7 and 1 trace synthesized from an array of length 6,1
" as with the original npi architecture, the experiments for this paper always used a 3-tuple of integers at = (at(1), at(2), at(3))",1
" although the model is exposed to more and more data, it might learn spurious and overly complex representations of the program, as suggested in zaremba   (2016)",0
"in this paper, we propose that the key abstraction of recursion is necessary for neural programs to generalize",0
" for brevity, we refer the reader to the appendix for further details on the program set and non-recursive and recursive trace-generating functions used for topological sort",1
5,1
" for each function we use to implement the recursive version of topological sort, we need to consider the set of possible environment observation sequences we can create from all valid inputs and test that the learned program produces the correct behavior on each of these inputs",0
" for each length, we test each program on 30 randomly generated problems",0
" each time a subprogram is called, the stack depth increases",0
"
algorithm 2 depth first search topological sort 1: color all vertices white",1
"
in addition, none of the current approaches to neural programming provide a method or even aim to enable provable guarantees about generalization",1
"when constructing a neural network for the purpose of learning a program, there are two orthogonal aspects to consider",0
" as with the others, we apply the procedure described in section 3",1
" however, note that there is no provable guarantee that the non-recursive learned program will generalize to all inputs, whereas we show later that the recursive learned program has a provable guarantee of perfect generalization",1
"
we experiment with two levels of recursion—partial and full",0
" for each length, we test each program on 30 randomly generated problems",0
" if a white vertex is found, set pstart to point to it; this signifies the start of a traversal of a new connected component",0
"
after finding v , we construct a set of problem inputs which, when executed on p , create exactly the step input sequences which make up v ",1
"1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 while (pstacklo 6= 1): 6 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 7 stack(stack_pop) 8 else: 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 stack(stack_pop) 12 partition() 13 stack(stack_push_call2) 14 stack(stack_push_call1) 15 } 16 17 partition() { 18 set_pivot_lo() 19 set_j_lo() 20 compswap_loop() 21 swap(ppivot, phi) 22 set_j_null() 23 } 24 25 compswap_loop() { 26 while (pj 6= phi): 27 compswap() 28 move(pj, up) 29 } 30 31 compswap() { 32 if (a[pj ] ≤ a[phi]): 33 swap(ppivot, pj) 34 move(ppivot, up) 35 } 36 37 stack(op) { 38 if (op == stack_push_call1): 39 write(env_stack_lo, plo) 40 write(env_stack_hi, ppivot − 1) 41 move(pstacklo, up) 42 move(pstackhi, up) 43 44 if (op == stack_push_call2): 45 write(env_stack_lo, ppivot + 1) 46 write(env_stack_hi, phi) 47 move(pstacklo, up) 48 move(pstackhi, up) 49 50 if (op == stack_pop): 51 write(env_stack_lo, reset) 52 write(env_stack_hi, reset) 53 move(pstacklo, down) 54 move(pstackhi, down) 55 }altered recursive functions 1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 6 partition() 7 stack(stack_push_call2) 8 stack(stack_push_call1) 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 quicksort() // recursive call 12 stack(stack_pop) 13 write(phi, env_stack_hi_peek) 14 write(plo, env_stack_lo_peek) 15 quicksort() // recursive call 16 stack(stack_pop) 17 } 18 19 compswap_loop() { 20 if (pj 6= phi): 21 compswap() 22 move(pj, up) 23 compswap_loop() // recursive call 24 }in this section, we describe the space of base cases and reduction rules that must be covered for each of the four sample tasks, in order to create the verification set",1
" to make an analogy to mdps, this procedure is analogous to how value iteration obtains the correct value for each state starting from any initialization",0
" the add reduction rule ensures that each call to add only covers two adjacent columns, and so the lstm only ever runs for a fixed number of steps necessary to process these two columns",1
" for the recursive formulation of addition, we analytically construct the set of input problems that cover all base cases and reduction rules",1
" the pointer at index i3 represents a counter internal to the environment that is incremented once after each pass of the algorithm (one cycle of bubble and reset); when incremented a number of times equal to the length of the array, the flag i3 == length becomes true and terminates the entire algorithm ",0
" we performed experiments on the partially recursive version in order to examine what happens when only one recursive call is implemented, when in reality three are required for perfect generalization",1
"
to evaluate a neural network that learns a neural program to accomplish a certain task, one common evaluation metric is how well the learned model m generalizes",1
"
one common strategy to improve generalization is to use curriculum learning, where the model is trained on inputs of gradually increasing complexity",1
"
the non-recursive trace loops on cycles of add1 and lshift",0
"
is critical for implementing recursion, since it permits us to restrict our attention to the currently relevant recursive call, ignoring irrelevant details about other contexts",1
" for instance, for recursive addition, consider the family f of addition problems anan−1 ",0
"
the training set for quicksort contains 4 traces, synthesized from arrays of length 5",0
" empirically, existing models suffer from a common limitation—generalization becomes poor beyond a threshold level of complexity",0
"
an alternative way of representing the environment slice is to expose the values of the absolute vertices to the model; however, this makes it difficult to scale the model to larger graphs, since large vertex values are not seen during training time",0
" however, models that make use of this strategy eventually fail after a certain level of complexity (e",0
 we adapt machinery from the original paper slightly to fit our needs,0
" via the recursive call, we effectively forget that the column just added exists, since the recursive call to add starts with a new hidden state for the lstm controller",1
"
it is sufficient to construct problems where every transition between two adjacent columns is covered",1
" for bubble sort, the domain-specific encoder is
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1), q(1, i2), i3 == length, at(1), at(2), at(3)]),
where the environment q ∈ r1×n×k is a scratch-pad that contains 1 row, to represent the state of the array as sorting proceeds in-place, andn columns",0
" we also hope to decrease supervision, for example by training with only partial or non-recursive traces, and to develop novel neural programming architectures integrated directly with a notion of recursion",0
" we make the trace recursive by adding a tail recursive call into the trace for the add program after calling add1 and lshift, the original paper uses k = 10, but we found it necessary to augment the range with an end token, in order to terminate properly",1
 indentation indicates the stack is one level deeper than before,1
"
arg 2 (auxiliary variable): color grey, color black
color grey and color black color the given vertex grey and black, respectively",1
" we can obtain this mapping by first considering the possible observations that can arise at the beginning of the entry function (add, bubblesort, toposort, and quicksort) for some valid program input, and iteratively applying the observation-to-observation mapping implied by the reference implementation’s step output at that point in the execution",0
" the recursive version of topological sort solves all graph instances we tried, from graphs of size 5 through 70",0
" on the other hand, the non-recursive version has low accuracy, beginning from size 5, and fails completely for graphs of size 8 and beyond",1
" as with the original npi architecture, the experiments for this paper always used a 3-tuple of integers at = (at(1), at(2), at(3))",1
"
recursion drastically reduces the number of configurations we need to consider during the verification phase and makes the proof tractable, because it introduces structure that eliminates infinitely long sequences of step inputs that would otherwise need to be considered",0
" an example of part of an addition task trace, written in shorthand, is given in figure 1",0
" observe that the non-recursive program’s correctness degrades for length 11 and beyond, while the recursive program can sort any given array",1
"
as mentioned in section 1, all approaches to neural programming today fare poorly on this generalization issue",1
 we adapt machinery from the original paper slightly to fit our needs,0
" to make an analogy to mdps, this procedure is analogous to how value iteration obtains the correct value for each state starting from any initialization",1
 the training set for addition contains 200 traces,0
"
as a concrete instantiation, we show in this paper that we can enable recursive neural programs in the npi model, and thus enable perfectly generalizable neural programs for tasks such as sorting where the original, non-recursive npi program fails",0
 this procedure verifies that all base cases and reductions are handled properly by the model via explicit tests,0
" even though the earlier version also generalized just as well in practice, relatively small differences in the formulation of the traces and the environment observations can drastically change the difficulty of verification",1
" both the non-recursive and recursive learned programs generalize on all graphs we tried, up to 120 vertices",0
" note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values, and arg 2 can only take one of 7 values",1
"
the dag is represented as an adjacency list where dag[i][j] refers to the j-th child of vertex i",0
" thus, any general neural programming architecture supporting such a call structure can be made to support recursion",1
 the program terminates when seeing no numbers in the current column,0
" recall that this verification procedure cannot be performed for the non-recursive versions, since the propagation of the hidden state in the core lstm module makes reasoning difficult and so we would need to check an unbounded number of examples",1
" we also hope to decrease supervision, for example by training with only partial or non-recursive traces, and to develop novel neural programming architectures integrated directly with a notion of recursion",0
"
bubble sort",0
" the stack operation has the following arguments:
arg 1 (operation): stack push call1, stack push call2, stack pop
stack push call1 pushes lo and pivot−1 toqstacklo andqstackhi",0
" in all experiments, α is set to 0",0
" twc-1409915, darpa under grant no",1
"1, we hypothesize the non-recursive programs do not generalize well because they have learned spurious dependencies specific to the training set, such as length of the input problems",1
" in this architecture, we consider a core controller, e",0
 stack push call2 pushes pivot + 1 and hi to qstacklo and qstackhi,0
g,1
" m is considered to have perfect generalization if m can give the right answer for any input, such as inputs of arbitrary length",1
 this modification also enables us to sort arrays containing arbitrary comparable elements,1
 reset represents a −∞ value,0
 ,0
" if it is too small, then execution of the program on some input might require evaluation of m(i) on some i /∈ v , and so the behavior ofm(i) might deviate from p (i)",1
" we performed experiments on the partially recursive version in order to examine what happens when only one recursive call is implemented, when in reality three are required for perfect generalization",0
" the write operation has the following arguments:
arg 1 (main action): color curr, color next, active start, active neighb, active stack, save, stack push, stack pop, result
color curr colors vactive, color next colors vertex dag[vactive][childlist[vactive]], active start writes pstart to vactive, active neighb writes dag[vactive][childlist[vactive]] to vactive, active stack writes qstack(pstack) to vactive, save writes vactive to vsave, stack push pushes vactive to the top of the stack, stack pop writes a null value to the top of the stack, and result writes vactive to qresult(presult)",0
we emphasize that the notion of a neural recursive program has not been presented in the literature before: this is our main contribution,0
" formally, the npi is represented by the following set of equations:
st = fenc(et, at)
ht = flstm(st, pt, ht−1)
rt = fend(ht), pt+1 = fprog(ht), at+1 = farg(ht)
t is a subscript denoting the time-step; fenc is a domain-specific encoder (to be described later) that takes in the environment slice et and arguments at; flstm represents the core module, which takes in the state st generated by fenc, a program embedding pt ∈ rp , and hidden lstm state ht; fend decodes the return probability rt; fprog decodes a program key embedding pt+1;1 and farg decodes arguments at+1",0
 stack push call2 pushes pivot + 1 and hi to qstacklo and qstackhi,1
" ppivot and pj point to the pivot and j indices of the array, used in the partition function in algorithm 3",0
"
to construct this set, by using the reference implementation of each subprogram, we construct a mapping between two sets of environment observations: the first set consists of all observations that can occur at the beginning of a particular subprogram’s invocation, and the second set contains the observations at the end of that subprogram",0
 stack push call2 pushes pivot + 1 and hi to qstacklo and qstackhi,1
 pstacklo and pstackhi point to the top (empty) positions in qstacklo and qstackhi,0
" 2: initialize lo and hi to be 1 and n, where n is the length of a",0
" however, to be sure that the generated v is complete (covers all the cases needed), we need to check all pairs of observations seen in adjacent step inputs (in particular, those before and after a primitive function call), in a similar way as if we were constructing v from scratch",0
" 2: initialize lo and hi to be 1 and n, where n is the length of a",0
" if the program is primitive, the next environmental state et+1 will be affected by pt and at, i",0
" the core module of the npi is an lstm controller that takes as input a slice of the current external environment, via a set of pointers, and a program and arguments to execute",1
"
the operations we need to be concerned most about are carry and lshift, which induce partial environment states spanning two columns",1
", to change the value of phi) none described belowstack",0
" the pointers at index i1 and i2 are used to compare the pair of numbers considered during the bubble sweep, swapping them if the number at i1 is greater than that in i2",1
" the assumption of leading 0’s can be easily removed, at the cost of slightly increasing the size of the verification set",0
" via the recursive call, we effectively forget that the column just added exists, since the recursive call to add starts with a new hidden state for the lstm controller",0
 we adapt machinery from the original paper slightly to fit our needs,1
" the partial recursion is not enough for perfect generalization, as will be presented later in section 4",1
 we choose to implement a topological sort task for graphs,0
3,1
" as described in the verification procedure, it is possible to prove our learned recursive program generalizes perfectly by testing on an appropriate set of problem inputs, i",0
 we found that changing the npi training traces is a simple way to enable this,1
" we further have qresult ∈ nu , a scratch-pad which contains the sorted list of vertices at the end of execution, and qstack ∈ nu , which serves the role of the stack s in algorithm 2",0
"we describe how models trained with recursive traces can be proven to generalize, by using the verification procedure described in section 3",1
"
arg 2 (increment or decrement): up, down1 // top level topological sort call 2 toposort() { 3 while (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 } 0 11 traverse() { 12 check_child() 13 explore() 14 } 15 16 check_child() { 17 while (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 18 move(childlist[vactive], up) 19 } 20 21 explore() { 22 do 23 if (qcolor(dag[vactive][childlist[vactive]]) is white): 24 write(color_next, color_grey) 25 stack(push) 26 write(save) 27 write(active_neighb) 28 move(childlist[vsave], up) 29 else: 30 write(color_curr, color_black) 31 write(result) 32 move(presult, up) 33 if(pstack == 1): 34 break 35 else: 36 stack(pop) 37 check_child() 38 while (true) 39 } 40 41 stack(op) { 42 if (op == push): 43 write(stack_push) 44 move(pstack, up) 45 46 if (op == pop): 47 write(active_stack) 48 write(stack_pop) 49 move(pstack, down) 50 } 51 52 next_start() { 53 while(qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 54 move(pstart, up) 55 }altered recursive functions  // top level topological sort call 2 toposort() { 3 if (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 toposort() // recursive call 0 } 11 12 check_child() { 13 if (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 14 move(childlist[vactive], up) 15 check_child() // recursive call 16 } 17 18 explore() { 19 if (qcolor(dag[vactive][childlist[vactive]]) is white): 20 write(color_next, color_grey) 21 stack(push) 22 write(save) 23 write(active_neighb) 24 move(childlist[vsave], up) 25 else: 26 write(color_curr, color_black) 27 write(result) 28 move(presult, up) 29 if(pstack == 1): 30 return 31 else: 32 stack(pop) 33 check_child() 34 explore() // recursive call 35 } 36 37 next_start() { 38 if (qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 39 move(pstart, up) 40 next_start() // recursive call 41 }algorithm 4 iterative quicksort 1: initialize an array a to sort and two empty stacks slo and shi",0
" formally, a function exhibits recursive behavior when it possesses two properties: (1) base cases— terminating scenarios that do not use recursion to produce answers; (2) a set of rules that reduces all other problems toward the base cases",0
" consequently, there is no concept of length relevant to the problem, which has traditionally been an important focus of length-based curriculum learning",0
" the write operation has the following arguments:
arg 1 (main action): color curr, color next, active start, active neighb, active stack, save, stack push, stack pop, result
color curr colors vactive, color next colors vertex dag[vactive][childlist[vactive]], active start writes pstart to vactive, active neighb writes dag[vactive][childlist[vactive]] to vactive, active stack writes qstack(pstack) to vactive, save writes vactive to vsave, stack push pushes vactive to the top of the stack, stack pop writes a null value to the top of the stack, and result writes vactive to qresult(presult)",0
", an array for quicksort or a dag for topological sort)",1
" qcolor(dag[vactive][childlist[vactive]]) refers to the color of the next child of vactive, used in the evaluation of the condition in the if branch in line 9 of algorithm 2",0
 we choose to implement a topological sort task for graphs,1
" here as a concrete and general example, we consider a general neural programming architecture (npa), similar to neural programmer-interpreter (npi) in reed & de freitas (2016)",1
"
bubble sort",0
 errors arise due to undesirable and uninterpretable dependencies and associations the architecture learns to store in some high-dimensional hidden state,0
" in the non-recursive trace, there are four functions that can be made recursive—toposort, check child, explore, and next start, and we add a tail recursive call to each of these functions in order to make the recursive trace",0
 ,1
"
in this general neural programming architecture, we show it is easy to support recursion",0
" without using a recursive program, such a proof is not possible, because the non-recursive program runs on an arbitrarily long addition problem that creates correspondingly long sequences of step inputs; in the non-recursive formulation of addition, add calls add1 a number of times that is dependent on the length of the input",1
" the non-recursive and partially recursive versions are unable to sort long arrays, beyond length 8",0
" as an application, we incorporate recursion into the neural programmer-interpreter architecture and consider four sample tasks: grade-school addition, bubble sort, topological sort, and quicksort",1
" in practice, we can additionally use tail recursion optimization to avoid problems with the call stack growing too deep",1
"
we also report on generalization results for the non-recursive and recursive versions of this variant of bubble sort",0
" , n , where the dag contains n vertices",1
"
training setup",0
"
algorithm 2 depth first search topological sort 1: color all vertices white",1
"
quicksort",0
" in particular, in the explore function, adding a tail recursive call resets and stores the hidden states associated with vertices in a stack-like fashion",0
 we adapt machinery from the original paper slightly to fit our needs,1
" this is possible if and only if the graph has no directed cycles; that is to say, it must be a directed acyclic graph (dag)",1
" the space of possible programs is extremely large, and composing a program that performs robustly on the infinite space of possible inputs is difficult—in part because it is impractical to obtain enough training examples to easily disambiguate amongst all possible programs",0
" empirically, we observe that the learned recursive programs solve all valid inputs with 100% accuracy after training on a very small number of examples, out-performing previous generalization results",0
", the verification set",0
" because the context of the caller is stored on a stack when it calls another program and the callee starts in a fresh context, this enables recursion simply by allowing a program to call itself",0
"
it is sufficient to construct problems where every transition between two adjacent columns is covered",0
 the training set for addition contains 200 traces,0
"
the non-recursive trace loops on cycles of add1 and lshift",0
"
under the assumption that there are no leading 0’s (except in the case of single digits) and the two numbers to be added have the same number of digits, the verification set for addition contains 20,181 input problems",1
 the first is the actual model architecture,0
" finally, we implement recursive traces for our own topological sort and quicksort tasks",1
" our results show that for all learned, to-be-verified neural programs, they all produced the same traces as those produced by p on the verification set",1
"
to construct this set, by using the reference implementation of each subprogram, we construct a mapping between two sets of environment observations: the first set consists of all observations that can occur at the beginning of a particular subprogram’s invocation, and the second set contains the observations at the end of that subprogram",0
", an array for quicksort or a dag for topological sort)",0
" this makes it so that we only need to consider the vertices in the subgraph that are currently relevant for computing the sort, allowing simpler reasoning about behavior for large graphs",0
" we show that by modifying the training procedure, we enable the npi model to learn recursive neural programs",0
" in order to compensate for this, ppivot and pj require special functions (move pivot lo and move j lo) to properly set them to lo in lines 11 and 12 of the partition function in algorithm 3",1
g,0
"we show that if we incorporate recursion, the learned npi programs can achieve provably perfect generalization for different tasks",0
"2 at any given time, the npi has access to values pointed to by four pointers in each of the four rows, represented by q(1, i1), q(2, i2), q(3, i3), and q(4, i4)",0
g,0
"
we propose and describe our verification procedure",1
" if the program is primitive, the next environmental state et+1 will be affected by pt and at, i",0
"
algorithm 2 depth first search topological sort 1: color all vertices white",1
" k is set to 11, to represent the range of 10 possible digits, along with a token representing the end of input",0
 there is a (changing) list of neural programs used to accomplish a given task,0
"
as mentioned in section 1, all approaches to neural programming today fare poorly on this generalization issue",0
"
in order to demonstrate that recursion can help learn and generalize better, for addition, we trained only on traces for 5 arbitrarily chosen 1-digit addition sum examples",0
" recall that this verification procedure cannot be performed for the non-recursive versions, since the propagation of the hidden state in the core lstm module makes reasoning difficult and so we would need to check an unbounded number of examples",0
" if a white vertex is found, set pstart to point to it; this signifies the start of a traversal of a new connected component",0
" that is to say, the network does not learn the true program semantics",0
"
the training data for the neural programmer-interpreter consists of full execution traces for the program of interest",0
"6) on the verification set, and ensure they match the traces produced by the true program p ",0
 ,0
 the original version of the bubblesort implementation exposes the values within the array,1
" in computer science, recursion (as opposed to iteration) involves solving a larger problem by combining solutions to smaller instances of the same problem",1
" formally, a function exhibits recursive behavior when it possesses two properties: (1) base cases— terminating scenarios that do not use recursion to produce answers; (2) a set of rules that reduces all other problems toward the base cases",0
g,0
"
under the assumption that there are no leading 0’s (except in the case of single digits) and the two numbers to be added have the same number of digits, the verification set for addition contains 20,181 input problems",1
" to the best of our knowledge, this is the first time verification has been applied to a neural program, providing provable guarantees about its behavior",1
" empirically, existing models suffer from a common limitation—generalization becomes poor beyond a threshold level of complexity",0
" as this complexity interfered with determining the base cases and reductions, we changed the algorithm to its current form",0
" in particular, given that a recursion is defined by two properties as mentioned before, the base cases and the set of reduction rules, we can prove a recursive neural program generalizes perfectly if we can prove that (1) it performs correctly on the base cases; (2) it learns the reduction rules correctly",0
", to change the value of vactive) none described below move move a pointer (e",1
"
move",1
" recursion is an important concept in programming languages and a critical tool to
reduce the complexity of programs",0
" even though the earlier version also generalized just as well in practice, relatively small differences in the formulation of the traces and the environment observations can drastically change the difficulty of verification",0
" 4: function toposort(dag) 5: while there is still a white vertex u: do 6: color[u] = grey 7: vactive = u 8: do 9: if vactive has a white child v then 10: color[v] = grey 11: push vactive onto s 12: vactive = v 13: else 14: color[vactive] = black 15: write vactive to result 16: if s is empty then pass 17: else pop the top vertex off s and set it to vactive 18: while s is not empty
topological sort",1
3,0
g,1
"1; and for bubble sort, appendix a",1
", pstart or childlist[vactive]) up or down none described belowwrite",0
" the stack operation has the following arguments:
arg 1 (operation): stack push call1, stack push call2, stack pop
stack push call1 pushes lo and pivot−1 toqstacklo andqstackhi",1
 errors arise due to undesirable and uninterpretable dependencies and associations the architecture learns to store in some high-dimensional hidden state,1
 ,0
" for example, for the addition task, the model is trained on short inputs and then tested on its ability to sum inputs with much longer numbers of digits",0
" training on the full recursive trace leads to perfect generalization, as shown in section 4",1
" we note that an earlier version of quicksort we tried lacked primitive operations to directly move a pointer to another, and therefore needed more functions and observations",1
g,0
" when the program is called, the current context including the caller’s memory state is stored on a stack; when the program returns, the stored context is popped off the stack to resume execution in the previous caller’s context",0
"
we note that for tasks with very large input domains, such as ones involving mnist digits or speech samples, the state space of base cases and reduction rules could be prohibitively large, possibly infinite",1
" for grade-school addition, the domain-specific encoder is
fenc(q, i1, i2, i3, i4, at) =mlp ([q(1, i1), q(2, i2), q(3, i3), q(4, i4), at(1), at(2), at(3)]),
where the environment q ∈ r4×n×k is a scratch-pad that contains four rows (the first input number, the second input number, the carry bits, and the output) and n columns",0
"
the three environment observations aid with control flow in algorithm 2",0
"
base cases and reduction rules for quicksort",0
"
one common strategy to improve generalization is to use curriculum learning, where the model is trained on inputs of gradually increasing complexity",0
" consequently, it is infeasible to construct a verification set that covers all cases, and the verification procedure we have described is inadequate",0
" for each length, we test each program on 30 randomly generated problems",0
 we adapt machinery from the original paper slightly to fit our needs,0
 ,1
" we prove every member of f is added properly, given that subproblems s = {anan−1 + bnbn−1, an−1an−2 + bn−1bn−2, ",0
 our experiments use a small number of training examples,0
" the pointers at index i1 and i2 are used to compare the pair of numbers considered during the bubble sweep, swapping them if the number at i1 is greater than that in i2",1
"2 at any given time, the npi has access to values pointed to by four pointers in each of the four rows, represented by q(1, i1), q(2, i2), q(3, i3), and q(4, i4)",0
" however, to be sure that the generated v is complete (covers all the cases needed), we need to check all pairs of observations seen in adjacent step inputs (in particular, those before and after a primitive function call), in a similar way as if we were constructing v from scratch",0
training neural networks to synthesize robust programs from a small number of examples is a challenging task,0
"
in what follows, we describe the way in which we constructed npi training traces so as to make them contain recursive elements and thus enable npi to learn recursive programs",0
" plo and phi point to the lo and hi indices of the array, as in algorithm 3",1
 table 4 also demonstrates that the (verified) recursive program generalizes perfectly,1
 the degree for any vertex in the dag is variable,0
" at end, pointer ppivot is moved to the pivot compswap loop, move pivot lo, move j lo, swap
none
compswap loop runs the for loop inside the partition function compswap, move none compswap compares a[pivot] ≤ a[j]; if so, perform a swap and increment ppivot swap, move none set pivot lo sets ppivot to lo index none none set j lo sets pj to lo index none none set j null sets pj to −∞ none none stack pushes lo/hi states
onto stacks slo and shi according to argument (described below)
write, move described below
move moves pointer one unit up or down none described below swap swaps elements at given array indices none described below write write a value either to stack (e",1
 u varies with the number of vertices in the graph,0
"
the non-recursive trace loops on cycles of bubble and reset, which logically represents one bubble sweep through the array and reset of the two bubble pointers to the very beginning of the array, respectively",0
g,1
 this procedure verifies that all base cases and reductions are handled properly by the model via explicit tests,0
g,0
" a topological sort is a linear ordering of vertices such that for every directed edge (u, v) from u to v, u comes before v in the ordering",1
" recall that this verification procedure cannot be performed for the non-recursive versions, since the propagation of the hidden state in the core lstm module makes reasoning difficult and so we would need to check an unbounded number of examples",0
 the memory updates of these neural programs are so complex and interdependent that it is difficult to reason about the behaviors of the learned neural program under previously unseen situations (such as problems with longer inputs),0
" the verification set can be very small, as we found a 10-element array ([8,2,1,2,0,8,5,8,3,7]) is sufficient to cover all of v ",0
" , n , where the dag contains n vertices",0
" for problems where the base cases may be extremely large or infinite, such as certain forms of motor control, recursion can still help reduce the problem of generalization to these two aspects and make the generalization problem significantly simpler to handle and reason about",0
"
we also report on generalization results for the non-recursive and recursive versions of this variant of bubble sort",1
"
move",1
" the move operation has the following arguments:
arg 1 (pointer): pstacklo, pstackhi, pj , ppivot
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values",0
 we adapt machinery from the original paper slightly to fit our needs,0
" the move operation has the following arguments:
arg 1 (pointer): pstacklo, pstackhi, pj , ppivot
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values",1
" for grade-school addition, the domain-specific encoder is
fenc(q, i1, i2, i3, i4, at) =mlp ([q(1, i1), q(2, i2), q(3, i3), q(4, i4), at(1), at(2), at(3)]),
where the environment q ∈ r4×n×k is a scratch-pad that contains four rows (the first input number, the second input number, the carry bits, and the output) and n columns",0
" the pointer at index i3 represents a counter internal to the environment that is incremented once after each pass of the algorithm (one cycle of bubble and reset); when incremented a number of times equal to the length of the array, the flag i3 == length becomes true and terminates the entire algorithm ",0
3,1
 this modification also enables us to sort arrays containing arbitrary comparable elements,0
" the recursive version can generalize perfectly to long problems constructed from these components (such as the sum “822+233”, where “8+2” and “2+3” are in the training set), but the non-recursive version fails to sum these long problems properly",1
" in fact, it is one of our future directions to explore new ways to train a recursive neural program without providing explicit training execution traces or with only partial or non-recursive traces",0
we now report on generalization for the varying tasks,1
"
the three environment observations aid with control flow in algorithm 2",1
 we use the lomuto partition scheme; the logic for the recursive trace is shown in algorithm 3,1
" we note that an earlier version of quicksort we tried lacked primitive operations to directly move a pointer to another, and therefore needed more functions and observations",0
"
in what follows, we describe the way in which we constructed npi training traces so as to make them contain recursive elements and thus enable npi to learn recursive programs",0
" nevertheless, we would like the model to quickly learn to represent the right semantics of the underlying program from a small number of training examples, not an exhaustive number of them",0
" table 1 presents results on randomly generated arrays of varying length for the learned non-recursive, partially recursive, and full recursive programs",1
 reset represents a −∞ value,1
 we then construct a verification set of size 73 by ensuring that randomly generated graphs cover the analytically derived v ,1
" the outputs rt, pt+1, at+1 are used to determine the next action, as described in algorithm 1",1
" empirically, existing models suffer from a common limitation—generalization becomes poor beyond a threshold level of complexity",1
 we find that recursion makes it easier for the network to learn the right program and generalize to unknown situations,1
" in this version, there is a dependence on length: bstep and lshift are called a number of times equivalent to one less than the length of the input array, in bubble and reset respectively",1
"
the training set for quicksort contains 4 traces, synthesized from arrays of length 5",0
"
move",1
"
algorithm 1 neural programming inference 1: inputs: environment observation e, program p, arguments a, stop threshold α 2: function run(e, p, a) 3: h← 0, r ← 0 4: while r < α do 5: s← fenc(e, a), h← flstm(s, p, h) 6: r ← fend(h), p2 ← fprog(h), a2 ← farg(h) 7: if p is a primitive function then 8: e← fenv(e, p, a)",1
"
we note that for tasks with very large input domains, such as ones involving mnist digits or speech samples, the state space of base cases and reduction rules could be prohibitively large, possibly infinite",0
" we implement a quicksort task, in order to demonstrate that recursion helps with learning divide-and-conquer algorithms",0
" as with the original npi architecture, the experiments for this paper always used a 3-tuple of integers at = (at(1), at(2), at(3))",1
" for these models, table 2 presents results on randomly generated dags of varying graph sizes (varying in the number of vertices)",0
" we performed experiments on the partially recursive version in order to examine what happens when only one recursive call is implemented, when in reality three are required for perfect generalization",0
" for grade-school addition, the domain-specific encoder is
fenc(q, i1, i2, i3, i4, at) =mlp ([q(1, i1), q(2, i2), q(3, i3), q(4, i4), at(1), at(2), at(3)]),
where the environment q ∈ r4×n×k is a scratch-pad that contains four rows (the first input number, the second input number, the carry bits, and the output) and n columns",0
" childlist ∈ nu is a vector of pointers, where childlist[i] points to the next child under consideration for vertex i",0
"program descriptions calls arguments quicksort run the quicksort
routine in place for the array a, for indices from lo to hi non-recursive: partition, stack, write recursive: same as non-recursive version, along with quicksort implicitly: array a to sort, lo, hi
partition runs the partition function",0
 these pointers are referred to as bubble pointers,0
" in this section, we give a brief review of the npi architecture from reed & de freitas (2016) as background",0
"
move",0
" furthermore, given a verification input set that covers all base cases and reduction rules, we can formally prove that the learned neural programs achieve perfect generalization after verifying its behavior on the verification input set",1
" finally, we implement recursive traces for our own topological sort and quicksort tasks",0
g,0
 this makes it difficult to reason about what the model will do when given complex inputs,0
g,0
" in this version, there is a dependence on length: bstep and lshift are called a number of times equivalent to one less than the length of the input array, in bubble and reset respectively",0
" the assumption of leading 0’s can be easily removed, at the cost of slightly increasing the size of the verification set",0
" as with the original npi architecture, the experiments for this paper always used a 3-tuple of integers at = (at(1), at(2), at(3))",0
 pstacklo and pstackhi point to the top (empty) positions in qstacklo and qstackhi,0
 reset represents a −∞ value,0
" there are 3 pointers (presult, pstack, pstart), presult points to the next empty location in qresult,
pstack points to the top of the stack in qstack, and pstart points to the candidate starting node for a connected component",1
" the assumption of leading 0’s can be easily removed, at the cost of slightly increasing the size of the verification set",1
" fa8750-15-2-0104, and berkeley deep drive",0
 figure 2 shows examples of traces for the different versions of bubble sort,1
"
the same set of problems was used to generate the training traces for all formulations of the task, for non-recursive and recursive versions",0
 these pointers are referred to as bubble pointers,0
" for these programs, we re-use the appropriate program sets (the associated subprograms), and we refer the reader to the appendix of reed & de freitas (2016) for further details on the subprograms used in addition and bubble sort",0
" et+1 ∼ fenv(et, pt, at)",0
"
grade-school addition",1
 this procedure verifies that all base cases and reductions are handled properly by the model via explicit tests,0
" we have three observations: the color of the start node, the color of the active node’s next child to be considered, and whether the stack is empty",0
"
through careful reasoning about the possible set of environment observations created by all valid inputs, and how each of the operations in the execution trace affects the environment, we can construct v using the procedure described in section 3",1
"
quicksort",0
" note that recursion helps make this process tractable, because we only need to test a finite number of inputs to show that the model will work correctly on inputs of unbounded complexity",0
"
bubble sort",1
" note that recursion helps make this process tractable, because we only need to test a finite number of inputs to show that the model will work correctly on inputs of unbounded complexity",0
" we also hope to decrease supervision, for example by training with only partial or non-recursive traces, and to develop novel neural programming architectures integrated directly with a notion of recursion",0
" consequently, there is no concept of length relevant to the problem, which has traditionally been an important focus of length-based curriculum learning",0
" if yes, then this indicates that the learned neural program achieves provably perfect generalization",1
 the program terminates when seeing no numbers in the current column,1
" we further have qresult ∈ nu , a scratch-pad which contains the sorted list of vertices at the end of execution, and qstack ∈ nu , which serves the role of the stack s in algorithm 2",0
" as with the others, we apply the procedure described in section 3",1
"
note that v should also contain the necessary reductions, which corresponds to making the recursive calls at the correct time, as indicated by p ",0
" in this architecture, we consider a core controller, e",1
" as an application, we incorporate recursion into the neural programmer-interpreter architecture and consider four sample tasks: grade-school addition, bubble sort, topological sort, and quicksort",1
" each step during an execution of the program does one of three things: (1) another subprogram along with associated arguments is called, as in line 10, (2) the program writes to the environment if it is primitive, as in line 8, or (3) the loop is terminated if the return probability exceeds a threshold α, after which the stack frame is popped and control is returned to the caller",1
" full recursion, in addition to making the aforementioned tail recursive call, adds two additional recursive calls; bstep and lshift are made tail recursive",1
" for each length, we test each program on 30 randomly generated problems",1
 table 3 presents results on randomly generated arrays of varying length for the learned non-recursive and recursive programs,0
" observe that partially recursive does slightly better than non-recursive for the setting in which the length of the array is 3, and that the fully recursive version is able to sort every array given to it",0
"
since npi naturally supports the notion of recursion, a key question is how to enable npi to learn recursive programs",1
 we describe the recursive re-formulation of traces for two tasks from the original npi paper—grade-school addition and bubble sort,0
", the trace corresponding to the array [3,2])",1
g,0
" given a precise definition of p , it may be possible to automate the generation of v from p in future work",0
g,1
" when the program is called, the current context including the caller’s memory state is stored on a stack; when the program returns, the stored context is popped off the stack to resume execution in the previous caller’s context",0
"
the training data for the neural programmer-interpreter consists of full execution traces for the program of interest",1
" given a precise definition of p , it may be possible to automate the generation of v from p in future work",0
" we performed experiments on the partially recursive version in order to examine what happens when only one recursive call is implemented, when in reality three are required for perfect generalization",0
"
in order to demonstrate that recursion can help learn and generalize better, we trained a non-recursive and recursive model on just a single execution trace generated from a graph containing 5 nodes3 for the topological sort task",0
"
through careful reasoning about the possible set of environment observations created by all valid inputs, and how each of the operations in the execution trace affects the environment, we can construct v using the procedure described in section 3",0
 2: initialize an empty stack s and a directed acyclic graph dag to traverse,0
g,1
", to color a vertex) or variable (e",0
 we then construct a verification set of size 73 by ensuring that randomly generated graphs cover the analytically derived v ,1
"
topological sort",0
" for the recursive formulation of addition, we analytically construct the set of input problems that cover all base cases and reduction rules",1
" if the program is primitive, the next environmental state et+1 will be affected by pt and at, i",0
"
topological sort",0
" we hypothesize that the reason for this is that the neural network learns to spuriously depend on specific characteristics of the training examples that are irrelevant to the true program semantics, such as length of the training inputs, and thus fails to generalize to more complex inputs",0
 the environment and return probability are omitted for readability,0
 we use the lomuto partition scheme; the logic for the recursive trace is shown in algorithm 3,1
" the add reduction rule ensures that each call to add only covers two adjacent columns, and so the lstm only ever runs for a fixed number of steps necessary to process these two columns",0
"
the training set for quicksort contains 4 traces, synthesized from arrays of length 5",0
" hence, if all subproblems in s are added correctly, we have proven that any member of f will be added correctly, thus eliminating an infinite family of inputs that need to be tested",0
2,1
 the sequence of primitive operations (move and write operations) for the non-recursive and recursive versions are exactly the same,0
"program descriptions calls arguments toposort perform topological
sort on graph traverse, next start, write, move
none
traverse traverse graph until stack is empty check child, explore none check child check if a white child exists; if so, set childlist[vactive] to point to it move none explore repeatedly traverse subgraphs until stack is empty stack, check child, write, move none stack interact with stack, either pushing or popping write, move push, pop next start move pstart until reaching a white vertex",0
" if yes, then this indicates that the learned neural program achieves provably perfect generalization",0
 our result shows that the training procedure and the npi architecture is capable of generalizing from the step input-output pairs seen in the training data to the unseen ones present in the verification set,1
"
quicksort",0
" in future work, we seek to enable more tasks with recursive structure",1
 the degree for any vertex in the dag is variable,0
"
figure 1 shows examples of non-recursive and recursive addition traces",0
"
algorithm 3 recursive quicksort 1: initialize an array a to sort",1
"when constructing a neural network for the purpose of learning a program, there are two orthogonal aspects to consider",0
6,0
"
in what follows, we describe the way in which we constructed npi training traces so as to make them contain recursive elements and thus enable npi to learn recursive programs",0
"
algorithm 1 neural programming inference 1: inputs: environment observation e, program p, arguments a, stop threshold α 2: function run(e, p, a) 3: h← 0, r ← 0 4: while r < α do 5: s← fenc(e, a), h← flstm(s, p, h) 6: r ← fend(h), p2 ← fprog(h), a2 ← farg(h) 7: if p is a primitive function then 8: e← fenv(e, p, a)",1
" for each task, given the verification set, we check the traces and results of the learned, to-be-verified neural program (described in section 4",1
"we train using the adam optimizer and use a 2-layer lstm and task-specific state encoders for the external environments, as described in reed & de freitas (2016)",1
 figure 2 shows examples of traces for the different versions of bubble sort,1
" finally, the boolean pstack == 1 is used to check whether the stack is empty in line 18 of algorithm 2",0
" for instance, for recursive addition, consider the family f of addition problems anan−1 ",0
" that is to say, the network does not learn the true program semantics",0
 stack pop pushes −∞ values to qstacklo and qstackhi,0
 we describe the recursive re-formulation of traces for two tasks from the original npi paper—grade-school addition and bubble sort,0
" we can obtain this mapping by first considering the possible observations that can arise at the beginning of the entry function (add, bubblesort, toposort, and quicksort) for some valid program input, and iteratively applying the observation-to-observation mapping implied by the reference implementation’s step output at that point in the execution",1
" plo and phi point to the lo and hi indices of the array, as in algorithm 3",0
" the swap operation has the following arguments:
arg 1 (swap object 1): ppivot
arg 2 (swap object 2): phi, pj
write",0
", the verification set",1
"
as in line 13 of the right-hand side of figure 1",0
" we made the assumption of equivalent lengths in order to parametrize the input format with respect to length, but this assumption can be removed as well",0
we emphasize the overall goal of this work is to enable the learning of a recursive program,0
 ,0
" for instance, for recursive addition, consider the family f of addition problems anan−1 ",0
" this is highly undesirable, since being able to provide the correct answer in all possible settings is one of the most important aspects of any learned neural program",0
" this is possible if and only if the graph has no directed cycles; that is to say, it must be a directed acyclic graph (dag)",1
" full recursion, in addition to making the aforementioned tail recursive call, adds two additional recursive calls; bstep and lshift are made tail recursive",1
g,1
" 3: 4: function partition(a, lo, hi) 5: pivot = lo 6: for j ∈ [lo, hi− 1] : do 7: if a[j] ≤ a[hi] then 8: swap a[pivot] with a[j] 9: pivot = pivot+ 1 0: swap a[pivot] with a[hi] 11: return pivot 12: 13: function quicksort(a, lo, hi) 14: while slo and shi are not empty: do 15: pop states off slo and shi, writing them to lo and hi",1
" on the other hand, the recursive programs have learned the true program semantics",0
" on the other hand, the non-recursive version has low accuracy, beginning from size 5, and fails completely for graphs of size 8 and beyond",1
e,1
" the stack operation has the following arguments:
arg 1 (operation): stack push call1, stack push call2, stack pop
stack push call1 pushes lo and pivot−1 toqstacklo andqstackhi",0
"
base cases and reduction rules for addition",0
" however, to be sure that the generated v is complete (covers all the cases needed), we need to check all pairs of observations seen in adjacent step inputs (in particular, those before and after a primitive function call), in a similar way as if we were constructing v from scratch",0
" k is set to 11, to denote the range of possible numbers (0 through 9), along with the start/end token (represented with the same encoding) which is observed when a pointer reaches beyond the bounds of the input",1
"
we also report on generalization results for the non-recursive and recursive versions of this variant of bubble sort",1
" for tasks other than addition, it is difficult to analytically determine the verification set, so instead, we randomly generate input candidates until they completely cover the base cases and reduction rules",0
" table 1 presents results on randomly generated arrays of varying length for the learned non-recursive, partially recursive, and full recursive programs",0
g,1
" in this pair, the step input runs a subprogram add1 that has no arguments, and the step output contains a program write that has arguments of out and 1",0
" for grade-school addition, the domain-specific encoder is
fenc(q, i1, i2, i3, i4, at) =mlp ([q(1, i1), q(2, i2), q(3, i3), q(4, i4), at(1), at(2), at(3)]),
where the environment q ∈ r4×n×k is a scratch-pad that contains four rows (the first input number, the second input number, the carry bits, and the output) and n columns",0
" training on the full recursive trace leads to perfect generalization, as shown in section 4",1
"
the training set for topological sort contains 6 traces, with one synthesized from a graph of size 5 and the rest synthesized from graphs of size 7",0
" in future work, we would like to decrease supervision and construct models that are capable of coming up with recursive abstractions themselves",0
" in the non-recursive trace, there are four functions that can be made recursive—toposort, check child, explore, and next start, and we add a tail recursive call to each of these functions in order to make the recursive trace",1
" if no white vertex is found, the entire execution is terminated move none write write a value either to environment (e",0
"
as a concrete instantiation, we show in this paper that we can enable recursive neural programs in the npi model, and thus enable perfectly generalizable neural programs for tasks such as sorting where the original, non-recursive npi program fails",1
 we find that recursion makes it easier for the network to learn the right program and generalize to unknown situations,0
" also the dag can have potentially more than one connected component, meaning it is necessary to transition between these components appropriately",0
" hence, if all subproblems in s are added correctly, we have proven that any member of f will be added correctly, thus eliminating an infinite family of inputs that need to be tested",0
" for grade-school addition, the domain-specific encoder is
fenc(q, i1, i2, i3, i4, at) =mlp ([q(1, i1), q(2, i2), q(3, i3), q(4, i4), at(1), at(2), at(3)]),
where the environment q ∈ r4×n×k is a scratch-pad that contains four rows (the first input number, the second input number, the carry bits, and the output) and n columns",0
" to the best of our knowledge, this is the first time verification has been applied to a neural program, providing provable guarantees about its behavior",1
 18: push lo and p− 1 to slo and shi,0
" an example of part of an addition task trace, written in shorthand, is given in figure 1",1
g,1
"
we propose and describe our verification procedure",1
 this procedure verifies that all base cases and reductions are handled properly by the model via explicit tests,1
"
move",1
" for example, a step input-step output pair in lines 2 and 3 of the left-hand side of figure 1 is (add1, write out 1)",0
" although the model is exposed to more and more data, it might learn spurious and overly complex representations of the program, as suggested in zaremba   (2016)",0
"
in order to demonstrate that recursion can help learn and generalize better, for addition, we trained only on traces for 5 arbitrarily chosen 1-digit addition sum examples",0
"
note that v should also contain the necessary reductions, which corresponds to making the recursive calls at the correct time, as indicated by p ",0
 this algorithm is a variant of depth first search,0
"
the training set for topological sort contains 6 traces, with one synthesized from a graph of size 5 and the rest synthesized from graphs of size 7",0
 this procedure verifies that all base cases and reductions are handled properly by the model via explicit tests,1
" recursion is an important concept in programming languages and a critical tool to
reduce the complexity of programs",0
" in future work, we seek to enable more tasks with recursive structure",1
"
base cases and reduction rules for addition",0
" for instance, for recursive addition, consider the family f of addition problems anan−1 ",1
" the outputs rt, pt+1, at+1 are used to determine the next action, as described in algorithm 1",0
g,0
" the pointers at index i1 and i2 are used to compare the pair of numbers considered during the bubble sweep, swapping them if the number at i1 is greater than that in i2",0
"
algorithm 1 neural programming inference 1: inputs: environment observation e, program p, arguments a, stop threshold α 2: function run(e, p, a) 3: h← 0, r ← 0 4: while r < α do 5: s← fenc(e, a), h← flstm(s, p, h) 6: r ← fend(h), p2 ← fprog(h), a2 ← farg(h) 7: if p is a primitive function then 8: e← fenv(e, p, a)",1
" a topological sort is a linear ordering of vertices such that for every directed edge (u, v) from u to v, u comes before v in the ordering",0
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,1
" in this section, we give a brief review of the npi architecture from reed & de freitas (2016) as background",0
" in particular, in the explore function, adding a tail recursive call resets and stores the hidden states associated with vertices in a stack-like fashion",0
"
training setup",0
" table 1 presents results on randomly generated arrays of varying length for the learned non-recursive, partially recursive, and full recursive programs",0
" formally, the npi is represented by the following set of equations:
st = fenc(et, at)
ht = flstm(st, pt, ht−1)
rt = fend(ht), pt+1 = fprog(ht), at+1 = farg(ht)
t is a subscript denoting the time-step; fenc is a domain-specific encoder (to be described later) that takes in the environment slice et and arguments at; flstm represents the core module, which takes in the state st generated by fenc, a program embedding pt ∈ rp , and hidden lstm state ht; fend decodes the return probability rt; fprog decodes a program key embedding pt+1;1 and farg decodes arguments at+1",1
"
algorithm 2 shows the topological sort task of interest",0
"program descriptions calls arguments toposort perform topological
sort on graph traverse, next start, write, move
none
traverse traverse graph until stack is empty check child, explore none check child check if a white child exists; if so, set childlist[vactive] to point to it move none explore repeatedly traverse subgraphs until stack is empty stack, check child, write, move none stack interact with stack, either pushing or popping write, move push, pop next start move pstart until reaching a white vertex",1
" both the non-recursive and recursive learned programs generalize on all graphs we tried, up to 120 vertices",1
" for example, for the addition task, the model is trained on short inputs and then tested on its ability to sum inputs with much longer numbers of digits",0
" because the context of the caller is stored on a stack when it calls another program and the callee starts in a fresh context, this enables recursion simply by allowing a program to call itself",1
" we implement a quicksort task, in order to demonstrate that recursion helps with learning divide-and-conquer algorithms",0
 3: begin traversing from vertex 1 in the dag,1
 pstacklo and pstackhi point to the top (empty) positions in qstacklo and qstackhi,1
" in the non-recursive trace, there are four functions that can be made recursive—toposort, check child, explore, and next start, and we add a tail recursive call to each of these functions in order to make the recursive trace",1
" given a precise definition of p , it may be possible to automate the generation of v from p in future work",0
" we further have qresult ∈ nu , a scratch-pad which contains the sorted list of vertices at the end of execution, and qstack ∈ nu , which serves the role of the stack s in algorithm 2",1
we emphasize that the notion of a neural recursive program has not been presented in the literature before: this is our main contribution,1
" in the non-recursive trace, there are four functions that can be made recursive—toposort, check child, explore, and next start, and we add a tail recursive call to each of these functions in order to make the recursive trace",1
 some functional programming languages go so far as not to define any looping constructs but rely solely on recursion to enable repeated execution of the same code,0
" for the recursive formulation of addition, we analytically construct the set of input problems that cover all base cases and reduction rules",0
 there are 2 variables (vactive and vsave); vactive holds the active vertex (as in algorithm 2) and vsave holds the value of vactive before executing line 12 of algorithm 2,0
" we performed experiments on the partially recursive version in order to examine what happens when only one recursive call is implemented, when in reality three are required for perfect generalization",0
"
as in line 13 of the right-hand side of figure 1",0
 this algorithm is a variant of depth first search,1
 recursion enables provable guarantees on neural programs’ behavior without needing to exhaustively enumerate all possible inputs to the programs,0
"
the training data for the neural programmer-interpreter consists of full execution traces for the program of interest",0
" the pointers at index i1 and i2 are used to compare the pair of numbers considered during the bubble sweep, swapping them if the number at i1 is greater than that in i2",0
" recursion is an important concept in programming languages and a critical tool to
reduce the complexity of programs",0
" if the program is primitive, the next environmental state et+1 will be affected by pt and at, i",1
" that is to say, the network does not learn the true program semantics",0
we emphasize that the notion of a neural recursive program has not been presented in the literature before: this is our main contribution,0
" our results show that for all learned, to-be-verified neural programs, they all produced the same traces as those produced by p on the verification set",1
"
we describe the details of the npi model relevant to our contributions",1
 errors arise due to undesirable and uninterpretable dependencies and associations the architecture learns to store in some high-dimensional hidden state,0
" the stack operation has the following arguments:
arg 1 (operation): stack push call1, stack push call2, stack pop
stack push call1 pushes lo and pivot−1 toqstacklo andqstackhi",1
"
algorithm 1 neural programming inference 1: inputs: environment observation e, program p, arguments a, stop threshold α 2: function run(e, p, a) 3: h← 0, r ← 0 4: while r < α do 5: s← fenc(e, a), h← flstm(s, p, h) 6: r ← fend(h), p2 ← fprog(h), a2 ← farg(h) 7: if p is a primitive function then 8: e← fenv(e, p, a)",1
 we created a program set that reflects the semantics of algorithm 2,0
" lshift, used in reset, shifts the pointers left until reaching the start token",0
" in our experiments, we only present dag’s as inputs and represent the vertices as values ranging from 1, ",1
" we implement a quicksort task, in order to demonstrate that recursion helps with learning divide-and-conquer algorithms",1
" nevertheless, we would like the model to quickly learn to represent the right semantics of the underlying program from a small number of training examples, not an exhaustive number of them",1
" for instance, for recursive addition, consider the family f of addition problems anan−1 ",0
 we found that changing the npi training traces is a simple way to enable this,0
"
one common strategy to improve generalization is to use curriculum learning, where the model is trained on inputs of gradually increasing complexity",0
"
given a verification set, we can then run the model on the verification set to check if the produced traces and results are correct",1
"1; and for bubble sort, appendix a",1
" for instance, for recursive addition, consider the family f of addition problems anan−1 ",0
" if it is too small, then execution of the program on some input might require evaluation of m(i) on some i /∈ v , and so the behavior ofm(i) might deviate from p (i)",0
" thus, we demonstrate that recursion enables provably perfect generalization for different tasks, including addition, topological sort, quicksort, and a variant of bubble sort",1
" in this version of curriculum learning, even though the inputs are gradually becoming more complex, the semantics of the program is succinct and does not change",1
" an example of part of an addition task trace, written in shorthand, is given in figure 1",0
 we use the lomuto partition scheme; the logic for the recursive trace is shown in algorithm 3,0
" without using a recursive program, such a proof is not possible, because the non-recursive program runs on an arbitrarily long addition problem that creates correspondingly long sequences of step inputs; in the non-recursive formulation of addition, add calls add1 a number of times that is dependent on the length of the input",1
"1, we hypothesize the non-recursive programs do not generalize well because they have learned spurious dependencies specific to the training set, such as length of the input problems",0
" full recursion, in addition to making the aforementioned tail recursive call, adds two additional recursive calls; bstep and lshift are made tail recursive",1
" et+1 ∼ fenv(et, pt, at)",0
" as aforementioned, the npi model naturally supports recursion",0
" 4: function toposort(dag) 5: while there is still a white vertex u: do 6: color[u] = grey 7: vactive = u 8: do 9: if vactive has a white child v then 10: color[v] = grey 11: push vactive onto s 12: vactive = v 13: else 14: color[vactive] = black 15: write vactive to result 16: if s is empty then pass 17: else pop the top vertex off s and set it to vactive 18: while s is not empty
topological sort",0
" , a1a0 + b1b0} are added properly",0
"
inside bubble and reset, there are two operations that can be made recursive",1
 ,1
", to change the value of vactive) none described below move move a pointer (e",0
" the theorem states that for the same sequence of step inputs, the model produces the exact same step output as the target program it aims to learn",0
" the core module of the npi is an lstm controller that takes as input a slice of the current external environment, via a set of pointers, and a program and arguments to execute",0
"
we would also like to point out that in this paper, we provide as an example one way to train a recursive neural program, by providing a certain training execution trace to the npi model",1
 u varies with the number of vertices in the graph,0
 we find that recursion makes it easier for the network to learn the right program and generalize to unknown situations,0
"
we also report on generalization results for the non-recursive and recursive versions of this variant of bubble sort",0
" in particular, in the explore function, adding a tail recursive call resets and stores the hidden states associated with vertices in a stack-like fashion",1
"
in order to demonstrate that recursion can help learn and generalize better, for addition, we trained only on traces for 5 arbitrarily chosen 1-digit addition sum examples",0
" even though the earlier version also generalized just as well in practice, relatively small differences in the formulation of the traces and the environment observations can drastically change the difficulty of verification",0
" given a precise definition of p , it may be possible to automate the generation of v from p in future work",0
" also the dag can have potentially more than one connected component, meaning it is necessary to transition between these components appropriately",1
" 3: 4: function partition(a, lo, hi) 5: pivot = lo 6: for j ∈ [lo, hi− 1] : do 7: if a[j] ≤ a[hi] then 8: swap a[pivot] with a[j] 9: pivot = pivot+ 1 0: swap a[pivot] with a[hi] 11: return pivot 12: 13: function quicksort(a, lo, hi) 14: while slo and shi are not empty: do 15: pop states off slo and shi, writing them to lo and hi",1
" for each graph size, we test the learned programs on 30 randomly generated dags",0
 ,0
 reset represents a −∞ value,1
" we further have qresult ∈ nu , a scratch-pad which contains the sorted list of vertices at the end of execution, and qstack ∈ nu , which serves the role of the stack s in algorithm 2",0
"
the non-recursive trace loops on cycles of add1 and lshift",0
" lshift moves the four pointers to the left, to move to the next column",0
", the trace corresponding to the array [3,2])",0
" however, to be sure that the generated v is complete (covers all the cases needed), we need to check all pairs of observations seen in adjacent step inputs (in particular, those before and after a primitive function call), in a similar way as if we were constructing v from scratch",1
" here as a concrete and general example, we consider a general neural programming architecture (npa), similar to neural programmer-interpreter (npi) in reed & de freitas (2016)",0
" also, compared to the bubble pointers in bubble sort, the pointers that perform the comparison for quicksort (the compswap function) are usually not adjacent to each other, making quicksort less local than bubble sort",0
"in this paper, we propose that the key abstraction of recursion is necessary for neural programs to generalize",1
"as there is no public implementation of npi, we implemented a version of it in keras that is as faithful to the paper as possible",0
" if yes, then this indicates that the learned neural program achieves provably perfect generalization",1
" in our experiments, we only present dag’s as inputs and represent the vertices as values ranging from 1, ",0
 add1 is a subprogram that adds the current column (writing the appropriate digit to the output row and carrying a bit to the next column if needed),0
5,0
 the original version of the bubblesort implementation exposes the values within the array,0
" we implement a quicksort task, in order to demonstrate that recursion helps with learning divide-and-conquer algorithms",1
 our experiments use a small number of training examples,1
" for each task, given the verification set, we check the traces and results of the learned, to-be-verified neural program (described in section 4",1
"as discussed in section 2, the neural programmer-interpreter (npi) is an instance of a neural programmer architecture and hence it naturally supports recursion",0
 recursion enables provable guarantees on neural programs’ behavior without needing to exhaustively enumerate all possible inputs to the programs,1
" m , as described in algorithm 1, processes the sequence of step inputs by using an lstm",0
 stack pop pushes −∞ values to qstacklo and qstackhi,0
" for purposes of verification, we replace the domain-specific encoder with the following:
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1) ≤ q(1, i2), 1 ≤ i1 ≤ length, 1 ≤ i2 ≤ length, i3 == length, at(1), at(2), at(3)]),which directly exposes which of the two values pointed to is larger",1
 18: push lo and p− 1 to slo and shi,0
"
in this paper, we propose to resolve these issues by explicitly incorporating recursion into neural architectures",1
 2: initialize an empty stack s and a directed acyclic graph dag to traverse,1
" in this architecture, we consider a core controller, e",1
" recursion is an important concept in programming languages and a critical tool to
reduce the complexity of programs",0
" 2: initialize lo and hi to be 1 and n, where n is the length of a",1
" fa8750-15-2-0104, and berkeley deep drive",0
 pstacklo and pstackhi point to the top (empty) positions in qstacklo and qstackhi,0
training neural networks to synthesize robust programs from a small number of examples is a challenging task,0
" the second is the training procedure, which consists of the form of the training data and the optimization process",1
" furthermore, given a verification input set that covers all base cases and reduction rules, we can formally prove that the learned neural programs achieve perfect generalization after verifying its behavior on the verification input set",0
" in this section, we give a brief review of the npi architecture from reed & de freitas (2016) as background",1
"
algorithm 3 recursive quicksort 1: initialize an array a to sort",1
 we find that recursion makes it easier for the network to learn the right program and generalize to unknown situations,0
 we use the lomuto partition scheme; the logic for the recursive trace is shown in algorithm 3,1
 b1b0 where no carry operations occur,0
" the 4 environment observations aid with control flow; qstacklo(pstacklo− 1) < qstackhi(pstackhi− 1) implements the lo < hi comparison in line 5 of algorithm 3, pstacklo == 1 checks if the stacks are empty in line 18 of algorithm 4, and the other observations (all involving ppivot or pj) deal with logic in the partition function",1
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,0
"
the non-recursive trace loops on cycles of bubble and reset, which logically represents one bubble sweep through the array and reset of the two bubble pointers to the very beginning of the array, respectively",1
 add1 is a subprogram that adds the current column (writing the appropriate digit to the output row and carrying a bit to the next column if needed),1
"
in order to demonstrate that recursion can help learn and generalize better, we trained a non-recursive and recursive model on just a single execution trace generated from a graph containing 5 nodes3 for the topological sort task",0
"
in what follows, we describe the way in which we constructed npi training traces so as to make them contain recursive elements and thus enable npi to learn recursive programs",0
g,0
" if it is too small, then execution of the program on some input might require evaluation of m(i) on some i /∈ v , and so the behavior ofm(i) might deviate from p (i)",1
" k is set to 11, to represent the range of 10 possible digits, along with a token representing the end of input",0
"program descriptions calls arguments toposort perform topological
sort on graph traverse, next start, write, move
none
traverse traverse graph until stack is empty check child, explore none check child check if a white child exists; if so, set childlist[vactive] to point to it move none explore repeatedly traverse subgraphs until stack is empty stack, check child, write, move none stack interact with stack, either pushing or popping write, move push, pop next start move pstart until reaching a white vertex",1
 recursion enables provable guarantees on neural programs’ behavior without needing to exhaustively enumerate all possible inputs to the programs,0
"
we experiment with two levels of recursion—partial and full",0
"
training setup",1
"
in order to demonstrate that recursion can help learn and generalize better, for addition, we trained only on traces for 5 arbitrarily chosen 1-digit addition sum examples",0
", an array for quicksort or a dag for topological sort)",1
" thus, we demonstrate that recursion enables provably perfect generalization for different tasks, including addition, topological sort, quicksort, and a variant of bubble sort",1
"2 at any given time, the npi has access to values pointed to by four pointers in each of the four rows, represented by q(1, i1), q(2, i2), q(3, i3), and q(4, i4)",0
", qstacklo or qstackhi) or to pointer (e",0
" k is set to 11, to represent the range of 10 possible digits, along with a token representing the end of input",0
" 2: initialize lo and hi to be 1 and n, where n is the length of a",1
"
by nature, recursion reduces the complexity of a problem to simpler instances",1
" without using a recursive program, such a proof is not possible, because the non-recursive program runs on an arbitrarily long addition problem that creates correspondingly long sequences of step inputs; in the non-recursive formulation of addition, add calls add1 a number of times that is dependent on the length of the input",0
"
note that the recursion for quicksort is not purely tail recursive and therefore represents a more complex kind of recursion that is harder to learn than in the previous tasks",0
" in order to compensate for this, ppivot and pj require special functions (move pivot lo and move j lo) to properly set them to lo in lines 11 and 12 of the partition function in algorithm 3",1
" on the other hand, the recursive programs have learned the true program semantics",0
" however, to be sure that the generated v is complete (covers all the cases needed), we need to check all pairs of observations seen in adjacent step inputs (in particular, those before and after a primitive function call), in a similar way as if we were constructing v from scratch",0
 this makes it difficult to reason about what the model will do when given complex inputs,1
"we describe how models trained with recursive traces can be proven to generalize, by using the verification procedure described in section 3",0
"we show that if we incorporate recursion, the learned npi programs can achieve provably perfect generalization for different tasks",1
g,1
" we note that an earlier version of quicksort we tried lacked primitive operations to directly move a pointer to another, and therefore needed more functions and observations",0
" at each time step, the core controller can decide to select one of the programs to call with certain arguments",0
 stack pop pushes −∞ values to qstacklo and qstackhi,0
" 2: initialize lo and hi to be 1 and n, where n is the length of a",0
" in particular, this condition is satisfied by npi, and thus the npi model naturally supports recursion (even though the authors of npi did not consider this aspect explicitly)",1
 u varies with the number of vertices in the graph,1
" if yes, then this indicates that the learned neural program achieves provably perfect generalization",1
" in our experiments, we only present dag’s as inputs and represent the vertices as values ranging from 1, ",0
 ,0
" this agrees with the generalization of non-recursive addition in reed & de freitas (2016), where they reported generalization up to 3000 digits",0
 recursion enables provably perfect generalization,1
" in this architecture, we consider a core controller, e",0
"
note that the recursion for quicksort is not purely tail recursive and therefore represents a more complex kind of recursion that is harder to learn than in the previous tasks",1
"
directed acyclic graphs are structurally more diverse than inputs in the two tasks of grade-school addition and bubble sort",1
" for example, for the addition task, the model is trained on short inputs and then tested on its ability to sum inputs with much longer numbers of digits",0
" in particular, we construct new training traces which explicitly contain recursive elements and show that with this type of trace, npi easily learns recursive programs",0
 ,0
"program descriptions calls arguments toposort perform topological
sort on graph traverse, next start, write, move
none
traverse traverse graph until stack is empty check child, explore none check child check if a white child exists; if so, set childlist[vactive] to point to it move none explore repeatedly traverse subgraphs until stack is empty stack, check child, write, move none stack interact with stack, either pushing or popping write, move push, pop next start move pstart until reaching a white vertex",0
" in computer science, recursion (as opposed to iteration) involves solving a larger problem by combining solutions to smaller instances of the same problem",0
"
training setup",1
"
figure 1 shows examples of non-recursive and recursive addition traces",1
" any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of national science foundation and darpa",0
" as before, the non-recursive learned program lacks a provable guarantee of generalization, whereas we show later that the recursive learned program has one",0
" an example of part of an addition task trace, written in shorthand, is given in figure 1",1
"
by nature, recursion reduces the complexity of a problem to simpler instances",0
" the core module of the npi is an lstm controller that takes as input a slice of the current external environment, via a set of pointers, and a program and arguments to execute",1
 we then construct a verification set of size 73 by ensuring that randomly generated graphs cover the analytically derived v ,0
" a single element of an execution trace consists of a step input-step output pair, which can be synthesized from algorithm 1: this corresponds to, for a given time-step, the step input tuple (e, p, a) and step output tuple (r, p2, a2)",0
"
after finding v , we construct a set of problem inputs which, when executed on p , create exactly the step input sequences which make up v ",0
" for each task, given the verification set, we check the traces and results of the learned, to-be-verified neural program (described in section 4",0
"
the non-recursive trace loops on cycles of add1 and lshift",0
" without using a recursive program, such a proof is not possible, because the non-recursive program runs on an arbitrarily long addition problem that creates correspondingly long sequences of step inputs; in the non-recursive formulation of addition, add calls add1 a number of times that is dependent on the length of the input",0
" we hypothesize that the reason for this is that the neural network learns to spuriously depend on specific characteristics of the training examples that are irrelevant to the true program semantics, such as length of the training inputs, and thus fails to generalize to more complex inputs",0
we emphasize that the notion of a neural recursive program has not been presented in the literature before: this is our main contribution,0
" we made the assumption of equivalent lengths in order to parametrize the input format with respect to length, but this assumption can be removed as well",0
" while this matches the description from reed & de freitas (2016), we found that this causes an unnecessary blowup in the size of v and makes it much more difficult to construct the verification set",0
" for instance, for recursive addition, consider the family f of addition problems anan−1 ",1
" the recursive version of topological sort solves all graph instances we tried, from graphs of size 5 through 70",0
" consequently, it is infeasible to construct a verification set that covers all cases, and the verification procedure we have described is inadequate",1
g,0
" in the non-recursive trace, there are four functions that can be made recursive—toposort, check child, explore, and next start, and we add a tail recursive call to each of these functions in order to make the recursive trace",0
this material is in part based upon work supported by the national science foundation under grant no,0
" if it is too large, then the semantics of p might not be well-defined on some elements in v , or the spurious step input sequences may not be reachable from any valid problem input (e",0
 u varies with the number of vertices in the graph,0
" the swap operation has the following arguments:
arg 1 (swap object 1): ppivot
arg 2 (swap object 2): phi, pj
write",1
" in order to compensate for this, ppivot and pj require special functions (move pivot lo and move j lo) to properly set them to lo in lines 11 and 12 of the partition function in algorithm 3",0
" consequently, it is infeasible to construct a verification set that covers all cases, and the verification procedure we have described is inadequate",0
" partial recursion only adds a tail recursive call to bubblesort after bubble and reset, similar to the tail recursive call described previously for addition",0
" the assumption of leading 0’s can be easily removed, at the cost of slightly increasing the size of the verification set",1
" fa8750-15-2-0104, and berkeley deep drive",1
we emphasize the overall goal of this work is to enable the learning of a recursive program,0
" a topological sort is a linear ordering of vertices such that for every directed edge (u, v) from u to v, u comes before v in the ordering",1
" as a consequence, our learned neural programs empirically achieve perfect generalization from a very small number of training examples",1
" the outputs rt, pt+1, at+1 are used to determine the next action, as described in algorithm 1",1
" empirically, existing models suffer from a common limitation—generalization becomes poor beyond a threshold level of complexity",0
" given a precise definition of p , it may be possible to automate the generation of v from p in future work",1
"1; and for bubble sort, appendix a",1
" a topological sort is a linear ordering of vertices such that for every directed edge (u, v) from u to v, u comes before v in the ordering",0
" fa8750-15-2-0104, and berkeley deep drive",0
" it is straightforward to deal with all other operations, which do not induce partial environment states",1
"
by nature, recursion reduces the complexity of a problem to simpler instances",0
" qcolor(dag[vactive][childlist[vactive]]) refers to the color of the next child of vactive, used in the evaluation of the condition in the if branch in line 9 of algorithm 2",0
 this makes it difficult to reason about what the model will do when given complex inputs,0
" as with the original npi architecture, the experiments for this paper always used a 3-tuple of integers at = (at(1), at(2), at(3))",0
" for these models, table 2 presents results on randomly generated dags of varying graph sizes (varying in the number of vertices)",1
" as with the original npi architecture, the experiments for this paper always used a 3-tuple of integers at = (at(1), at(2), at(3))",0
" for the recursive formulation of addition, we analytically construct the set of input problems that cover all base cases and reduction rules",0
" 3: 4: function partition(a, lo, hi) 5: pivot = lo 6: for j ∈ [lo, hi− 1] : do 7: if a[j] ≤ a[hi] then 8: swap a[pivot] with a[j] 9: pivot = pivot+ 1 0: swap a[pivot] with a[hi] 11: return pivot 12: 13: function quicksort(a, lo, hi) 14: while slo and shi are not empty: do 15: pop states off slo and shi, writing them to lo and hi",0
" for these programs, we re-use the appropriate program sets (the associated subprograms), and we refer the reader to the appendix of reed & de freitas (2016) for further details on the subprograms used in addition and bubble sort",0
" our results show that for all learned, to-be-verified neural programs, they all produced the same traces as those produced by p on the verification set",0
" however, to be sure that the generated v is complete (covers all the cases needed), we need to check all pairs of observations seen in adjacent step inputs (in particular, those before and after a primitive function call), in a similar way as if we were constructing v from scratch",0
" thus, any general neural programming architecture supporting such a call structure can be made to support recursion",0
" , n , where the dag contains n vertices",0
", the verification set",1
"
to perform the verification as described here, it is critical to construct v correctly",1
" for example, a step input-step output pair in lines 2 and 3 of the left-hand side of figure 1 is (add1, write out 1)",0
 ,1
" as with the others, we apply the procedure described in section 3",0
"
as a concrete instantiation, we show in this paper that we can enable recursive neural programs in the npi model, and thus enable perfectly generalizable neural programs for tasks such as sorting where the original, non-recursive npi program fails",1
" for grade-school addition, the domain-specific encoder is
fenc(q, i1, i2, i3, i4, at) =mlp ([q(1, i1), q(2, i2), q(3, i3), q(4, i4), at(1), at(2), at(3)]),
where the environment q ∈ r4×n×k is a scratch-pad that contains four rows (the first input number, the second input number, the carry bits, and the output) and n columns",0
"
note that the training set can often be considerably smaller than the verification set, and despite this, the learned model can still pass the entire verification set",1
" the theorem states that for the same sequence of step inputs, the model produces the exact same step output as the target program it aims to learn",0
", qstacklo or qstackhi) or to pointer (e",0
 stack pop pushes −∞ values to qstacklo and qstackhi,1
" 3: 4: function quicksort(a, lo, hi) 5: if lo < hi: then 6: p = partition(a, lo, hi) 7: quicksort(a, lo, p− 1) 8: quicksort(a, p+ 1, hi) 9: 10: function partition(a, lo, hi) 11: pivot = lo 12: for j ∈ [lo, hi− 1] : do 13: if a[j] ≤ a[hi] then 14: swap a[pivot] with a[j] 15: pivot = pivot+ 1 16: swap a[pivot] with a[hi] 17: return pivot
for quicksort, the domain-specific encoder is
fenc(qarray, qstacklo, qstackhi, plo, phi, pstacklo, pstackhi, ppivot, pj , at) =
mlp ([qarray(pj) ≤ qarray(phi), pj == phi, qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1), pstacklo == 1, at(1), at(2), at(3)]),
whereqarray ∈ ru×11 is a scratch-pad that contains u rows, each containing one of 11 values (one of the numbers 0 through 9 or an invalid state)",0
"
it is sufficient to construct problems where every transition between two adjacent columns is covered",0
5,0
"
the operations we need to be concerned most about are carry and lshift, which induce partial environment states spanning two columns",0
" the architecture usually possesses some form of memory, which could be internal (such as the hidden state of a recurrent neural network) or external (such as a discrete “scratch pad” or a memory block with differentiable access)",0
e,1
" in the non-recursive trace, there are four functions that can be made recursive—toposort, check child, explore, and next start, and we add a tail recursive call to each of these functions in order to make the recursive trace",1
"
since npi naturally supports the notion of recursion, a key question is how to enable npi to learn recursive programs",0
 ,0
" 9: else 10: function run(e, p2, a2)
a description of the inference procedure is given in algorithm 1",0
" for each task, given the verification set, we check the traces and results of the learned, to-be-verified neural program (described in section 4",1
" , n , where the dag contains n vertices",0
" the verification set can be very small, as we found a 10-element array ([8,2,1,2,0,8,5,8,3,7]) is sufficient to cover all of v ",0
"
note that the training set can often be considerably smaller than the verification set, and despite this, the learned model can still pass the entire verification set",0
"
by reasoning about the possible set of environment observations created by all valid inputs, we construct v using the procedure described in section 3",0
 recursion enables provable guarantees on neural programs’ behavior without needing to exhaustively enumerate all possible inputs to the programs,0
"
bubble sort",0
" to the best of our knowledge, this is the first time verification has been applied to a neural program, providing provable guarantees about its behavior",1
 the model described in the training setup of section 4 (trained on 6 traces) was verified to be correct via the matching procedure described in section 4,0
 our result shows that the training procedure and the npi architecture is capable of generalizing from the step input-output pairs seen in the training data to the unseen ones present in the verification set,1
" also, compared to the bubble pointers in bubble sort, the pointers that perform the comparison for quicksort (the compswap function) are usually not adjacent to each other, making quicksort less local than bubble sort",0
" as described in the verification procedure, it is possible to prove our learned recursive program generalizes perfectly by testing on an appropriate set of problem inputs, i",0
" for instance, for recursive addition, consider the family f of addition problems anan−1 ",1
" we have three observations: the color of the start node, the color of the active node’s next child to be considered, and whether the stack is empty",0
"
in what follows, we describe the way in which we constructed npi training traces so as to make them contain recursive elements and thus enable npi to learn recursive programs",1
" however, note that there is no provable guarantee that the non-recursive learned program will generalize to all inputs, whereas we show later that the recursive learned program has a provable guarantee of perfect generalization",0
 figure 2 shows examples of traces for the different versions of bubble sort,0
 stack push call2 pushes pivot + 1 and hi to qstacklo and qstackhi,0
" an example of part of an addition task trace, written in shorthand, is given in figure 1",0
" the single-digit multiplication task in zaremba   (2016), the bubble sort task in reed & de freitas (2016), and the graph tasks in graves   (2016))",0
 we found that changing the npi training traces is a simple way to enable this,0
" , a1a0 + b1b0} are added properly",1
as mentioned in section 2,1
" childlist ∈ nu is a vector of pointers, where childlist[i] points to the next child under consideration for vertex i",0
" for problems where the base cases may be extremely large or infinite, such as certain forms of motor control, recursion can still help reduce the problem of generalization to these two aspects and make the generalization problem significantly simpler to handle and reason about",0
"
as in line 13 of the right-hand side of figure 1",0
"
there are 6 pointers (plo, phi, pstacklo, pstackhi, ppivot, pj)",0
" we call this set of inputs the verification set, sv ",0
" the core lstm module’s hidden state is preserved over all these add1 calls, and it is difficult to interpret with certainty what happens over longer timesteps without concretely evaluating the lstm with an input of that length",0
" note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values, and arg 2 can only take one of 7 values",0
" the core module of the npi is an lstm controller that takes as input a slice of the current external environment, via a set of pointers, and a program and arguments to execute",1
 the degree for any vertex in the dag is variable,1
" finally, the boolean pstack == 1 is used to check whether the stack is empty in line 18 of algorithm 2",0
 add1 is a subprogram that adds the current column (writing the appropriate digit to the output row and carrying a bit to the next column if needed),1
 the program terminates when seeing no numbers in the current column,0
"
algorithm 1 neural programming inference 1: inputs: environment observation e, program p, arguments a, stop threshold α 2: function run(e, p, a) 3: h← 0, r ← 0 4: while r < α do 5: s← fenc(e, a), h← flstm(s, p, h) 6: r ← fend(h), p2 ← fprog(h), a2 ← farg(h) 7: if p is a primitive function then 8: e← fenv(e, p, a)",0
", to change the value of vactive) none described below move move a pointer (e",0
" recall that this verification procedure cannot be performed for the non-recursive versions, since the propagation of the hidden state in the core lstm module makes reasoning difficult and so we would need to check an unbounded number of examples",0
"
note that the training set can often be considerably smaller than the verification set, and despite this, the learned model can still pass the entire verification set",1
"
quicksort",1
" 16: p = partition(a, lo, hi) 17: push p+ 1 and hi to slo and shi",0
" via the recursive call, we effectively forget that the column just added exists, since the recursive call to add starts with a new hidden state for the lstm controller",1
" for these programs, we re-use the appropriate program sets (the associated subprograms), and we refer the reader to the appendix of reed & de freitas (2016) for further details on the subprograms used in addition and bubble sort",0
 we outline how to construct this set,0
 the sequence of primitive operations (move and write operations) for the non-recursive and recursive versions are exactly the same,0
"
we describe the base cases, reduction rules, and the verification set for each task in appendix a",0
 ,1
" in computer science, recursion (as opposed to iteration) involves solving a larger problem by combining solutions to smaller instances of the same problem",0
" for instance, for recursive addition, consider the family f of addition problems anan−1 ",0
" the core module of the npi is an lstm controller that takes as input a slice of the current external environment, via a set of pointers, and a program and arguments to execute",1
 this verification phase only needs to be performed once after training,0
 errors arise due to undesirable and uninterpretable dependencies and associations the architecture learns to store in some high-dimensional hidden state,1
 we find that recursion makes it easier for the network to learn the right program and generalize to unknown situations,0
" the recursive version of topological sort solves all graph instances we tried, from graphs of size 5 through 70",1
" qcolor(dag[vactive][childlist[vactive]]) refers to the color of the next child of vactive, used in the evaluation of the condition in the if branch in line 9 of algorithm 2",1
"
an alternative method is to run p on many different program inputs and then observe step input sequences which occur, to create v ",1
" we then construct problem configurations that span all possible valid environment states (for instance, in order to force the carry bit in a column to be 1, one can add the sum “1+9” in the column to the right)",1
"
the training data for the neural programmer-interpreter consists of full execution traces for the program of interest",1
" thus, any general neural programming architecture supporting such a call structure can be made to support recursion",1
" for each graph size, we test the learned programs on 30 randomly generated dags",1
" observe that the non-recursive program’s correctness degrades for length 11 and beyond, while the recursive program can sort any given array",1
" the theorem states that for the same sequence of step inputs, the model produces the exact same step output as the target program it aims to learn",0
"
under the assumption that there are no leading 0’s (except in the case of single digits) and the two numbers to be added have the same number of digits, the verification set for addition contains 20,181 input problems",0
 some functional programming languages go so far as not to define any looping constructs but rely solely on recursion to enable repeated execution of the same code,0
 add1 is a subprogram that adds the current column (writing the appropriate digit to the output row and carrying a bit to the next column if needed),0
", the verification set",0
" on the other hand, the non-recursive version has low accuracy, beginning from size 5, and fails completely for graphs of size 8 and beyond",0
"
the non-recursive trace loops on cycles of add1 and lshift",0
"
we construct input problems by splitting into two cases: one case in which the left column contains a null value and another in which the left column does not contain any null values",1
" fa8750-15-2-0104, and berkeley deep drive",0
 u varies with the number of vertices in the graph,0
"
the npi accesses an external environment, q, which varies according to the task",0
" in fact, we argue that recursion is an essential element for neural programs to generalize, and makes it tractable to prove the generalization of neural programs",0
" the recursive version can generalize perfectly to long problems constructed from these components (such as the sum “822+233”, where “8+2” and “2+3” are in the training set), but the non-recursive version fails to sum these long problems properly",1
" on the other hand, the recursive programs have learned the true program semantics",0
" given a precise definition of p , it may be possible to automate the generation of v from p in future work",0
"as discussed in section 2, the neural programmer-interpreter (npi) is an instance of a neural programmer architecture and hence it naturally supports recursion",1
" in this architecture, we consider a core controller, e",0
" hence, if all subproblems in s are added correctly, we have proven that any member of f will be added correctly, thus eliminating an infinite family of inputs that need to be tested",0
"
the training set for topological sort contains 6 traces, with one synthesized from a graph of size 5 and the rest synthesized from graphs of size 7",1
" in contrast, each call to the recursive add always runs for a fixed number of steps, even on arbitrarily long problems in f , so we can test that it performs correctly on a small, fixed number of step input sequences",0
" in fact, we argue that recursion is an essential element for neural programs to generalize, and makes it tractable to prove the generalization of neural programs",1
"2 at any given time, the npi has access to values pointed to by four pointers in each of the four rows, represented by q(1, i1), q(2, i2), q(3, i3), and q(4, i4)",1
"
in order to demonstrate that recursion can help learn and generalize better, we trained a non-recursive and recursive model on just a single execution trace generated from a graph containing 5 nodes3 for the topological sort task",0
 the sequence of primitive operations (move and write operations) for the non-recursive and recursive versions are exactly the same,1
" the write operation has the following arguments:
arg 1 (object to write): env stack lo, env stack hi, phi, plo
env stack lo and env stack hi represent qstacklo(pstacklo) and qstackhi(pstackhi), respectively",1
" 4: function toposort(dag) 5: while there is still a white vertex u: do 6: color[u] = grey 7: vactive = u 8: do 9: if vactive has a white child v then 10: color[v] = grey 11: push vactive onto s 12: vactive = v 13: else 14: color[vactive] = black 15: write vactive to result 16: if s is empty then pass 17: else pop the top vertex off s and set it to vactive 18: while s is not empty
topological sort",0
" the add reduction rule ensures that each call to add only covers two adjacent columns, and so the lstm only ever runs for a fixed number of steps necessary to process these two columns",0
training neural networks to synthesize robust programs from a small number of examples is a challenging task,0
" the learned recursive program is different from neural programs learned in all previous work in an important aspect: previous approaches do not explicitly incorporate this abstraction, and hence generalize poorly, whereas our learned neural programs incorporate recursion and achieve perfect generalization",1
" for many problems, the base cases and reduction rules usually consist of a finite (often small) number of cases",0
"1; and for bubble sort, appendix a",0
" childlist ∈ nu is a vector of pointers, where childlist[i] points to the next child under consideration for vertex i",0
" qcolor(pstart) contains the color of the current start vertex, used in the evaluation of the condition in the while loop in line 5 of algorithm 2",1
" for brevity, we refer the reader to the appendix for further details on the program set and non-recursive and recursive trace-generating functions used for topological sort",1
g,0
"
we experiment with two levels of recursion—partial and full",1
3,1
", an lstm in npi’s case, but possibly other networks in different cases",0
" twc-1409915, darpa under grant no",0
g,0
 the original version of the bubblesort implementation exposes the values within the array,0
"
given a verification set, we can then run the model on the verification set to check if the produced traces and results are correct",0
"
we would also like to point out that in this paper, we provide as an example one way to train a recursive neural program, by providing a certain training execution trace to the npi model",0
", an lstm in npi’s case, but possibly other networks in different cases",1
"
topological sort",1
"
it is sufficient to construct problems where every transition between two adjacent columns is covered",0
" each time a subprogram is called, the stack depth increases",0
"when constructing a neural network for the purpose of learning a program, there are two orthogonal aspects to consider",0
 a1a0 + bnbn−1 ,0
" in particular, this condition is satisfied by npi, and thus the npi model naturally supports recursion (even though the authors of npi did not consider this aspect explicitly)",0
" if the step output specifies a primitive function call, we need to reason about how it can affect the environment so as to change the observation in the next step input",1
"
bubble sort",0
"
base cases and reduction rules for topological sort",0
" 3: 4: function quicksort(a, lo, hi) 5: if lo < hi: then 6: p = partition(a, lo, hi) 7: quicksort(a, lo, p− 1) 8: quicksort(a, p+ 1, hi) 9: 10: function partition(a, lo, hi) 11: pivot = lo 12: for j ∈ [lo, hi− 1] : do 13: if a[j] ≤ a[hi] then 14: swap a[pivot] with a[j] 15: pivot = pivot+ 1 16: swap a[pivot] with a[hi] 17: return pivot
for quicksort, the domain-specific encoder is
fenc(qarray, qstacklo, qstackhi, plo, phi, pstacklo, pstackhi, ppivot, pj , at) =
mlp ([qarray(pj) ≤ qarray(phi), pj == phi, qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1), pstacklo == 1, at(1), at(2), at(3)]),
whereqarray ∈ ru×11 is a scratch-pad that contains u rows, each containing one of 11 values (one of the numbers 0 through 9 or an invalid state)",1
"
in this paper, we propose that recursion is an important concept for neural programs as well",1
" both the non-recursive and recursive learned programs generalize on all input lengths we tried, up to 5000 digits",1
" the architecture usually possesses some form of memory, which could be internal (such as the hidden state of a recurrent neural network) or external (such as a discrete “scratch pad” or a memory block with differentiable access)",0
" 3: 4: function quicksort(a, lo, hi) 5: if lo < hi: then 6: p = partition(a, lo, hi) 7: quicksort(a, lo, p− 1) 8: quicksort(a, p+ 1, hi) 9: 10: function partition(a, lo, hi) 11: pivot = lo 12: for j ∈ [lo, hi− 1] : do 13: if a[j] ≤ a[hi] then 14: swap a[pivot] with a[j] 15: pivot = pivot+ 1 16: swap a[pivot] with a[hi] 17: return pivot
for quicksort, the domain-specific encoder is
fenc(qarray, qstacklo, qstackhi, plo, phi, pstacklo, pstackhi, ppivot, pj , at) =
mlp ([qarray(pj) ≤ qarray(phi), pj == phi, qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1), pstacklo == 1, at(1), at(2), at(3)]),
whereqarray ∈ ru×11 is a scratch-pad that contains u rows, each containing one of 11 values (one of the numbers 0 through 9 or an invalid state)",0
"
after finding v , we construct a set of problem inputs which, when executed on p , create exactly the step input sequences which make up v ",1
" on the other hand, the non-recursive version has low accuracy, beginning from size 5, and fails completely for graphs of size 8 and beyond",0
 2: initialize an empty stack s and a directed acyclic graph dag to traverse,0
"6) on the verification set, and ensure they match the traces produced by the true program p ",0
 we adapt machinery from the original paper slightly to fit our needs,0
this material is in part based upon work supported by the national science foundation under grant no,0
" this guarantees that the step input sequences considered during verification contain all step input sequences which arise during execution of an unseen problem in f , leading to generalization to any problem in f ",1
5,0
" this guarantees that the step input sequences considered during verification contain all step input sequences which arise during execution of an unseen problem in f , leading to generalization to any problem in f ",1
" the move operation has the following arguments:
arg 1 (pointer): pstacklo, pstackhi, pj , ppivot
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values",0
" 3: 4: function quicksort(a, lo, hi) 5: if lo < hi: then 6: p = partition(a, lo, hi) 7: quicksort(a, lo, p− 1) 8: quicksort(a, p+ 1, hi) 9: 10: function partition(a, lo, hi) 11: pivot = lo 12: for j ∈ [lo, hi− 1] : do 13: if a[j] ≤ a[hi] then 14: swap a[pivot] with a[j] 15: pivot = pivot+ 1 16: swap a[pivot] with a[hi] 17: return pivot
for quicksort, the domain-specific encoder is
fenc(qarray, qstacklo, qstackhi, plo, phi, pstacklo, pstackhi, ppivot, pj , at) =
mlp ([qarray(pj) ≤ qarray(phi), pj == phi, qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1), pstacklo == 1, at(1), at(2), at(3)]),
whereqarray ∈ ru×11 is a scratch-pad that contains u rows, each containing one of 11 values (one of the numbers 0 through 9 or an invalid state)",0
" if yes, then this indicates that the learned neural program achieves provably perfect generalization",0
 table 4 demonstrates that the accuracy of the non-recursive program degrades sharply when moving from arrays of length 7 to arrays of length 8,0
"
it is sufficient to construct problems where every transition between two adjacent columns is covered",0
"
recursion drastically reduces the number of configurations we need to consider during the verification phase and makes the proof tractable, because it introduces structure that eliminates infinitely long sequences of step inputs that would otherwise need to be considered",0
" the contents ofqresult andqstack are not exposed directly through the domain-specific encoder; rather, we define primitive functions which manipulate these scratch-pads",1
 ,0
" the outputs rt, pt+1, at+1 are used to determine the next action, as described in algorithm 1",0
" for purposes of verification, we replace the domain-specific encoder with the following:
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1) ≤ q(1, i2), 1 ≤ i1 ≤ length, 1 ≤ i2 ≤ length, i3 == length, at(1), at(2), at(3)]),which directly exposes which of the two values pointed to is larger",0
" k is set to 11, to denote the range of possible numbers (0 through 9), along with the start/end token (represented with the same encoding) which is observed when a pointer reaches beyond the bounds of the input",0
3 to construct v and then empirically create a verification set which covers v ,1
"
the dag is represented as an adjacency list where dag[i][j] refers to the j-th child of vertex i",1
" at any given time, the npi has access to the values referred to by two pointers, represented by q(1, i1) and q(1, i2),",0
" as described in the verification procedure, it is possible to prove our learned recursive program generalizes perfectly by testing on an appropriate set of problem inputs, i",1
" our implementation uses two stacks qstacklo and
qstackhi, each in ru , that store the arguments to the recursive quicksort calls in algorithm 3; before each recursive call, the appropriate arguments are popped off the stack and written to plo and phi",0
" we then construct problem configurations that span all possible valid environment states (for instance, in order to force the carry bit in a column to be 1, one can add the sum “1+9” in the column to the right)",1
" however, to be sure that the generated v is complete (covers all the cases needed), we need to check all pairs of observations seen in adjacent step inputs (in particular, those before and after a primitive function call), in a similar way as if we were constructing v from scratch",0
" the write operation has the following arguments:
arg 1 (object to write): env stack lo, env stack hi, phi, plo
env stack lo and env stack hi represent qstacklo(pstacklo) and qstackhi(pstackhi), respectively",0
" in order to compensate for this, ppivot and pj require special functions (move pivot lo and move j lo) to properly set them to lo in lines 11 and 12 of the partition function in algorithm 3",1
 18: push lo and p− 1 to slo and shi,0
"
as mentioned in section 1, all approaches to neural programming today fare poorly on this generalization issue",0
 we instantiated recursion for the neural programmerinterpreter by changing the training traces,0
 the program terminates when seeing no numbers in the current column,0
"
note that the training set can often be considerably smaller than the verification set, and despite this, the learned model can still pass the entire verification set",0
we now report on generalization for the varying tasks,1
g,1
 the original version of the bubblesort implementation exposes the values within the array,0
" we note that an earlier version of quicksort we tried lacked primitive operations to directly move a pointer to another, and therefore needed more functions and observations",0
"
topological sort",0
"
training setup",0
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,1
" we performed experiments on the partially recursive version in order to examine what happens when only one recursive call is implemented, when in reality three are required for perfect generalization",0
" twc-1409915, darpa under grant no",0
"
base cases and reduction rules for topological sort",0
 the core controller acts as a dispatcher for the programs,0
"
in this paper, we propose to resolve these issues by explicitly incorporating recursion into neural architectures",1
" we make the trace recursive by adding a tail recursive call into the trace for the add program after calling add1 and lshift, the original paper uses k = 10, but we found it necessary to augment the range with an end token, in order to terminate properly",0
"
in what follows, we describe the way in which we constructed npi training traces so as to make them contain recursive elements and thus enable npi to learn recursive programs",1
"
the training set for topological sort contains 6 traces, with one synthesized from a graph of size 5 and the rest synthesized from graphs of size 7",1
"
directed acyclic graphs are structurally more diverse than inputs in the two tasks of grade-school addition and bubble sort",1
"
the training data for the neural programmer-interpreter consists of full execution traces for the program of interest",0
" the arrow in the theorem refers to evaluation, as in big-step semantics",1
 table 4 also demonstrates that the (verified) recursive program generalizes perfectly,0
" to the best of our knowledge, this is the first time verification has been applied to a neural program, providing provable guarantees about its behavior",1
" the 4 environment observations aid with control flow; qstacklo(pstacklo− 1) < qstackhi(pstackhi− 1) implements the lo < hi comparison in line 5 of algorithm 3, pstacklo == 1 checks if the stacks are empty in line 18 of algorithm 4, and the other observations (all involving ppivot or pj) deal with logic in the partition function",0
" the stack operation has the following arguments:
arg 1 (operation): stack push call1, stack push call2, stack pop
stack push call1 pushes lo and pivot−1 toqstacklo andqstackhi",1
" the single-digit multiplication task in zaremba   (2016), the bubble sort task in reed & de freitas (2016), and the graph tasks in graves   (2016))",0
" the add reduction rule ensures that each call to add only covers two adjacent columns, and so the lstm only ever runs for a fixed number of steps necessary to process these two columns",0
g,0
" for these models, table 2 presents results on randomly generated dags of varying graph sizes (varying in the number of vertices)",0
"
algorithm 2 depth first search topological sort 1: color all vertices white",0
" in order to compensate for this, ppivot and pj require special functions (move pivot lo and move j lo) to properly set them to lo in lines 11 and 12 of the partition function in algorithm 3",1
"
since npi naturally supports the notion of recursion, a key question is how to enable npi to learn recursive programs",0
"
the non-recursive trace loops on cycles of bubble and reset, which logically represents one bubble sweep through the array and reset of the two bubble pointers to the very beginning of the array, respectively",1
"
in this paper, we propose to resolve these issues by explicitly incorporating recursion into neural architectures",1
" the npi model has three learnable components: a task-agnostic core, a program-key embedding, and domain-specific encoders that allow the npi to operate in diverse environments",1
" however, the authors of npi did not consider explicitly the notion of recursion and as a consequence, did not learn recursive programs",1
" more specifically, when m is trained on simpler inputs, such as inputs of a small length, the generalization metric evaluates how well m will do on more complex inputs, such as inputs of much longer length",1
"
algorithm 1 neural programming inference 1: inputs: environment observation e, program p, arguments a, stop threshold α 2: function run(e, p, a) 3: h← 0, r ← 0 4: while r < α do 5: s← fenc(e, a), h← flstm(s, p, h) 6: r ← fend(h), p2 ← fprog(h), a2 ← farg(h) 7: if p is a primitive function then 8: e← fenv(e, p, a)",0
5,0
" formally, the npi is represented by the following set of equations:
st = fenc(et, at)
ht = flstm(st, pt, ht−1)
rt = fend(ht), pt+1 = fprog(ht), at+1 = farg(ht)
t is a subscript denoting the time-step; fenc is a domain-specific encoder (to be described later) that takes in the environment slice et and arguments at; flstm represents the core module, which takes in the state st generated by fenc, a program embedding pt ∈ rp , and hidden lstm state ht; fend decodes the return probability rt; fprog decodes a program key embedding pt+1;1 and farg decodes arguments at+1",0
 ,1
" m , as described in algorithm 1, processes the sequence of step inputs by using an lstm",0
"
is critical for implementing recursion, since it permits us to restrict our attention to the currently relevant recursive call, ignoring irrelevant details about other contexts",0
" if it is too large, then the semantics of p might not be well-defined on some elements in v , or the spurious step input sequences may not be reachable from any valid problem input (e",0
" this functionality the original npi paper decodes to a program key embedding kt ∈ rk and then computes a program embedding pt+1, which we also did in our implementation, but we omit this for brevity",1
" for example, for the addition task, the model is trained on short inputs and then tested on its ability to sum inputs with much longer numbers of digits",0
g,1
 almost all architectures train on program input/output pairs,0
" we implement a quicksort task, in order to demonstrate that recursion helps with learning divide-and-conquer algorithms",1
we now report on generalization for the varying tasks,0
" bstep, used in bubble, compares pairs of numbers, continuously moving the bubble pointers once to the right each time until reaching the end of the array",1
" we then construct problem configurations that span all possible valid environment states (for instance, in order to force the carry bit in a column to be 1, one can add the sum “1+9” in the column to the right)",1
"
quicksort",0
" this functionality the original npi paper decodes to a program key embedding kt ∈ rk and then computes a program embedding pt+1, which we also did in our implementation, but we omit this for brevity",0
 ,1
", to change the value of phi) none described belowstack",0
" numerous models have been proposed for learning programs; to name a few, this includes the differentiable neural computer (graves  , 2016), neural turing machine (graves  , 2014), neural gpu (kaiser & sutskever, 2015), neural programmer (neelakantan  , 2015), pointer network (vinyals  , 2015), hierarchical attentive memory (andrychowicz & kurach, 2016), and neural random access machine (kurach  , 2016)",0
" , n , where the dag contains n vertices",0
" in future work, we would like to decrease supervision and construct models that are capable of coming up with recursive abstractions themselves",0
", to change the value of phi) none described belowstack",0
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,0
 errors arise due to undesirable and uninterpretable dependencies and associations the architecture learns to store in some high-dimensional hidden state,0
" this guarantees that the step input sequences considered during verification contain all step input sequences which arise during execution of an unseen problem in f , leading to generalization to any problem in f ",0
" the pointer at index i3 represents a counter internal to the environment that is incremented once after each pass of the algorithm (one cycle of bubble and reset); when incremented a number of times equal to the length of the array, the flag i3 == length becomes true and terminates the entire algorithm ",0
" without using a recursive program, such a proof is not possible, because the non-recursive program runs on an arbitrarily long addition problem that creates correspondingly long sequences of step inputs; in the non-recursive formulation of addition, add calls add1 a number of times that is dependent on the length of the input",0
"
move",1
"we train using the adam optimizer and use a 2-layer lstm and task-specific state encoders for the external environments, as described in reed & de freitas (2016)",0
" the recursive version of topological sort solves all graph instances we tried, from graphs of size 5 through 70",1
 the training set for addition contains 200 traces,0
"
formally, verification consists of proving the following theorem:
∀i ∈ v,m(i) ⇓ p (i) where i denotes a sequence of step inputs (within one function call), v denotes the set of valid sequences of step inputs, m denotes the neural network model, p denotes the correct program, and p (i) denotes the next step output from the correct program",1
"1; and for bubble sort, appendix a",0
 this verification phase only needs to be performed once after training,0
"
there are 6 pointers (plo, phi, pstacklo, pstackhi, ppivot, pj)",1
g,1
" the write operation has the following arguments:
arg 1 (object to write): env stack lo, env stack hi, phi, plo
env stack lo and env stack hi represent qstacklo(pstacklo) and qstackhi(pstackhi), respectively",1
"
after finding v , we construct a set of problem inputs which, when executed on p , create exactly the step input sequences which make up v ",0
"
topological sort",0
" numerous models have been proposed for learning programs; to name a few, this includes the differentiable neural computer (graves  , 2016), neural turing machine (graves  , 2014), neural gpu (kaiser & sutskever, 2015), neural programmer (neelakantan  , 2015), pointer network (vinyals  , 2015), hierarchical attentive memory (andrychowicz & kurach, 2016), and neural random access machine (kurach  , 2016)",0
", an lstm in npi’s case, but possibly other networks in different cases",0
" we performed experiments on the partially recursive version in order to examine what happens when only one recursive call is implemented, when in reality three are required for perfect generalization",0
", an array for quicksort or a dag for topological sort)",0
 ,1
" note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values, and arg 2 can only take one of 7 values",0
" in this architecture, we consider a core controller, e",1
" this is possible if and only if the graph has no directed cycles; that is to say, it must be a directed acyclic graph (dag)",0
" thus, we demonstrate that recursion enables provably perfect generalization for different tasks, including addition, topological sort, quicksort, and a variant of bubble sort",0
" the pointer at index i3 represents a counter internal to the environment that is incremented once after each pass of the algorithm (one cycle of bubble and reset); when incremented a number of times equal to the length of the array, the flag i3 == length becomes true and terminates the entire algorithm ",0
" m , as described in algorithm 1, processes the sequence of step inputs by using an lstm",0
"
base cases and reduction rules for addition",0
"
to evaluate a neural network that learns a neural program to accomplish a certain task, one common evaluation metric is how well the learned model m generalizes",1
" for non-primitive subprograms, we can update the observation-to-observation mapping currently associated with the subprogram and then apply that mapping to the current set",1
" if it is too small, then execution of the program on some input might require evaluation of m(i) on some i /∈ v , and so the behavior ofm(i) might deviate from p (i)",0
" for each task, given the verification set, we check the traces and results of the learned, to-be-verified neural program (described in section 4",0
"
base cases and reduction rules for bubble sort",0
" 9: else 10: function run(e, p2, a2)
a description of the inference procedure is given in algorithm 1",0
g,0
"
we refer the reader to the appendix for the non-recursive trace generating functions",0
" in future work, we seek to enable more tasks with recursive structure",0
 this is due to the properties of the training set – we trained on 2 traces synthesized from arrays of length 7 and 1 trace synthesized from an array of length 6,1
"
the non-recursive trace loops on cycles of add1 and lshift",0
" at end, pointer ppivot is moved to the pivot compswap loop, move pivot lo, move j lo, swap
none
compswap loop runs the for loop inside the partition function compswap, move none compswap compares a[pivot] ≤ a[j]; if so, perform a swap and increment ppivot swap, move none set pivot lo sets ppivot to lo index none none set j lo sets pj to lo index none none set j null sets pj to −∞ none none stack pushes lo/hi states
onto stacks slo and shi according to argument (described below)
write, move described below
move moves pointer one unit up or down none described below swap swaps elements at given array indices none described below write write a value either to stack (e",1
" finally, we implement recursive traces for our own topological sort and quicksort tasks",1
"
as in line 13 of the right-hand side of figure 1",1
"
quicksort",0
" consequently, it is infeasible to construct a verification set that covers all cases, and the verification procedure we have described is inadequate",1
" a single element of an execution trace consists of a step input-step output pair, which can be synthesized from algorithm 1: this corresponds to, for a given time-step, the step input tuple (e, p, a) and step output tuple (r, p2, a2)",0
" in this version of curriculum learning, even though the inputs are gradually becoming more complex, the semantics of the program is succinct and does not change",0
" partial recursion only adds a tail recursive call to bubblesort after bubble and reset, similar to the tail recursive call described previously for addition",0
" m , as described in algorithm 1, processes the sequence of step inputs by using an lstm",1
" the stack operation has the following arguments:
arg 1 (operation): stack push call1, stack push call2, stack pop
stack push call1 pushes lo and pivot−1 toqstacklo andqstackhi",0
"1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 while (pstacklo 6= 1): 6 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 7 stack(stack_pop) 8 else: 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 stack(stack_pop) 12 partition() 13 stack(stack_push_call2) 14 stack(stack_push_call1) 15 } 16 17 partition() { 18 set_pivot_lo() 19 set_j_lo() 20 compswap_loop() 21 swap(ppivot, phi) 22 set_j_null() 23 } 24 25 compswap_loop() { 26 while (pj 6= phi): 27 compswap() 28 move(pj, up) 29 } 30 31 compswap() { 32 if (a[pj ] ≤ a[phi]): 33 swap(ppivot, pj) 34 move(ppivot, up) 35 } 36 37 stack(op) { 38 if (op == stack_push_call1): 39 write(env_stack_lo, plo) 40 write(env_stack_hi, ppivot − 1) 41 move(pstacklo, up) 42 move(pstackhi, up) 43 44 if (op == stack_push_call2): 45 write(env_stack_lo, ppivot + 1) 46 write(env_stack_hi, phi) 47 move(pstacklo, up) 48 move(pstackhi, up) 49 50 if (op == stack_pop): 51 write(env_stack_lo, reset) 52 write(env_stack_hi, reset) 53 move(pstacklo, down) 54 move(pstackhi, down) 55 }altered recursive functions 1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 6 partition() 7 stack(stack_push_call2) 8 stack(stack_push_call1) 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 quicksort() // recursive call 12 stack(stack_pop) 13 write(phi, env_stack_hi_peek) 14 write(plo, env_stack_lo_peek) 15 quicksort() // recursive call 16 stack(stack_pop) 17 } 18 19 compswap_loop() { 20 if (pj 6= phi): 21 compswap() 22 move(pj, up) 23 compswap_loop() // recursive call 24 }in this section, we describe the space of base cases and reduction rules that must be covered for each of the four sample tasks, in order to create the verification set",0
" et+1 ∼ fenv(et, pt, at)",1
"
base cases and reduction rules for bubble sort",1
"
through careful reasoning about the possible set of environment observations created by all valid inputs, and how each of the operations in the execution trace affects the environment, we can construct v using the procedure described in section 3",0
" to make an analogy to mdps, this procedure is analogous to how value iteration obtains the correct value for each state starting from any initialization",0
"
thus far, to evaluate the efficacy of neural models on programming tasks, the only metric that has been used is generalization of expected behavior to inputs of greater complexity (vinyals   (2015), kaiser & sutskever (2015), reed & de freitas (2016), graves   (2016), zaremba   (2016))",1
3,0
" we make the trace recursive by adding a tail recursive call into the trace for the add program after calling add1 and lshift, the original paper uses k = 10, but we found it necessary to augment the range with an end token, in order to terminate properly",0
" also, compared to the bubble pointers in bubble sort, the pointers that perform the comparison for quicksort (the compswap function) are usually not adjacent to each other, making quicksort less local than bubble sort",0
we emphasize the overall goal of this work is to enable the learning of a recursive program,1
this material is in part based upon work supported by the national science foundation under grant no,0
"
in order to demonstrate that recursion can help learn and generalize better, for addition, we trained only on traces for 5 arbitrarily chosen 1-digit addition sum examples",1
"
base cases and reduction rules for bubble sort",0
" we then construct problem configurations that span all possible valid environment states (for instance, in order to force the carry bit in a column to be 1, one can add the sum “1+9” in the column to the right)",0
"
as in line 13 of the right-hand side of figure 1",1
"
in order to demonstrate that recursion can help learn and generalize better, for addition, we trained only on traces for 5 arbitrarily chosen 1-digit addition sum examples",0
" we can obtain this mapping by first considering the possible observations that can arise at the beginning of the entry function (add, bubblesort, toposort, and quicksort) for some valid program input, and iteratively applying the observation-to-observation mapping implied by the reference implementation’s step output at that point in the execution",0
"
algorithm 1 neural programming inference 1: inputs: environment observation e, program p, arguments a, stop threshold α 2: function run(e, p, a) 3: h← 0, r ← 0 4: while r < α do 5: s← fenc(e, a), h← flstm(s, p, h) 6: r ← fend(h), p2 ← fprog(h), a2 ← farg(h) 7: if p is a primitive function then 8: e← fenv(e, p, a)",0
" we performed experiments on the partially recursive version in order to examine what happens when only one recursive call is implemented, when in reality three are required for perfect generalization",1
"
move",1
"
in what follows, we describe the way in which we constructed npi training traces so as to make them contain recursive elements and thus enable npi to learn recursive programs",1
"
base cases and reduction rules for quicksort",1
we emphasize that the notion of a neural recursive program has not been presented in the literature before: this is our main contribution,1
 ,0
" for instance, for recursive addition, consider the family f of addition problems anan−1 ",0
" both the non-recursive and recursive learned programs generalize on all input lengths we tried, up to 5000 digits",0
 there is a (changing) list of neural programs used to accomplish a given task,0
"
as in line 13 of the right-hand side of figure 1",0
"
there are 6 pointers (plo, phi, pstacklo, pstackhi, ppivot, pj)",0
" we can obtain this mapping by first considering the possible observations that can arise at the beginning of the entry function (add, bubblesort, toposort, and quicksort) for some valid program input, and iteratively applying the observation-to-observation mapping implied by the reference implementation’s step output at that point in the execution",0
" an example of part of an addition task trace, written in shorthand, is given in figure 1",1
e,1
" the npi model has three learnable components: a task-agnostic core, a program-key embedding, and domain-specific encoders that allow the npi to operate in diverse environments",1
" here as a concrete and general example, we consider a general neural programming architecture (npa), similar to neural programmer-interpreter (npi) in reed & de freitas (2016)",1
" for tasks other than addition, it is difficult to analytically determine the verification set, so instead, we randomly generate input candidates until they completely cover the base cases and reduction rules",0
" observe that the non-recursive program’s correctness degrades for length 11 and beyond, while the recursive program can sort any given array",0
 recursion can be implemented differently for different neural programming models,0
" for non-primitive subprograms, we can update the observation-to-observation mapping currently associated with the subprogram and then apply that mapping to the current set",0
6,0
" for each length, we test each program on 30 randomly generated problems",0
 the sequence of primitive operations (move and write operations) for the non-recursive and recursive versions are exactly the same,1
" 4: function toposort(dag) 5: while there is still a white vertex u: do 6: color[u] = grey 7: vactive = u 8: do 9: if vactive has a white child v then 10: color[v] = grey 11: push vactive onto s 12: vactive = v 13: else 14: color[vactive] = black 15: write vactive to result 16: if s is empty then pass 17: else pop the top vertex off s and set it to vactive 18: while s is not empty
topological sort",0
" 3: 4: function partition(a, lo, hi) 5: pivot = lo 6: for j ∈ [lo, hi− 1] : do 7: if a[j] ≤ a[hi] then 8: swap a[pivot] with a[j] 9: pivot = pivot+ 1 0: swap a[pivot] with a[hi] 11: return pivot 12: 13: function quicksort(a, lo, hi) 14: while slo and shi are not empty: do 15: pop states off slo and shi, writing them to lo and hi",0
"
we describe the details of the npi model relevant to our contributions",1
"
for addition, we analytically determine the verification set",1
" for example, for the addition task, the model is trained on short inputs and then tested on its ability to sum inputs with much longer numbers of digits",0
3,1
 the model described in the training setup of section 4 (trained on 6 traces) was verified to be correct via the matching procedure described in section 4,0
"
we describe the details of the npi model relevant to our contributions",0
" in particular, given that a recursion is defined by two properties as mentioned before, the base cases and the set of reduction rules, we can prove a recursive neural program generalizes perfectly if we can prove that (1) it performs correctly on the base cases; (2) it learns the reduction rules correctly",0
" hence, if all subproblems in s are added correctly, we have proven that any member of f will be added correctly, thus eliminating an infinite family of inputs that need to be tested",0
" the stack operation has the following arguments:
arg 1 (operation): stack push call1, stack push call2, stack pop
stack push call1 pushes lo and pivot−1 toqstacklo andqstackhi",0
 this procedure verifies that all base cases and reductions are handled properly by the model via explicit tests,0
" full recursion, in addition to making the aforementioned tail recursive call, adds two additional recursive calls; bstep and lshift are made tail recursive",0
"
quicksort",0
" we then construct problem configurations that span all possible valid environment states (for instance, in order to force the carry bit in a column to be 1, one can add the sum “1+9” in the column to the right)",0
" as with the original npi architecture, the experiments for this paper always used a 3-tuple of integers at = (at(1), at(2), at(3))",0
" thus, we demonstrate that recursion enables provably perfect generalization for different tasks, including addition, topological sort, quicksort, and a variant of bubble sort",0
" in order to claim a proof, we must verify the model produces correct behavior over all base cases and reductions, as described in section 2",0
3,0
we emphasize that the notion of a neural recursive program has not been presented in the literature before: this is our main contribution,1
g,0
" 16: p = partition(a, lo, hi) 17: push p+ 1 and hi to slo and shi",1
" 16: p = partition(a, lo, hi) 17: push p+ 1 and hi to slo and shi",0
" to the best of our knowledge, this is the first time verification has been applied to a neural program, providing provable guarantees about its behavior",1
 our result shows that the training procedure and the npi architecture is capable of generalizing from the step input-output pairs seen in the training data to the unseen ones present in the verification set,0
"
we experiment with two levels of recursion—partial and full",0
" 3: 4: function quicksort(a, lo, hi) 5: if lo < hi: then 6: p = partition(a, lo, hi) 7: quicksort(a, lo, p− 1) 8: quicksort(a, p+ 1, hi) 9: 10: function partition(a, lo, hi) 11: pivot = lo 12: for j ∈ [lo, hi− 1] : do 13: if a[j] ≤ a[hi] then 14: swap a[pivot] with a[j] 15: pivot = pivot+ 1 16: swap a[pivot] with a[hi] 17: return pivot
for quicksort, the domain-specific encoder is
fenc(qarray, qstacklo, qstackhi, plo, phi, pstacklo, pstackhi, ppivot, pj , at) =
mlp ([qarray(pj) ≤ qarray(phi), pj == phi, qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1), pstacklo == 1, at(1), at(2), at(3)]),
whereqarray ∈ ru×11 is a scratch-pad that contains u rows, each containing one of 11 values (one of the numbers 0 through 9 or an invalid state)",0
"
we describe the details of the npi model relevant to our contributions",0
" for these models, table 2 presents results on randomly generated dags of varying graph sizes (varying in the number of vertices)",0
" in order to claim a proof, we must verify the model produces correct behavior over all base cases and reductions, as described in section 2",1
"
recursion drastically reduces the number of configurations we need to consider during the verification phase and makes the proof tractable, because it introduces structure that eliminates infinitely long sequences of step inputs that would otherwise need to be considered",0
" the move operation has the following arguments:
arg 1 (pointer): pstacklo, pstackhi, pj , ppivot
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values",1
" the only model, to our knowledge, that does not train on input-output pairs is the neural programmer-interpreter (reed & de freitas, 2016), which trains on synthetic execution traces",1
" for each graph size, we test the learned programs on 30 randomly generated dags",0
" we can obtain this mapping by first considering the possible observations that can arise at the beginning of the entry function (add, bubblesort, toposort, and quicksort) for some valid program input, and iteratively applying the observation-to-observation mapping implied by the reference implementation’s step output at that point in the execution",0
" lshift moves the four pointers to the left, to move to the next column",1
" training on the full recursive trace leads to perfect generalization, as shown in section 4",0
 we leave this as future work to devise a verification procedure more appropriate to this setting,1
6,1
" for bubble sort, the domain-specific encoder is
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1), q(1, i2), i3 == length, at(1), at(2), at(3)]),
where the environment q ∈ r1×n×k is a scratch-pad that contains 1 row, to represent the state of the array as sorting proceeds in-place, andn columns",0
 we then construct a verification set of size 73 by ensuring that randomly generated graphs cover the analytically derived v ,1
" the recursive version can generalize perfectly to long problems constructed from these components (such as the sum “822+233”, where “8+2” and “2+3” are in the training set), but the non-recursive version fails to sum these long problems properly",1
"
the training set for quicksort contains 4 traces, synthesized from arrays of length 5",0
" for these models, table 2 presents results on randomly generated dags of varying graph sizes (varying in the number of vertices)",0
" in future work, we seek to enable more tasks with recursive structure",0
" the recursive version of topological sort solves all graph instances we tried, from graphs of size 5 through 70",0
 recursion enables provably perfect generalization,0
" in the non-recursive trace, there are four functions that can be made recursive—toposort, check child, explore, and next start, and we add a tail recursive call to each of these functions in order to make the recursive trace",0
"
thus far, to evaluate the efficacy of neural models on programming tasks, the only metric that has been used is generalization of expected behavior to inputs of greater complexity (vinyals   (2015), kaiser & sutskever (2015), reed & de freitas (2016), graves   (2016), zaremba   (2016))",1
" the space of possible programs is extremely large, and composing a program that performs robustly on the infinite space of possible inputs is difficult—in part because it is impractical to obtain enough training examples to easily disambiguate amongst all possible programs",1
"6) on the verification set, and ensure they match the traces produced by the true program p ",1
"
one common strategy to improve generalization is to use curriculum learning, where the model is trained on inputs of gradually increasing complexity",0
" also the dag can have potentially more than one connected component, meaning it is necessary to transition between these components appropriately",0
 ,0
" a topological sort is a linear ordering of vertices such that for every directed edge (u, v) from u to v, u comes before v in the ordering",0
" as with the others, we apply the procedure described in section 3",0
" the write operation has the following arguments:
arg 1 (main action): color curr, color next, active start, active neighb, active stack, save, stack push, stack pop, result
color curr colors vactive, color next colors vertex dag[vactive][childlist[vactive]], active start writes pstart to vactive, active neighb writes dag[vactive][childlist[vactive]] to vactive, active stack writes qstack(pstack) to vactive, save writes vactive to vsave, stack push pushes vactive to the top of the stack, stack pop writes a null value to the top of the stack, and result writes vactive to qresult(presult)",1
 there is a (changing) list of neural programs used to accomplish a given task,1
" the only model, to our knowledge, that does not train on input-output pairs is the neural programmer-interpreter (reed & de freitas, 2016), which trains on synthetic execution traces",0
" we have three observations: the color of the start node, the color of the active node’s next child to be considered, and whether the stack is empty",0
"
as mentioned in section 1, all approaches to neural programming today fare poorly on this generalization issue",1
 stack pop pushes −∞ values to qstacklo and qstackhi,0
" our implementation uses two stacks qstacklo and
qstackhi, each in ru , that store the arguments to the recursive quicksort calls in algorithm 3; before each recursive call, the appropriate arguments are popped off the stack and written to plo and phi",0
 figure 2 shows examples of traces for the different versions of bubble sort,0
g,0
" given verification sets that cover all the base cases and reduction rules, we can provide proofs that these learned programs generalize perfectly",1
" we made the assumption of equivalent lengths in order to parametrize the input format with respect to length, but this assumption can be removed as well",1
 ,0
" for tasks other than addition, it is difficult to analytically determine the verification set, so instead, we randomly generate input candidates until they completely cover the base cases and reduction rules",1
" for instance, for recursive addition, consider the family f of addition problems anan−1 ",0
 table 4 also demonstrates that the (verified) recursive program generalizes perfectly,1
"
we describe the details of the npi model relevant to our contributions",0
"
after finding v , we construct a set of problem inputs which, when executed on p , create exactly the step input sequences which make up v ",1
" the add reduction rule ensures that each call to add only covers two adjacent columns, and so the lstm only ever runs for a fixed number of steps necessary to process these two columns",1
g,0
" the move operation has the following arguments:
arg 1 (pointer): pstacklo, pstackhi, pj , ppivot
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values",1
" both the non-recursive and recursive learned programs generalize on all input lengths we tried, up to 5000 digits",0
"
the operations we need to be concerned most about are carry and lshift, which induce partial environment states spanning two columns",0
" in particular, given that a recursion is defined by two properties as mentioned before, the base cases and the set of reduction rules, we can prove a recursive neural program generalizes perfectly if we can prove that (1) it performs correctly on the base cases; (2) it learns the reduction rules correctly",1
 we describe the recursive re-formulation of traces for two tasks from the original npi paper—grade-school addition and bubble sort,0
 some functional programming languages go so far as not to define any looping constructs but rely solely on recursion to enable repeated execution of the same code,1
"
we refer the reader to the appendix for the non-recursive trace generating functions",0
" the verification set can be very small, as we found a 10-element array ([8,2,1,2,0,8,5,8,3,7]) is sufficient to cover all of v ",1
" thus, any general neural programming architecture supporting such a call structure can be made to support recursion",1
" note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values, and arg 2 can only take one of 7 values",0
" the second is the training procedure, which consists of the form of the training data and the optimization process",0
"
base cases and reduction rules for topological sort",1
"
in this general neural programming architecture, we show it is easy to support recursion",1
" we call this set of inputs the verification set, sv ",1
"
the dag is represented as an adjacency list where dag[i][j] refers to the j-th child of vertex i",0
" any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of national science foundation and darpa",1
" using this modification, we constructed a verification set consisting of one array of size 10",0
" thus, any general neural programming architecture supporting such a call structure can be made to support recursion",0
"
move",0
"
quicksort",1
"
algorithm 2 shows the topological sort task of interest",0
" 16: p = partition(a, lo, hi) 17: push p+ 1 and hi to slo and shi",0
" in computer science, recursion (as opposed to iteration) involves solving a larger problem by combining solutions to smaller instances of the same problem",0
"
we would also like to point out that in this paper, we provide as an example one way to train a recursive neural program, by providing a certain training execution trace to the npi model",0
 we use the lomuto partition scheme; the logic for the recursive trace is shown in algorithm 3,0
"
training setup",0
 figure 2 shows examples of traces for the different versions of bubble sort,0
 we leave this as future work to devise a verification procedure more appropriate to this setting,1
"
the dag is represented as an adjacency list where dag[i][j] refers to the j-th child of vertex i",0
"
the training set for bubble sort contains 100 traces, with maximum problem length of 2 (e",0
"
as a concrete instantiation, we show in this paper that we can enable recursive neural programs in the npi model, and thus enable perfectly generalizable neural programs for tasks such as sorting where the original, non-recursive npi program fails",0
"
base cases and reduction rules for topological sort",0
"
we experiment with two levels of recursion—partial and full",0
"
in what follows, we describe the way in which we constructed npi training traces so as to make them contain recursive elements and thus enable npi to learn recursive programs",1
"
through careful reasoning about the possible set of environment observations created by all valid inputs, and how each of the operations in the execution trace affects the environment, we can construct v using the procedure described in section 3",1
"1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 while (pstacklo 6= 1): 6 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 7 stack(stack_pop) 8 else: 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 stack(stack_pop) 12 partition() 13 stack(stack_push_call2) 14 stack(stack_push_call1) 15 } 16 17 partition() { 18 set_pivot_lo() 19 set_j_lo() 20 compswap_loop() 21 swap(ppivot, phi) 22 set_j_null() 23 } 24 25 compswap_loop() { 26 while (pj 6= phi): 27 compswap() 28 move(pj, up) 29 } 30 31 compswap() { 32 if (a[pj ] ≤ a[phi]): 33 swap(ppivot, pj) 34 move(ppivot, up) 35 } 36 37 stack(op) { 38 if (op == stack_push_call1): 39 write(env_stack_lo, plo) 40 write(env_stack_hi, ppivot − 1) 41 move(pstacklo, up) 42 move(pstackhi, up) 43 44 if (op == stack_push_call2): 45 write(env_stack_lo, ppivot + 1) 46 write(env_stack_hi, phi) 47 move(pstacklo, up) 48 move(pstackhi, up) 49 50 if (op == stack_pop): 51 write(env_stack_lo, reset) 52 write(env_stack_hi, reset) 53 move(pstacklo, down) 54 move(pstackhi, down) 55 }altered recursive functions 1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 6 partition() 7 stack(stack_push_call2) 8 stack(stack_push_call1) 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 quicksort() // recursive call 12 stack(stack_pop) 13 write(phi, env_stack_hi_peek) 14 write(plo, env_stack_lo_peek) 15 quicksort() // recursive call 16 stack(stack_pop) 17 } 18 19 compswap_loop() { 20 if (pj 6= phi): 21 compswap() 22 move(pj, up) 23 compswap_loop() // recursive call 24 }in this section, we describe the space of base cases and reduction rules that must be covered for each of the four sample tasks, in order to create the verification set",1
" we hypothesize that the reason for this is that the neural network learns to spuriously depend on specific characteristics of the training examples that are irrelevant to the true program semantics, such as length of the training inputs, and thus fails to generalize to more complex inputs",0
"
to perform the verification as described here, it is critical to construct v correctly",1
"
arg 2 (auxiliary variable): color grey, color black
color grey and color black color the given vertex grey and black, respectively",0
"
one common strategy to improve generalization is to use curriculum learning, where the model is trained on inputs of gradually increasing complexity",0
"
move",1
"
to construct this set, by using the reference implementation of each subprogram, we construct a mapping between two sets of environment observations: the first set consists of all observations that can occur at the beginning of a particular subprogram’s invocation, and the second set contains the observations at the end of that subprogram",1
"
note that v should also contain the necessary reductions, which corresponds to making the recursive calls at the correct time, as indicated by p ",0
"
algorithm 2 depth first search topological sort 1: color all vertices white",0
"
note that v should also contain the necessary reductions, which corresponds to making the recursive calls at the correct time, as indicated by p ",1
"
an alternative method is to run p on many different program inputs and then observe step input sequences which occur, to create v ",1
", an lstm in npi’s case, but possibly other networks in different cases",0
"
base cases and reduction rules for topological sort",1
"
the same set of problems was used to generate the training traces for all formulations of the task, for non-recursive and recursive versions",0
"
algorithm 1 neural programming inference 1: inputs: environment observation e, program p, arguments a, stop threshold α 2: function run(e, p, a) 3: h← 0, r ← 0 4: while r < α do 5: s← fenc(e, a), h← flstm(s, p, h) 6: r ← fend(h), p2 ← fprog(h), a2 ← farg(h) 7: if p is a primitive function then 8: e← fenv(e, p, a)",1
"
note that the training set can often be considerably smaller than the verification set, and despite this, the learned model can still pass the entire verification set",1
" this is highly undesirable, since being able to provide the correct answer in all possible settings is one of the most important aspects of any learned neural program",0
"
algorithm 2 depth first search topological sort 1: color all vertices white",1
"1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 while (pstacklo 6= 1): 6 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 7 stack(stack_pop) 8 else: 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 stack(stack_pop) 12 partition() 13 stack(stack_push_call2) 14 stack(stack_push_call1) 15 } 16 17 partition() { 18 set_pivot_lo() 19 set_j_lo() 20 compswap_loop() 21 swap(ppivot, phi) 22 set_j_null() 23 } 24 25 compswap_loop() { 26 while (pj 6= phi): 27 compswap() 28 move(pj, up) 29 } 30 31 compswap() { 32 if (a[pj ] ≤ a[phi]): 33 swap(ppivot, pj) 34 move(ppivot, up) 35 } 36 37 stack(op) { 38 if (op == stack_push_call1): 39 write(env_stack_lo, plo) 40 write(env_stack_hi, ppivot − 1) 41 move(pstacklo, up) 42 move(pstackhi, up) 43 44 if (op == stack_push_call2): 45 write(env_stack_lo, ppivot + 1) 46 write(env_stack_hi, phi) 47 move(pstacklo, up) 48 move(pstackhi, up) 49 50 if (op == stack_pop): 51 write(env_stack_lo, reset) 52 write(env_stack_hi, reset) 53 move(pstacklo, down) 54 move(pstackhi, down) 55 }altered recursive functions 1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 6 partition() 7 stack(stack_push_call2) 8 stack(stack_push_call1) 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 quicksort() // recursive call 12 stack(stack_pop) 13 write(phi, env_stack_hi_peek) 14 write(plo, env_stack_lo_peek) 15 quicksort() // recursive call 16 stack(stack_pop) 17 } 18 19 compswap_loop() { 20 if (pj 6= phi): 21 compswap() 22 move(pj, up) 23 compswap_loop() // recursive call 24 }in this section, we describe the space of base cases and reduction rules that must be covered for each of the four sample tasks, in order to create the verification set",0
" thus, any general neural programming architecture supporting such a call structure can be made to support recursion",1
 the sequence of primitive operations (move and write operations) for the non-recursive and recursive versions are exactly the same,0
"
in order to demonstrate that recursion can help learn and generalize better, we trained a non-recursive and recursive model on just a single execution trace generated from a graph containing 5 nodes3 for the topological sort task",0
"as discussed in section 2, the neural programmer-interpreter (npi) is an instance of a neural programmer architecture and hence it naturally supports recursion",1
" for many problems, the base cases and reduction rules usually consist of a finite (often small) number of cases",0
 ,0
" the add reduction rule ensures that each call to add only covers two adjacent columns, and so the lstm only ever runs for a fixed number of steps necessary to process these two columns",0
"
arg 2 (increment or decrement): up, down
swap",1
 table 4 demonstrates that the accuracy of the non-recursive program degrades sharply when moving from arrays of length 7 to arrays of length 8,1
" to make an analogy to mdps, this procedure is analogous to how value iteration obtains the correct value for each state starting from any initialization",1
g,0
"
under the assumption that there are no leading 0’s (except in the case of single digits) and the two numbers to be added have the same number of digits, the verification set for addition contains 20,181 input problems",0
" the npi model has three learnable components: a task-agnostic core, a program-key embedding, and domain-specific encoders that allow the npi to operate in diverse environments",0
3,0
" for these models, table 2 presents results on randomly generated dags of varying graph sizes (varying in the number of vertices)",0
" the verification set can be very small, as we found a 10-element array ([8,2,1,2,0,8,5,8,3,7]) is sufficient to cover all of v ",0
"
base cases and reduction rules for bubble sort",0
" for each graph size, we test the learned programs on 30 randomly generated dags",0
" 4: function toposort(dag) 5: while there is still a white vertex u: do 6: color[u] = grey 7: vactive = u 8: do 9: if vactive has a white child v then 10: color[v] = grey 11: push vactive onto s 12: vactive = v 13: else 14: color[vactive] = black 15: write vactive to result 16: if s is empty then pass 17: else pop the top vertex off s and set it to vactive 18: while s is not empty
topological sort",0
"
topological sort",1
" in fact, it is one of our future directions to explore new ways to train a recursive neural program without providing explicit training execution traces or with only partial or non-recursive traces",0
 the logic for the non-recursive trace is shown in algorithm 4 in the appendix,0
"
bubble sort",0
", the trace corresponding to the array [3,2])",1
g,0
" hence, if all subproblems in s are added correctly, we have proven that any member of f will be added correctly, thus eliminating an infinite family of inputs that need to be tested",0
" for each length, we test each program on 30 randomly generated problems",0
" the move operation has the following arguments:
arg 1 (pointer): pstacklo, pstackhi, pj , ppivot
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values",0
" for each length, we test each program on 30 randomly generated problems",1
" 2: initialize lo and hi to be 1 and n, where n is the length of a",1
 ,1
" using this modification, we constructed a verification set consisting of one array of size 10",0
 ,0
"
training setup",0
" the corresponding edge list is [(1, 2), (1, 5), (2, 4), (2, 5), (3, 5)]",0
" the pointer at index i3 represents a counter internal to the environment that is incremented once after each pass of the algorithm (one cycle of bubble and reset); when incremented a number of times equal to the length of the array, the flag i3 == length becomes true and terminates the entire algorithm ",0
" we note that an earlier version of quicksort we tried lacked primitive operations to directly move a pointer to another, and therefore needed more functions and observations",0
 we leave this as future work to devise a verification procedure more appropriate to this setting,0
" we then construct problem configurations that span all possible valid environment states (for instance, in order to force the carry bit in a column to be 1, one can add the sum “1+9” in the column to the right)",1
" using this modification, we constructed a verification set consisting of one array of size 10",0
we emphasize the overall goal of this work is to enable the learning of a recursive program,0
"
base cases and reduction rules for topological sort",1
" in particular, in the explore function, adding a tail recursive call resets and stores the hidden states associated with vertices in a stack-like fashion",0
 ,1
" the corresponding edge list is [(1, 2), (1, 5), (2, 4), (2, 5), (3, 5)]",0
" the swap operation has the following arguments:
arg 1 (swap object 1): ppivot
arg 2 (swap object 2): phi, pj
write",1
" in order to compensate for this, ppivot and pj require special functions (move pivot lo and move j lo) to properly set them to lo in lines 11 and 12 of the partition function in algorithm 3",1
" training on the full recursive trace leads to perfect generalization, as shown in section 4",1
" in computer science, recursion (as opposed to iteration) involves solving a larger problem by combining solutions to smaller instances of the same problem",1
" this guarantees that the step input sequences considered during verification contain all step input sequences which arise during execution of an unseen problem in f , leading to generalization to any problem in f ",0
" 2: initialize lo and hi to be 1 and n, where n is the length of a",1
 npi then outputs the return probability and next program and arguments to execute,1
5,0
" the theorem states that for the same sequence of step inputs, the model produces the exact same step output as the target program it aims to learn",1
" the architecture usually possesses some form of memory, which could be internal (such as the hidden state of a recurrent neural network) or external (such as a discrete “scratch pad” or a memory block with differentiable access)",1
" the learned recursive program is different from neural programs learned in all previous work in an important aspect: previous approaches do not explicitly incorporate this abstraction, and hence generalize poorly, whereas our learned neural programs incorporate recursion and achieve perfect generalization",0
"
the operations we need to be concerned most about are carry and lshift, which induce partial environment states spanning two columns",0
" we show that by modifying the training procedure, we enable the npi model to learn recursive neural programs",1
" qcolor(pstart) contains the color of the current start vertex, used in the evaluation of the condition in the while loop in line 5 of algorithm 2",0
" in order to compensate for this, ppivot and pj require special functions (move pivot lo and move j lo) to properly set them to lo in lines 11 and 12 of the partition function in algorithm 3",0
" observe that the non-recursive program’s correctness degrades for length 11 and beyond, while the recursive program can sort any given array",0
" however, note that there is no provable guarantee that the non-recursive learned program will generalize to all inputs, whereas we show later that the recursive learned program has a provable guarantee of perfect generalization",1
g,0
"as discussed in section 2, the neural programmer-interpreter (npi) is an instance of a neural programmer architecture and hence it naturally supports recursion",0
"program descriptions calls arguments quicksort run the quicksort
routine in place for the array a, for indices from lo to hi non-recursive: partition, stack, write recursive: same as non-recursive version, along with quicksort implicitly: array a to sort, lo, hi
partition runs the partition function",0
 table 4 demonstrates that the accuracy of the non-recursive program degrades sharply when moving from arrays of length 7 to arrays of length 8,0
 ,0
" in this version, there is a dependence on length: bstep and lshift are called a number of times equivalent to one less than the length of the input array, in bubble and reset respectively",0
" we implement a quicksort task, in order to demonstrate that recursion helps with learning divide-and-conquer algorithms",1
"
base cases and reduction rules for quicksort",0
" full recursion, in addition to making the aforementioned tail recursive call, adds two additional recursive calls; bstep and lshift are made tail recursive",0
"
an alternative method is to run p on many different program inputs and then observe step input sequences which occur, to create v ",0
 ,0
" recursion is an important concept in programming languages and a critical tool to
reduce the complexity of programs",1
" ppivot and pj point to the pivot and j indices of the array, used in the partition function in algorithm 3",0
 3: begin traversing from vertex 1 in the dag,0
 the degree for any vertex in the dag is variable,0
 table 4 demonstrates that the accuracy of the non-recursive program degrades sharply when moving from arrays of length 7 to arrays of length 8,1
"
by nature, recursion reduces the complexity of a problem to simpler instances",1
5,0
 there are 2 variables (vactive and vsave); vactive holds the active vertex (as in algorithm 2) and vsave holds the value of vactive before executing line 12 of algorithm 2,0
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,0
 recursion enables provably perfect generalization,1
 the model described in the training setup of section 4 (trained on 6 traces) was verified to be correct via the matching procedure described in section 4,0
" naı̈vely, we might expect to synthesize and test an input for any sequence created by combining the four possible colors in two variables and another boolean variable for whether the stack is empty (so 32 possible observations at any point), but for various reasons, most of these combinations are impossible to occur at any given point in the execution trace",0
" the core lstm module’s hidden state is preserved over all these add1 calls, and it is difficult to interpret with certainty what happens over longer timesteps without concretely evaluating the lstm with an input of that length",0
"
the three environment observations aid with control flow in algorithm 2",1
" this functionality the original npi paper decodes to a program key embedding kt ∈ rk and then computes a program embedding pt+1, which we also did in our implementation, but we omit this for brevity",0
training neural networks to synthesize robust programs from a small number of examples is a challenging task,0
" in order to claim a proof, we must verify the model produces correct behavior over all base cases and reductions, as described in section 2",1
" plo and phi point to the lo and hi indices of the array, as in algorithm 3",1
"
to evaluate a neural network that learns a neural program to accomplish a certain task, one common evaluation metric is how well the learned model m generalizes",1
 the model described in the training setup of section 4 (trained on 6 traces) was verified to be correct via the matching procedure described in section 4,1
 ,0
" for non-primitive subprograms, we can update the observation-to-observation mapping currently associated with the subprogram and then apply that mapping to the current set",0
3,0
" as an application, we incorporate recursion into the neural programmer-interpreter architecture and consider four sample tasks: grade-school addition, bubble sort, topological sort, and quicksort",1
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,1
" when the program is called, the current context including the caller’s memory state is stored on a stack; when the program returns, the stored context is popped off the stack to resume execution in the previous caller’s context",1
" observe that partially recursive does slightly better than non-recursive for the setting in which the length of the array is 3, and that the fully recursive version is able to sort every array given to it",0
" ppivot and pj point to the pivot and j indices of the array, used in the partition function in algorithm 3",0
 these pointers are referred to as bubble pointers,1
"
to perform the verification as described here, it is critical to construct v correctly",1
 pstacklo and pstackhi point to the top (empty) positions in qstacklo and qstackhi,1
" if it is too large, then the semantics of p might not be well-defined on some elements in v , or the spurious step input sequences may not be reachable from any valid problem input (e",0
"
thus far, to evaluate the efficacy of neural models on programming tasks, the only metric that has been used is generalization of expected behavior to inputs of greater complexity (vinyals   (2015), kaiser & sutskever (2015), reed & de freitas (2016), graves   (2016), zaremba   (2016))",0
" there are 3 pointers (presult, pstack, pstart), presult points to the next empty location in qresult,
pstack points to the top of the stack in qstack, and pstart points to the candidate starting node for a connected component",0
"
for addition, we analytically determine the verification set",0
"
thus far, to evaluate the efficacy of neural models on programming tasks, the only metric that has been used is generalization of expected behavior to inputs of greater complexity (vinyals   (2015), kaiser & sutskever (2015), reed & de freitas (2016), graves   (2016), zaremba   (2016))",0
g,0
" fa8750-15-2-0104, and berkeley deep drive",0
 there are 2 variables (vactive and vsave); vactive holds the active vertex (as in algorithm 2) and vsave holds the value of vactive before executing line 12 of algorithm 2,1
"
an alternative method is to run p on many different program inputs and then observe step input sequences which occur, to create v ",1
" hence, if all subproblems in s are added correctly, we have proven that any member of f will be added correctly, thus eliminating an infinite family of inputs that need to be tested",0
" by iterating with this procedure, and then running p on the input observation set that we obtain for the entry point function, we can obtain v precisely",0
"1; and for bubble sort, appendix a",1
" for these programs, we re-use the appropriate program sets (the associated subprograms), and we refer the reader to the appendix of reed & de freitas (2016) for further details on the subprograms used in addition and bubble sort",1
" recursion is an important concept in programming languages and a critical tool to
reduce the complexity of programs",0
 we adapt machinery from the original paper slightly to fit our needs,0
g,1
 reset represents a −∞ value,1
 this verification phase only needs to be performed once after training,1
"as discussed in section 2, the neural programmer-interpreter (npi) is an instance of a neural programmer architecture and hence it naturally supports recursion",1
 the first is the actual model architecture,1
" the second is the training procedure, which consists of the form of the training data and the optimization process",1
" on the other hand, the non-recursive version has low accuracy, beginning from size 5, and fails completely for graphs of size 8 and beyond",0
" for purposes of verification, we replace the domain-specific encoder with the following:
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1) ≤ q(1, i2), 1 ≤ i1 ≤ length, 1 ≤ i2 ≤ length, i3 == length, at(1), at(2), at(3)]),which directly exposes which of the two values pointed to is larger",1
 ,1
" our results show that for all learned, to-be-verified neural programs, they all produced the same traces as those produced by p on the verification set",1
e,1
"
we note that for tasks with very large input domains, such as ones involving mnist digits or speech samples, the state space of base cases and reduction rules could be prohibitively large, possibly infinite",1
 our experiments use a small number of training examples,0
 the memory updates of these neural programs are so complex and interdependent that it is difficult to reason about the behaviors of the learned neural program under previously unseen situations (such as problems with longer inputs),1
" by iterating with this procedure, and then running p on the input observation set that we obtain for the entry point function, we can obtain v precisely",0
 we choose to implement a topological sort task for graphs,0
as mentioned in section 2,0
" at end, pointer ppivot is moved to the pivot compswap loop, move pivot lo, move j lo, swap
none
compswap loop runs the for loop inside the partition function compswap, move none compswap compares a[pivot] ≤ a[j]; if so, perform a swap and increment ppivot swap, move none set pivot lo sets ppivot to lo index none none set j lo sets pj to lo index none none set j null sets pj to −∞ none none stack pushes lo/hi states
onto stacks slo and shi according to argument (described below)
write, move described below
move moves pointer one unit up or down none described below swap swaps elements at given array indices none described below write write a value either to stack (e",0
"
as in line 13 of the right-hand side of figure 1",0
" m is considered to have perfect generalization if m can give the right answer for any input, such as inputs of arbitrary length",1
" our results show that for all learned, to-be-verified neural programs, they all produced the same traces as those produced by p on the verification set",0
" the contents ofqresult andqstack are not exposed directly through the domain-specific encoder; rather, we define primitive functions which manipulate these scratch-pads",0
" this functionality the original npi paper decodes to a program key embedding kt ∈ rk and then computes a program embedding pt+1, which we also did in our implementation, but we omit this for brevity",0
" for each length, we test each program on 30 randomly generated problems",1
" if it is too large, then the semantics of p might not be well-defined on some elements in v , or the spurious step input sequences may not be reachable from any valid problem input (e",0
"
in this paper, we propose to resolve these issues by explicitly incorporating recursion into neural architectures",0
"
since npi naturally supports the notion of recursion, a key question is how to enable npi to learn recursive programs",1
"
quicksort",0
"
it is important to emphasize that at inference time in the npi, the hidden state of the lstm controller is reset (to zero) at each subprogram call, as in line 3 of algorithm 1 (h ← 0)",0
 ,1
" hence, if all subproblems in s are added correctly, we have proven that any member of f will be added correctly, thus eliminating an infinite family of inputs that need to be tested",0
"1, we hypothesize the non-recursive programs do not generalize well because they have learned spurious dependencies specific to the training set, such as length of the input problems",0
" in computer science, recursion (as opposed to iteration) involves solving a larger problem by combining solutions to smaller instances of the same problem",0
 b1b0 where no carry operations occur,0
" the architecture usually possesses some form of memory, which could be internal (such as the hidden state of a recurrent neural network) or external (such as a discrete “scratch pad” or a memory block with differentiable access)",0
" furthermore, given a verification input set that covers all base cases and reduction rules, we can formally prove that the learned neural programs achieve perfect generalization after verifying its behavior on the verification input set",1
 we describe the recursive re-formulation of traces for two tasks from the original npi paper—grade-school addition and bubble sort,1
"
algorithm 3 recursive quicksort 1: initialize an array a to sort",0
 the logic for the non-recursive trace is shown in algorithm 4 in the appendix,0
 these pointers are referred to as bubble pointers,0
g,0
" observe that the non-recursive program’s correctness degrades for length 11 and beyond, while the recursive program can sort any given array",0
"
to evaluate a neural network that learns a neural program to accomplish a certain task, one common evaluation metric is how well the learned model m generalizes",0
" 16: p = partition(a, lo, hi) 17: push p+ 1 and hi to slo and shi",0
"
the training set for topological sort contains 6 traces, with one synthesized from a graph of size 5 and the rest synthesized from graphs of size 7",0
g,0
"1; and for bubble sort, appendix a",0
 the maximum problem length in this training set is 3 (e,1
 we created a program set that reflects the semantics of algorithm 2,0
" if a white vertex is found, set pstart to point to it; this signifies the start of a traversal of a new connected component",0
" the add reduction rule ensures that each call to add only covers two adjacent columns, and so the lstm only ever runs for a fixed number of steps necessary to process these two columns",0
" we call this set of inputs the verification set, sv ",0
" the 4 environment observations aid with control flow; qstacklo(pstacklo− 1) < qstackhi(pstackhi− 1) implements the lo < hi comparison in line 5 of algorithm 3, pstacklo == 1 checks if the stacks are empty in line 18 of algorithm 4, and the other observations (all involving ppivot or pj) deal with logic in the partition function",0
" the single-digit multiplication task in zaremba   (2016), the bubble sort task in reed & de freitas (2016), and the graph tasks in graves   (2016))",0
" in fact, it is one of our future directions to explore new ways to train a recursive neural program without providing explicit training execution traces or with only partial or non-recursive traces",0
 ,0
g,1
"
under the assumption that there are no leading 0’s (except in the case of single digits) and the two numbers to be added have the same number of digits, the verification set for addition contains 20,181 input problems",1
" in this version of curriculum learning, even though the inputs are gradually becoming more complex, the semantics of the program is succinct and does not change",0
"
bubble sort",1
 our experiments use a small number of training examples,0
" 9: else 10: function run(e, p2, a2)
a description of the inference procedure is given in algorithm 1",0
g,0
3 to construct v and then empirically create a verification set which covers v ,0
" via the recursive call, we effectively forget that the column just added exists, since the recursive call to add starts with a new hidden state for the lstm controller",0
" empirically, we observe that the learned recursive programs solve all valid inputs with 100% accuracy after training on a very small number of examples, out-performing previous generalization results",0
" in this architecture, we consider a core controller, e",0
"
for topological sort, the domain-specific encoder is
fenc(dag,qcolor, pstack, pstart, vactive, childlist, at)
=mlp ([qcolor(pstart), qcolor(dag[vactive][childlist[vactive]]), pstack == 1, at(1), at(2), at(3)]),
where qcolor ∈ ru×4 is a scratch-pad that contains u rows, each containing one of four colors (white, gray, black, invalid) with one-hot encoding",1
 18: push lo and p− 1 to slo and shi,0
" the recursive version of topological sort solves all graph instances we tried, from graphs of size 5 through 70",1
 the first is the actual model architecture,1
 we created a program set that reflects the semantics of algorithm 2,1
 recursion enables provably perfect generalization,1
" for each function we use to implement the recursive version of topological sort, we need to consider the set of possible environment observation sequences we can create from all valid inputs and test that the learned program produces the correct behavior on each of these inputs",1
", an lstm in npi’s case, but possibly other networks in different cases",0
"
the training set for bubble sort contains 100 traces, with maximum problem length of 2 (e",0
"program descriptions calls arguments quicksort run the quicksort
routine in place for the array a, for indices from lo to hi non-recursive: partition, stack, write recursive: same as non-recursive version, along with quicksort implicitly: array a to sort, lo, hi
partition runs the partition function",0
" the pointer at index i3 represents a counter internal to the environment that is incremented once after each pass of the algorithm (one cycle of bubble and reset); when incremented a number of times equal to the length of the array, the flag i3 == length becomes true and terminates the entire algorithm ",1
"we describe how models trained with recursive traces can be proven to generalize, by using the verification procedure described in section 3",0
" for problems where the base cases may be extremely large or infinite, such as certain forms of motor control, recursion can still help reduce the problem of generalization to these two aspects and make the generalization problem significantly simpler to handle and reason about",0
2,0
"in this paper, we propose that the key abstraction of recursion is necessary for neural programs to generalize",1
" to the best of our knowledge, this is the first time verification has been applied to a neural program, providing provable guarantees about its behavior",0
", the verification set",0
"we show that if we incorporate recursion, the learned npi programs can achieve provably perfect generalization for different tasks",0
" also the dag can have potentially more than one connected component, meaning it is necessary to transition between these components appropriately",0
"
the three environment observations aid with control flow in algorithm 2",0
 there are 2 variables (vactive and vsave); vactive holds the active vertex (as in algorithm 2) and vsave holds the value of vactive before executing line 12 of algorithm 2,0
"
in this paper, we propose to resolve these issues by explicitly incorporating recursion into neural architectures",0
"
base cases and reduction rules for topological sort",1
" in this version, there is a dependence on length: bstep and lshift are called a number of times equivalent to one less than the length of the input array, in bubble and reset respectively",1
g,0
 the model described in the training setup of section 4 (trained on 6 traces) was verified to be correct via the matching procedure described in section 4,0
" for example, a step input-step output pair in lines 2 and 3 of the left-hand side of figure 1 is (add1, write out 1)",1
 stack push call2 pushes pivot + 1 and hi to qstacklo and qstackhi,0
 stack pop pushes −∞ values to qstacklo and qstackhi,0
" in this version of curriculum learning, even though the inputs are gradually becoming more complex, the semantics of the program is succinct and does not change",0
 table 4 demonstrates that the accuracy of the non-recursive program degrades sharply when moving from arrays of length 7 to arrays of length 8,0
" any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of national science foundation and darpa",0
"
directed acyclic graphs are structurally more diverse than inputs in the two tasks of grade-school addition and bubble sort",0
" the space of possible programs is extremely large, and composing a program that performs robustly on the infinite space of possible inputs is difficult—in part because it is impractical to obtain enough training examples to easily disambiguate amongst all possible programs",0
" plo and phi point to the lo and hi indices of the array, as in algorithm 3",0
" when the program is called, the current context including the caller’s memory state is stored on a stack; when the program returns, the stored context is popped off the stack to resume execution in the previous caller’s context",1
"as there is no public implementation of npi, we implemented a version of it in keras that is as faithful to the paper as possible",0
" the general notion of recursion has been an important concept in many domains, including mathematics and computer science",0
" however, our concept of recursion for neural programs is general",1
" for bubble sort, the domain-specific encoder is
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1), q(1, i2), i3 == length, at(1), at(2), at(3)]),
where the environment q ∈ r1×n×k is a scratch-pad that contains 1 row, to represent the state of the array as sorting proceeds in-place, andn columns",1
"
there are 6 pointers (plo, phi, pstacklo, pstackhi, ppivot, pj)",0
" if a white vertex is found, set pstart to point to it; this signifies the start of a traversal of a new connected component",0
" for each function we use to implement the recursive version of topological sort, we need to consider the set of possible environment observation sequences we can create from all valid inputs and test that the learned program produces the correct behavior on each of these inputs",0
" if it is too small, then execution of the program on some input might require evaluation of m(i) on some i /∈ v , and so the behavior ofm(i) might deviate from p (i)",1
" because the context of the caller is stored on a stack when it calls another program and the callee starts in a fresh context, this enables recursion simply by allowing a program to call itself",0
"6) on the verification set, and ensure they match the traces produced by the true program p ",1
training neural networks to synthesize robust programs from a small number of examples is a challenging task,0
"
the training set for bubble sort contains 100 traces, with maximum problem length of 2 (e",1
"program descriptions calls arguments toposort perform topological
sort on graph traverse, next start, write, move
none
traverse traverse graph until stack is empty check child, explore none check child check if a white child exists; if so, set childlist[vactive] to point to it move none explore repeatedly traverse subgraphs until stack is empty stack, check child, write, move none stack interact with stack, either pushing or popping write, move push, pop next start move pstart until reaching a white vertex",0
 we find that recursion makes it easier for the network to learn the right program and generalize to unknown situations,0
" the move operation has the following arguments:
arg 1 (pointer): presult, pstack, pstart, childlist[vactive], childlist[vsave]
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 5 values",0
"
as mentioned in section 1, all approaches to neural programming today fare poorly on this generalization issue",1
" the only model, to our knowledge, that does not train on input-output pairs is the neural programmer-interpreter (reed & de freitas, 2016), which trains on synthetic execution traces",0
 our result shows that the training procedure and the npi architecture is capable of generalizing from the step input-output pairs seen in the training data to the unseen ones present in the verification set,0
"
through careful reasoning about the possible set of environment observations created by all valid inputs, and how each of the operations in the execution trace affects the environment, we can construct v using the procedure described in section 3",0
" in order to claim a proof, we must verify the model produces correct behavior over all base cases and reductions, as described in section 2",1
"
algorithm 1 neural programming inference 1: inputs: environment observation e, program p, arguments a, stop threshold α 2: function run(e, p, a) 3: h← 0, r ← 0 4: while r < α do 5: s← fenc(e, a), h← flstm(s, p, h) 6: r ← fend(h), p2 ← fprog(h), a2 ← farg(h) 7: if p is a primitive function then 8: e← fenv(e, p, a)",0
"
bubble sort",0
"program descriptions calls arguments toposort perform topological
sort on graph traverse, next start, write, move
none
traverse traverse graph until stack is empty check child, explore none check child check if a white child exists; if so, set childlist[vactive] to point to it move none explore repeatedly traverse subgraphs until stack is empty stack, check child, write, move none stack interact with stack, either pushing or popping write, move push, pop next start move pstart until reaching a white vertex",0
"
the three environment observations aid with control flow in algorithm 2",1
" formally, a function exhibits recursive behavior when it possesses two properties: (1) base cases— terminating scenarios that do not use recursion to produce answers; (2) a set of rules that reduces all other problems toward the base cases",1
" for instance, for recursive addition, consider the family f of addition problems anan−1 ",1
" as with the original npi architecture, the experiments for this paper always used a 3-tuple of integers at = (at(1), at(2), at(3))",0
"we show that if we incorporate recursion, the learned npi programs can achieve provably perfect generalization for different tasks",0
 the first is the actual model architecture,1
" partial recursion only adds a tail recursive call to bubblesort after bubble and reset, similar to the tail recursive call described previously for addition",0
" 9: else 10: function run(e, p2, a2)
a description of the inference procedure is given in algorithm 1",0
"
by reasoning about the possible set of environment observations created by all valid inputs, we construct v using the procedure described in section 3",0
"
by reasoning about the possible set of environment observations created by all valid inputs, we construct v using the procedure described in section 3",1
" we note that an earlier version of quicksort we tried lacked primitive operations to directly move a pointer to another, and therefore needed more functions and observations",0
"we train using the adam optimizer and use a 2-layer lstm and task-specific state encoders for the external environments, as described in reed & de freitas (2016)",0
" via the recursive call, we effectively forget that the column just added exists, since the recursive call to add starts with a new hidden state for the lstm controller",0
 errors arise due to undesirable and uninterpretable dependencies and associations the architecture learns to store in some high-dimensional hidden state,1
" however, our concept of recursion for neural programs is general",1
" in particular, given that a recursion is defined by two properties as mentioned before, the base cases and the set of reduction rules, we can prove a recursive neural program generalizes perfectly if we can prove that (1) it performs correctly on the base cases; (2) it learns the reduction rules correctly",1
" on the other hand, the recursive programs have learned the true program semantics",0
"
arg 2 (object to copy): env stack lo peek, env stack hi peek, phi, plo, ppivot − 1, ppivot + 1, reset
env stack lo peek and env stack hi peek represent qstacklo(pstacklo − 1) and qstackhi(pstackhi − 1), respectively",1
 ,1
" as an application, we incorporate recursion into the neural programmer-interpreter architecture and consider four sample tasks: grade-school addition, bubble sort, topological sort, and quicksort",0
 b1b0 where no carry operations occur,0
 this makes it difficult to reason about what the model will do when given complex inputs,0
" et+1 ∼ fenv(et, pt, at)",0
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,0
" the single-digit multiplication task in zaremba   (2016), the bubble sort task in reed & de freitas (2016), and the graph tasks in graves   (2016))",0
 the degree for any vertex in the dag is variable,1
 recursion enables provable guarantees on neural programs’ behavior without needing to exhaustively enumerate all possible inputs to the programs,0
g,0
"
we also report on generalization results for the non-recursive and recursive versions of this variant of bubble sort",1
" observe that the non-recursive program’s correctness degrades for length 11 and beyond, while the recursive program can sort any given array",0
"
algorithm 2 shows the topological sort task of interest",1
"
we note that for tasks with very large input domains, such as ones involving mnist digits or speech samples, the state space of base cases and reduction rules could be prohibitively large, possibly infinite",0
" both the non-recursive and recursive learned programs generalize on all input lengths we tried, up to 5000 digits",1
" plo and phi point to the lo and hi indices of the array, as in algorithm 3",1
" consequently, there is no concept of length relevant to the problem, which has traditionally been an important focus of length-based curriculum learning",1
" the verification set can be very small, as we found a 10-element array ([8,2,1,2,0,8,5,8,3,7]) is sufficient to cover all of v ",0
"
algorithm 2 shows the topological sort task of interest",0
 we describe the recursive re-formulation of traces for two tasks from the original npi paper—grade-school addition and bubble sort,0
 pstacklo and pstackhi point to the top (empty) positions in qstacklo and qstackhi,0
"
bubble sort",0
" we also hope to decrease supervision, for example by training with only partial or non-recursive traces, and to develop novel neural programming architectures integrated directly with a notion of recursion",0
" qcolor(dag[vactive][childlist[vactive]]) refers to the color of the next child of vactive, used in the evaluation of the condition in the if branch in line 9 of algorithm 2",1
 we choose to implement a topological sort task for graphs,0
" empirically, we observe that the learned recursive programs solve all valid inputs with 100% accuracy after training on a very small number of examples, out-performing previous generalization results",0
 errors arise due to undesirable and uninterpretable dependencies and associations the architecture learns to store in some high-dimensional hidden state,0
 table 4 demonstrates that the accuracy of the non-recursive program degrades sharply when moving from arrays of length 7 to arrays of length 8,0
" in particular, recursion can be implemented as a program calling itself",0
", the verification set",1
 2: initialize an empty stack s and a directed acyclic graph dag to traverse,0
" also the dag can have potentially more than one connected component, meaning it is necessary to transition between these components appropriately",0
 this is due to the properties of the training set – we trained on 2 traces synthesized from arrays of length 7 and 1 trace synthesized from an array of length 6,0
 table 4 also demonstrates that the (verified) recursive program generalizes perfectly,0
" at any given time, the npi has access to the values referred to by two pointers, represented by q(1, i1) and q(1, i2),",0
" we also hope to decrease supervision, for example by training with only partial or non-recursive traces, and to develop novel neural programming architectures integrated directly with a notion of recursion",0
 ,1
" as aforementioned, the npi model naturally supports recursion",0
"6) on the verification set, and ensure they match the traces produced by the true program p ",0
" for the recursive formulation of addition, we analytically construct the set of input problems that cover all base cases and reduction rules",0
" we call this set of inputs the verification set, sv ",1
 we outline how to construct this set,0
" this is possible if and only if the graph has no directed cycles; that is to say, it must be a directed acyclic graph (dag)",0
 b1b0 where no carry operations occur,0
"
the training set for topological sort contains 6 traces, with one synthesized from a graph of size 5 and the rest synthesized from graphs of size 7",1
this material is in part based upon work supported by the national science foundation under grant no,1
 errors arise due to undesirable and uninterpretable dependencies and associations the architecture learns to store in some high-dimensional hidden state,1
 the model described in the training setup of section 4 (trained on 6 traces) was verified to be correct via the matching procedure described in section 4,1
 the maximum problem length in this training set is 3 (e,0
 b1b0 where no carry operations occur,1
" recursion is an important concept in programming languages and a critical tool to
reduce the complexity of programs",0
" for each graph size, we test the learned programs on 30 randomly generated dags",0
3 to construct v and then empirically create a verification set which covers v ,1
" in contrast, each call to the recursive add always runs for a fixed number of steps, even on arbitrarily long problems in f , so we can test that it performs correctly on a small, fixed number of step input sequences",0
"
arg 2 (auxiliary variable): color grey, color black
color grey and color black color the given vertex grey and black, respectively",0
"
in what follows, we describe the way in which we constructed npi training traces so as to make them contain recursive elements and thus enable npi to learn recursive programs",1
" if the program is primitive, the next environmental state et+1 will be affected by pt and at, i",1
"
bubble sort",0
e,0
" the stack operation has the following arguments:
arg 1 (operation): stack push call1, stack push call2, stack pop
stack push call1 pushes lo and pivot−1 toqstacklo andqstackhi",0
"
in order to demonstrate that recursion can help learn and generalize better, we trained a non-recursive and recursive model on just a single execution trace generated from a graph containing 5 nodes3 for the topological sort task",0
"program descriptions calls arguments quicksort run the quicksort
routine in place for the array a, for indices from lo to hi non-recursive: partition, stack, write recursive: same as non-recursive version, along with quicksort implicitly: array a to sort, lo, hi
partition runs the partition function",1
"
to evaluate a neural network that learns a neural program to accomplish a certain task, one common evaluation metric is how well the learned model m generalizes",1
" in this pair, the step input runs a subprogram add1 that has no arguments, and the step output contains a program write that has arguments of out and 1",0
" fa8750-15-2-0104, and berkeley deep drive",0
"
to perform the verification as described here, it is critical to construct v correctly",1
" full recursion, in addition to making the aforementioned tail recursive call, adds two additional recursive calls; bstep and lshift are made tail recursive",0
" the theorem states that for the same sequence of step inputs, the model produces the exact same step output as the target program it aims to learn",0
" furthermore, given a verification input set that covers all base cases and reduction rules, we can formally prove that the learned neural programs achieve perfect generalization after verifying its behavior on the verification input set",0
"
arg 2 (increment or decrement): up, down1 // top level topological sort call 2 toposort() { 3 while (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 } 0 11 traverse() { 12 check_child() 13 explore() 14 } 15 16 check_child() { 17 while (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 18 move(childlist[vactive], up) 19 } 20 21 explore() { 22 do 23 if (qcolor(dag[vactive][childlist[vactive]]) is white): 24 write(color_next, color_grey) 25 stack(push) 26 write(save) 27 write(active_neighb) 28 move(childlist[vsave], up) 29 else: 30 write(color_curr, color_black) 31 write(result) 32 move(presult, up) 33 if(pstack == 1): 34 break 35 else: 36 stack(pop) 37 check_child() 38 while (true) 39 } 40 41 stack(op) { 42 if (op == push): 43 write(stack_push) 44 move(pstack, up) 45 46 if (op == pop): 47 write(active_stack) 48 write(stack_pop) 49 move(pstack, down) 50 } 51 52 next_start() { 53 while(qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 54 move(pstart, up) 55 }altered recursive functions  // top level topological sort call 2 toposort() { 3 if (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 toposort() // recursive call 0 } 11 12 check_child() { 13 if (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 14 move(childlist[vactive], up) 15 check_child() // recursive call 16 } 17 18 explore() { 19 if (qcolor(dag[vactive][childlist[vactive]]) is white): 20 write(color_next, color_grey) 21 stack(push) 22 write(save) 23 write(active_neighb) 24 move(childlist[vsave], up) 25 else: 26 write(color_curr, color_black) 27 write(result) 28 move(presult, up) 29 if(pstack == 1): 30 return 31 else: 32 stack(pop) 33 check_child() 34 explore() // recursive call 35 } 36 37 next_start() { 38 if (qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 39 move(pstart, up) 40 next_start() // recursive call 41 }algorithm 4 iterative quicksort 1: initialize an array a to sort and two empty stacks slo and shi",1
"
topological sort",0
" as before, the non-recursive learned program lacks a provable guarantee of generalization, whereas we show later that the recursive learned program has one",0
", the trace corresponding to the array [3,2])",0
" this agrees with the generalization of non-recursive addition in reed & de freitas (2016), where they reported generalization up to 3000 digits",0
 figure 2 shows examples of traces for the different versions of bubble sort,1
" both the non-recursive and recursive learned programs generalize on all input lengths we tried, up to 5000 digits",0
"
to perform the verification as described here, it is critical to construct v correctly",0
"
given a verification set, we can then run the model on the verification set to check if the produced traces and results are correct",1
" childlist ∈ nu is a vector of pointers, where childlist[i] points to the next child under consideration for vertex i",0
" as aforementioned, the npi model naturally supports recursion",1
"
in this general neural programming architecture, we show it is easy to support recursion",0
" while this matches the description from reed & de freitas (2016), we found that this causes an unnecessary blowup in the size of v and makes it much more difficult to construct the verification set",0
" formally, a function exhibits recursive behavior when it possesses two properties: (1) base cases— terminating scenarios that do not use recursion to produce answers; (2) a set of rules that reduces all other problems toward the base cases",0
"
the training data for the neural programmer-interpreter consists of full execution traces for the program of interest",1
" we call this set of inputs the verification set, sv ",1
" in particular, this condition is satisfied by npi, and thus the npi model naturally supports recursion (even though the authors of npi did not consider this aspect explicitly)",0
" the swap operation has the following arguments:
arg 1 (swap object 1): ppivot
arg 2 (swap object 2): phi, pj
write",1
" the core module of the npi is an lstm controller that takes as input a slice of the current external environment, via a set of pointers, and a program and arguments to execute",0
" the recursive version can generalize perfectly to long problems constructed from these components (such as the sum “822+233”, where “8+2” and “2+3” are in the training set), but the non-recursive version fails to sum these long problems properly",0
"
the non-recursive trace loops on cycles of bubble and reset, which logically represents one bubble sweep through the array and reset of the two bubble pointers to the very beginning of the array, respectively",1
" lshift moves the four pointers to the left, to move to the next column",0
"
quicksort",0
" hence, if all subproblems in s are added correctly, we have proven that any member of f will be added correctly, thus eliminating an infinite family of inputs that need to be tested",0
" the recursive version of topological sort solves all graph instances we tried, from graphs of size 5 through 70",1
" in this pair, the step input runs a subprogram add1 that has no arguments, and the step output contains a program write that has arguments of out and 1",1
"
quicksort",1
" in all experiments, α is set to 0",0
"
the training data for the neural programmer-interpreter consists of full execution traces for the program of interest",0
as mentioned in section 2,1
"
move",1
" if the step output specifies a primitive function call, we need to reason about how it can affect the environment so as to change the observation in the next step input",1
g,0
" finally, the boolean pstack == 1 is used to check whether the stack is empty in line 18 of algorithm 2",0
" the pointers at index i1 and i2 are used to compare the pair of numbers considered during the bubble sweep, swapping them if the number at i1 is greater than that in i2",0
" consequently, there is no concept of length relevant to the problem, which has traditionally been an important focus of length-based curriculum learning",0
" if a white vertex is found, set pstart to point to it; this signifies the start of a traversal of a new connected component",1
 pstacklo and pstackhi point to the top (empty) positions in qstacklo and qstackhi,0
"
in order to demonstrate that recursion can help learn and generalize better, for addition, we trained only on traces for 5 arbitrarily chosen 1-digit addition sum examples",1
" , a1a0 + b1b0} are added properly",1
training neural networks to synthesize robust programs from a small number of examples is a challenging task,0
" childlist ∈ nu is a vector of pointers, where childlist[i] points to the next child under consideration for vertex i",0
"
we experiment with two levels of recursion—partial and full",1
"
for topological sort, the domain-specific encoder is
fenc(dag,qcolor, pstack, pstart, vactive, childlist, at)
=mlp ([qcolor(pstart), qcolor(dag[vactive][childlist[vactive]]), pstack == 1, at(1), at(2), at(3)]),
where qcolor ∈ ru×4 is a scratch-pad that contains u rows, each containing one of four colors (white, gray, black, invalid) with one-hot encoding",1
" the pointers at index i1 and i2 are used to compare the pair of numbers considered during the bubble sweep, swapping them if the number at i1 is greater than that in i2",0
" as an application, we incorporate recursion into the neural programmer-interpreter architecture and consider four sample tasks: grade-school addition, bubble sort, topological sort, and quicksort",1
"
arg 2 (increment or decrement): up, down
swap",0
"
the training set for quicksort contains 4 traces, synthesized from arrays of length 5",0
3 to construct v and then empirically create a verification set which covers v ,0
 figure 2 shows examples of traces for the different versions of bubble sort,1
" for tasks other than addition, it is difficult to analytically determine the verification set, so instead, we randomly generate input candidates until they completely cover the base cases and reduction rules",0
 the core controller acts as a dispatcher for the programs,1
" as this complexity interfered with determining the base cases and reductions, we changed the algorithm to its current form",1
g,1
" we show that by modifying the training procedure, we enable the npi model to learn recursive neural programs",1
" for tasks other than addition, it is difficult to analytically determine the verification set, so instead, we randomly generate input candidates until they completely cover the base cases and reduction rules",1
" plo and phi point to the lo and hi indices of the array, as in algorithm 3",0
" the non-recursive and partially recursive versions are unable to sort long arrays, beyond length 8",1
" in the non-recursive trace, there are four functions that can be made recursive—toposort, check child, explore, and next start, and we add a tail recursive call to each of these functions in order to make the recursive trace",0
" the stack operation has the following arguments:
arg 1 (operation): stack push call1, stack push call2, stack pop
stack push call1 pushes lo and pivot−1 toqstacklo andqstackhi",0
" this agrees with the generalization of non-recursive addition in reed & de freitas (2016), where they reported generalization up to 3000 digits",1
"
quicksort",0
"
bubble sort",0
"
we construct input problems by splitting into two cases: one case in which the left column contains a null value and another in which the left column does not contain any null values",1
" both the non-recursive and recursive learned programs generalize on all graphs we tried, up to 120 vertices",0
"
directed acyclic graphs are structurally more diverse than inputs in the two tasks of grade-school addition and bubble sort",0
" for brevity, we refer the reader to the appendix for information about the program set and non-recursive and recursive trace-generating functions for quicksort",0
 there is a (changing) list of neural programs used to accomplish a given task,1
" in order to compensate for this, ppivot and pj require special functions (move pivot lo and move j lo) to properly set them to lo in lines 11 and 12 of the partition function in algorithm 3",0
" 3: 4: function quicksort(a, lo, hi) 5: if lo < hi: then 6: p = partition(a, lo, hi) 7: quicksort(a, lo, p− 1) 8: quicksort(a, p+ 1, hi) 9: 10: function partition(a, lo, hi) 11: pivot = lo 12: for j ∈ [lo, hi− 1] : do 13: if a[j] ≤ a[hi] then 14: swap a[pivot] with a[j] 15: pivot = pivot+ 1 16: swap a[pivot] with a[hi] 17: return pivot
for quicksort, the domain-specific encoder is
fenc(qarray, qstacklo, qstackhi, plo, phi, pstacklo, pstackhi, ppivot, pj , at) =
mlp ([qarray(pj) ≤ qarray(phi), pj == phi, qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1), pstacklo == 1, at(1), at(2), at(3)]),
whereqarray ∈ ru×11 is a scratch-pad that contains u rows, each containing one of 11 values (one of the numbers 0 through 9 or an invalid state)",1
as mentioned in section 2,1
" as with the original npi architecture, the experiments for this paper always used a 3-tuple of integers at = (at(1), at(2), at(3))",1
" 2: initialize lo and hi to be 1 and n, where n is the length of a",0
" in the non-recursive trace, there are four functions that can be made recursive—toposort, check child, explore, and next start, and we add a tail recursive call to each of these functions in order to make the recursive trace",0
"
bubble sort",0
 this makes it difficult to reason about what the model will do when given complex inputs,0
" to make an analogy to mdps, this procedure is analogous to how value iteration obtains the correct value for each state starting from any initialization",0
" for brevity, we refer the reader to the appendix for information about the program set and non-recursive and recursive trace-generating functions for quicksort",0
 we found that changing the npi training traces is a simple way to enable this,1
"
to perform the verification as described here, it is critical to construct v correctly",0
 we outline how to construct this set,1
" m , as described in algorithm 1, processes the sequence of step inputs by using an lstm",1
" the pointer at index i3 represents a counter internal to the environment that is incremented once after each pass of the algorithm (one cycle of bubble and reset); when incremented a number of times equal to the length of the array, the flag i3 == length becomes true and terminates the entire algorithm ",0
"
we describe the details of the npi model relevant to our contributions",0
 there is a (changing) list of neural programs used to accomplish a given task,0
 the memory updates of these neural programs are so complex and interdependent that it is difficult to reason about the behaviors of the learned neural program under previously unseen situations (such as problems with longer inputs),1
"we show that if we incorporate recursion, the learned npi programs can achieve provably perfect generalization for different tasks",0
"we show that if we incorporate recursion, the learned npi programs can achieve provably perfect generalization for different tasks",0
" the contents ofqresult andqstack are not exposed directly through the domain-specific encoder; rather, we define primitive functions which manipulate these scratch-pads",0
 our experiments use a small number of training examples,0
"program descriptions calls arguments toposort perform topological
sort on graph traverse, next start, write, move
none
traverse traverse graph until stack is empty check child, explore none check child check if a white child exists; if so, set childlist[vactive] to point to it move none explore repeatedly traverse subgraphs until stack is empty stack, check child, write, move none stack interact with stack, either pushing or popping write, move push, pop next start move pstart until reaching a white vertex",1
" if no white vertex is found, the entire execution is terminated move none write write a value either to environment (e",0
g,0
"
algorithm 3 recursive quicksort 1: initialize an array a to sort",0
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,0
 b1b0 where no carry operations occur,0
" as this complexity interfered with determining the base cases and reductions, we changed the algorithm to its current form",1
" we call this set of inputs the verification set, sv ",0
" this functionality the original npi paper decodes to a program key embedding kt ∈ rk and then computes a program embedding pt+1, which we also did in our implementation, but we omit this for brevity",1
" lshift moves the four pointers to the left, to move to the next column",0
" we can obtain this mapping by first considering the possible observations that can arise at the beginning of the entry function (add, bubblesort, toposort, and quicksort) for some valid program input, and iteratively applying the observation-to-observation mapping implied by the reference implementation’s step output at that point in the execution",0
e,0
" the single-digit multiplication task in zaremba   (2016), the bubble sort task in reed & de freitas (2016), and the graph tasks in graves   (2016))",1
" in the non-recursive trace, there are four functions that can be made recursive—toposort, check child, explore, and next start, and we add a tail recursive call to each of these functions in order to make the recursive trace",0
" we make the trace recursive by adding a tail recursive call into the trace for the add program after calling add1 and lshift, the original paper uses k = 10, but we found it necessary to augment the range with an end token, in order to terminate properly",1
" the architecture usually possesses some form of memory, which could be internal (such as the hidden state of a recurrent neural network) or external (such as a discrete “scratch pad” or a memory block with differentiable access)",1
" we show that by modifying the training procedure, we enable the npi model to learn recursive neural programs",0
" for example, for the addition task, the model is trained on short inputs and then tested on its ability to sum inputs with much longer numbers of digits",1
"
to perform the verification as described here, it is critical to construct v correctly",1
 almost all architectures train on program input/output pairs,0
 we use the lomuto partition scheme; the logic for the recursive trace is shown in algorithm 3,1
"
algorithm 3 recursive quicksort 1: initialize an array a to sort",0
" using this modification, we constructed a verification set consisting of one array of size 10",0
" empirically, existing models suffer from a common limitation—generalization becomes poor beyond a threshold level of complexity",1
" for bubble sort, the domain-specific encoder is
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1), q(1, i2), i3 == length, at(1), at(2), at(3)]),
where the environment q ∈ r1×n×k is a scratch-pad that contains 1 row, to represent the state of the array as sorting proceeds in-place, andn columns",0
" for grade-school addition, the domain-specific encoder is
fenc(q, i1, i2, i3, i4, at) =mlp ([q(1, i1), q(2, i2), q(3, i3), q(4, i4), at(1), at(2), at(3)]),
where the environment q ∈ r4×n×k is a scratch-pad that contains four rows (the first input number, the second input number, the carry bits, and the output) and n columns",1
" the core lstm module’s hidden state is preserved over all these add1 calls, and it is difficult to interpret with certainty what happens over longer timesteps without concretely evaluating the lstm with an input of that length",1
" in our experiments, we only present dag’s as inputs and represent the vertices as values ranging from 1, ",1
" 2: initialize lo and hi to be 1 and n, where n is the length of a",1
 ,0
" if yes, then this indicates that the learned neural program achieves provably perfect generalization",0
" as an application, we incorporate recursion into the neural programmer-interpreter architecture and consider four sample tasks: grade-school addition, bubble sort, topological sort, and quicksort",0
 our experiments use a small number of training examples,0
" k is set to 11, to denote the range of possible numbers (0 through 9), along with the start/end token (represented with the same encoding) which is observed when a pointer reaches beyond the bounds of the input",1
 this modification also enables us to sort arrays containing arbitrary comparable elements,0
"
it is sufficient to construct problems where every transition between two adjacent columns is covered",0
"we describe how models trained with recursive traces can be proven to generalize, by using the verification procedure described in section 3",1
", an lstm in npi’s case, but possibly other networks in different cases",0
this material is in part based upon work supported by the national science foundation under grant no,1
" in our experiments, we only present dag’s as inputs and represent the vertices as values ranging from 1, ",1
" in particular, we construct new training traces which explicitly contain recursive elements and show that with this type of trace, npi easily learns recursive programs",0
"we train using the adam optimizer and use a 2-layer lstm and task-specific state encoders for the external environments, as described in reed & de freitas (2016)",0
 this modification also enables us to sort arrays containing arbitrary comparable elements,0
 npi then outputs the return probability and next program and arguments to execute,0
"
the npi accesses an external environment, q, which varies according to the task",0
" consequently, there is no concept of length relevant to the problem, which has traditionally been an important focus of length-based curriculum learning",1
" lshift, used in reset, shifts the pointers left until reaching the start token",0
" training on the full recursive trace leads to perfect generalization, as shown in section 4",0
"
base cases and reduction rules for topological sort",0
" our results show that for all learned, to-be-verified neural programs, they all produced the same traces as those produced by p on the verification set",1
"
the same set of problems was used to generate the training traces for all formulations of the task, for non-recursive and recursive versions",1
" via the recursive call, we effectively forget that the column just added exists, since the recursive call to add starts with a new hidden state for the lstm controller",0
 we use the lomuto partition scheme; the logic for the recursive trace is shown in algorithm 3,0
 u varies with the number of vertices in the graph,0
"
for topological sort, the domain-specific encoder is
fenc(dag,qcolor, pstack, pstart, vactive, childlist, at)
=mlp ([qcolor(pstart), qcolor(dag[vactive][childlist[vactive]]), pstack == 1, at(1), at(2), at(3)]),
where qcolor ∈ ru×4 is a scratch-pad that contains u rows, each containing one of four colors (white, gray, black, invalid) with one-hot encoding",1
" for these programs, we re-use the appropriate program sets (the associated subprograms), and we refer the reader to the appendix of reed & de freitas (2016) for further details on the subprograms used in addition and bubble sort",1
 the original version of the bubblesort implementation exposes the values within the array,1
" as this complexity interfered with determining the base cases and reductions, we changed the algorithm to its current form",1
"
we also report on generalization results for the non-recursive and recursive versions of this variant of bubble sort",0
" 9: else 10: function run(e, p2, a2)
a description of the inference procedure is given in algorithm 1",0
 this modification also enables us to sort arrays containing arbitrary comparable elements,1
"
given a verification set, we can then run the model on the verification set to check if the produced traces and results are correct",0
"we show that if we incorporate recursion, the learned npi programs can achieve provably perfect generalization for different tasks",0
" also the dag can have potentially more than one connected component, meaning it is necessary to transition between these components appropriately",0
" thus, we demonstrate that recursion enables provably perfect generalization for different tasks, including addition, topological sort, quicksort, and a variant of bubble sort",0
" finally, we implement recursive traces for our own topological sort and quicksort tasks",1
g,1
"
base cases and reduction rules for quicksort",0
" as with the original npi architecture, the experiments for this paper always used a 3-tuple of integers at = (at(1), at(2), at(3))",0
" table 1 presents results on randomly generated arrays of varying length for the learned non-recursive, partially recursive, and full recursive programs",0
"
figure 1 shows examples of non-recursive and recursive addition traces",0
"
we describe the details of the npi model relevant to our contributions",0
we now report on generalization for the varying tasks,0
"
one common strategy to improve generalization is to use curriculum learning, where the model is trained on inputs of gradually increasing complexity",0
"
grade school addition",1
" the arrow in the theorem refers to evaluation, as in big-step semantics",1
" in particular, recursion can be implemented as a program calling itself",0
" in particular, recursion can be implemented as a program calling itself",1
" in order to claim a proof, we must verify the model produces correct behavior over all base cases and reductions, as described in section 2",1
"
the non-recursive trace loops on cycles of add1 and lshift",0
 almost all architectures train on program input/output pairs,0
"
to perform the verification as described here, it is critical to construct v correctly",0
"
move",0
 the original version of the bubblesort implementation exposes the values within the array,1
 stack pop pushes −∞ values to qstacklo and qstackhi,1
"
to perform the verification as described here, it is critical to construct v correctly",1
" also the dag can have potentially more than one connected component, meaning it is necessary to transition between these components appropriately",1
"
the three environment observations aid with control flow in algorithm 2",0
 figure 2 shows examples of traces for the different versions of bubble sort,0
" as an application, we incorporate recursion into the neural programmer-interpreter architecture and consider four sample tasks: grade-school addition, bubble sort, topological sort, and quicksort",1
"
bubble sort",0
" , a1a0 + b1b0} are added properly",0
" it is straightforward to deal with all other operations, which do not induce partial environment states",0
" although the model is exposed to more and more data, it might learn spurious and overly complex representations of the program, as suggested in zaremba   (2016)",0
" in particular, this condition is satisfied by npi, and thus the npi model naturally supports recursion (even though the authors of npi did not consider this aspect explicitly)",0
 the sequence of primitive operations (move and write operations) for the non-recursive and recursive versions are exactly the same,0
 this procedure verifies that all base cases and reductions are handled properly by the model via explicit tests,0
" to make an analogy to mdps, this procedure is analogous to how value iteration obtains the correct value for each state starting from any initialization",0
" if yes, then this indicates that the learned neural program achieves provably perfect generalization",1
" we show that by modifying the training procedure, we enable the npi model to learn recursive neural programs",0
" we have three observations: the color of the start node, the color of the active node’s next child to be considered, and whether the stack is empty",0
"
in order to demonstrate that recursion can help learn and generalize better, for addition, we trained only on traces for 5 arbitrarily chosen 1-digit addition sum examples",0
" note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values, and arg 2 can only take one of 7 values",1
" recursion is an important concept in programming languages and a critical tool to
reduce the complexity of programs",0
" the second is the training procedure, which consists of the form of the training data and the optimization process",0
e,1
" , a1a0 + b1b0} are added properly",0
 the sequence of primitive operations (move and write operations) for the non-recursive and recursive versions are exactly the same,1
" our results show that for all learned, to-be-verified neural programs, they all produced the same traces as those produced by p on the verification set",1
" for each function we use to implement the recursive version of topological sort, we need to consider the set of possible environment observation sequences we can create from all valid inputs and test that the learned program produces the correct behavior on each of these inputs",0
 we describe the recursive re-formulation of traces for two tasks from the original npi paper—grade-school addition and bubble sort,1
" we show that by modifying the training procedure, we enable the npi model to learn recursive neural programs",1
" in this architecture, we consider a core controller, e",0
"
in this paper, we propose that recursion is an important concept for neural programs as well",0
" we implement a quicksort task, in order to demonstrate that recursion helps with learning divide-and-conquer algorithms",1
" this functionality the original npi paper decodes to a program key embedding kt ∈ rk and then computes a program embedding pt+1, which we also did in our implementation, but we omit this for brevity",0
as mentioned in section 2,0
 our result shows that the training procedure and the npi architecture is capable of generalizing from the step input-output pairs seen in the training data to the unseen ones present in the verification set,1
"
bubble sort",1
" we have three observations: the color of the start node, the color of the active node’s next child to be considered, and whether the stack is empty",0
"
the non-recursive trace loops on cycles of bubble and reset, which logically represents one bubble sweep through the array and reset of the two bubble pointers to the very beginning of the array, respectively",1
"
grade school addition",0
 ,0
"
the training set for bubble sort contains 100 traces, with maximum problem length of 2 (e",0
" twc-1409915, darpa under grant no",0
 we describe the recursive re-formulation of traces for two tasks from the original npi paper—grade-school addition and bubble sort,1
"program descriptions calls arguments quicksort run the quicksort
routine in place for the array a, for indices from lo to hi non-recursive: partition, stack, write recursive: same as non-recursive version, along with quicksort implicitly: array a to sort, lo, hi
partition runs the partition function",0
 ,1
" nevertheless, we would like the model to quickly learn to represent the right semantics of the underlying program from a small number of training examples, not an exhaustive number of them",0
g,1
" although the model is exposed to more and more data, it might learn spurious and overly complex representations of the program, as suggested in zaremba   (2016)",1
", an lstm in npi’s case, but possibly other networks in different cases",1
" as an application, we incorporate recursion into the neural programmer-interpreter architecture and consider four sample tasks: grade-school addition, bubble sort, topological sort, and quicksort",0
" 2: initialize lo and hi to be 1 and n, where n is the length of a",1
" if the program is primitive, the next environmental state et+1 will be affected by pt and at, i",0
" the add reduction rule ensures that each call to add only covers two adjacent columns, and so the lstm only ever runs for a fixed number of steps necessary to process these two columns",1
" each step during an execution of the program does one of three things: (1) another subprogram along with associated arguments is called, as in line 10, (2) the program writes to the environment if it is primitive, as in line 8, or (3) the loop is terminated if the return probability exceeds a threshold α, after which the stack frame is popped and control is returned to the caller",1
" thus, recursion helps decompose a problem and makes it easier to reason about a program’s behavior for previously unseen situations such as longer inputs",1
"
in this paper, we propose that recursion is an important concept for neural programs as well",0
" even though the earlier version also generalized just as well in practice, relatively small differences in the formulation of the traces and the environment observations can drastically change the difficulty of verification",0
" as with the original npi architecture, the experiments for this paper always used a 3-tuple of integers at = (at(1), at(2), at(3))",1
" ppivot and pj point to the pivot and j indices of the array, used in the partition function in algorithm 3",1
" in this version of curriculum learning, even though the inputs are gradually becoming more complex, the semantics of the program is succinct and does not change",0
" we note that an earlier version of quicksort we tried lacked primitive operations to directly move a pointer to another, and therefore needed more functions and observations",0
" the pointers at index i1 and i2 are used to compare the pair of numbers considered during the bubble sweep, swapping them if the number at i1 is greater than that in i2",0
" as this complexity interfered with determining the base cases and reductions, we changed the algorithm to its current form",1
 the model described in the training setup of section 4 (trained on 6 traces) was verified to be correct via the matching procedure described in section 4,0
" the core lstm module’s hidden state is preserved over all these add1 calls, and it is difficult to interpret with certainty what happens over longer timesteps without concretely evaluating the lstm with an input of that length",1
"
given a verification set, we can then run the model on the verification set to check if the produced traces and results are correct",0
" both the non-recursive and recursive learned programs generalize on all input lengths we tried, up to 5000 digits",0
" as aforementioned, the npi model naturally supports recursion",1
" in this pair, the step input runs a subprogram add1 that has no arguments, and the step output contains a program write that has arguments of out and 1",0
"
recursion drastically reduces the number of configurations we need to consider during the verification phase and makes the proof tractable, because it introduces structure that eliminates infinitely long sequences of step inputs that would otherwise need to be considered",1
"
base cases and reduction rules for bubble sort",1
" in future work, we seek to enable more tasks with recursive structure",0
" even though the earlier version also generalized just as well in practice, relatively small differences in the formulation of the traces and the environment observations can drastically change the difficulty of verification",0
" the 4 environment observations aid with control flow; qstacklo(pstacklo− 1) < qstackhi(pstackhi− 1) implements the lo < hi comparison in line 5 of algorithm 3, pstacklo == 1 checks if the stacks are empty in line 18 of algorithm 4, and the other observations (all involving ppivot or pj) deal with logic in the partition function",1
" training on the full recursive trace leads to perfect generalization, as shown in section 4",1
 we found that changing the npi training traces is a simple way to enable this,0
 u varies with the number of vertices in the graph,1
 the model described in the training setup of section 4 (trained on 6 traces) was verified to be correct via the matching procedure described in section 4,0
" empirically, we observe that the learned recursive programs solve all valid inputs with 100% accuracy after training on a very small number of examples, out-performing previous generalization results",1
" however, to be sure that the generated v is complete (covers all the cases needed), we need to check all pairs of observations seen in adjacent step inputs (in particular, those before and after a primitive function call), in a similar way as if we were constructing v from scratch",1
" the pointer at index i3 represents a counter internal to the environment that is incremented once after each pass of the algorithm (one cycle of bubble and reset); when incremented a number of times equal to the length of the array, the flag i3 == length becomes true and terminates the entire algorithm ",0
" the move operation has the following arguments:
arg 1 (pointer): presult, pstack, pstart, childlist[vactive], childlist[vsave]
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 5 values",0
" for purposes of verification, we replace the domain-specific encoder with the following:
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1) ≤ q(1, i2), 1 ≤ i1 ≤ length, 1 ≤ i2 ≤ length, i3 == length, at(1), at(2), at(3)]),which directly exposes which of the two values pointed to is larger",1
" finally, the boolean pstack == 1 is used to check whether the stack is empty in line 18 of algorithm 2",0
"
we note that for tasks with very large input domains, such as ones involving mnist digits or speech samples, the state space of base cases and reduction rules could be prohibitively large, possibly infinite",1
g,1
" via the recursive call, we effectively forget that the column just added exists, since the recursive call to add starts with a new hidden state for the lstm controller",1
" the move operation has the following arguments:
arg 1 (pointer): pstacklo, pstackhi, pj , ppivot
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values",0
 npi then outputs the return probability and next program and arguments to execute,0
 some functional programming languages go so far as not to define any looping constructs but rely solely on recursion to enable repeated execution of the same code,1
" the pointers at index i1 and i2 are used to compare the pair of numbers considered during the bubble sweep, swapping them if the number at i1 is greater than that in i2",0
" the non-recursive and partially recursive versions are unable to sort long arrays, beyond length 8",0
" fa8750-15-2-0104, and berkeley deep drive",0
" this functionality the original npi paper decodes to a program key embedding kt ∈ rk and then computes a program embedding pt+1, which we also did in our implementation, but we omit this for brevity",0
 ,1
"
arg 2 (increment or decrement): up, down1 // top level topological sort call 2 toposort() { 3 while (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 } 0 11 traverse() { 12 check_child() 13 explore() 14 } 15 16 check_child() { 17 while (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 18 move(childlist[vactive], up) 19 } 20 21 explore() { 22 do 23 if (qcolor(dag[vactive][childlist[vactive]]) is white): 24 write(color_next, color_grey) 25 stack(push) 26 write(save) 27 write(active_neighb) 28 move(childlist[vsave], up) 29 else: 30 write(color_curr, color_black) 31 write(result) 32 move(presult, up) 33 if(pstack == 1): 34 break 35 else: 36 stack(pop) 37 check_child() 38 while (true) 39 } 40 41 stack(op) { 42 if (op == push): 43 write(stack_push) 44 move(pstack, up) 45 46 if (op == pop): 47 write(active_stack) 48 write(stack_pop) 49 move(pstack, down) 50 } 51 52 next_start() { 53 while(qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 54 move(pstart, up) 55 }altered recursive functions  // top level topological sort call 2 toposort() { 3 if (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 toposort() // recursive call 0 } 11 12 check_child() { 13 if (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 14 move(childlist[vactive], up) 15 check_child() // recursive call 16 } 17 18 explore() { 19 if (qcolor(dag[vactive][childlist[vactive]]) is white): 20 write(color_next, color_grey) 21 stack(push) 22 write(save) 23 write(active_neighb) 24 move(childlist[vsave], up) 25 else: 26 write(color_curr, color_black) 27 write(result) 28 move(presult, up) 29 if(pstack == 1): 30 return 31 else: 32 stack(pop) 33 check_child() 34 explore() // recursive call 35 } 36 37 next_start() { 38 if (qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 39 move(pstart, up) 40 next_start() // recursive call 41 }algorithm 4 iterative quicksort 1: initialize an array a to sort and two empty stacks slo and shi",0
" 9: else 10: function run(e, p2, a2)
a description of the inference procedure is given in algorithm 1",0
" thus, we demonstrate that recursion enables provably perfect generalization for different tasks, including addition, topological sort, quicksort, and a variant of bubble sort",0
" 3: 4: function quicksort(a, lo, hi) 5: if lo < hi: then 6: p = partition(a, lo, hi) 7: quicksort(a, lo, p− 1) 8: quicksort(a, p+ 1, hi) 9: 10: function partition(a, lo, hi) 11: pivot = lo 12: for j ∈ [lo, hi− 1] : do 13: if a[j] ≤ a[hi] then 14: swap a[pivot] with a[j] 15: pivot = pivot+ 1 16: swap a[pivot] with a[hi] 17: return pivot
for quicksort, the domain-specific encoder is
fenc(qarray, qstacklo, qstackhi, plo, phi, pstacklo, pstackhi, ppivot, pj , at) =
mlp ([qarray(pj) ≤ qarray(phi), pj == phi, qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1), pstacklo == 1, at(1), at(2), at(3)]),
whereqarray ∈ ru×11 is a scratch-pad that contains u rows, each containing one of 11 values (one of the numbers 0 through 9 or an invalid state)",0
" the core lstm module’s hidden state is preserved over all these add1 calls, and it is difficult to interpret with certainty what happens over longer timesteps without concretely evaluating the lstm with an input of that length",1
 the sequence of primitive operations (move and write operations) for the non-recursive and recursive versions are exactly the same,1
" in the non-recursive trace, there are four functions that can be made recursive—toposort, check child, explore, and next start, and we add a tail recursive call to each of these functions in order to make the recursive trace",1
 we instantiated recursion for the neural programmerinterpreter by changing the training traces,0
 we find that recursion makes it easier for the network to learn the right program and generalize to unknown situations,0
g,0
" there are 3 pointers (presult, pstack, pstart), presult points to the next empty location in qresult,
pstack points to the top of the stack in qstack, and pstart points to the candidate starting node for a connected component",1
" 3: 4: function partition(a, lo, hi) 5: pivot = lo 6: for j ∈ [lo, hi− 1] : do 7: if a[j] ≤ a[hi] then 8: swap a[pivot] with a[j] 9: pivot = pivot+ 1 0: swap a[pivot] with a[hi] 11: return pivot 12: 13: function quicksort(a, lo, hi) 14: while slo and shi are not empty: do 15: pop states off slo and shi, writing them to lo and hi",1
"
quicksort",1
 we instantiated recursion for the neural programmerinterpreter by changing the training traces,0
" in contrast, each call to the recursive add always runs for a fixed number of steps, even on arbitrarily long problems in f , so we can test that it performs correctly on a small, fixed number of step input sequences",1
" because the context of the caller is stored on a stack when it calls another program and the callee starts in a fresh context, this enables recursion simply by allowing a program to call itself",0
"2 at any given time, the npi has access to values pointed to by four pointers in each of the four rows, represented by q(1, i1), q(2, i2), q(3, i3), and q(4, i4)",1
" for brevity, we refer the reader to the appendix for information about the program set and non-recursive and recursive trace-generating functions for quicksort",0
" the assumption of leading 0’s can be easily removed, at the cost of slightly increasing the size of the verification set",0
6,1
" in practice, we can additionally use tail recursion optimization to avoid problems with the call stack growing too deep",1
 indentation indicates the stack is one level deeper than before,0
" to make an analogy to mdps, this procedure is analogous to how value iteration obtains the correct value for each state starting from any initialization",0
" 2: initialize lo and hi to be 1 and n, where n is the length of a",0
 we then construct a verification set of size 73 by ensuring that randomly generated graphs cover the analytically derived v ,0
" the learned recursive program is different from neural programs learned in all previous work in an important aspect: previous approaches do not explicitly incorporate this abstraction, and hence generalize poorly, whereas our learned neural programs incorporate recursion and achieve perfect generalization",1
" qcolor(dag[vactive][childlist[vactive]]) refers to the color of the next child of vactive, used in the evaluation of the condition in the if branch in line 9 of algorithm 2",0
"
base cases and reduction rules for addition",1
" the recursive version of topological sort solves all graph instances we tried, from graphs of size 5 through 70",0
e,0
" thus, any general neural programming architecture supporting such a call structure can be made to support recursion",0
" qcolor(dag[vactive][childlist[vactive]]) refers to the color of the next child of vactive, used in the evaluation of the condition in the if branch in line 9 of algorithm 2",1
e,1
"
as a concrete instantiation, we show in this paper that we can enable recursive neural programs in the npi model, and thus enable perfectly generalizable neural programs for tasks such as sorting where the original, non-recursive npi program fails",1
" at each time step, the core controller can decide to select one of the programs to call with certain arguments",0
"
for topological sort, the domain-specific encoder is
fenc(dag,qcolor, pstack, pstart, vactive, childlist, at)
=mlp ([qcolor(pstart), qcolor(dag[vactive][childlist[vactive]]), pstack == 1, at(1), at(2), at(3)]),
where qcolor ∈ ru×4 is a scratch-pad that contains u rows, each containing one of four colors (white, gray, black, invalid) with one-hot encoding",0
"
the non-recursive trace loops on cycles of add1 and lshift",0
g,0
 the environment and return probability are omitted for readability,0
" however, our concept of recursion for neural programs is general",1
"
move",1
"1; and for bubble sort, appendix a",0
"
training setup",0
" if a white vertex is found, set pstart to point to it; this signifies the start of a traversal of a new connected component",1
", an lstm in npi’s case, but possibly other networks in different cases",0
 add1 is a subprogram that adds the current column (writing the appropriate digit to the output row and carrying a bit to the next column if needed),0
"
algorithm 1 neural programming inference 1: inputs: environment observation e, program p, arguments a, stop threshold α 2: function run(e, p, a) 3: h← 0, r ← 0 4: while r < α do 5: s← fenc(e, a), h← flstm(s, p, h) 6: r ← fend(h), p2 ← fprog(h), a2 ← farg(h) 7: if p is a primitive function then 8: e← fenv(e, p, a)",0
 these pointers are referred to as bubble pointers,0
 we outline how to construct this set,0
" in our experiments, we only present dag’s as inputs and represent the vertices as values ranging from 1, ",0
", the trace corresponding to the problem “109 + 101”)",1
" we implement a quicksort task, in order to demonstrate that recursion helps with learning divide-and-conquer algorithms",1
"
one common strategy to improve generalization is to use curriculum learning, where the model is trained on inputs of gradually increasing complexity",1
"
to perform the verification as described here, it is critical to construct v correctly",0
 add1 is a subprogram that adds the current column (writing the appropriate digit to the output row and carrying a bit to the next column if needed),0
"
training setup",1
g,0
" the arrow in the theorem refers to evaluation, as in big-step semantics",0
"
topological sort",0
 ,0
 we find that recursion makes it easier for the network to learn the right program and generalize to unknown situations,0
"
algorithm 3 recursive quicksort 1: initialize an array a to sort",1
"
the non-recursive trace loops on cycles of add1 and lshift",0
"
the training data for the neural programmer-interpreter consists of full execution traces for the program of interest",0
" we show that by modifying the training procedure, we enable the npi model to learn recursive neural programs",0
 this makes it difficult to reason about what the model will do when given complex inputs,0
" the swap operation has the following arguments:
arg 1 (swap object 1): ppivot
arg 2 (swap object 2): phi, pj
write",1
"
thus far, to evaluate the efficacy of neural models on programming tasks, the only metric that has been used is generalization of expected behavior to inputs of greater complexity (vinyals   (2015), kaiser & sutskever (2015), reed & de freitas (2016), graves   (2016), zaremba   (2016))",0
"
move",1
 indentation indicates the stack is one level deeper than before,0
"
quicksort",0
"
to evaluate a neural network that learns a neural program to accomplish a certain task, one common evaluation metric is how well the learned model m generalizes",1
" for many problems, the base cases and reduction rules usually consist of a finite (often small) number of cases",0
 18: push lo and p− 1 to slo and shi,0
"
an alternative way of representing the environment slice is to expose the values of the absolute vertices to the model; however, this makes it difficult to scale the model to larger graphs, since large vertex values are not seen during training time",0
"
the three environment observations aid with control flow in algorithm 2",0
", pstart or childlist[vactive]) up or down none described belowwrite",0
 figure 2 shows examples of traces for the different versions of bubble sort,1
"
to perform the verification as described here, it is critical to construct v correctly",0
g,0
"
the non-recursive trace loops on cycles of bubble and reset, which logically represents one bubble sweep through the array and reset of the two bubble pointers to the very beginning of the array, respectively",0
" in particular, we construct new training traces which explicitly contain recursive elements and show that with this type of trace, npi easily learns recursive programs",0
" for bubble sort, the domain-specific encoder is
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1), q(1, i2), i3 == length, at(1), at(2), at(3)]),
where the environment q ∈ r1×n×k is a scratch-pad that contains 1 row, to represent the state of the array as sorting proceeds in-place, andn columns",0
 we use the lomuto partition scheme; the logic for the recursive trace is shown in algorithm 3,1
" lshift, used in reset, shifts the pointers left until reaching the start token",1
"1, we hypothesize the non-recursive programs do not generalize well because they have learned spurious dependencies specific to the training set, such as length of the input problems",0
"
the training set for quicksort contains 4 traces, synthesized from arrays of length 5",0
" for many problems, the base cases and reduction rules usually consist of a finite (often small) number of cases",1
" ppivot and pj point to the pivot and j indices of the array, used in the partition function in algorithm 3",0
 a1a0 + bnbn−1 ,0
" given a precise definition of p , it may be possible to automate the generation of v from p in future work",0
" , n , where the dag contains n vertices",1
"
there are 6 pointers (plo, phi, pstacklo, pstackhi, ppivot, pj)",0
 recursion can be implemented differently for different neural programming models,1
"
through careful reasoning about the possible set of environment observations created by all valid inputs, and how each of the operations in the execution trace affects the environment, we can construct v using the procedure described in section 3",0
"program descriptions calls arguments toposort perform topological
sort on graph traverse, next start, write, move
none
traverse traverse graph until stack is empty check child, explore none check child check if a white child exists; if so, set childlist[vactive] to point to it move none explore repeatedly traverse subgraphs until stack is empty stack, check child, write, move none stack interact with stack, either pushing or popping write, move push, pop next start move pstart until reaching a white vertex",1
 npi then outputs the return probability and next program and arguments to execute,0
" in particular, this condition is satisfied by npi, and thus the npi model naturally supports recursion (even though the authors of npi did not consider this aspect explicitly)",0
" the contents ofqresult andqstack are not exposed directly through the domain-specific encoder; rather, we define primitive functions which manipulate these scratch-pads",0
" thus, recursion helps decompose a problem and makes it easier to reason about a program’s behavior for previously unseen situations such as longer inputs",1
" in this architecture, we consider a core controller, e",1
"
in addition, none of the current approaches to neural programming provide a method or even aim to enable provable guarantees about generalization",0
"
we propose and describe our verification procedure",0
", to change the value of phi) none described belowstack",1
 these pointers are referred to as bubble pointers,0
" although the model is exposed to more and more data, it might learn spurious and overly complex representations of the program, as suggested in zaremba   (2016)",0
" in this architecture, we consider a core controller, e",1
" , n , where the dag contains n vertices",0
"
bubble sort",0
" nevertheless, we would like the model to quickly learn to represent the right semantics of the underlying program from a small number of training examples, not an exhaustive number of them",0
" for many problems, the base cases and reduction rules usually consist of a finite (often small) number of cases",0
 we choose to implement a topological sort task for graphs,0
 there are 2 variables (vactive and vsave); vactive holds the active vertex (as in algorithm 2) and vsave holds the value of vactive before executing line 12 of algorithm 2,0
" for brevity, we refer the reader to the appendix for information about the program set and non-recursive and recursive trace-generating functions for quicksort",1
" thus, we demonstrate that recursion enables provably perfect generalization for different tasks, including addition, topological sort, quicksort, and a variant of bubble sort",0
" for non-primitive subprograms, we can update the observation-to-observation mapping currently associated with the subprogram and then apply that mapping to the current set",1
 the degree for any vertex in the dag is variable,1
"
an alternative way of representing the environment slice is to expose the values of the absolute vertices to the model; however, this makes it difficult to scale the model to larger graphs, since large vertex values are not seen during training time",0
" partial recursion only adds a tail recursive call to bubblesort after bubble and reset, similar to the tail recursive call described previously for addition",0
", the trace corresponding to the array [3,2])",0
 table 3 presents results on randomly generated arrays of varying length for the learned non-recursive and recursive programs,0
" the pointer at index i3 represents a counter internal to the environment that is incremented once after each pass of the algorithm (one cycle of bubble and reset); when incremented a number of times equal to the length of the array, the flag i3 == length becomes true and terminates the entire algorithm ",1
" the add reduction rule ensures that each call to add only covers two adjacent columns, and so the lstm only ever runs for a fixed number of steps necessary to process these two columns",0
" nevertheless, we would like the model to quickly learn to represent the right semantics of the underlying program from a small number of training examples, not an exhaustive number of them",1
" at end, pointer ppivot is moved to the pivot compswap loop, move pivot lo, move j lo, swap
none
compswap loop runs the for loop inside the partition function compswap, move none compswap compares a[pivot] ≤ a[j]; if so, perform a swap and increment ppivot swap, move none set pivot lo sets ppivot to lo index none none set j lo sets pj to lo index none none set j null sets pj to −∞ none none stack pushes lo/hi states
onto stacks slo and shi according to argument (described below)
write, move described below
move moves pointer one unit up or down none described below swap swaps elements at given array indices none described below write write a value either to stack (e",0
" as aforementioned, the npi model naturally supports recursion",1
" the space of possible programs is extremely large, and composing a program that performs robustly on the infinite space of possible inputs is difficult—in part because it is impractical to obtain enough training examples to easily disambiguate amongst all possible programs",0
" in particular, given that a recursion is defined by two properties as mentioned before, the base cases and the set of reduction rules, we can prove a recursive neural program generalizes perfectly if we can prove that (1) it performs correctly on the base cases; (2) it learns the reduction rules correctly",0
", qstacklo or qstackhi) or to pointer (e",1
" the space of possible programs is extremely large, and composing a program that performs robustly on the infinite space of possible inputs is difficult—in part because it is impractical to obtain enough training examples to easily disambiguate amongst all possible programs",0
 the sequence of primitive operations (move and write operations) for the non-recursive and recursive versions are exactly the same,1
 the training set for addition contains 200 traces,1
" for problems where the base cases may be extremely large or infinite, such as certain forms of motor control, recursion can still help reduce the problem of generalization to these two aspects and make the generalization problem significantly simpler to handle and reason about",1
" we make the trace recursive by adding a tail recursive call into the trace for the add program after calling add1 and lshift, the original paper uses k = 10, but we found it necessary to augment the range with an end token, in order to terminate properly",0
g,0
" 16: p = partition(a, lo, hi) 17: push p+ 1 and hi to slo and shi",0
g,0
" in contrast, each call to the recursive add always runs for a fixed number of steps, even on arbitrarily long problems in f , so we can test that it performs correctly on a small, fixed number of step input sequences",1
" we made the assumption of equivalent lengths in order to parametrize the input format with respect to length, but this assumption can be removed as well",1
" we implement a quicksort task, in order to demonstrate that recursion helps with learning divide-and-conquer algorithms",0
" if the step output specifies a primitive function call, we need to reason about how it can affect the environment so as to change the observation in the next step input",0
"
the three environment observations aid with control flow in algorithm 2",1
"
we construct input problems by splitting into two cases: one case in which the left column contains a null value and another in which the left column does not contain any null values",0
"
in addition, none of the current approaches to neural programming provide a method or even aim to enable provable guarantees about generalization",1
" 4: function toposort(dag) 5: while there is still a white vertex u: do 6: color[u] = grey 7: vactive = u 8: do 9: if vactive has a white child v then 10: color[v] = grey 11: push vactive onto s 12: vactive = v 13: else 14: color[vactive] = black 15: write vactive to result 16: if s is empty then pass 17: else pop the top vertex off s and set it to vactive 18: while s is not empty
topological sort",0
" partial recursion only adds a tail recursive call to bubblesort after bubble and reset, similar to the tail recursive call described previously for addition",1
"when constructing a neural network for the purpose of learning a program, there are two orthogonal aspects to consider",1
" in contrast, each call to the recursive add always runs for a fixed number of steps, even on arbitrarily long problems in f , so we can test that it performs correctly on a small, fixed number of step input sequences",0
 we use the lomuto partition scheme; the logic for the recursive trace is shown in algorithm 3,1
"program descriptions calls arguments quicksort run the quicksort
routine in place for the array a, for indices from lo to hi non-recursive: partition, stack, write recursive: same as non-recursive version, along with quicksort implicitly: array a to sort, lo, hi
partition runs the partition function",1
" as with the others, we apply the procedure described in section 3",0
"as there is no public implementation of npi, we implemented a version of it in keras that is as faithful to the paper as possible",1
 we leave this as future work to devise a verification procedure more appropriate to this setting,0
 the degree for any vertex in the dag is variable,0
", to change the value of vactive) none described below move move a pointer (e",0
" finally, we implement recursive traces for our own topological sort and quicksort tasks",1
g,0
"
in this paper, we propose to resolve these issues by explicitly incorporating recursion into neural architectures",0
"
recursion drastically reduces the number of configurations we need to consider during the verification phase and makes the proof tractable, because it introduces structure that eliminates infinitely long sequences of step inputs that would otherwise need to be considered",1
"
since npi naturally supports the notion of recursion, a key question is how to enable npi to learn recursive programs",1
" the 4 environment observations aid with control flow; qstacklo(pstacklo− 1) < qstackhi(pstackhi− 1) implements the lo < hi comparison in line 5 of algorithm 3, pstacklo == 1 checks if the stacks are empty in line 18 of algorithm 4, and the other observations (all involving ppivot or pj) deal with logic in the partition function",0
" the architecture usually possesses some form of memory, which could be internal (such as the hidden state of a recurrent neural network) or external (such as a discrete “scratch pad” or a memory block with differentiable access)",0
 table 4 also demonstrates that the (verified) recursive program generalizes perfectly,0
" on the other hand, the recursive programs have learned the true program semantics",1
"
algorithm 2 depth first search topological sort 1: color all vertices white",1
" the contents ofqresult andqstack are not exposed directly through the domain-specific encoder; rather, we define primitive functions which manipulate these scratch-pads",1
" any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of national science foundation and darpa",0
 we created a program set that reflects the semantics of algorithm 2,1
 the original version of the bubblesort implementation exposes the values within the array,0
e,0
" , a1a0 + b1b0} are added properly",0
" as a consequence, our learned neural programs empirically achieve perfect generalization from a very small number of training examples",1
"
note that v should also contain the necessary reductions, which corresponds to making the recursive calls at the correct time, as indicated by p ",0
this material is in part based upon work supported by the national science foundation under grant no,1
"we show that if we incorporate recursion, the learned npi programs can achieve provably perfect generalization for different tasks",0
" this agrees with the generalization of non-recursive addition in reed & de freitas (2016), where they reported generalization up to 3000 digits",0
 table 3 presents results on randomly generated arrays of varying length for the learned non-recursive and recursive programs,0
"
grade-school addition",0
" thus, any general neural programming architecture supporting such a call structure can be made to support recursion",1
" the pointers at index i1 and i2 are used to compare the pair of numbers considered during the bubble sweep, swapping them if the number at i1 is greater than that in i2",1
" a topological sort is a linear ordering of vertices such that for every directed edge (u, v) from u to v, u comes before v in the ordering",1
" for problems where the base cases may be extremely large or infinite, such as certain forms of motor control, recursion can still help reduce the problem of generalization to these two aspects and make the generalization problem significantly simpler to handle and reason about",1
" bstep, used in bubble, compares pairs of numbers, continuously moving the bubble pointers once to the right each time until reaching the end of the array",1
" if no white vertex is found, the entire execution is terminated move none write write a value either to environment (e",1
" an example of part of an addition task trace, written in shorthand, is given in figure 1",1
" the theorem states that for the same sequence of step inputs, the model produces the exact same step output as the target program it aims to learn",1
" we then construct problem configurations that span all possible valid environment states (for instance, in order to force the carry bit in a column to be 1, one can add the sum “1+9” in the column to the right)",0
" m is considered to have perfect generalization if m can give the right answer for any input, such as inputs of arbitrary length",0
" in particular, given that a recursion is defined by two properties as mentioned before, the base cases and the set of reduction rules, we can prove a recursive neural program generalizes perfectly if we can prove that (1) it performs correctly on the base cases; (2) it learns the reduction rules correctly",0
" also the dag can have potentially more than one connected component, meaning it is necessary to transition between these components appropriately",0
 the core controller acts as a dispatcher for the programs,1
 this is due to the properties of the training set – we trained on 2 traces synthesized from arrays of length 7 and 1 trace synthesized from an array of length 6,1
" finally, we implement recursive traces for our own topological sort and quicksort tasks",1
"
algorithm 2 depth first search topological sort 1: color all vertices white",0
"
note that v should also contain the necessary reductions, which corresponds to making the recursive calls at the correct time, as indicated by p ",1
" et+1 ∼ fenv(et, pt, at)",0
"
bubble sort",1
 the program terminates when seeing no numbers in the current column,1
3,0
 add1 is a subprogram that adds the current column (writing the appropriate digit to the output row and carrying a bit to the next column if needed),0
" the learned recursive program is different from neural programs learned in all previous work in an important aspect: previous approaches do not explicitly incorporate this abstraction, and hence generalize poorly, whereas our learned neural programs incorporate recursion and achieve perfect generalization",1
 this is due to the properties of the training set – we trained on 2 traces synthesized from arrays of length 7 and 1 trace synthesized from an array of length 6,0
" the architecture usually possesses some form of memory, which could be internal (such as the hidden state of a recurrent neural network) or external (such as a discrete “scratch pad” or a memory block with differentiable access)",0
 these pointers are referred to as bubble pointers,0
 the maximum problem length in this training set is 3 (e,0
" the general notion of recursion has been an important concept in many domains, including mathematics and computer science",0
" the only model, to our knowledge, that does not train on input-output pairs is the neural programmer-interpreter (reed & de freitas, 2016), which trains on synthetic execution traces",0
" the move operation has the following arguments:
arg 1 (pointer): presult, pstack, pstart, childlist[vactive], childlist[vsave]
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 5 values",1
" we hypothesize that the reason for this is that the neural network learns to spuriously depend on specific characteristics of the training examples that are irrelevant to the true program semantics, such as length of the training inputs, and thus fails to generalize to more complex inputs",0
 table 4 also demonstrates that the (verified) recursive program generalizes perfectly,1
"
the training set for quicksort contains 4 traces, synthesized from arrays of length 5",0
 we adapt machinery from the original paper slightly to fit our needs,1
" formally, a function exhibits recursive behavior when it possesses two properties: (1) base cases— terminating scenarios that do not use recursion to produce answers; (2) a set of rules that reduces all other problems toward the base cases",1
 this is due to the properties of the training set – we trained on 2 traces synthesized from arrays of length 7 and 1 trace synthesized from an array of length 6,0
" for problems where the base cases may be extremely large or infinite, such as certain forms of motor control, recursion can still help reduce the problem of generalization to these two aspects and make the generalization problem significantly simpler to handle and reason about",1
 the memory updates of these neural programs are so complex and interdependent that it is difficult to reason about the behaviors of the learned neural program under previously unseen situations (such as problems with longer inputs),0
"
we describe the base cases, reduction rules, and the verification set for each task in appendix a",1
" the move operation has the following arguments:
arg 1 (pointer): presult, pstack, pstart, childlist[vactive], childlist[vsave]
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 5 values",0
 npi then outputs the return probability and next program and arguments to execute,0
3,0
"
algorithm 1 neural programming inference 1: inputs: environment observation e, program p, arguments a, stop threshold α 2: function run(e, p, a) 3: h← 0, r ← 0 4: while r < α do 5: s← fenc(e, a), h← flstm(s, p, h) 6: r ← fend(h), p2 ← fprog(h), a2 ← farg(h) 7: if p is a primitive function then 8: e← fenv(e, p, a)",0
" , n , where the dag contains n vertices",1
"we describe how models trained with recursive traces can be proven to generalize, by using the verification procedure described in section 3",0
" full recursion, in addition to making the aforementioned tail recursive call, adds two additional recursive calls; bstep and lshift are made tail recursive",0
" if the step output specifies a primitive function call, we need to reason about how it can affect the environment so as to change the observation in the next step input",1
"
topological sort",1
we emphasize the overall goal of this work is to enable the learning of a recursive program,1
" given a precise definition of p , it may be possible to automate the generation of v from p in future work",0
" we make the trace recursive by adding a tail recursive call into the trace for the add program after calling add1 and lshift, the original paper uses k = 10, but we found it necessary to augment the range with an end token, in order to terminate properly",0
"
in what follows, we describe the way in which we constructed npi training traces so as to make them contain recursive elements and thus enable npi to learn recursive programs",0
" this functionality the original npi paper decodes to a program key embedding kt ∈ rk and then computes a program embedding pt+1, which we also did in our implementation, but we omit this for brevity",1
"we describe how models trained with recursive traces can be proven to generalize, by using the verification procedure described in section 3",1
" twc-1409915, darpa under grant no",0
 recursion can be implemented differently for different neural programming models,1
" our implementation uses two stacks qstacklo and
qstackhi, each in ru , that store the arguments to the recursive quicksort calls in algorithm 3; before each recursive call, the appropriate arguments are popped off the stack and written to plo and phi",1
"
we note that for tasks with very large input domains, such as ones involving mnist digits or speech samples, the state space of base cases and reduction rules could be prohibitively large, possibly infinite",1
"
is critical for implementing recursion, since it permits us to restrict our attention to the currently relevant recursive call, ignoring irrelevant details about other contexts",0
"
the training data for the neural programmer-interpreter consists of full execution traces for the program of interest",1
" recursion is an important concept in programming languages and a critical tool to
reduce the complexity of programs",1
"
inside bubble and reset, there are two operations that can be made recursive",0
" k is set to 11, to denote the range of possible numbers (0 through 9), along with the start/end token (represented with the same encoding) which is observed when a pointer reaches beyond the bounds of the input",0
"
the dag is represented as an adjacency list where dag[i][j] refers to the j-th child of vertex i",0
" for brevity, we refer the reader to the appendix for information about the program set and non-recursive and recursive trace-generating functions for quicksort",0
" for many problems, the base cases and reduction rules usually consist of a finite (often small) number of cases",0
as mentioned in section 2,0
"
arg 2 (object to copy): env stack lo peek, env stack hi peek, phi, plo, ppivot − 1, ppivot + 1, reset
env stack lo peek and env stack hi peek represent qstacklo(pstacklo − 1) and qstackhi(pstackhi − 1), respectively",0
g,1
" empirically, existing models suffer from a common limitation—generalization becomes poor beyond a threshold level of complexity",0
" furthermore, given a verification input set that covers all base cases and reduction rules, we can formally prove that the learned neural programs achieve perfect generalization after verifying its behavior on the verification input set",0
"
grade-school addition",0
" recall that this verification procedure cannot be performed for the non-recursive versions, since the propagation of the hidden state in the core lstm module makes reasoning difficult and so we would need to check an unbounded number of examples",0
", the verification set",0
" a single element of an execution trace consists of a step input-step output pair, which can be synthesized from algorithm 1: this corresponds to, for a given time-step, the step input tuple (e, p, a) and step output tuple (r, p2, a2)",1
" also, compared to the bubble pointers in bubble sort, the pointers that perform the comparison for quicksort (the compswap function) are usually not adjacent to each other, making quicksort less local than bubble sort",1
" for each length, we test each program on 30 randomly generated problems",0
" the write operation has the following arguments:
arg 1 (main action): color curr, color next, active start, active neighb, active stack, save, stack push, stack pop, result
color curr colors vactive, color next colors vertex dag[vactive][childlist[vactive]], active start writes pstart to vactive, active neighb writes dag[vactive][childlist[vactive]] to vactive, active stack writes qstack(pstack) to vactive, save writes vactive to vsave, stack push pushes vactive to the top of the stack, stack pop writes a null value to the top of the stack, and result writes vactive to qresult(presult)",0
we now report on generalization for the varying tasks,1
"
base cases and reduction rules for topological sort",0
" consequently, there is no concept of length relevant to the problem, which has traditionally been an important focus of length-based curriculum learning",0
"1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 while (pstacklo 6= 1): 6 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 7 stack(stack_pop) 8 else: 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 stack(stack_pop) 12 partition() 13 stack(stack_push_call2) 14 stack(stack_push_call1) 15 } 16 17 partition() { 18 set_pivot_lo() 19 set_j_lo() 20 compswap_loop() 21 swap(ppivot, phi) 22 set_j_null() 23 } 24 25 compswap_loop() { 26 while (pj 6= phi): 27 compswap() 28 move(pj, up) 29 } 30 31 compswap() { 32 if (a[pj ] ≤ a[phi]): 33 swap(ppivot, pj) 34 move(ppivot, up) 35 } 36 37 stack(op) { 38 if (op == stack_push_call1): 39 write(env_stack_lo, plo) 40 write(env_stack_hi, ppivot − 1) 41 move(pstacklo, up) 42 move(pstackhi, up) 43 44 if (op == stack_push_call2): 45 write(env_stack_lo, ppivot + 1) 46 write(env_stack_hi, phi) 47 move(pstacklo, up) 48 move(pstackhi, up) 49 50 if (op == stack_pop): 51 write(env_stack_lo, reset) 52 write(env_stack_hi, reset) 53 move(pstacklo, down) 54 move(pstackhi, down) 55 }altered recursive functions 1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 6 partition() 7 stack(stack_push_call2) 8 stack(stack_push_call1) 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 quicksort() // recursive call 12 stack(stack_pop) 13 write(phi, env_stack_hi_peek) 14 write(plo, env_stack_lo_peek) 15 quicksort() // recursive call 16 stack(stack_pop) 17 } 18 19 compswap_loop() { 20 if (pj 6= phi): 21 compswap() 22 move(pj, up) 23 compswap_loop() // recursive call 24 }in this section, we describe the space of base cases and reduction rules that must be covered for each of the four sample tasks, in order to create the verification set",0
 18: push lo and p− 1 to slo and shi,1
" there are 3 pointers (presult, pstack, pstart), presult points to the next empty location in qresult,
pstack points to the top of the stack in qstack, and pstart points to the candidate starting node for a connected component",0
" m , as described in algorithm 1, processes the sequence of step inputs by using an lstm",0
" in our experiments, we only present dag’s as inputs and represent the vertices as values ranging from 1, ",0
" the contents ofqresult andqstack are not exposed directly through the domain-specific encoder; rather, we define primitive functions which manipulate these scratch-pads",0
" for the recursive formulation of addition, we analytically construct the set of input problems that cover all base cases and reduction rules",1
" on the other hand, the non-recursive version has low accuracy, beginning from size 5, and fails completely for graphs of size 8 and beyond",0
" 16: p = partition(a, lo, hi) 17: push p+ 1 and hi to slo and shi",0
" training on the full recursive trace leads to perfect generalization, as shown in section 4",1
g,0
 ,1
"
note that v should also contain the necessary reductions, which corresponds to making the recursive calls at the correct time, as indicated by p ",0
" the recursive version of topological sort solves all graph instances we tried, from graphs of size 5 through 70",1
" in practice, we can additionally use tail recursion optimization to avoid problems with the call stack growing too deep",1
"
algorithm 3 recursive quicksort 1: initialize an array a to sort",0
g,1
" even though the earlier version also generalized just as well in practice, relatively small differences in the formulation of the traces and the environment observations can drastically change the difficulty of verification",0
", the trace corresponding to the array [3,2])",0
"
the non-recursive trace loops on cycles of bubble and reset, which logically represents one bubble sweep through the array and reset of the two bubble pointers to the very beginning of the array, respectively",0
 errors arise due to undesirable and uninterpretable dependencies and associations the architecture learns to store in some high-dimensional hidden state,1
" for brevity, we refer the reader to the appendix for further details on the program set and non-recursive and recursive trace-generating functions used for topological sort",1
" as an application, we incorporate recursion into the neural programmer-interpreter architecture and consider four sample tasks: grade-school addition, bubble sort, topological sort, and quicksort",1
" it is straightforward to deal with all other operations, which do not induce partial environment states",0
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,0
 we created a program set that reflects the semantics of algorithm 2,0
 npi then outputs the return probability and next program and arguments to execute,1
we emphasize the overall goal of this work is to enable the learning of a recursive program,0
" in this architecture, we consider a core controller, e",1
" the general notion of recursion has been an important concept in many domains, including mathematics and computer science",1
"
it is sufficient to construct problems where every transition between two adjacent columns is covered",0
 ,0
" k is set to 11, to represent the range of 10 possible digits, along with a token representing the end of input",0
"in this paper, we propose that the key abstraction of recursion is necessary for neural programs to generalize",0
" numerous models have been proposed for learning programs; to name a few, this includes the differentiable neural computer (graves  , 2016), neural turing machine (graves  , 2014), neural gpu (kaiser & sutskever, 2015), neural programmer (neelakantan  , 2015), pointer network (vinyals  , 2015), hierarchical attentive memory (andrychowicz & kurach, 2016), and neural random access machine (kurach  , 2016)",1
"1, we hypothesize the non-recursive programs do not generalize well because they have learned spurious dependencies specific to the training set, such as length of the input problems",1
" the write operation has the following arguments:
arg 1 (main action): color curr, color next, active start, active neighb, active stack, save, stack push, stack pop, result
color curr colors vactive, color next colors vertex dag[vactive][childlist[vactive]], active start writes pstart to vactive, active neighb writes dag[vactive][childlist[vactive]] to vactive, active stack writes qstack(pstack) to vactive, save writes vactive to vsave, stack push pushes vactive to the top of the stack, stack pop writes a null value to the top of the stack, and result writes vactive to qresult(presult)",0
 the environment and return probability are omitted for readability,0
this material is in part based upon work supported by the national science foundation under grant no,1
"
arg 2 (increment or decrement): up, down1 // top level topological sort call 2 toposort() { 3 while (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 } 0 11 traverse() { 12 check_child() 13 explore() 14 } 15 16 check_child() { 17 while (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 18 move(childlist[vactive], up) 19 } 20 21 explore() { 22 do 23 if (qcolor(dag[vactive][childlist[vactive]]) is white): 24 write(color_next, color_grey) 25 stack(push) 26 write(save) 27 write(active_neighb) 28 move(childlist[vsave], up) 29 else: 30 write(color_curr, color_black) 31 write(result) 32 move(presult, up) 33 if(pstack == 1): 34 break 35 else: 36 stack(pop) 37 check_child() 38 while (true) 39 } 40 41 stack(op) { 42 if (op == push): 43 write(stack_push) 44 move(pstack, up) 45 46 if (op == pop): 47 write(active_stack) 48 write(stack_pop) 49 move(pstack, down) 50 } 51 52 next_start() { 53 while(qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 54 move(pstart, up) 55 }altered recursive functions  // top level topological sort call 2 toposort() { 3 if (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 toposort() // recursive call 0 } 11 12 check_child() { 13 if (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 14 move(childlist[vactive], up) 15 check_child() // recursive call 16 } 17 18 explore() { 19 if (qcolor(dag[vactive][childlist[vactive]]) is white): 20 write(color_next, color_grey) 21 stack(push) 22 write(save) 23 write(active_neighb) 24 move(childlist[vsave], up) 25 else: 26 write(color_curr, color_black) 27 write(result) 28 move(presult, up) 29 if(pstack == 1): 30 return 31 else: 32 stack(pop) 33 check_child() 34 explore() // recursive call 35 } 36 37 next_start() { 38 if (qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 39 move(pstart, up) 40 next_start() // recursive call 41 }algorithm 4 iterative quicksort 1: initialize an array a to sort and two empty stacks slo and shi",0
" for brevity, we refer the reader to the appendix for information about the program set and non-recursive and recursive trace-generating functions for quicksort",0
 this verification phase only needs to be performed once after training,0
 the memory updates of these neural programs are so complex and interdependent that it is difficult to reason about the behaviors of the learned neural program under previously unseen situations (such as problems with longer inputs),0
" the single-digit multiplication task in zaremba   (2016), the bubble sort task in reed & de freitas (2016), and the graph tasks in graves   (2016))",0
" a single element of an execution trace consists of a step input-step output pair, which can be synthesized from algorithm 1: this corresponds to, for a given time-step, the step input tuple (e, p, a) and step output tuple (r, p2, a2)",0
"
we refer the reader to the appendix for the non-recursive trace generating functions",1
" the recursive version can generalize perfectly to long problems constructed from these components (such as the sum “822+233”, where “8+2” and “2+3” are in the training set), but the non-recursive version fails to sum these long problems properly",1
" each time a subprogram is called, the stack depth increases",0
"
we construct input problems by splitting into two cases: one case in which the left column contains a null value and another in which the left column does not contain any null values",0
" the recursive version of topological sort solves all graph instances we tried, from graphs of size 5 through 70",0
 this paper is the first (to our knowledge) to investigate the important problem of provable generalization properties of neural programs,0
" formally, a function exhibits recursive behavior when it possesses two properties: (1) base cases— terminating scenarios that do not use recursion to produce answers; (2) a set of rules that reduces all other problems toward the base cases",0
" the move operation has the following arguments:
arg 1 (pointer): presult, pstack, pstart, childlist[vactive], childlist[vsave]
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 5 values",0
" our results show that for all learned, to-be-verified neural programs, they all produced the same traces as those produced by p on the verification set",0
" this is highly undesirable, since being able to provide the correct answer in all possible settings is one of the most important aspects of any learned neural program",0
" k is set to 11, to represent the range of 10 possible digits, along with a token representing the end of input",0
" if the step output specifies a primitive function call, we need to reason about how it can affect the environment so as to change the observation in the next step input",0
" the architecture usually possesses some form of memory, which could be internal (such as the hidden state of a recurrent neural network) or external (such as a discrete “scratch pad” or a memory block with differentiable access)",0
"
to perform the verification as described here, it is critical to construct v correctly",0
 reset represents a −∞ value,1
 ,1
 ,0
" if yes, then this indicates that the learned neural program achieves provably perfect generalization",1
" the corresponding edge list is [(1, 2), (1, 5), (2, 4), (2, 5), (3, 5)]",0
" also, compared to the bubble pointers in bubble sort, the pointers that perform the comparison for quicksort (the compswap function) are usually not adjacent to each other, making quicksort less local than bubble sort",0
 our experiments use a small number of training examples,0
we emphasize that the notion of a neural recursive program has not been presented in the literature before: this is our main contribution,1
"
in this general neural programming architecture, we show it is easy to support recursion",0
 our experiments use a small number of training examples,1
 our experiments use a small number of training examples,0
"
we experiment with two levels of recursion—partial and full",0
we emphasize that the notion of a neural recursive program has not been presented in the literature before: this is our main contribution,1
" this makes it so that we only need to consider the vertices in the subgraph that are currently relevant for computing the sort, allowing simpler reasoning about behavior for large graphs",1
e,0
" fa8750-15-2-0104, and berkeley deep drive",1
"
base cases and reduction rules for quicksort",0
"
in addition, none of the current approaches to neural programming provide a method or even aim to enable provable guarantees about generalization",0
" in particular, given that a recursion is defined by two properties as mentioned before, the base cases and the set of reduction rules, we can prove a recursive neural program generalizes perfectly if we can prove that (1) it performs correctly on the base cases; (2) it learns the reduction rules correctly",0
" recursion is an important concept in programming languages and a critical tool to
reduce the complexity of programs",0
 we instantiated recursion for the neural programmerinterpreter by changing the training traces,1
" for each graph size, we test the learned programs on 30 randomly generated dags",0
 we then construct a verification set of size 73 by ensuring that randomly generated graphs cover the analytically derived v ,0
"
is critical for implementing recursion, since it permits us to restrict our attention to the currently relevant recursive call, ignoring irrelevant details about other contexts",0
"
arg 2 (increment or decrement): up, down
swap",1
" the npi model has three learnable components: a task-agnostic core, a program-key embedding, and domain-specific encoders that allow the npi to operate in diverse environments",0
" in particular, in the explore function, adding a tail recursive call resets and stores the hidden states associated with vertices in a stack-like fashion",1
" as with the others, we apply the procedure described in section 3",0
 this modification also enables us to sort arrays containing arbitrary comparable elements,0
"program descriptions calls arguments toposort perform topological
sort on graph traverse, next start, write, move
none
traverse traverse graph until stack is empty check child, explore none check child check if a white child exists; if so, set childlist[vactive] to point to it move none explore repeatedly traverse subgraphs until stack is empty stack, check child, write, move none stack interact with stack, either pushing or popping write, move push, pop next start move pstart until reaching a white vertex",0
"we train using the adam optimizer and use a 2-layer lstm and task-specific state encoders for the external environments, as described in reed & de freitas (2016)",0
" twc-1409915, darpa under grant no",1
"
by nature, recursion reduces the complexity of a problem to simpler instances",0
" it is straightforward to deal with all other operations, which do not induce partial environment states",0
" if the step output specifies a primitive function call, we need to reason about how it can affect the environment so as to change the observation in the next step input",0
" however, the authors of npi did not consider explicitly the notion of recursion and as a consequence, did not learn recursive programs",1
" thus, we demonstrate that recursion enables provably perfect generalization for different tasks, including addition, topological sort, quicksort, and a variant of bubble sort",0
" formally, a function exhibits recursive behavior when it possesses two properties: (1) base cases— terminating scenarios that do not use recursion to produce answers; (2) a set of rules that reduces all other problems toward the base cases",0
" table 1 presents results on randomly generated arrays of varying length for the learned non-recursive, partially recursive, and full recursive programs",0
 pstacklo and pstackhi point to the top (empty) positions in qstacklo and qstackhi,0
" ppivot and pj point to the pivot and j indices of the array, used in the partition function in algorithm 3",1
"when constructing a neural network for the purpose of learning a program, there are two orthogonal aspects to consider",1
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,0
 indentation indicates the stack is one level deeper than before,0
"
in this paper, we propose that recursion is an important concept for neural programs as well",0
 recursion can be implemented differently for different neural programming models,0
" consequently, it is infeasible to construct a verification set that covers all cases, and the verification procedure we have described is inadequate",0
"
the non-recursive trace loops on cycles of add1 and lshift",1
" nevertheless, we would like the model to quickly learn to represent the right semantics of the underlying program from a small number of training examples, not an exhaustive number of them",1
"
in this general neural programming architecture, we show it is easy to support recursion",0
", an array for quicksort or a dag for topological sort)",0
" this functionality the original npi paper decodes to a program key embedding kt ∈ rk and then computes a program embedding pt+1, which we also did in our implementation, but we omit this for brevity",0
"
note that the recursion for quicksort is not purely tail recursive and therefore represents a more complex kind of recursion that is harder to learn than in the previous tasks",0
" we also hope to decrease supervision, for example by training with only partial or non-recursive traces, and to develop novel neural programming architectures integrated directly with a notion of recursion",0
"
we note that for tasks with very large input domains, such as ones involving mnist digits or speech samples, the state space of base cases and reduction rules could be prohibitively large, possibly infinite",0
"
to construct this set, by using the reference implementation of each subprogram, we construct a mapping between two sets of environment observations: the first set consists of all observations that can occur at the beginning of a particular subprogram’s invocation, and the second set contains the observations at the end of that subprogram",0
" training on the full recursive trace leads to perfect generalization, as shown in section 4",1
 ,0
g,0
"
inside bubble and reset, there are two operations that can be made recursive",0
g,0
" using this modification, we constructed a verification set consisting of one array of size 10",0
", an array for quicksort or a dag for topological sort)",0
6,1
" in this architecture, we consider a core controller, e",0
" however, to be sure that the generated v is complete (covers all the cases needed), we need to check all pairs of observations seen in adjacent step inputs (in particular, those before and after a primitive function call), in a similar way as if we were constructing v from scratch",0
" we made the assumption of equivalent lengths in order to parametrize the input format with respect to length, but this assumption can be removed as well",0
6,0
" we further have qresult ∈ nu , a scratch-pad which contains the sorted list of vertices at the end of execution, and qstack ∈ nu , which serves the role of the stack s in algorithm 2",1
" for brevity, we refer the reader to the appendix for information about the program set and non-recursive and recursive trace-generating functions for quicksort",1
g,0
" we call this set of inputs the verification set, sv ",0
" the verification set can be very small, as we found a 10-element array ([8,2,1,2,0,8,5,8,3,7]) is sufficient to cover all of v ",1
 ,0
" the core lstm module’s hidden state is preserved over all these add1 calls, and it is difficult to interpret with certainty what happens over longer timesteps without concretely evaluating the lstm with an input of that length",0
 this makes it difficult to reason about what the model will do when given complex inputs,1
" because the context of the caller is stored on a stack when it calls another program and the callee starts in a fresh context, this enables recursion simply by allowing a program to call itself",0
" at each time step, the core controller can decide to select one of the programs to call with certain arguments",1
"
to construct this set, by using the reference implementation of each subprogram, we construct a mapping between two sets of environment observations: the first set consists of all observations that can occur at the beginning of a particular subprogram’s invocation, and the second set contains the observations at the end of that subprogram",0
" finally, we implement recursive traces for our own topological sort and quicksort tasks",0
" recursion is an important concept in programming languages and a critical tool to
reduce the complexity of programs",0
", pstart or childlist[vactive]) up or down none described belowwrite",0
" thus, any general neural programming architecture supporting such a call structure can be made to support recursion",1
" this guarantees that the step input sequences considered during verification contain all step input sequences which arise during execution of an unseen problem in f , leading to generalization to any problem in f ",0
g,0
" 3: 4: function quicksort(a, lo, hi) 5: if lo < hi: then 6: p = partition(a, lo, hi) 7: quicksort(a, lo, p− 1) 8: quicksort(a, p+ 1, hi) 9: 10: function partition(a, lo, hi) 11: pivot = lo 12: for j ∈ [lo, hi− 1] : do 13: if a[j] ≤ a[hi] then 14: swap a[pivot] with a[j] 15: pivot = pivot+ 1 16: swap a[pivot] with a[hi] 17: return pivot
for quicksort, the domain-specific encoder is
fenc(qarray, qstacklo, qstackhi, plo, phi, pstacklo, pstackhi, ppivot, pj , at) =
mlp ([qarray(pj) ≤ qarray(phi), pj == phi, qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1), pstacklo == 1, at(1), at(2), at(3)]),
whereqarray ∈ ru×11 is a scratch-pad that contains u rows, each containing one of 11 values (one of the numbers 0 through 9 or an invalid state)",1
"
we propose and describe our verification procedure",0
" a single element of an execution trace consists of a step input-step output pair, which can be synthesized from algorithm 1: this corresponds to, for a given time-step, the step input tuple (e, p, a) and step output tuple (r, p2, a2)",0
"
base cases and reduction rules for topological sort",0
" for brevity, we refer the reader to the appendix for further details on the program set and non-recursive and recursive trace-generating functions used for topological sort",0
 add1 is a subprogram that adds the current column (writing the appropriate digit to the output row and carrying a bit to the next column if needed),0
" numerous models have been proposed for learning programs; to name a few, this includes the differentiable neural computer (graves  , 2016), neural turing machine (graves  , 2014), neural gpu (kaiser & sutskever, 2015), neural programmer (neelakantan  , 2015), pointer network (vinyals  , 2015), hierarchical attentive memory (andrychowicz & kurach, 2016), and neural random access machine (kurach  , 2016)",0
" as before, the non-recursive learned program lacks a provable guarantee of generalization, whereas we show later that the recursive learned program has one",1
" twc-1409915, darpa under grant no",1
" given a precise definition of p , it may be possible to automate the generation of v from p in future work",0
" however, note that there is no provable guarantee that the non-recursive learned program will generalize to all inputs, whereas we show later that the recursive learned program has a provable guarantee of perfect generalization",0
" in future work, we seek to enable more tasks with recursive structure",1
" in this pair, the step input runs a subprogram add1 that has no arguments, and the step output contains a program write that has arguments of out and 1",1
"
we would also like to point out that in this paper, we provide as an example one way to train a recursive neural program, by providing a certain training execution trace to the npi model",0
" provably perfect generalization implies the model will behave correctly, given any valid input",0
"
note that the training set can often be considerably smaller than the verification set, and despite this, the learned model can still pass the entire verification set",1
 almost all architectures train on program input/output pairs,0
" the space of possible programs is extremely large, and composing a program that performs robustly on the infinite space of possible inputs is difficult—in part because it is impractical to obtain enough training examples to easily disambiguate amongst all possible programs",1
" for each task, given the verification set, we check the traces and results of the learned, to-be-verified neural program (described in section 4",0
 the core controller acts as a dispatcher for the programs,0
" full recursion, in addition to making the aforementioned tail recursive call, adds two additional recursive calls; bstep and lshift are made tail recursive",0
" qcolor(dag[vactive][childlist[vactive]]) refers to the color of the next child of vactive, used in the evaluation of the condition in the if branch in line 9 of algorithm 2",1
" formally, the npi is represented by the following set of equations:
st = fenc(et, at)
ht = flstm(st, pt, ht−1)
rt = fend(ht), pt+1 = fprog(ht), at+1 = farg(ht)
t is a subscript denoting the time-step; fenc is a domain-specific encoder (to be described later) that takes in the environment slice et and arguments at; flstm represents the core module, which takes in the state st generated by fenc, a program embedding pt ∈ rp , and hidden lstm state ht; fend decodes the return probability rt; fprog decodes a program key embedding pt+1;1 and farg decodes arguments at+1",0
"
we would also like to point out that in this paper, we provide as an example one way to train a recursive neural program, by providing a certain training execution trace to the npi model",1
" even though the earlier version also generalized just as well in practice, relatively small differences in the formulation of the traces and the environment observations can drastically change the difficulty of verification",1
g,1
" in this version of curriculum learning, even though the inputs are gradually becoming more complex, the semantics of the program is succinct and does not change",0
"
base cases and reduction rules for addition",1
" if it is too large, then the semantics of p might not be well-defined on some elements in v , or the spurious step input sequences may not be reachable from any valid problem input (e",0
 reset represents a −∞ value,1
g,1
" 3: 4: function partition(a, lo, hi) 5: pivot = lo 6: for j ∈ [lo, hi− 1] : do 7: if a[j] ≤ a[hi] then 8: swap a[pivot] with a[j] 9: pivot = pivot+ 1 0: swap a[pivot] with a[hi] 11: return pivot 12: 13: function quicksort(a, lo, hi) 14: while slo and shi are not empty: do 15: pop states off slo and shi, writing them to lo and hi",1
" k is set to 11, to denote the range of possible numbers (0 through 9), along with the start/end token (represented with the same encoding) which is observed when a pointer reaches beyond the bounds of the input",0
" 9: else 10: function run(e, p2, a2)
a description of the inference procedure is given in algorithm 1",0
"
base cases and reduction rules for bubble sort",0
" the npi model has three learnable components: a task-agnostic core, a program-key embedding, and domain-specific encoders that allow the npi to operate in diverse environments",0
 this algorithm is a variant of depth first search,0
"
algorithm 2 shows the topological sort task of interest",1
" for many problems, the base cases and reduction rules usually consist of a finite (often small) number of cases",1
" qcolor(pstart) contains the color of the current start vertex, used in the evaluation of the condition in the while loop in line 5 of algorithm 2",0
 this algorithm is a variant of depth first search,0
"
note that the training set can often be considerably smaller than the verification set, and despite this, the learned model can still pass the entire verification set",1
g,1
"
for addition, we analytically determine the verification set",1
"
arg 2 (increment or decrement): up, down
swap",1
 ,0
" this makes it so that we only need to consider the vertices in the subgraph that are currently relevant for computing the sort, allowing simpler reasoning about behavior for large graphs",0
" table 1 presents results on randomly generated arrays of varying length for the learned non-recursive, partially recursive, and full recursive programs",1
" the core module of the npi is an lstm controller that takes as input a slice of the current external environment, via a set of pointers, and a program and arguments to execute",1
 the environment and return probability are omitted for readability,1
" for tasks other than addition, it is difficult to analytically determine the verification set, so instead, we randomly generate input candidates until they completely cover the base cases and reduction rules",0
 the core controller acts as a dispatcher for the programs,0
g,0
"
quicksort",0
"
in this paper, we propose that recursion is an important concept for neural programs as well",1
" empirically, we observe that the learned recursive programs solve all valid inputs with 100% accuracy after training on a very small number of examples, out-performing previous generalization results",1
"1, we hypothesize the non-recursive programs do not generalize well because they have learned spurious dependencies specific to the training set, such as length of the input problems",1
" in future work, we would like to decrease supervision and construct models that are capable of coming up with recursive abstractions themselves",0
" k is set to 11, to denote the range of possible numbers (0 through 9), along with the start/end token (represented with the same encoding) which is observed when a pointer reaches beyond the bounds of the input",1
 u varies with the number of vertices in the graph,1
 ,1
g,1
" we note that an earlier version of quicksort we tried lacked primitive operations to directly move a pointer to another, and therefore needed more functions and observations",0
" at end, pointer ppivot is moved to the pivot compswap loop, move pivot lo, move j lo, swap
none
compswap loop runs the for loop inside the partition function compswap, move none compswap compares a[pivot] ≤ a[j]; if so, perform a swap and increment ppivot swap, move none set pivot lo sets ppivot to lo index none none set j lo sets pj to lo index none none set j null sets pj to −∞ none none stack pushes lo/hi states
onto stacks slo and shi according to argument (described below)
write, move described below
move moves pointer one unit up or down none described below swap swaps elements at given array indices none described below write write a value either to stack (e",0
"
arg 2 (auxiliary variable): color grey, color black
color grey and color black color the given vertex grey and black, respectively",0
" in particular, given that a recursion is defined by two properties as mentioned before, the base cases and the set of reduction rules, we can prove a recursive neural program generalizes perfectly if we can prove that (1) it performs correctly on the base cases; (2) it learns the reduction rules correctly",1
 our experiments use a small number of training examples,0
" however, our concept of recursion for neural programs is general",1
" we hypothesize that the reason for this is that the neural network learns to spuriously depend on specific characteristics of the training examples that are irrelevant to the true program semantics, such as length of the training inputs, and thus fails to generalize to more complex inputs",0
" ppivot and pj point to the pivot and j indices of the array, used in the partition function in algorithm 3",1
3,1
 we find that recursion makes it easier for the network to learn the right program and generalize to unknown situations,1
 this modification also enables us to sort arrays containing arbitrary comparable elements,0
" for purposes of verification, we replace the domain-specific encoder with the following:
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1) ≤ q(1, i2), 1 ≤ i1 ≤ length, 1 ≤ i2 ≤ length, i3 == length, at(1), at(2), at(3)]),which directly exposes which of the two values pointed to is larger",0
" , a1a0 + b1b0} are added properly",0
"
algorithm 1 neural programming inference 1: inputs: environment observation e, program p, arguments a, stop threshold α 2: function run(e, p, a) 3: h← 0, r ← 0 4: while r < α do 5: s← fenc(e, a), h← flstm(s, p, h) 6: r ← fend(h), p2 ← fprog(h), a2 ← farg(h) 7: if p is a primitive function then 8: e← fenv(e, p, a)",0
"we describe how models trained with recursive traces can be proven to generalize, by using the verification procedure described in section 3",1
" in order to claim a proof, we must verify the model produces correct behavior over all base cases and reductions, as described in section 2",0
" the theorem states that for the same sequence of step inputs, the model produces the exact same step output as the target program it aims to learn",1
 the logic for the non-recursive trace is shown in algorithm 4 in the appendix,0
 these pointers are referred to as bubble pointers,0
 the memory updates of these neural programs are so complex and interdependent that it is difficult to reason about the behaviors of the learned neural program under previously unseen situations (such as problems with longer inputs),1
"
note that the recursion for quicksort is not purely tail recursive and therefore represents a more complex kind of recursion that is harder to learn than in the previous tasks",1
" for the recursive formulation of addition, we analytically construct the set of input problems that cover all base cases and reduction rules",0
"
bubble sort",0
 u varies with the number of vertices in the graph,1
 the training set for addition contains 200 traces,0
", to color a vertex) or variable (e",0
"
by nature, recursion reduces the complexity of a problem to simpler instances",0
" by iterating with this procedure, and then running p on the input observation set that we obtain for the entry point function, we can obtain v precisely",0
" for these models, table 2 presents results on randomly generated dags of varying graph sizes (varying in the number of vertices)",1
" lshift, used in reset, shifts the pointers left until reaching the start token",0
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,1
 we adapt machinery from the original paper slightly to fit our needs,0
" partial recursion only adds a tail recursive call to bubblesort after bubble and reset, similar to the tail recursive call described previously for addition",1
"
the three environment observations aid with control flow in algorithm 2",0
 the original version of the bubblesort implementation exposes the values within the array,0
" however, note that there is no provable guarantee that the non-recursive learned program will generalize to all inputs, whereas we show later that the recursive learned program has a provable guarantee of perfect generalization",0
" as described in the verification procedure, it is possible to prove our learned recursive program generalizes perfectly by testing on an appropriate set of problem inputs, i",0
" table 1 presents results on randomly generated arrays of varying length for the learned non-recursive, partially recursive, and full recursive programs",0
"
note that the training set can often be considerably smaller than the verification set, and despite this, the learned model can still pass the entire verification set",0
" if the step output specifies a primitive function call, we need to reason about how it can affect the environment so as to change the observation in the next step input",0
"
after finding v , we construct a set of problem inputs which, when executed on p , create exactly the step input sequences which make up v ",0
" fa8750-15-2-0104, and berkeley deep drive",0
"in this paper, we propose that the key abstraction of recursion is necessary for neural programs to generalize",1
" we have three observations: the color of the start node, the color of the active node’s next child to be considered, and whether the stack is empty",0
" childlist ∈ nu is a vector of pointers, where childlist[i] points to the next child under consideration for vertex i",0
 some functional programming languages go so far as not to define any looping constructs but rely solely on recursion to enable repeated execution of the same code,0
 our experiments use a small number of training examples,1
" the pointer at index i3 represents a counter internal to the environment that is incremented once after each pass of the algorithm (one cycle of bubble and reset); when incremented a number of times equal to the length of the array, the flag i3 == length becomes true and terminates the entire algorithm ",0
"as there is no public implementation of npi, we implemented a version of it in keras that is as faithful to the paper as possible",0
 ,0
" the only model, to our knowledge, that does not train on input-output pairs is the neural programmer-interpreter (reed & de freitas, 2016), which trains on synthetic execution traces",0
 this makes it difficult to reason about what the model will do when given complex inputs,0
 stack push call2 pushes pivot + 1 and hi to qstacklo and qstackhi,0
" in this section, we give a brief review of the npi architecture from reed & de freitas (2016) as background",1
" we also hope to decrease supervision, for example by training with only partial or non-recursive traces, and to develop novel neural programming architectures integrated directly with a notion of recursion",1
"2 at any given time, the npi has access to values pointed to by four pointers in each of the four rows, represented by q(1, i1), q(2, i2), q(3, i3), and q(4, i4)",0
we now report on generalization for the varying tasks,0
"
thus far, to evaluate the efficacy of neural models on programming tasks, the only metric that has been used is generalization of expected behavior to inputs of greater complexity (vinyals   (2015), kaiser & sutskever (2015), reed & de freitas (2016), graves   (2016), zaremba   (2016))",1
" in particular, we construct new training traces which explicitly contain recursive elements and show that with this type of trace, npi easily learns recursive programs",1
" consequently, there is no concept of length relevant to the problem, which has traditionally been an important focus of length-based curriculum learning",0
" this is possible if and only if the graph has no directed cycles; that is to say, it must be a directed acyclic graph (dag)",1
" the pointer at index i3 represents a counter internal to the environment that is incremented once after each pass of the algorithm (one cycle of bubble and reset); when incremented a number of times equal to the length of the array, the flag i3 == length becomes true and terminates the entire algorithm ",0
3,0
" at any given time, the npi has access to the values referred to by two pointers, represented by q(1, i1) and q(1, i2),",1
" for example, for the addition task, the model is trained on short inputs and then tested on its ability to sum inputs with much longer numbers of digits",1
 recursion enables provably perfect generalization,1
" finally, we implement recursive traces for our own topological sort and quicksort tasks",0
 ,0
" observe that partially recursive does slightly better than non-recursive for the setting in which the length of the array is 3, and that the fully recursive version is able to sort every array given to it",0
" for brevity, we refer the reader to the appendix for further details on the program set and non-recursive and recursive trace-generating functions used for topological sort",1
" training on the full recursive trace leads to perfect generalization, as shown in section 4",0
" to make an analogy to mdps, this procedure is analogous to how value iteration obtains the correct value for each state starting from any initialization",1
" both the non-recursive and recursive learned programs generalize on all graphs we tried, up to 120 vertices",1
" the second is the training procedure, which consists of the form of the training data and the optimization process",1
 reset represents a −∞ value,0
"
arg 2 (object to copy): env stack lo peek, env stack hi peek, phi, plo, ppivot − 1, ppivot + 1, reset
env stack lo peek and env stack hi peek represent qstacklo(pstacklo − 1) and qstackhi(pstackhi − 1), respectively",0
 ,0
" this is possible if and only if the graph has no directed cycles; that is to say, it must be a directed acyclic graph (dag)",0
" both the non-recursive and recursive learned programs generalize on all input lengths we tried, up to 5000 digits",1
"as there is no public implementation of npi, we implemented a version of it in keras that is as faithful to the paper as possible",1
"
bubble sort",1
" in computer science, recursion (as opposed to iteration) involves solving a larger problem by combining solutions to smaller instances of the same problem",0
 almost all architectures train on program input/output pairs,0
"
the training set for quicksort contains 4 traces, synthesized from arrays of length 5",1
" the architecture usually possesses some form of memory, which could be internal (such as the hidden state of a recurrent neural network) or external (such as a discrete “scratch pad” or a memory block with differentiable access)",0
 we outline how to construct this set,0
 recursion enables provable guarantees on neural programs’ behavior without needing to exhaustively enumerate all possible inputs to the programs,0
" the move operation has the following arguments:
arg 1 (pointer): presult, pstack, pstart, childlist[vactive], childlist[vsave]
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 5 values",0
" the only model, to our knowledge, that does not train on input-output pairs is the neural programmer-interpreter (reed & de freitas, 2016), which trains on synthetic execution traces",1
 table 3 presents results on randomly generated arrays of varying length for the learned non-recursive and recursive programs,0
 the degree for any vertex in the dag is variable,0
" the core module of the npi is an lstm controller that takes as input a slice of the current external environment, via a set of pointers, and a program and arguments to execute",0
" as a consequence, our learned neural programs empirically achieve perfect generalization from a very small number of training examples",0
" we have three observations: the color of the start node, the color of the active node’s next child to be considered, and whether the stack is empty",0
" also, compared to the bubble pointers in bubble sort, the pointers that perform the comparison for quicksort (the compswap function) are usually not adjacent to each other, making quicksort less local than bubble sort",1
 we then construct a verification set of size 73 by ensuring that randomly generated graphs cover the analytically derived v ,0
" there are 3 pointers (presult, pstack, pstart), presult points to the next empty location in qresult,
pstack points to the top of the stack in qstack, and pstart points to the candidate starting node for a connected component",0
"
algorithm 2 depth first search topological sort 1: color all vertices white",1
 the original version of the bubblesort implementation exposes the values within the array,0
" 9: else 10: function run(e, p2, a2)
a description of the inference procedure is given in algorithm 1",0
" 2: initialize lo and hi to be 1 and n, where n is the length of a",1
" the space of possible programs is extremely large, and composing a program that performs robustly on the infinite space of possible inputs is difficult—in part because it is impractical to obtain enough training examples to easily disambiguate amongst all possible programs",1
" it is straightforward to deal with all other operations, which do not induce partial environment states",1
"
there are 6 pointers (plo, phi, pstacklo, pstackhi, ppivot, pj)",0
" for these models, table 2 presents results on randomly generated dags of varying graph sizes (varying in the number of vertices)",0
" any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of national science foundation and darpa",1
" in contrast, each call to the recursive add always runs for a fixed number of steps, even on arbitrarily long problems in f , so we can test that it performs correctly on a small, fixed number of step input sequences",0
" also, compared to the bubble pointers in bubble sort, the pointers that perform the comparison for quicksort (the compswap function) are usually not adjacent to each other, making quicksort less local than bubble sort",0
 the environment and return probability are omitted for readability,0
" for grade-school addition, the domain-specific encoder is
fenc(q, i1, i2, i3, i4, at) =mlp ([q(1, i1), q(2, i2), q(3, i3), q(4, i4), at(1), at(2), at(3)]),
where the environment q ∈ r4×n×k is a scratch-pad that contains four rows (the first input number, the second input number, the carry bits, and the output) and n columns",0
"
the operations we need to be concerned most about are carry and lshift, which induce partial environment states spanning two columns",0
" qcolor(pstart) contains the color of the current start vertex, used in the evaluation of the condition in the while loop in line 5 of algorithm 2",0
"
grade school addition",1
g,0
" qcolor(pstart) contains the color of the current start vertex, used in the evaluation of the condition in the while loop in line 5 of algorithm 2",1
" to make an analogy to mdps, this procedure is analogous to how value iteration obtains the correct value for each state starting from any initialization",0
6,0
", the verification set",1
" because the context of the caller is stored on a stack when it calls another program and the callee starts in a fresh context, this enables recursion simply by allowing a program to call itself",0
" in this section, we give a brief review of the npi architecture from reed & de freitas (2016) as background",0
g,0
" we made the assumption of equivalent lengths in order to parametrize the input format with respect to length, but this assumption can be removed as well",1
" in this architecture, we consider a core controller, e",1
" numerous models have been proposed for learning programs; to name a few, this includes the differentiable neural computer (graves  , 2016), neural turing machine (graves  , 2014), neural gpu (kaiser & sutskever, 2015), neural programmer (neelakantan  , 2015), pointer network (vinyals  , 2015), hierarchical attentive memory (andrychowicz & kurach, 2016), and neural random access machine (kurach  , 2016)",0
" in contrast, each call to the recursive add always runs for a fixed number of steps, even on arbitrarily long problems in f , so we can test that it performs correctly on a small, fixed number of step input sequences",1
" ppivot and pj point to the pivot and j indices of the array, used in the partition function in algorithm 3",0
" an example of part of an addition task trace, written in shorthand, is given in figure 1",0
" observe that the non-recursive program’s correctness degrades for length 11 and beyond, while the recursive program can sort any given array",0
" the non-recursive and partially recursive versions are unable to sort long arrays, beyond length 8",1
"
the training set for bubble sort contains 100 traces, with maximum problem length of 2 (e",1
"
as a concrete instantiation, we show in this paper that we can enable recursive neural programs in the npi model, and thus enable perfectly generalizable neural programs for tasks such as sorting where the original, non-recursive npi program fails",0
"
note that the recursion for quicksort is not purely tail recursive and therefore represents a more complex kind of recursion that is harder to learn than in the previous tasks",1
" , a1a0 + b1b0} are added properly",1
" k is set to 11, to represent the range of 10 possible digits, along with a token representing the end of input",0
" however, our concept of recursion for neural programs is general",0
g,0
 table 4 demonstrates that the accuracy of the non-recursive program degrades sharply when moving from arrays of length 7 to arrays of length 8,0
" in particular, in the explore function, adding a tail recursive call resets and stores the hidden states associated with vertices in a stack-like fashion",1
" while this matches the description from reed & de freitas (2016), we found that this causes an unnecessary blowup in the size of v and makes it much more difficult to construct the verification set",1
" using this modification, we constructed a verification set consisting of one array of size 10",1
"
as in line 13 of the right-hand side of figure 1",0
"
figure 1 shows examples of non-recursive and recursive addition traces",0
" in order to claim a proof, we must verify the model produces correct behavior over all base cases and reductions, as described in section 2",0
 u varies with the number of vertices in the graph,0
 ,0
 we found that changing the npi training traces is a simple way to enable this,0
"
in this general neural programming architecture, we show it is easy to support recursion",1
" consequently, there is no concept of length relevant to the problem, which has traditionally been an important focus of length-based curriculum learning",0
"
as mentioned in section 1, all approaches to neural programming today fare poorly on this generalization issue",1
" the core lstm module’s hidden state is preserved over all these add1 calls, and it is difficult to interpret with certainty what happens over longer timesteps without concretely evaluating the lstm with an input of that length",0
2,1
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,0
" if yes, then this indicates that the learned neural program achieves provably perfect generalization",0
" the general notion of recursion has been an important concept in many domains, including mathematics and computer science",0
 recursion can be implemented differently for different neural programming models,1
 the environment and return probability are omitted for readability,0
" each time a subprogram is called, the stack depth increases",1
" in fact, we argue that recursion is an essential element for neural programs to generalize, and makes it tractable to prove the generalization of neural programs",1
" this makes it so that we only need to consider the vertices in the subgraph that are currently relevant for computing the sort, allowing simpler reasoning about behavior for large graphs",1
" a single element of an execution trace consists of a step input-step output pair, which can be synthesized from algorithm 1: this corresponds to, for a given time-step, the step input tuple (e, p, a) and step output tuple (r, p2, a2)",1
 18: push lo and p− 1 to slo and shi,1
" 9: else 10: function run(e, p2, a2)
a description of the inference procedure is given in algorithm 1",1
 we describe the recursive re-formulation of traces for two tasks from the original npi paper—grade-school addition and bubble sort,1
" for tasks other than addition, it is difficult to analytically determine the verification set, so instead, we randomly generate input candidates until they completely cover the base cases and reduction rules",1
" the pointer at index i3 represents a counter internal to the environment that is incremented once after each pass of the algorithm (one cycle of bubble and reset); when incremented a number of times equal to the length of the array, the flag i3 == length becomes true and terminates the entire algorithm ",1
" training on the full recursive trace leads to perfect generalization, as shown in section 4",0
" for problems where the base cases may be extremely large or infinite, such as certain forms of motor control, recursion can still help reduce the problem of generalization to these two aspects and make the generalization problem significantly simpler to handle and reason about",1
" training on the full recursive trace leads to perfect generalization, as shown in section 4",0
" for brevity, we refer the reader to the appendix for further details on the program set and non-recursive and recursive trace-generating functions used for topological sort",0
" full recursion, in addition to making the aforementioned tail recursive call, adds two additional recursive calls; bstep and lshift are made tail recursive",0
"
we describe the details of the npi model relevant to our contributions",0
" for tasks other than addition, it is difficult to analytically determine the verification set, so instead, we randomly generate input candidates until they completely cover the base cases and reduction rules",0
" finally, we implement recursive traces for our own topological sort and quicksort tasks",0
"
grade school addition",0
", an lstm in npi’s case, but possibly other networks in different cases",0
"
base cases and reduction rules for quicksort",1
", an lstm in npi’s case, but possibly other networks in different cases",1
" the npi model has three learnable components: a task-agnostic core, a program-key embedding, and domain-specific encoders that allow the npi to operate in diverse environments",1
 errors arise due to undesirable and uninterpretable dependencies and associations the architecture learns to store in some high-dimensional hidden state,1
 this paper is the first (to our knowledge) to investigate the important problem of provable generalization properties of neural programs,0
" if it is too large, then the semantics of p might not be well-defined on some elements in v , or the spurious step input sequences may not be reachable from any valid problem input (e",1
"
in what follows, we describe the way in which we constructed npi training traces so as to make them contain recursive elements and thus enable npi to learn recursive programs",1
", to change the value of phi) none described belowstack",1
 the logic for the non-recursive trace is shown in algorithm 4 in the appendix,1
"
an alternative method is to run p on many different program inputs and then observe step input sequences which occur, to create v ",0
"
it is sufficient to construct problems where every transition between two adjacent columns is covered",0
" in fact, it is one of our future directions to explore new ways to train a recursive neural program without providing explicit training execution traces or with only partial or non-recursive traces",0
" qcolor(dag[vactive][childlist[vactive]]) refers to the color of the next child of vactive, used in the evaluation of the condition in the if branch in line 9 of algorithm 2",1
"
by nature, recursion reduces the complexity of a problem to simpler instances",0
" a topological sort is a linear ordering of vertices such that for every directed edge (u, v) from u to v, u comes before v in the ordering",1
"
grade school addition",0
 we created a program set that reflects the semantics of algorithm 2,1
 table 4 also demonstrates that the (verified) recursive program generalizes perfectly,0
 this procedure verifies that all base cases and reductions are handled properly by the model via explicit tests,0
" 4: function toposort(dag) 5: while there is still a white vertex u: do 6: color[u] = grey 7: vactive = u 8: do 9: if vactive has a white child v then 10: color[v] = grey 11: push vactive onto s 12: vactive = v 13: else 14: color[vactive] = black 15: write vactive to result 16: if s is empty then pass 17: else pop the top vertex off s and set it to vactive 18: while s is not empty
topological sort",0
" in fact, we argue that recursion is an essential element for neural programs to generalize, and makes it tractable to prove the generalization of neural programs",0
 the degree for any vertex in the dag is variable,0
" more specifically, when m is trained on simpler inputs, such as inputs of a small length, the generalization metric evaluates how well m will do on more complex inputs, such as inputs of much longer length",0
" the corresponding edge list is [(1, 2), (1, 5), (2, 4), (2, 5), (3, 5)]",0
g,1
g,0
"
in this general neural programming architecture, we show it is easy to support recursion",1
3,1
6,1
" for purposes of verification, we replace the domain-specific encoder with the following:
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1) ≤ q(1, i2), 1 ≤ i1 ≤ length, 1 ≤ i2 ≤ length, i3 == length, at(1), at(2), at(3)]),which directly exposes which of the two values pointed to is larger",0
" for each length, we test each program on 30 randomly generated problems",1
"
figure 1 shows examples of non-recursive and recursive addition traces",0
" the arrow in the theorem refers to evaluation, as in big-step semantics",0
" 4: function toposort(dag) 5: while there is still a white vertex u: do 6: color[u] = grey 7: vactive = u 8: do 9: if vactive has a white child v then 10: color[v] = grey 11: push vactive onto s 12: vactive = v 13: else 14: color[vactive] = black 15: write vactive to result 16: if s is empty then pass 17: else pop the top vertex off s and set it to vactive 18: while s is not empty
topological sort",1
" via the recursive call, we effectively forget that the column just added exists, since the recursive call to add starts with a new hidden state for the lstm controller",0
", an lstm in npi’s case, but possibly other networks in different cases",1
" k is set to 11, to denote the range of possible numbers (0 through 9), along with the start/end token (represented with the same encoding) which is observed when a pointer reaches beyond the bounds of the input",1
" also the dag can have potentially more than one connected component, meaning it is necessary to transition between these components appropriately",1
" this agrees with the generalization of non-recursive addition in reed & de freitas (2016), where they reported generalization up to 3000 digits",1
" more specifically, when m is trained on simpler inputs, such as inputs of a small length, the generalization metric evaluates how well m will do on more complex inputs, such as inputs of much longer length",0
" empirically, we observe that the learned recursive programs solve all valid inputs with 100% accuracy after training on a very small number of examples, out-performing previous generalization results",0
" for the recursive formulation of addition, we analytically construct the set of input problems that cover all base cases and reduction rules",1
" for brevity, we refer the reader to the appendix for information about the program set and non-recursive and recursive trace-generating functions for quicksort",0
 pstacklo and pstackhi point to the top (empty) positions in qstacklo and qstackhi,1
"
in this paper, we propose to resolve these issues by explicitly incorporating recursion into neural architectures",0
" empirically, we observe that the learned recursive programs solve all valid inputs with 100% accuracy after training on a very small number of examples, out-performing previous generalization results",0
 almost all architectures train on program input/output pairs,0
" for problems where the base cases may be extremely large or infinite, such as certain forms of motor control, recursion can still help reduce the problem of generalization to these two aspects and make the generalization problem significantly simpler to handle and reason about",0
"
for topological sort, the domain-specific encoder is
fenc(dag,qcolor, pstack, pstart, vactive, childlist, at)
=mlp ([qcolor(pstart), qcolor(dag[vactive][childlist[vactive]]), pstack == 1, at(1), at(2), at(3)]),
where qcolor ∈ ru×4 is a scratch-pad that contains u rows, each containing one of four colors (white, gray, black, invalid) with one-hot encoding",0
"
move",0
 our result shows that the training procedure and the npi architecture is capable of generalizing from the step input-output pairs seen in the training data to the unseen ones present in the verification set,0
" if it is too large, then the semantics of p might not be well-defined on some elements in v , or the spurious step input sequences may not be reachable from any valid problem input (e",1
" using this modification, we constructed a verification set consisting of one array of size 10",0
" the pointers at index i1 and i2 are used to compare the pair of numbers considered during the bubble sweep, swapping them if the number at i1 is greater than that in i2",0
" as described in the verification procedure, it is possible to prove our learned recursive program generalizes perfectly by testing on an appropriate set of problem inputs, i",1
" if a white vertex is found, set pstart to point to it; this signifies the start of a traversal of a new connected component",1
 pstacklo and pstackhi point to the top (empty) positions in qstacklo and qstackhi,1
" more specifically, when m is trained on simpler inputs, such as inputs of a small length, the generalization metric evaluates how well m will do on more complex inputs, such as inputs of much longer length",1
"
inside bubble and reset, there are two operations that can be made recursive",1
" we then construct problem configurations that span all possible valid environment states (for instance, in order to force the carry bit in a column to be 1, one can add the sum “1+9” in the column to the right)",1
" , n , where the dag contains n vertices",1
"
since npi naturally supports the notion of recursion, a key question is how to enable npi to learn recursive programs",0
" partial recursion only adds a tail recursive call to bubblesort after bubble and reset, similar to the tail recursive call described previously for addition",0
e,0
" however, to be sure that the generated v is complete (covers all the cases needed), we need to check all pairs of observations seen in adjacent step inputs (in particular, those before and after a primitive function call), in a similar way as if we were constructing v from scratch",0
" we show that by modifying the training procedure, we enable the npi model to learn recursive neural programs",0
" this is possible if and only if the graph has no directed cycles; that is to say, it must be a directed acyclic graph (dag)",0
" each step during an execution of the program does one of three things: (1) another subprogram along with associated arguments is called, as in line 10, (2) the program writes to the environment if it is primitive, as in line 8, or (3) the loop is terminated if the return probability exceeds a threshold α, after which the stack frame is popped and control is returned to the caller",1
5,1
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,0
" even though the earlier version also generalized just as well in practice, relatively small differences in the formulation of the traces and the environment observations can drastically change the difficulty of verification",0
"
one common strategy to improve generalization is to use curriculum learning, where the model is trained on inputs of gradually increasing complexity",1
" 2: initialize lo and hi to be 1 and n, where n is the length of a",0
", the trace corresponding to the array [3,2])",1
" for purposes of verification, we replace the domain-specific encoder with the following:
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1) ≤ q(1, i2), 1 ≤ i1 ≤ length, 1 ≤ i2 ≤ length, i3 == length, at(1), at(2), at(3)]),which directly exposes which of the two values pointed to is larger",1
 our experiments use a small number of training examples,0
 ,0
" it is straightforward to deal with all other operations, which do not induce partial environment states",0
 indentation indicates the stack is one level deeper than before,0
"
to perform the verification as described here, it is critical to construct v correctly",0
" in our experiments, we only present dag’s as inputs and represent the vertices as values ranging from 1, ",0
" using this modification, we constructed a verification set consisting of one array of size 10",0
"
inside bubble and reset, there are two operations that can be made recursive",1
 we instantiated recursion for the neural programmerinterpreter by changing the training traces,1
" as described in the verification procedure, it is possible to prove our learned recursive program generalizes perfectly by testing on an appropriate set of problem inputs, i",0
"
in order to demonstrate that recursion can help learn and generalize better, for addition, we trained only on traces for 5 arbitrarily chosen 1-digit addition sum examples",0
e,0
" in all experiments, α is set to 0",1
"
algorithm 1 neural programming inference 1: inputs: environment observation e, program p, arguments a, stop threshold α 2: function run(e, p, a) 3: h← 0, r ← 0 4: while r < α do 5: s← fenc(e, a), h← flstm(s, p, h) 6: r ← fend(h), p2 ← fprog(h), a2 ← farg(h) 7: if p is a primitive function then 8: e← fenv(e, p, a)",0
"
the same set of problems was used to generate the training traces for all formulations of the task, for non-recursive and recursive versions",0
" in particular, in the explore function, adding a tail recursive call resets and stores the hidden states associated with vertices in a stack-like fashion",0
" in future work, we would like to decrease supervision and construct models that are capable of coming up with recursive abstractions themselves",1
"
recursion drastically reduces the number of configurations we need to consider during the verification phase and makes the proof tractable, because it introduces structure that eliminates infinitely long sequences of step inputs that would otherwise need to be considered",1
3,0
e,0
" in future work, we seek to enable more tasks with recursive structure",1
" 2: initialize lo and hi to be 1 and n, where n is the length of a",0
" the assumption of leading 0’s can be easily removed, at the cost of slightly increasing the size of the verification set",1
 we describe the recursive re-formulation of traces for two tasks from the original npi paper—grade-school addition and bubble sort,1
" thus, recursion helps decompose a problem and makes it easier to reason about a program’s behavior for previously unseen situations such as longer inputs",0
 ,1
"
by nature, recursion reduces the complexity of a problem to simpler instances",1
" we have three observations: the color of the start node, the color of the active node’s next child to be considered, and whether the stack is empty",1
"
an alternative method is to run p on many different program inputs and then observe step input sequences which occur, to create v ",0
" thus, we demonstrate that recursion enables provably perfect generalization for different tasks, including addition, topological sort, quicksort, and a variant of bubble sort",1
"
move",0
"
as a concrete instantiation, we show in this paper that we can enable recursive neural programs in the npi model, and thus enable perfectly generalizable neural programs for tasks such as sorting where the original, non-recursive npi program fails",0
" we hypothesize that the reason for this is that the neural network learns to spuriously depend on specific characteristics of the training examples that are irrelevant to the true program semantics, such as length of the training inputs, and thus fails to generalize to more complex inputs",0
"
for addition, we analytically determine the verification set",1
"
base cases and reduction rules for bubble sort",1
 stack push call2 pushes pivot + 1 and hi to qstacklo and qstackhi,1
"
formally, verification consists of proving the following theorem:
∀i ∈ v,m(i) ⇓ p (i) where i denotes a sequence of step inputs (within one function call), v denotes the set of valid sequences of step inputs, m denotes the neural network model, p denotes the correct program, and p (i) denotes the next step output from the correct program",1
" at any given time, the npi has access to the values referred to by two pointers, represented by q(1, i1) and q(1, i2),",1
 stack pop pushes −∞ values to qstacklo and qstackhi,0
 add1 is a subprogram that adds the current column (writing the appropriate digit to the output row and carrying a bit to the next column if needed),1
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,1
" for non-primitive subprograms, we can update the observation-to-observation mapping currently associated with the subprogram and then apply that mapping to the current set",1
" for each function we use to implement the recursive version of topological sort, we need to consider the set of possible environment observation sequences we can create from all valid inputs and test that the learned program produces the correct behavior on each of these inputs",0
" a topological sort is a linear ordering of vertices such that for every directed edge (u, v) from u to v, u comes before v in the ordering",1
"
we refer the reader to the appendix for the non-recursive trace generating functions",0
" at each time step, the core controller can decide to select one of the programs to call with certain arguments",0
"
it is sufficient to construct problems where every transition between two adjacent columns is covered",0
" provably perfect generalization implies the model will behave correctly, given any valid input",1
"
in addition, none of the current approaches to neural programming provide a method or even aim to enable provable guarantees about generalization",0
 this modification also enables us to sort arrays containing arbitrary comparable elements,1
3 to construct v and then empirically create a verification set which covers v ,0
" to make an analogy to mdps, this procedure is analogous to how value iteration obtains the correct value for each state starting from any initialization",0
2,1
"
one common strategy to improve generalization is to use curriculum learning, where the model is trained on inputs of gradually increasing complexity",0
g,0
g,1
 u varies with the number of vertices in the graph,0
" we make the trace recursive by adding a tail recursive call into the trace for the add program after calling add1 and lshift, the original paper uses k = 10, but we found it necessary to augment the range with an end token, in order to terminate properly",1
" however, our concept of recursion for neural programs is general",0
" in future work, we seek to enable more tasks with recursive structure",1
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,0
" this makes it so that we only need to consider the vertices in the subgraph that are currently relevant for computing the sort, allowing simpler reasoning about behavior for large graphs",0
" for purposes of verification, we replace the domain-specific encoder with the following:
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1) ≤ q(1, i2), 1 ≤ i1 ≤ length, 1 ≤ i2 ≤ length, i3 == length, at(1), at(2), at(3)]),which directly exposes which of the two values pointed to is larger",0
" the learned recursive program is different from neural programs learned in all previous work in an important aspect: previous approaches do not explicitly incorporate this abstraction, and hence generalize poorly, whereas our learned neural programs incorporate recursion and achieve perfect generalization",1
" this is highly undesirable, since being able to provide the correct answer in all possible settings is one of the most important aspects of any learned neural program",0
"
the training set for quicksort contains 4 traces, synthesized from arrays of length 5",1
" for grade-school addition, the domain-specific encoder is
fenc(q, i1, i2, i3, i4, at) =mlp ([q(1, i1), q(2, i2), q(3, i3), q(4, i4), at(1), at(2), at(3)]),
where the environment q ∈ r4×n×k is a scratch-pad that contains four rows (the first input number, the second input number, the carry bits, and the output) and n columns",0
"
in addition, none of the current approaches to neural programming provide a method or even aim to enable provable guarantees about generalization",0
 we found that changing the npi training traces is a simple way to enable this,1
" we prove every member of f is added properly, given that subproblems s = {anan−1 + bnbn−1, an−1an−2 + bn−1bn−2, ",0
"
we describe the base cases, reduction rules, and the verification set for each task in appendix a",0
" for each length, we test each program on 30 randomly generated problems",0
 errors arise due to undesirable and uninterpretable dependencies and associations the architecture learns to store in some high-dimensional hidden state,0
", the trace corresponding to the problem “109 + 101”)",0
" finally, the boolean pstack == 1 is used to check whether the stack is empty in line 18 of algorithm 2",0
"
algorithm 2 shows the topological sort task of interest",0
"
one common strategy to improve generalization is to use curriculum learning, where the model is trained on inputs of gradually increasing complexity",0
" observe that the non-recursive program’s correctness degrades for length 11 and beyond, while the recursive program can sort any given array",0
" thus, we demonstrate that recursion enables provably perfect generalization for different tasks, including addition, topological sort, quicksort, and a variant of bubble sort",1
" in order to compensate for this, ppivot and pj require special functions (move pivot lo and move j lo) to properly set them to lo in lines 11 and 12 of the partition function in algorithm 3",0
"
under the assumption that there are no leading 0’s (except in the case of single digits) and the two numbers to be added have the same number of digits, the verification set for addition contains 20,181 input problems",0
" thus, we demonstrate that recursion enables provably perfect generalization for different tasks, including addition, topological sort, quicksort, and a variant of bubble sort",1
as mentioned in section 2,0
", qstacklo or qstackhi) or to pointer (e",0
" when the program is called, the current context including the caller’s memory state is stored on a stack; when the program returns, the stored context is popped off the stack to resume execution in the previous caller’s context",0
" we call this set of inputs the verification set, sv ",1
" for example, for the addition task, the model is trained on short inputs and then tested on its ability to sum inputs with much longer numbers of digits",1
 we created a program set that reflects the semantics of algorithm 2,0
g,1
 indentation indicates the stack is one level deeper than before,1
"
the training set for topological sort contains 6 traces, with one synthesized from a graph of size 5 and the rest synthesized from graphs of size 7",0
" in fact, it is one of our future directions to explore new ways to train a recursive neural program without providing explicit training execution traces or with only partial or non-recursive traces",0
" a topological sort is a linear ordering of vertices such that for every directed edge (u, v) from u to v, u comes before v in the ordering",1
", an lstm in npi’s case, but possibly other networks in different cases",1
"
formally, verification consists of proving the following theorem:
∀i ∈ v,m(i) ⇓ p (i) where i denotes a sequence of step inputs (within one function call), v denotes the set of valid sequences of step inputs, m denotes the neural network model, p denotes the correct program, and p (i) denotes the next step output from the correct program",0
" any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of national science foundation and darpa",0
 this algorithm is a variant of depth first search,1
" table 1 presents results on randomly generated arrays of varying length for the learned non-recursive, partially recursive, and full recursive programs",0
 18: push lo and p− 1 to slo and shi,0
"we describe how models trained with recursive traces can be proven to generalize, by using the verification procedure described in section 3",0
 errors arise due to undesirable and uninterpretable dependencies and associations the architecture learns to store in some high-dimensional hidden state,0
" the learned recursive program is different from neural programs learned in all previous work in an important aspect: previous approaches do not explicitly incorporate this abstraction, and hence generalize poorly, whereas our learned neural programs incorporate recursion and achieve perfect generalization",1
 ,1
"
bubble sort",0
" note that recursion helps make this process tractable, because we only need to test a finite number of inputs to show that the model will work correctly on inputs of unbounded complexity",0
" the contents ofqresult andqstack are not exposed directly through the domain-specific encoder; rather, we define primitive functions which manipulate these scratch-pads",0
"
in what follows, we describe the way in which we constructed npi training traces so as to make them contain recursive elements and thus enable npi to learn recursive programs",0
" qcolor(pstart) contains the color of the current start vertex, used in the evaluation of the condition in the while loop in line 5 of algorithm 2",0
" the second is the training procedure, which consists of the form of the training data and the optimization process",1
"1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 while (pstacklo 6= 1): 6 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 7 stack(stack_pop) 8 else: 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 stack(stack_pop) 12 partition() 13 stack(stack_push_call2) 14 stack(stack_push_call1) 15 } 16 17 partition() { 18 set_pivot_lo() 19 set_j_lo() 20 compswap_loop() 21 swap(ppivot, phi) 22 set_j_null() 23 } 24 25 compswap_loop() { 26 while (pj 6= phi): 27 compswap() 28 move(pj, up) 29 } 30 31 compswap() { 32 if (a[pj ] ≤ a[phi]): 33 swap(ppivot, pj) 34 move(ppivot, up) 35 } 36 37 stack(op) { 38 if (op == stack_push_call1): 39 write(env_stack_lo, plo) 40 write(env_stack_hi, ppivot − 1) 41 move(pstacklo, up) 42 move(pstackhi, up) 43 44 if (op == stack_push_call2): 45 write(env_stack_lo, ppivot + 1) 46 write(env_stack_hi, phi) 47 move(pstacklo, up) 48 move(pstackhi, up) 49 50 if (op == stack_pop): 51 write(env_stack_lo, reset) 52 write(env_stack_hi, reset) 53 move(pstacklo, down) 54 move(pstackhi, down) 55 }altered recursive functions 1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 6 partition() 7 stack(stack_push_call2) 8 stack(stack_push_call1) 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 quicksort() // recursive call 12 stack(stack_pop) 13 write(phi, env_stack_hi_peek) 14 write(plo, env_stack_lo_peek) 15 quicksort() // recursive call 16 stack(stack_pop) 17 } 18 19 compswap_loop() { 20 if (pj 6= phi): 21 compswap() 22 move(pj, up) 23 compswap_loop() // recursive call 24 }in this section, we describe the space of base cases and reduction rules that must be covered for each of the four sample tasks, in order to create the verification set",0
 a1a0 + bnbn−1 ,0
" empirically, we observe that the learned recursive programs solve all valid inputs with 100% accuracy after training on a very small number of examples, out-performing previous generalization results",0
 the training set for addition contains 200 traces,1
" observe that the non-recursive program’s correctness degrades for length 11 and beyond, while the recursive program can sort any given array",0
" if it is too small, then execution of the program on some input might require evaluation of m(i) on some i /∈ v , and so the behavior ofm(i) might deviate from p (i)",0
" 4: function toposort(dag) 5: while there is still a white vertex u: do 6: color[u] = grey 7: vactive = u 8: do 9: if vactive has a white child v then 10: color[v] = grey 11: push vactive onto s 12: vactive = v 13: else 14: color[vactive] = black 15: write vactive to result 16: if s is empty then pass 17: else pop the top vertex off s and set it to vactive 18: while s is not empty
topological sort",1
 this modification also enables us to sort arrays containing arbitrary comparable elements,0
g,1
"
arg 2 (auxiliary variable): color grey, color black
color grey and color black color the given vertex grey and black, respectively",1
" for non-primitive subprograms, we can update the observation-to-observation mapping currently associated with the subprogram and then apply that mapping to the current set",1
" plo and phi point to the lo and hi indices of the array, as in algorithm 3",1
" for example, a step input-step output pair in lines 2 and 3 of the left-hand side of figure 1 is (add1, write out 1)",0
"
base cases and reduction rules for addition",0
5,1
" lshift, used in reset, shifts the pointers left until reaching the start token",0
" via the recursive call, we effectively forget that the column just added exists, since the recursive call to add starts with a new hidden state for the lstm controller",0
g,0
"
we construct input problems by splitting into two cases: one case in which the left column contains a null value and another in which the left column does not contain any null values",1
" consequently, there is no concept of length relevant to the problem, which has traditionally been an important focus of length-based curriculum learning",1
"
to construct this set, by using the reference implementation of each subprogram, we construct a mapping between two sets of environment observations: the first set consists of all observations that can occur at the beginning of a particular subprogram’s invocation, and the second set contains the observations at the end of that subprogram",0
"we train using the adam optimizer and use a 2-layer lstm and task-specific state encoders for the external environments, as described in reed & de freitas (2016)",0
this material is in part based upon work supported by the national science foundation under grant no,0
" the move operation has the following arguments:
arg 1 (pointer): presult, pstack, pstart, childlist[vactive], childlist[vsave]
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 5 values",1
" as with the others, we apply the procedure described in section 3",0
e,0
"
one common strategy to improve generalization is to use curriculum learning, where the model is trained on inputs of gradually increasing complexity",0
 table 3 presents results on randomly generated arrays of varying length for the learned non-recursive and recursive programs,0
", to color a vertex) or variable (e",0
we emphasize that the notion of a neural recursive program has not been presented in the literature before: this is our main contribution,1
" we prove every member of f is added properly, given that subproblems s = {anan−1 + bnbn−1, an−1an−2 + bn−1bn−2, ",0
"
for addition, we analytically determine the verification set",0
 we describe the recursive re-formulation of traces for two tasks from the original npi paper—grade-school addition and bubble sort,1
" we then construct problem configurations that span all possible valid environment states (for instance, in order to force the carry bit in a column to be 1, one can add the sum “1+9” in the column to the right)",1
 these pointers are referred to as bubble pointers,1
 the first is the actual model architecture,1
" at each time step, the core controller can decide to select one of the programs to call with certain arguments",0
" note that recursion helps make this process tractable, because we only need to test a finite number of inputs to show that the model will work correctly on inputs of unbounded complexity",1
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,0
" given verification sets that cover all the base cases and reduction rules, we can provide proofs that these learned programs generalize perfectly",1
" we further have qresult ∈ nu , a scratch-pad which contains the sorted list of vertices at the end of execution, and qstack ∈ nu , which serves the role of the stack s in algorithm 2",1
" we show that by modifying the training procedure, we enable the npi model to learn recursive neural programs",0
" for many problems, the base cases and reduction rules usually consist of a finite (often small) number of cases",0
g,1
"as discussed in section 2, the neural programmer-interpreter (npi) is an instance of a neural programmer architecture and hence it naturally supports recursion",1
 ,1
" we made the assumption of equivalent lengths in order to parametrize the input format with respect to length, but this assumption can be removed as well",0
" if a white vertex is found, set pstart to point to it; this signifies the start of a traversal of a new connected component",0
" the stack operation has the following arguments:
arg 1 (operation): stack push call1, stack push call2, stack pop
stack push call1 pushes lo and pivot−1 toqstacklo andqstackhi",0
" for brevity, we refer the reader to the appendix for information about the program set and non-recursive and recursive trace-generating functions for quicksort",1
" it is straightforward to deal with all other operations, which do not induce partial environment states",1
"
in order to demonstrate that recursion can help learn and generalize better, for addition, we trained only on traces for 5 arbitrarily chosen 1-digit addition sum examples",0
we emphasize that the notion of a neural recursive program has not been presented in the literature before: this is our main contribution,0
"
the same set of problems was used to generate the training traces for all formulations of the task, for non-recursive and recursive versions",0
"
move",1
g,0
" at any given time, the npi has access to the values referred to by two pointers, represented by q(1, i1) and q(1, i2),",0
 the core controller acts as a dispatcher for the programs,1
" empirically, existing models suffer from a common limitation—generalization becomes poor beyond a threshold level of complexity",1
g,1
6,1
g,0
" if the step output specifies a primitive function call, we need to reason about how it can affect the environment so as to change the observation in the next step input",1
"as there is no public implementation of npi, we implemented a version of it in keras that is as faithful to the paper as possible",0
" , a1a0 + b1b0} are added properly",1
 ,0
" however, models that make use of this strategy eventually fail after a certain level of complexity (e",1
"
the training set for topological sort contains 6 traces, with one synthesized from a graph of size 5 and the rest synthesized from graphs of size 7",0
" for each length, we test each program on 30 randomly generated problems",1
 ,0
" in particular, this condition is satisfied by npi, and thus the npi model naturally supports recursion (even though the authors of npi did not consider this aspect explicitly)",0
" the learned recursive program is different from neural programs learned in all previous work in an important aspect: previous approaches do not explicitly incorporate this abstraction, and hence generalize poorly, whereas our learned neural programs incorporate recursion and achieve perfect generalization",1
 some functional programming languages go so far as not to define any looping constructs but rely solely on recursion to enable repeated execution of the same code,0
 recursion can be implemented differently for different neural programming models,0
 the degree for any vertex in the dag is variable,1
 recursion can be implemented differently for different neural programming models,1
"1, we hypothesize the non-recursive programs do not generalize well because they have learned spurious dependencies specific to the training set, such as length of the input problems",1
" there are 3 pointers (presult, pstack, pstart), presult points to the next empty location in qresult,
pstack points to the top of the stack in qstack, and pstart points to the candidate starting node for a connected component",1
" formally, the npi is represented by the following set of equations:
st = fenc(et, at)
ht = flstm(st, pt, ht−1)
rt = fend(ht), pt+1 = fprog(ht), at+1 = farg(ht)
t is a subscript denoting the time-step; fenc is a domain-specific encoder (to be described later) that takes in the environment slice et and arguments at; flstm represents the core module, which takes in the state st generated by fenc, a program embedding pt ∈ rp , and hidden lstm state ht; fend decodes the return probability rt; fprog decodes a program key embedding pt+1;1 and farg decodes arguments at+1",0
" however, our concept of recursion for neural programs is general",0
" the core lstm module’s hidden state is preserved over all these add1 calls, and it is difficult to interpret with certainty what happens over longer timesteps without concretely evaluating the lstm with an input of that length",1
 table 3 presents results on randomly generated arrays of varying length for the learned non-recursive and recursive programs,0
g,0
 the environment and return probability are omitted for readability,1
 the original version of the bubblesort implementation exposes the values within the array,0
 the logic for the non-recursive trace is shown in algorithm 4 in the appendix,1
" in all experiments, α is set to 0",1
"
we note that for tasks with very large input domains, such as ones involving mnist digits or speech samples, the state space of base cases and reduction rules could be prohibitively large, possibly infinite",0
we emphasize the overall goal of this work is to enable the learning of a recursive program,1
" m , as described in algorithm 1, processes the sequence of step inputs by using an lstm",1
 ,1
as mentioned in section 2,1
 a1a0 + bnbn−1 ,0
3 to construct v and then empirically create a verification set which covers v ,1
" numerous models have been proposed for learning programs; to name a few, this includes the differentiable neural computer (graves  , 2016), neural turing machine (graves  , 2014), neural gpu (kaiser & sutskever, 2015), neural programmer (neelakantan  , 2015), pointer network (vinyals  , 2015), hierarchical attentive memory (andrychowicz & kurach, 2016), and neural random access machine (kurach  , 2016)",0
" in particular, this condition is satisfied by npi, and thus the npi model naturally supports recursion (even though the authors of npi did not consider this aspect explicitly)",0
"
to evaluate a neural network that learns a neural program to accomplish a certain task, one common evaluation metric is how well the learned model m generalizes",0
" nevertheless, we would like the model to quickly learn to represent the right semantics of the underlying program from a small number of training examples, not an exhaustive number of them",1
"1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 while (pstacklo 6= 1): 6 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 7 stack(stack_pop) 8 else: 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 stack(stack_pop) 12 partition() 13 stack(stack_push_call2) 14 stack(stack_push_call1) 15 } 16 17 partition() { 18 set_pivot_lo() 19 set_j_lo() 20 compswap_loop() 21 swap(ppivot, phi) 22 set_j_null() 23 } 24 25 compswap_loop() { 26 while (pj 6= phi): 27 compswap() 28 move(pj, up) 29 } 30 31 compswap() { 32 if (a[pj ] ≤ a[phi]): 33 swap(ppivot, pj) 34 move(ppivot, up) 35 } 36 37 stack(op) { 38 if (op == stack_push_call1): 39 write(env_stack_lo, plo) 40 write(env_stack_hi, ppivot − 1) 41 move(pstacklo, up) 42 move(pstackhi, up) 43 44 if (op == stack_push_call2): 45 write(env_stack_lo, ppivot + 1) 46 write(env_stack_hi, phi) 47 move(pstacklo, up) 48 move(pstackhi, up) 49 50 if (op == stack_pop): 51 write(env_stack_lo, reset) 52 write(env_stack_hi, reset) 53 move(pstacklo, down) 54 move(pstackhi, down) 55 }altered recursive functions 1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 6 partition() 7 stack(stack_push_call2) 8 stack(stack_push_call1) 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 quicksort() // recursive call 12 stack(stack_pop) 13 write(phi, env_stack_hi_peek) 14 write(plo, env_stack_lo_peek) 15 quicksort() // recursive call 16 stack(stack_pop) 17 } 18 19 compswap_loop() { 20 if (pj 6= phi): 21 compswap() 22 move(pj, up) 23 compswap_loop() // recursive call 24 }in this section, we describe the space of base cases and reduction rules that must be covered for each of the four sample tasks, in order to create the verification set",1
"
inside bubble and reset, there are two operations that can be made recursive",0
" in this section, we give a brief review of the npi architecture from reed & de freitas (2016) as background",1
" in practice, we can additionally use tail recursion optimization to avoid problems with the call stack growing too deep",1
 we describe the recursive re-formulation of traces for two tasks from the original npi paper—grade-school addition and bubble sort,0
" the 4 environment observations aid with control flow; qstacklo(pstacklo− 1) < qstackhi(pstackhi− 1) implements the lo < hi comparison in line 5 of algorithm 3, pstacklo == 1 checks if the stacks are empty in line 18 of algorithm 4, and the other observations (all involving ppivot or pj) deal with logic in the partition function",0
" while this matches the description from reed & de freitas (2016), we found that this causes an unnecessary blowup in the size of v and makes it much more difficult to construct the verification set",0
" , a1a0 + b1b0} are added properly",1
"
arg 2 (auxiliary variable): color grey, color black
color grey and color black color the given vertex grey and black, respectively",0
 some functional programming languages go so far as not to define any looping constructs but rely solely on recursion to enable repeated execution of the same code,1
" the move operation has the following arguments:
arg 1 (pointer): pstacklo, pstackhi, pj , ppivot
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values",0
 b1b0 where no carry operations occur,1
" this guarantees that the step input sequences considered during verification contain all step input sequences which arise during execution of an unseen problem in f , leading to generalization to any problem in f ",0
" we have three observations: the color of the start node, the color of the active node’s next child to be considered, and whether the stack is empty",0
 figure 2 shows examples of traces for the different versions of bubble sort,1
" k is set to 11, to denote the range of possible numbers (0 through 9), along with the start/end token (represented with the same encoding) which is observed when a pointer reaches beyond the bounds of the input",0
" as this complexity interfered with determining the base cases and reductions, we changed the algorithm to its current form",0
"
by nature, recursion reduces the complexity of a problem to simpler instances",0
"program descriptions calls arguments toposort perform topological
sort on graph traverse, next start, write, move
none
traverse traverse graph until stack is empty check child, explore none check child check if a white child exists; if so, set childlist[vactive] to point to it move none explore repeatedly traverse subgraphs until stack is empty stack, check child, write, move none stack interact with stack, either pushing or popping write, move push, pop next start move pstart until reaching a white vertex",0
" full recursion, in addition to making the aforementioned tail recursive call, adds two additional recursive calls; bstep and lshift are made tail recursive",0
" if it is too small, then execution of the program on some input might require evaluation of m(i) on some i /∈ v , and so the behavior ofm(i) might deviate from p (i)",0
", pstart or childlist[vactive]) up or down none described belowwrite",0
training neural networks to synthesize robust programs from a small number of examples is a challenging task,1
g,0
"
we experiment with two levels of recursion—partial and full",0
" naı̈vely, we might expect to synthesize and test an input for any sequence created by combining the four possible colors in two variables and another boolean variable for whether the stack is empty (so 32 possible observations at any point), but for various reasons, most of these combinations are impossible to occur at any given point in the execution trace",1
"as there is no public implementation of npi, we implemented a version of it in keras that is as faithful to the paper as possible",0
"
in this paper, we propose to resolve these issues by explicitly incorporating recursion into neural architectures",0
" fa8750-15-2-0104, and berkeley deep drive",1
" as before, the non-recursive learned program lacks a provable guarantee of generalization, whereas we show later that the recursive learned program has one",0
"
given a verification set, we can then run the model on the verification set to check if the produced traces and results are correct",1
"
the same set of problems was used to generate the training traces for all formulations of the task, for non-recursive and recursive versions",0
"
quicksort",0
"
training setup",0
2,0
"
quicksort",1
"
we would also like to point out that in this paper, we provide as an example one way to train a recursive neural program, by providing a certain training execution trace to the npi model",0
" empirically, existing models suffer from a common limitation—generalization becomes poor beyond a threshold level of complexity",0
" the general notion of recursion has been an important concept in many domains, including mathematics and computer science",0
" as a consequence, our learned neural programs empirically achieve perfect generalization from a very small number of training examples",0
"
to construct this set, by using the reference implementation of each subprogram, we construct a mapping between two sets of environment observations: the first set consists of all observations that can occur at the beginning of a particular subprogram’s invocation, and the second set contains the observations at the end of that subprogram",0
 this algorithm is a variant of depth first search,0
"
the npi accesses an external environment, q, which varies according to the task",1
" in all experiments, α is set to 0",0
"we show that if we incorporate recursion, the learned npi programs can achieve provably perfect generalization for different tasks",0
" 2: initialize lo and hi to be 1 and n, where n is the length of a",0
" ppivot and pj point to the pivot and j indices of the array, used in the partition function in algorithm 3",1
 a1a0 + bnbn−1 ,1
" we implement a quicksort task, in order to demonstrate that recursion helps with learning divide-and-conquer algorithms",1
 table 3 presents results on randomly generated arrays of varying length for the learned non-recursive and recursive programs,0
" in particular, recursion can be implemented as a program calling itself",0
", to change the value of phi) none described belowstack",0
g,0
" fa8750-15-2-0104, and berkeley deep drive",1
3,1
" , a1a0 + b1b0} are added properly",1
" furthermore, given a verification input set that covers all base cases and reduction rules, we can formally prove that the learned neural programs achieve perfect generalization after verifying its behavior on the verification input set",0
" 3: 4: function partition(a, lo, hi) 5: pivot = lo 6: for j ∈ [lo, hi− 1] : do 7: if a[j] ≤ a[hi] then 8: swap a[pivot] with a[j] 9: pivot = pivot+ 1 0: swap a[pivot] with a[hi] 11: return pivot 12: 13: function quicksort(a, lo, hi) 14: while slo and shi are not empty: do 15: pop states off slo and shi, writing them to lo and hi",1
 ,0
" for bubble sort, the domain-specific encoder is
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1), q(1, i2), i3 == length, at(1), at(2), at(3)]),
where the environment q ∈ r1×n×k is a scratch-pad that contains 1 row, to represent the state of the array as sorting proceeds in-place, andn columns",1
g,0
" the recursive version of topological sort solves all graph instances we tried, from graphs of size 5 through 70",0
" our results show that for all learned, to-be-verified neural programs, they all produced the same traces as those produced by p on the verification set",1
" if no white vertex is found, the entire execution is terminated move none write write a value either to environment (e",0
" if yes, then this indicates that the learned neural program achieves provably perfect generalization",0
g,0
we emphasize the overall goal of this work is to enable the learning of a recursive program,0
 there is a (changing) list of neural programs used to accomplish a given task,1
" thus, we demonstrate that recursion enables provably perfect generalization for different tasks, including addition, topological sort, quicksort, and a variant of bubble sort",0
" the general notion of recursion has been an important concept in many domains, including mathematics and computer science",0
" in order to compensate for this, ppivot and pj require special functions (move pivot lo and move j lo) to properly set them to lo in lines 11 and 12 of the partition function in algorithm 3",0
", to color a vertex) or variable (e",1
" we further have qresult ∈ nu , a scratch-pad which contains the sorted list of vertices at the end of execution, and qstack ∈ nu , which serves the role of the stack s in algorithm 2",0
" for instance, for recursive addition, consider the family f of addition problems anan−1 ",0
" bstep, used in bubble, compares pairs of numbers, continuously moving the bubble pointers once to the right each time until reaching the end of the array",1
", the trace corresponding to the array [3,2])",0
"
arg 2 (increment or decrement): up, down
swap",0
"
algorithm 1 neural programming inference 1: inputs: environment observation e, program p, arguments a, stop threshold α 2: function run(e, p, a) 3: h← 0, r ← 0 4: while r < α do 5: s← fenc(e, a), h← flstm(s, p, h) 6: r ← fend(h), p2 ← fprog(h), a2 ← farg(h) 7: if p is a primitive function then 8: e← fenv(e, p, a)",0
 almost all architectures train on program input/output pairs,1
" as aforementioned, the npi model naturally supports recursion",0
" each step during an execution of the program does one of three things: (1) another subprogram along with associated arguments is called, as in line 10, (2) the program writes to the environment if it is primitive, as in line 8, or (3) the loop is terminated if the return probability exceeds a threshold α, after which the stack frame is popped and control is returned to the caller",1
 ,0
" at end, pointer ppivot is moved to the pivot compswap loop, move pivot lo, move j lo, swap
none
compswap loop runs the for loop inside the partition function compswap, move none compswap compares a[pivot] ≤ a[j]; if so, perform a swap and increment ppivot swap, move none set pivot lo sets ppivot to lo index none none set j lo sets pj to lo index none none set j null sets pj to −∞ none none stack pushes lo/hi states
onto stacks slo and shi according to argument (described below)
write, move described below
move moves pointer one unit up or down none described below swap swaps elements at given array indices none described below write write a value either to stack (e",0
" finally, the boolean pstack == 1 is used to check whether the stack is empty in line 18 of algorithm 2",1
 ,1
" given a precise definition of p , it may be possible to automate the generation of v from p in future work",1
 the model described in the training setup of section 4 (trained on 6 traces) was verified to be correct via the matching procedure described in section 4,1
"
the training set for topological sort contains 6 traces, with one synthesized from a graph of size 5 and the rest synthesized from graphs of size 7",0
" there are 3 pointers (presult, pstack, pstart), presult points to the next empty location in qresult,
pstack points to the top of the stack in qstack, and pstart points to the candidate starting node for a connected component",1
"
through careful reasoning about the possible set of environment observations created by all valid inputs, and how each of the operations in the execution trace affects the environment, we can construct v using the procedure described in section 3",1
" in this section, we give a brief review of the npi architecture from reed & de freitas (2016) as background",0
"
base cases and reduction rules for topological sort",0
 this procedure verifies that all base cases and reductions are handled properly by the model via explicit tests,1
" numerous models have been proposed for learning programs; to name a few, this includes the differentiable neural computer (graves  , 2016), neural turing machine (graves  , 2014), neural gpu (kaiser & sutskever, 2015), neural programmer (neelakantan  , 2015), pointer network (vinyals  , 2015), hierarchical attentive memory (andrychowicz & kurach, 2016), and neural random access machine (kurach  , 2016)",1
" for each length, we test each program on 30 randomly generated problems",0
" recall that this verification procedure cannot be performed for the non-recursive versions, since the propagation of the hidden state in the core lstm module makes reasoning difficult and so we would need to check an unbounded number of examples",0
" we made the assumption of equivalent lengths in order to parametrize the input format with respect to length, but this assumption can be removed as well",0
" 9: else 10: function run(e, p2, a2)
a description of the inference procedure is given in algorithm 1",1
"
recursion drastically reduces the number of configurations we need to consider during the verification phase and makes the proof tractable, because it introduces structure that eliminates infinitely long sequences of step inputs that would otherwise need to be considered",0
 ,0
"
in this paper, we propose that recursion is an important concept for neural programs as well",0
" for each function we use to implement the recursive version of topological sort, we need to consider the set of possible environment observation sequences we can create from all valid inputs and test that the learned program produces the correct behavior on each of these inputs",1
" however, models that make use of this strategy eventually fail after a certain level of complexity (e",1
" in this version of curriculum learning, even though the inputs are gradually becoming more complex, the semantics of the program is succinct and does not change",0
" also, compared to the bubble pointers in bubble sort, the pointers that perform the comparison for quicksort (the compswap function) are usually not adjacent to each other, making quicksort less local than bubble sort",1
"
the non-recursive trace loops on cycles of add1 and lshift",0
" this is possible if and only if the graph has no directed cycles; that is to say, it must be a directed acyclic graph (dag)",1
"program descriptions calls arguments quicksort run the quicksort
routine in place for the array a, for indices from lo to hi non-recursive: partition, stack, write recursive: same as non-recursive version, along with quicksort implicitly: array a to sort, lo, hi
partition runs the partition function",0
" the single-digit multiplication task in zaremba   (2016), the bubble sort task in reed & de freitas (2016), and the graph tasks in graves   (2016))",1
"
in addition, none of the current approaches to neural programming provide a method or even aim to enable provable guarantees about generalization",0
" as this complexity interfered with determining the base cases and reductions, we changed the algorithm to its current form",1
" if the step output specifies a primitive function call, we need to reason about how it can affect the environment so as to change the observation in the next step input",0
"
the dag is represented as an adjacency list where dag[i][j] refers to the j-th child of vertex i",0
 18: push lo and p− 1 to slo and shi,1
 there is a (changing) list of neural programs used to accomplish a given task,1
 our experiments use a small number of training examples,0
" as with the others, we apply the procedure described in section 3",1
 this procedure verifies that all base cases and reductions are handled properly by the model via explicit tests,1
" this is possible if and only if the graph has no directed cycles; that is to say, it must be a directed acyclic graph (dag)",1
"
in this general neural programming architecture, we show it is easy to support recursion",1
" however, the authors of npi did not consider explicitly the notion of recursion and as a consequence, did not learn recursive programs",0
"
the same set of problems was used to generate the training traces for all formulations of the task, for non-recursive and recursive versions",1
" childlist ∈ nu is a vector of pointers, where childlist[i] points to the next child under consideration for vertex i",0
" training on the full recursive trace leads to perfect generalization, as shown in section 4",1
 3: begin traversing from vertex 1 in the dag,1
 npi then outputs the return probability and next program and arguments to execute,0
" we further have qresult ∈ nu , a scratch-pad which contains the sorted list of vertices at the end of execution, and qstack ∈ nu , which serves the role of the stack s in algorithm 2",1
"
given a verification set, we can then run the model on the verification set to check if the produced traces and results are correct",1
3,1
" as a consequence, our learned neural programs empirically achieve perfect generalization from a very small number of training examples",1
" qcolor(pstart) contains the color of the current start vertex, used in the evaluation of the condition in the while loop in line 5 of algorithm 2",0
" thus, any general neural programming architecture supporting such a call structure can be made to support recursion",0
" however, models that make use of this strategy eventually fail after a certain level of complexity (e",0
" for each graph size, we test the learned programs on 30 randomly generated dags",1
"6) on the verification set, and ensure they match the traces produced by the true program p ",0
"
note that the recursion for quicksort is not purely tail recursive and therefore represents a more complex kind of recursion that is harder to learn than in the previous tasks",1
" we call this set of inputs the verification set, sv ",0
"
move",1
" as this complexity interfered with determining the base cases and reductions, we changed the algorithm to its current form",0
"we train using the adam optimizer and use a 2-layer lstm and task-specific state encoders for the external environments, as described in reed & de freitas (2016)",0
 these pointers are referred to as bubble pointers,0
" the second is the training procedure, which consists of the form of the training data and the optimization process",0
"
arg 2 (increment or decrement): up, down1 // top level topological sort call 2 toposort() { 3 while (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 } 0 11 traverse() { 12 check_child() 13 explore() 14 } 15 16 check_child() { 17 while (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 18 move(childlist[vactive], up) 19 } 20 21 explore() { 22 do 23 if (qcolor(dag[vactive][childlist[vactive]]) is white): 24 write(color_next, color_grey) 25 stack(push) 26 write(save) 27 write(active_neighb) 28 move(childlist[vsave], up) 29 else: 30 write(color_curr, color_black) 31 write(result) 32 move(presult, up) 33 if(pstack == 1): 34 break 35 else: 36 stack(pop) 37 check_child() 38 while (true) 39 } 40 41 stack(op) { 42 if (op == push): 43 write(stack_push) 44 move(pstack, up) 45 46 if (op == pop): 47 write(active_stack) 48 write(stack_pop) 49 move(pstack, down) 50 } 51 52 next_start() { 53 while(qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 54 move(pstart, up) 55 }altered recursive functions  // top level topological sort call 2 toposort() { 3 if (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 toposort() // recursive call 0 } 11 12 check_child() { 13 if (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 14 move(childlist[vactive], up) 15 check_child() // recursive call 16 } 17 18 explore() { 19 if (qcolor(dag[vactive][childlist[vactive]]) is white): 20 write(color_next, color_grey) 21 stack(push) 22 write(save) 23 write(active_neighb) 24 move(childlist[vsave], up) 25 else: 26 write(color_curr, color_black) 27 write(result) 28 move(presult, up) 29 if(pstack == 1): 30 return 31 else: 32 stack(pop) 33 check_child() 34 explore() // recursive call 35 } 36 37 next_start() { 38 if (qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 39 move(pstart, up) 40 next_start() // recursive call 41 }algorithm 4 iterative quicksort 1: initialize an array a to sort and two empty stacks slo and shi",0
" for each function we use to implement the recursive version of topological sort, we need to consider the set of possible environment observation sequences we can create from all valid inputs and test that the learned program produces the correct behavior on each of these inputs",0
" table 1 presents results on randomly generated arrays of varying length for the learned non-recursive, partially recursive, and full recursive programs",1
" each time a subprogram is called, the stack depth increases",0
" the recursive version of topological sort solves all graph instances we tried, from graphs of size 5 through 70",0
" this makes it so that we only need to consider the vertices in the subgraph that are currently relevant for computing the sort, allowing simpler reasoning about behavior for large graphs",0
"
the non-recursive trace loops on cycles of add1 and lshift",1
" for each graph size, we test the learned programs on 30 randomly generated dags",0
" for grade-school addition, the domain-specific encoder is
fenc(q, i1, i2, i3, i4, at) =mlp ([q(1, i1), q(2, i2), q(3, i3), q(4, i4), at(1), at(2), at(3)]),
where the environment q ∈ r4×n×k is a scratch-pad that contains four rows (the first input number, the second input number, the carry bits, and the output) and n columns",1
 the maximum problem length in this training set is 3 (e,0
"6) on the verification set, and ensure they match the traces produced by the true program p ",0
" formally, the npi is represented by the following set of equations:
st = fenc(et, at)
ht = flstm(st, pt, ht−1)
rt = fend(ht), pt+1 = fprog(ht), at+1 = farg(ht)
t is a subscript denoting the time-step; fenc is a domain-specific encoder (to be described later) that takes in the environment slice et and arguments at; flstm represents the core module, which takes in the state st generated by fenc, a program embedding pt ∈ rp , and hidden lstm state ht; fend decodes the return probability rt; fprog decodes a program key embedding pt+1;1 and farg decodes arguments at+1",1
" in order to claim a proof, we must verify the model produces correct behavior over all base cases and reductions, as described in section 2",0
 b1b0 where no carry operations occur,0
"
formally, verification consists of proving the following theorem:
∀i ∈ v,m(i) ⇓ p (i) where i denotes a sequence of step inputs (within one function call), v denotes the set of valid sequences of step inputs, m denotes the neural network model, p denotes the correct program, and p (i) denotes the next step output from the correct program",0
" for tasks other than addition, it is difficult to analytically determine the verification set, so instead, we randomly generate input candidates until they completely cover the base cases and reduction rules",0
 there is a (changing) list of neural programs used to accomplish a given task,0
 reset represents a −∞ value,1
" if no white vertex is found, the entire execution is terminated move none write write a value either to environment (e",0
 there are 2 variables (vactive and vsave); vactive holds the active vertex (as in algorithm 2) and vsave holds the value of vactive before executing line 12 of algorithm 2,1
 add1 is a subprogram that adds the current column (writing the appropriate digit to the output row and carrying a bit to the next column if needed),0
" for brevity, we refer the reader to the appendix for further details on the program set and non-recursive and recursive trace-generating functions used for topological sort",0
" in this section, we give a brief review of the npi architecture from reed & de freitas (2016) as background",0
" at each time step, the core controller can decide to select one of the programs to call with certain arguments",1
"
the same set of problems was used to generate the training traces for all formulations of the task, for non-recursive and recursive versions",0
"when constructing a neural network for the purpose of learning a program, there are two orthogonal aspects to consider",0
" for brevity, we refer the reader to the appendix for information about the program set and non-recursive and recursive trace-generating functions for quicksort",0
" each time a subprogram is called, the stack depth increases",1
" as described in the verification procedure, it is possible to prove our learned recursive program generalizes perfectly by testing on an appropriate set of problem inputs, i",1
" the core lstm module’s hidden state is preserved over all these add1 calls, and it is difficult to interpret with certainty what happens over longer timesteps without concretely evaluating the lstm with an input of that length",0
" we also hope to decrease supervision, for example by training with only partial or non-recursive traces, and to develop novel neural programming architectures integrated directly with a notion of recursion",0
training neural networks to synthesize robust programs from a small number of examples is a challenging task,1
"
the non-recursive trace loops on cycles of bubble and reset, which logically represents one bubble sweep through the array and reset of the two bubble pointers to the very beginning of the array, respectively",0
" plo and phi point to the lo and hi indices of the array, as in algorithm 3",0
" this is highly undesirable, since being able to provide the correct answer in all possible settings is one of the most important aspects of any learned neural program",1
" the partial recursion is not enough for perfect generalization, as will be presented later in section 4",0
"
move",0
"
as mentioned in section 1, all approaches to neural programming today fare poorly on this generalization issue",0
 there are 2 variables (vactive and vsave); vactive holds the active vertex (as in algorithm 2) and vsave holds the value of vactive before executing line 12 of algorithm 2,0
", to change the value of vactive) none described below move move a pointer (e",0
6,1
" for many problems, the base cases and reduction rules usually consist of a finite (often small) number of cases",0
"1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 while (pstacklo 6= 1): 6 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 7 stack(stack_pop) 8 else: 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 stack(stack_pop) 12 partition() 13 stack(stack_push_call2) 14 stack(stack_push_call1) 15 } 16 17 partition() { 18 set_pivot_lo() 19 set_j_lo() 20 compswap_loop() 21 swap(ppivot, phi) 22 set_j_null() 23 } 24 25 compswap_loop() { 26 while (pj 6= phi): 27 compswap() 28 move(pj, up) 29 } 30 31 compswap() { 32 if (a[pj ] ≤ a[phi]): 33 swap(ppivot, pj) 34 move(ppivot, up) 35 } 36 37 stack(op) { 38 if (op == stack_push_call1): 39 write(env_stack_lo, plo) 40 write(env_stack_hi, ppivot − 1) 41 move(pstacklo, up) 42 move(pstackhi, up) 43 44 if (op == stack_push_call2): 45 write(env_stack_lo, ppivot + 1) 46 write(env_stack_hi, phi) 47 move(pstacklo, up) 48 move(pstackhi, up) 49 50 if (op == stack_pop): 51 write(env_stack_lo, reset) 52 write(env_stack_hi, reset) 53 move(pstacklo, down) 54 move(pstackhi, down) 55 }altered recursive functions 1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 6 partition() 7 stack(stack_push_call2) 8 stack(stack_push_call1) 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 quicksort() // recursive call 12 stack(stack_pop) 13 write(phi, env_stack_hi_peek) 14 write(plo, env_stack_lo_peek) 15 quicksort() // recursive call 16 stack(stack_pop) 17 } 18 19 compswap_loop() { 20 if (pj 6= phi): 21 compswap() 22 move(pj, up) 23 compswap_loop() // recursive call 24 }in this section, we describe the space of base cases and reduction rules that must be covered for each of the four sample tasks, in order to create the verification set",0
" in order to claim a proof, we must verify the model produces correct behavior over all base cases and reductions, as described in section 2",0
" 2: initialize lo and hi to be 1 and n, where n is the length of a",0
" for these models, table 2 presents results on randomly generated dags of varying graph sizes (varying in the number of vertices)",1
g,1
 we then construct a verification set of size 73 by ensuring that randomly generated graphs cover the analytically derived v ,0
 ,1
 the memory updates of these neural programs are so complex and interdependent that it is difficult to reason about the behaviors of the learned neural program under previously unseen situations (such as problems with longer inputs),1
" observe that partially recursive does slightly better than non-recursive for the setting in which the length of the array is 3, and that the fully recursive version is able to sort every array given to it",0
g,0
"as there is no public implementation of npi, we implemented a version of it in keras that is as faithful to the paper as possible",1
3,0
"
base cases and reduction rules for bubble sort",0
 the program terminates when seeing no numbers in the current column,0
" the general notion of recursion has been an important concept in many domains, including mathematics and computer science",1
" note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values, and arg 2 can only take one of 7 values",0
" finally, the boolean pstack == 1 is used to check whether the stack is empty in line 18 of algorithm 2",1
"when constructing a neural network for the purpose of learning a program, there are two orthogonal aspects to consider",0
 there are 2 variables (vactive and vsave); vactive holds the active vertex (as in algorithm 2) and vsave holds the value of vactive before executing line 12 of algorithm 2,1
"
in order to demonstrate that recursion can help learn and generalize better, we trained a non-recursive and recursive model on just a single execution trace generated from a graph containing 5 nodes3 for the topological sort task",1
"
we propose and describe our verification procedure",1
" in fact, we argue that recursion is an essential element for neural programs to generalize, and makes it tractable to prove the generalization of neural programs",0
 this is due to the properties of the training set – we trained on 2 traces synthesized from arrays of length 7 and 1 trace synthesized from an array of length 6,0
" qcolor(pstart) contains the color of the current start vertex, used in the evaluation of the condition in the while loop in line 5 of algorithm 2",0
" in computer science, recursion (as opposed to iteration) involves solving a larger problem by combining solutions to smaller instances of the same problem",1
" , n , where the dag contains n vertices",0
" the learned recursive program is different from neural programs learned in all previous work in an important aspect: previous approaches do not explicitly incorporate this abstraction, and hence generalize poorly, whereas our learned neural programs incorporate recursion and achieve perfect generalization",1
" even though the earlier version also generalized just as well in practice, relatively small differences in the formulation of the traces and the environment observations can drastically change the difficulty of verification",0
 the core controller acts as a dispatcher for the programs,0
" the pointers at index i1 and i2 are used to compare the pair of numbers considered during the bubble sweep, swapping them if the number at i1 is greater than that in i2",1
" for instance, for recursive addition, consider the family f of addition problems anan−1 ",0
 we created a program set that reflects the semantics of algorithm 2,1
training neural networks to synthesize robust programs from a small number of examples is a challenging task,1
"
we note that for tasks with very large input domains, such as ones involving mnist digits or speech samples, the state space of base cases and reduction rules could be prohibitively large, possibly infinite",0
" the corresponding edge list is [(1, 2), (1, 5), (2, 4), (2, 5), (3, 5)]",0
", qstacklo or qstackhi) or to pointer (e",0
 a1a0 + bnbn−1 ,1
"
we propose and describe our verification procedure",1
6,0
"
the training set for topological sort contains 6 traces, with one synthesized from a graph of size 5 and the rest synthesized from graphs of size 7",0
"
it is important to emphasize that at inference time in the npi, the hidden state of the lstm controller is reset (to zero) at each subprogram call, as in line 3 of algorithm 1 (h ← 0)",1
"
the non-recursive trace loops on cycles of bubble and reset, which logically represents one bubble sweep through the array and reset of the two bubble pointers to the very beginning of the array, respectively",1
"program descriptions calls arguments quicksort run the quicksort
routine in place for the array a, for indices from lo to hi non-recursive: partition, stack, write recursive: same as non-recursive version, along with quicksort implicitly: array a to sort, lo, hi
partition runs the partition function",1
training neural networks to synthesize robust programs from a small number of examples is a challenging task,0
" this is highly undesirable, since being able to provide the correct answer in all possible settings is one of the most important aspects of any learned neural program",1
" at any given time, the npi has access to the values referred to by two pointers, represented by q(1, i1) and q(1, i2),",1
" we can obtain this mapping by first considering the possible observations that can arise at the beginning of the entry function (add, bubblesort, toposort, and quicksort) for some valid program input, and iteratively applying the observation-to-observation mapping implied by the reference implementation’s step output at that point in the execution",0
" as an application, we incorporate recursion into the neural programmer-interpreter architecture and consider four sample tasks: grade-school addition, bubble sort, topological sort, and quicksort",0
" hence, if all subproblems in s are added correctly, we have proven that any member of f will be added correctly, thus eliminating an infinite family of inputs that need to be tested",1
"
in this paper, we propose to resolve these issues by explicitly incorporating recursion into neural architectures",0
"we describe how models trained with recursive traces can be proven to generalize, by using the verification procedure described in section 3",0
" in this pair, the step input runs a subprogram add1 that has no arguments, and the step output contains a program write that has arguments of out and 1",0
" we call this set of inputs the verification set, sv ",1
" the outputs rt, pt+1, at+1 are used to determine the next action, as described in algorithm 1",0
" at any given time, the npi has access to the values referred to by two pointers, represented by q(1, i1) and q(1, i2),",1
"1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 while (pstacklo 6= 1): 6 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 7 stack(stack_pop) 8 else: 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 stack(stack_pop) 12 partition() 13 stack(stack_push_call2) 14 stack(stack_push_call1) 15 } 16 17 partition() { 18 set_pivot_lo() 19 set_j_lo() 20 compswap_loop() 21 swap(ppivot, phi) 22 set_j_null() 23 } 24 25 compswap_loop() { 26 while (pj 6= phi): 27 compswap() 28 move(pj, up) 29 } 30 31 compswap() { 32 if (a[pj ] ≤ a[phi]): 33 swap(ppivot, pj) 34 move(ppivot, up) 35 } 36 37 stack(op) { 38 if (op == stack_push_call1): 39 write(env_stack_lo, plo) 40 write(env_stack_hi, ppivot − 1) 41 move(pstacklo, up) 42 move(pstackhi, up) 43 44 if (op == stack_push_call2): 45 write(env_stack_lo, ppivot + 1) 46 write(env_stack_hi, phi) 47 move(pstacklo, up) 48 move(pstackhi, up) 49 50 if (op == stack_pop): 51 write(env_stack_lo, reset) 52 write(env_stack_hi, reset) 53 move(pstacklo, down) 54 move(pstackhi, down) 55 }altered recursive functions 1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 6 partition() 7 stack(stack_push_call2) 8 stack(stack_push_call1) 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 quicksort() // recursive call 12 stack(stack_pop) 13 write(phi, env_stack_hi_peek) 14 write(plo, env_stack_lo_peek) 15 quicksort() // recursive call 16 stack(stack_pop) 17 } 18 19 compswap_loop() { 20 if (pj 6= phi): 21 compswap() 22 move(pj, up) 23 compswap_loop() // recursive call 24 }in this section, we describe the space of base cases and reduction rules that must be covered for each of the four sample tasks, in order to create the verification set",0
" however, note that there is no provable guarantee that the non-recursive learned program will generalize to all inputs, whereas we show later that the recursive learned program has a provable guarantee of perfect generalization",0
" the pointers at index i1 and i2 are used to compare the pair of numbers considered during the bubble sweep, swapping them if the number at i1 is greater than that in i2",0
 ,1
 recursion can be implemented differently for different neural programming models,0
"
given a verification set, we can then run the model on the verification set to check if the produced traces and results are correct",0
" this functionality the original npi paper decodes to a program key embedding kt ∈ rk and then computes a program embedding pt+1, which we also did in our implementation, but we omit this for brevity",0
"
note that v should also contain the necessary reductions, which corresponds to making the recursive calls at the correct time, as indicated by p ",0
" we call this set of inputs the verification set, sv ",1
" the theorem states that for the same sequence of step inputs, the model produces the exact same step output as the target program it aims to learn",0
" also, compared to the bubble pointers in bubble sort, the pointers that perform the comparison for quicksort (the compswap function) are usually not adjacent to each other, making quicksort less local than bubble sort",1
g,1
" as aforementioned, the npi model naturally supports recursion",1
" however, to be sure that the generated v is complete (covers all the cases needed), we need to check all pairs of observations seen in adjacent step inputs (in particular, those before and after a primitive function call), in a similar way as if we were constructing v from scratch",0
" if a white vertex is found, set pstart to point to it; this signifies the start of a traversal of a new connected component",1
" partial recursion only adds a tail recursive call to bubblesort after bubble and reset, similar to the tail recursive call described previously for addition",0
" , a1a0 + b1b0} are added properly",0
e,0
" this functionality the original npi paper decodes to a program key embedding kt ∈ rk and then computes a program embedding pt+1, which we also did in our implementation, but we omit this for brevity",1
5,0
"
inside bubble and reset, there are two operations that can be made recursive",0
" via the recursive call, we effectively forget that the column just added exists, since the recursive call to add starts with a new hidden state for the lstm controller",0
" we hypothesize that the reason for this is that the neural network learns to spuriously depend on specific characteristics of the training examples that are irrelevant to the true program semantics, such as length of the training inputs, and thus fails to generalize to more complex inputs",1
 the degree for any vertex in the dag is variable,0
"as discussed in section 2, the neural programmer-interpreter (npi) is an instance of a neural programmer architecture and hence it naturally supports recursion",0
" our results show that for all learned, to-be-verified neural programs, they all produced the same traces as those produced by p on the verification set",1
" in future work, we would like to decrease supervision and construct models that are capable of coming up with recursive abstractions themselves",0
" empirically, existing models suffer from a common limitation—generalization becomes poor beyond a threshold level of complexity",1
" we further have qresult ∈ nu , a scratch-pad which contains the sorted list of vertices at the end of execution, and qstack ∈ nu , which serves the role of the stack s in algorithm 2",0
" we call this set of inputs the verification set, sv ",0
" in fact, it is one of our future directions to explore new ways to train a recursive neural program without providing explicit training execution traces or with only partial or non-recursive traces",0
" each step during an execution of the program does one of three things: (1) another subprogram along with associated arguments is called, as in line 10, (2) the program writes to the environment if it is primitive, as in line 8, or (3) the loop is terminated if the return probability exceeds a threshold α, after which the stack frame is popped and control is returned to the caller",1
"
as in line 13 of the right-hand side of figure 1",1
" for each length, we test each program on 30 randomly generated problems",1
" for each graph size, we test the learned programs on 30 randomly generated dags",0
"
as a concrete instantiation, we show in this paper that we can enable recursive neural programs in the npi model, and thus enable perfectly generalizable neural programs for tasks such as sorting where the original, non-recursive npi program fails",1
 this makes it difficult to reason about what the model will do when given complex inputs,0
" for tasks other than addition, it is difficult to analytically determine the verification set, so instead, we randomly generate input candidates until they completely cover the base cases and reduction rules",1
g,0
" because the context of the caller is stored on a stack when it calls another program and the callee starts in a fresh context, this enables recursion simply by allowing a program to call itself",0
" empirically, existing models suffer from a common limitation—generalization becomes poor beyond a threshold level of complexity",0
"
in this general neural programming architecture, we show it is easy to support recursion",1
" et+1 ∼ fenv(et, pt, at)",1
" in particular, this condition is satisfied by npi, and thus the npi model naturally supports recursion (even though the authors of npi did not consider this aspect explicitly)",0
"2 at any given time, the npi has access to values pointed to by four pointers in each of the four rows, represented by q(1, i1), q(2, i2), q(3, i3), and q(4, i4)",1
"we show that if we incorporate recursion, the learned npi programs can achieve provably perfect generalization for different tasks",0
 we adapt machinery from the original paper slightly to fit our needs,1
" observe that the non-recursive program’s correctness degrades for length 11 and beyond, while the recursive program can sort any given array",1
"
since npi naturally supports the notion of recursion, a key question is how to enable npi to learn recursive programs",1
" by iterating with this procedure, and then running p on the input observation set that we obtain for the entry point function, we can obtain v precisely",1
"
the dag is represented as an adjacency list where dag[i][j] refers to the j-th child of vertex i",1
", the trace corresponding to the array [3,2])",0
" 2: initialize lo and hi to be 1 and n, where n is the length of a",0
 there are 2 variables (vactive and vsave); vactive holds the active vertex (as in algorithm 2) and vsave holds the value of vactive before executing line 12 of algorithm 2,1
" each step during an execution of the program does one of three things: (1) another subprogram along with associated arguments is called, as in line 10, (2) the program writes to the environment if it is primitive, as in line 8, or (3) the loop is terminated if the return probability exceeds a threshold α, after which the stack frame is popped and control is returned to the caller",0
" we can obtain this mapping by first considering the possible observations that can arise at the beginning of the entry function (add, bubblesort, toposort, and quicksort) for some valid program input, and iteratively applying the observation-to-observation mapping implied by the reference implementation’s step output at that point in the execution",1
3 to construct v and then empirically create a verification set which covers v ,1
"
recursion drastically reduces the number of configurations we need to consider during the verification phase and makes the proof tractable, because it introduces structure that eliminates infinitely long sequences of step inputs that would otherwise need to be considered",0
" the learned recursive program is different from neural programs learned in all previous work in an important aspect: previous approaches do not explicitly incorporate this abstraction, and hence generalize poorly, whereas our learned neural programs incorporate recursion and achieve perfect generalization",0
" as with the original npi architecture, the experiments for this paper always used a 3-tuple of integers at = (at(1), at(2), at(3))",1
" we made the assumption of equivalent lengths in order to parametrize the input format with respect to length, but this assumption can be removed as well",1
 the memory updates of these neural programs are so complex and interdependent that it is difficult to reason about the behaviors of the learned neural program under previously unseen situations (such as problems with longer inputs),1
"
in what follows, we describe the way in which we constructed npi training traces so as to make them contain recursive elements and thus enable npi to learn recursive programs",0
 errors arise due to undesirable and uninterpretable dependencies and associations the architecture learns to store in some high-dimensional hidden state,1
" in future work, we seek to enable more tasks with recursive structure",0
" this guarantees that the step input sequences considered during verification contain all step input sequences which arise during execution of an unseen problem in f , leading to generalization to any problem in f ",1
" this is possible if and only if the graph has no directed cycles; that is to say, it must be a directed acyclic graph (dag)",0
" in particular, we construct new training traces which explicitly contain recursive elements and show that with this type of trace, npi easily learns recursive programs",1
"
the training data for the neural programmer-interpreter consists of full execution traces for the program of interest",0
we emphasize the overall goal of this work is to enable the learning of a recursive program,1
" for each length, we test each program on 30 randomly generated problems",1
 the memory updates of these neural programs are so complex and interdependent that it is difficult to reason about the behaviors of the learned neural program under previously unseen situations (such as problems with longer inputs),1
 ,0
" even though the earlier version also generalized just as well in practice, relatively small differences in the formulation of the traces and the environment observations can drastically change the difficulty of verification",0
" in future work, we seek to enable more tasks with recursive structure",1
" 16: p = partition(a, lo, hi) 17: push p+ 1 and hi to slo and shi",0
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,0
" as this complexity interfered with determining the base cases and reductions, we changed the algorithm to its current form",1
" formally, the npi is represented by the following set of equations:
st = fenc(et, at)
ht = flstm(st, pt, ht−1)
rt = fend(ht), pt+1 = fprog(ht), at+1 = farg(ht)
t is a subscript denoting the time-step; fenc is a domain-specific encoder (to be described later) that takes in the environment slice et and arguments at; flstm represents the core module, which takes in the state st generated by fenc, a program embedding pt ∈ rp , and hidden lstm state ht; fend decodes the return probability rt; fprog decodes a program key embedding pt+1;1 and farg decodes arguments at+1",0
" numerous models have been proposed for learning programs; to name a few, this includes the differentiable neural computer (graves  , 2016), neural turing machine (graves  , 2014), neural gpu (kaiser & sutskever, 2015), neural programmer (neelakantan  , 2015), pointer network (vinyals  , 2015), hierarchical attentive memory (andrychowicz & kurach, 2016), and neural random access machine (kurach  , 2016)",0
" we prove every member of f is added properly, given that subproblems s = {anan−1 + bnbn−1, an−1an−2 + bn−1bn−2, ",0
 this paper is the first (to our knowledge) to investigate the important problem of provable generalization properties of neural programs,1
" in particular, given that a recursion is defined by two properties as mentioned before, the base cases and the set of reduction rules, we can prove a recursive neural program generalizes perfectly if we can prove that (1) it performs correctly on the base cases; (2) it learns the reduction rules correctly",1
"
grade-school addition",1
" thus, any general neural programming architecture supporting such a call structure can be made to support recursion",1
 this algorithm is a variant of depth first search,1
 this procedure verifies that all base cases and reductions are handled properly by the model via explicit tests,0
 table 4 also demonstrates that the (verified) recursive program generalizes perfectly,0
 the model described in the training setup of section 4 (trained on 6 traces) was verified to be correct via the matching procedure described in section 4,0
" in our experiments, we only present dag’s as inputs and represent the vertices as values ranging from 1, ",1
"
we describe the base cases, reduction rules, and the verification set for each task in appendix a",0
 reset represents a −∞ value,0
3,0
" this is highly undesirable, since being able to provide the correct answer in all possible settings is one of the most important aspects of any learned neural program",1
" the partial recursion is not enough for perfect generalization, as will be presented later in section 4",0
 we instantiated recursion for the neural programmerinterpreter by changing the training traces,0
" finally, we implement recursive traces for our own topological sort and quicksort tasks",0
" for brevity, we refer the reader to the appendix for further details on the program set and non-recursive and recursive trace-generating functions used for topological sort",1
" at each time step, the core controller can decide to select one of the programs to call with certain arguments",0
" the outputs rt, pt+1, at+1 are used to determine the next action, as described in algorithm 1",0
" each time a subprogram is called, the stack depth increases",0
 our result shows that the training procedure and the npi architecture is capable of generalizing from the step input-output pairs seen in the training data to the unseen ones present in the verification set,0
"as discussed in section 2, the neural programmer-interpreter (npi) is an instance of a neural programmer architecture and hence it naturally supports recursion",1
" 2: initialize lo and hi to be 1 and n, where n is the length of a",0
" et+1 ∼ fenv(et, pt, at)",0
"
algorithm 2 shows the topological sort task of interest",0
"
there are 6 pointers (plo, phi, pstacklo, pstackhi, ppivot, pj)",1
 recursion enables provably perfect generalization,0
" also the dag can have potentially more than one connected component, meaning it is necessary to transition between these components appropriately",0
"
base cases and reduction rules for quicksort",1
" the write operation has the following arguments:
arg 1 (main action): color curr, color next, active start, active neighb, active stack, save, stack push, stack pop, result
color curr colors vactive, color next colors vertex dag[vactive][childlist[vactive]], active start writes pstart to vactive, active neighb writes dag[vactive][childlist[vactive]] to vactive, active stack writes qstack(pstack) to vactive, save writes vactive to vsave, stack push pushes vactive to the top of the stack, stack pop writes a null value to the top of the stack, and result writes vactive to qresult(presult)",0
" however, our concept of recursion for neural programs is general",0
" this is highly undesirable, since being able to provide the correct answer in all possible settings is one of the most important aspects of any learned neural program",1
"
for topological sort, the domain-specific encoder is
fenc(dag,qcolor, pstack, pstart, vactive, childlist, at)
=mlp ([qcolor(pstart), qcolor(dag[vactive][childlist[vactive]]), pstack == 1, at(1), at(2), at(3)]),
where qcolor ∈ ru×4 is a scratch-pad that contains u rows, each containing one of four colors (white, gray, black, invalid) with one-hot encoding",0
" training on the full recursive trace leads to perfect generalization, as shown in section 4",0
"
the training set for topological sort contains 6 traces, with one synthesized from a graph of size 5 and the rest synthesized from graphs of size 7",0
"
the training set for topological sort contains 6 traces, with one synthesized from a graph of size 5 and the rest synthesized from graphs of size 7",0
 almost all architectures train on program input/output pairs,0
" however, the authors of npi did not consider explicitly the notion of recursion and as a consequence, did not learn recursive programs",0
" naı̈vely, we might expect to synthesize and test an input for any sequence created by combining the four possible colors in two variables and another boolean variable for whether the stack is empty (so 32 possible observations at any point), but for various reasons, most of these combinations are impossible to occur at any given point in the execution trace",0
" the recursive version can generalize perfectly to long problems constructed from these components (such as the sum “822+233”, where “8+2” and “2+3” are in the training set), but the non-recursive version fails to sum these long problems properly",0
" an example of part of an addition task trace, written in shorthand, is given in figure 1",0
 we created a program set that reflects the semantics of algorithm 2,0
" while this matches the description from reed & de freitas (2016), we found that this causes an unnecessary blowup in the size of v and makes it much more difficult to construct the verification set",1
" via the recursive call, we effectively forget that the column just added exists, since the recursive call to add starts with a new hidden state for the lstm controller",1
" if a white vertex is found, set pstart to point to it; this signifies the start of a traversal of a new connected component",1
" for brevity, we refer the reader to the appendix for information about the program set and non-recursive and recursive trace-generating functions for quicksort",1
"
arg 2 (auxiliary variable): color grey, color black
color grey and color black color the given vertex grey and black, respectively",1
" if it is too small, then execution of the program on some input might require evaluation of m(i) on some i /∈ v , and so the behavior ofm(i) might deviate from p (i)",0
as mentioned in section 2,0
"
algorithm 2 shows the topological sort task of interest",1
 some functional programming languages go so far as not to define any looping constructs but rely solely on recursion to enable repeated execution of the same code,1
" the corresponding edge list is [(1, 2), (1, 5), (2, 4), (2, 5), (3, 5)]",0
" the recursive version of topological sort solves all graph instances we tried, from graphs of size 5 through 70",1
" qcolor(pstart) contains the color of the current start vertex, used in the evaluation of the condition in the while loop in line 5 of algorithm 2",0
e,0
" when the program is called, the current context including the caller’s memory state is stored on a stack; when the program returns, the stored context is popped off the stack to resume execution in the previous caller’s context",0
" for each graph size, we test the learned programs on 30 randomly generated dags",1
 18: push lo and p− 1 to slo and shi,1
" we hypothesize that the reason for this is that the neural network learns to spuriously depend on specific characteristics of the training examples that are irrelevant to the true program semantics, such as length of the training inputs, and thus fails to generalize to more complex inputs",1
3 to construct v and then empirically create a verification set which covers v ,1
2,1
" for each length, we test each program on 30 randomly generated problems",1
" in order to compensate for this, ppivot and pj require special functions (move pivot lo and move j lo) to properly set them to lo in lines 11 and 12 of the partition function in algorithm 3",0
" we then construct problem configurations that span all possible valid environment states (for instance, in order to force the carry bit in a column to be 1, one can add the sum “1+9” in the column to the right)",0
"as there is no public implementation of npi, we implemented a version of it in keras that is as faithful to the paper as possible",0
"
arg 2 (increment or decrement): up, down
swap",0
" we also hope to decrease supervision, for example by training with only partial or non-recursive traces, and to develop novel neural programming architectures integrated directly with a notion of recursion",0
 we outline how to construct this set,0
" in our experiments, we only present dag’s as inputs and represent the vertices as values ranging from 1, ",0
" the single-digit multiplication task in zaremba   (2016), the bubble sort task in reed & de freitas (2016), and the graph tasks in graves   (2016))",0
"
move",0
"
the non-recursive trace loops on cycles of add1 and lshift",0
"
the dag is represented as an adjacency list where dag[i][j] refers to the j-th child of vertex i",0
" lshift, used in reset, shifts the pointers left until reaching the start token",1
g,1
" as with the original npi architecture, the experiments for this paper always used a 3-tuple of integers at = (at(1), at(2), at(3))",0
3,0
"
directed acyclic graphs are structurally more diverse than inputs in the two tasks of grade-school addition and bubble sort",0
" twc-1409915, darpa under grant no",1
"
formally, verification consists of proving the following theorem:
∀i ∈ v,m(i) ⇓ p (i) where i denotes a sequence of step inputs (within one function call), v denotes the set of valid sequences of step inputs, m denotes the neural network model, p denotes the correct program, and p (i) denotes the next step output from the correct program",1
 ,0
we now report on generalization for the varying tasks,0
", an lstm in npi’s case, but possibly other networks in different cases",1
" this functionality the original npi paper decodes to a program key embedding kt ∈ rk and then computes a program embedding pt+1, which we also did in our implementation, but we omit this for brevity",0
" , a1a0 + b1b0} are added properly",0
"when constructing a neural network for the purpose of learning a program, there are two orthogonal aspects to consider",0
" this agrees with the generalization of non-recursive addition in reed & de freitas (2016), where they reported generalization up to 3000 digits",0
" here as a concrete and general example, we consider a general neural programming architecture (npa), similar to neural programmer-interpreter (npi) in reed & de freitas (2016)",1
this material is in part based upon work supported by the national science foundation under grant no,1
 3: begin traversing from vertex 1 in the dag,1
" we hypothesize that the reason for this is that the neural network learns to spuriously depend on specific characteristics of the training examples that are irrelevant to the true program semantics, such as length of the training inputs, and thus fails to generalize to more complex inputs",0
"
bubble sort",1
" in computer science, recursion (as opposed to iteration) involves solving a larger problem by combining solutions to smaller instances of the same problem",0
" the add reduction rule ensures that each call to add only covers two adjacent columns, and so the lstm only ever runs for a fixed number of steps necessary to process these two columns",0
"
base cases and reduction rules for topological sort",1
" the non-recursive and partially recursive versions are unable to sort long arrays, beyond length 8",0
" in practice, we can additionally use tail recursion optimization to avoid problems with the call stack growing too deep",0
"
quicksort",0
" empirically, existing models suffer from a common limitation—generalization becomes poor beyond a threshold level of complexity",0
" thus, recursion helps decompose a problem and makes it easier to reason about a program’s behavior for previously unseen situations such as longer inputs",0
" each step during an execution of the program does one of three things: (1) another subprogram along with associated arguments is called, as in line 10, (2) the program writes to the environment if it is primitive, as in line 8, or (3) the loop is terminated if the return probability exceeds a threshold α, after which the stack frame is popped and control is returned to the caller",1
" the core module of the npi is an lstm controller that takes as input a slice of the current external environment, via a set of pointers, and a program and arguments to execute",0
" 4: function toposort(dag) 5: while there is still a white vertex u: do 6: color[u] = grey 7: vactive = u 8: do 9: if vactive has a white child v then 10: color[v] = grey 11: push vactive onto s 12: vactive = v 13: else 14: color[vactive] = black 15: write vactive to result 16: if s is empty then pass 17: else pop the top vertex off s and set it to vactive 18: while s is not empty
topological sort",0
" a single element of an execution trace consists of a step input-step output pair, which can be synthesized from algorithm 1: this corresponds to, for a given time-step, the step input tuple (e, p, a) and step output tuple (r, p2, a2)",0
g,0
" the pointers at index i1 and i2 are used to compare the pair of numbers considered during the bubble sweep, swapping them if the number at i1 is greater than that in i2",1
"
given a verification set, we can then run the model on the verification set to check if the produced traces and results are correct",0
" a topological sort is a linear ordering of vertices such that for every directed edge (u, v) from u to v, u comes before v in the ordering",0
" by iterating with this procedure, and then running p on the input observation set that we obtain for the entry point function, we can obtain v precisely",1
" this functionality the original npi paper decodes to a program key embedding kt ∈ rk and then computes a program embedding pt+1, which we also did in our implementation, but we omit this for brevity",0
2,0
" the general notion of recursion has been an important concept in many domains, including mathematics and computer science",1
 npi then outputs the return probability and next program and arguments to execute,0
 we choose to implement a topological sort task for graphs,0
" in the non-recursive trace, there are four functions that can be made recursive—toposort, check child, explore, and next start, and we add a tail recursive call to each of these functions in order to make the recursive trace",1
 we outline how to construct this set,1
" for grade-school addition, the domain-specific encoder is
fenc(q, i1, i2, i3, i4, at) =mlp ([q(1, i1), q(2, i2), q(3, i3), q(4, i4), at(1), at(2), at(3)]),
where the environment q ∈ r4×n×k is a scratch-pad that contains four rows (the first input number, the second input number, the carry bits, and the output) and n columns",0
" 2: initialize lo and hi to be 1 and n, where n is the length of a",0
" given verification sets that cover all the base cases and reduction rules, we can provide proofs that these learned programs generalize perfectly",1
 figure 2 shows examples of traces for the different versions of bubble sort,0
" qcolor(pstart) contains the color of the current start vertex, used in the evaluation of the condition in the while loop in line 5 of algorithm 2",1
" 2: initialize lo and hi to be 1 and n, where n is the length of a",1
"
the dag is represented as an adjacency list where dag[i][j] refers to the j-th child of vertex i",1
 ,0
 ,1
" this makes it so that we only need to consider the vertices in the subgraph that are currently relevant for computing the sort, allowing simpler reasoning about behavior for large graphs",0
"
bubble sort",1
"
in this paper, we propose to resolve these issues by explicitly incorporating recursion into neural architectures",0
"
we refer the reader to the appendix for the non-recursive trace generating functions",0
" here as a concrete and general example, we consider a general neural programming architecture (npa), similar to neural programmer-interpreter (npi) in reed & de freitas (2016)",1
" however, models that make use of this strategy eventually fail after a certain level of complexity (e",0
"
there are 6 pointers (plo, phi, pstacklo, pstackhi, ppivot, pj)",0
" on the other hand, the non-recursive version has low accuracy, beginning from size 5, and fails completely for graphs of size 8 and beyond",0
"as there is no public implementation of npi, we implemented a version of it in keras that is as faithful to the paper as possible",1
"
note that v should also contain the necessary reductions, which corresponds to making the recursive calls at the correct time, as indicated by p ",1
g,1
 the first is the actual model architecture,0
"
the training set for topological sort contains 6 traces, with one synthesized from a graph of size 5 and the rest synthesized from graphs of size 7",0
"
we describe the details of the npi model relevant to our contributions",0
" full recursion, in addition to making the aforementioned tail recursive call, adds two additional recursive calls; bstep and lshift are made tail recursive",1
" formally, the npi is represented by the following set of equations:
st = fenc(et, at)
ht = flstm(st, pt, ht−1)
rt = fend(ht), pt+1 = fprog(ht), at+1 = farg(ht)
t is a subscript denoting the time-step; fenc is a domain-specific encoder (to be described later) that takes in the environment slice et and arguments at; flstm represents the core module, which takes in the state st generated by fenc, a program embedding pt ∈ rp , and hidden lstm state ht; fend decodes the return probability rt; fprog decodes a program key embedding pt+1;1 and farg decodes arguments at+1",0
"1; and for bubble sort, appendix a",1
" thus, we demonstrate that recursion enables provably perfect generalization for different tasks, including addition, topological sort, quicksort, and a variant of bubble sort",0
" formally, the npi is represented by the following set of equations:
st = fenc(et, at)
ht = flstm(st, pt, ht−1)
rt = fend(ht), pt+1 = fprog(ht), at+1 = farg(ht)
t is a subscript denoting the time-step; fenc is a domain-specific encoder (to be described later) that takes in the environment slice et and arguments at; flstm represents the core module, which takes in the state st generated by fenc, a program embedding pt ∈ rp , and hidden lstm state ht; fend decodes the return probability rt; fprog decodes a program key embedding pt+1;1 and farg decodes arguments at+1",1
" the outputs rt, pt+1, at+1 are used to determine the next action, as described in algorithm 1",1
", to color a vertex) or variable (e",1
" the partial recursion is not enough for perfect generalization, as will be presented later in section 4",0
", to change the value of vactive) none described below move move a pointer (e",1
"
arg 2 (auxiliary variable): color grey, color black
color grey and color black color the given vertex grey and black, respectively",0
 the original version of the bubblesort implementation exposes the values within the array,1
" for brevity, we refer the reader to the appendix for information about the program set and non-recursive and recursive trace-generating functions for quicksort",1
", qstacklo or qstackhi) or to pointer (e",0
" this is highly undesirable, since being able to provide the correct answer in all possible settings is one of the most important aspects of any learned neural program",1
"
the operations we need to be concerned most about are carry and lshift, which induce partial environment states spanning two columns",0
" thus, recursion helps decompose a problem and makes it easier to reason about a program’s behavior for previously unseen situations such as longer inputs",1
"
move",1
"
in addition, none of the current approaches to neural programming provide a method or even aim to enable provable guarantees about generalization",0
"
in order to demonstrate that recursion can help learn and generalize better, we trained a non-recursive and recursive model on just a single execution trace generated from a graph containing 5 nodes3 for the topological sort task",0
" the general notion of recursion has been an important concept in many domains, including mathematics and computer science",0
" there are 3 pointers (presult, pstack, pstart), presult points to the next empty location in qresult,
pstack points to the top of the stack in qstack, and pstart points to the candidate starting node for a connected component",1
"
move",0
 recursion can be implemented differently for different neural programming models,0
"
we also report on generalization results for the non-recursive and recursive versions of this variant of bubble sort",0
" hence, if all subproblems in s are added correctly, we have proven that any member of f will be added correctly, thus eliminating an infinite family of inputs that need to be tested",1
" the swap operation has the following arguments:
arg 1 (swap object 1): ppivot
arg 2 (swap object 2): phi, pj
write",0
" because the context of the caller is stored on a stack when it calls another program and the callee starts in a fresh context, this enables recursion simply by allowing a program to call itself",0
" on the other hand, the recursive programs have learned the true program semantics",1
 recursion enables provably perfect generalization,0
" in this section, we give a brief review of the npi architecture from reed & de freitas (2016) as background",0
"
is critical for implementing recursion, since it permits us to restrict our attention to the currently relevant recursive call, ignoring irrelevant details about other contexts",0
" the architecture usually possesses some form of memory, which could be internal (such as the hidden state of a recurrent neural network) or external (such as a discrete “scratch pad” or a memory block with differentiable access)",1
" an example of part of an addition task trace, written in shorthand, is given in figure 1",1
"
base cases and reduction rules for bubble sort",1
" the theorem states that for the same sequence of step inputs, the model produces the exact same step output as the target program it aims to learn",0
" for grade-school addition, the domain-specific encoder is
fenc(q, i1, i2, i3, i4, at) =mlp ([q(1, i1), q(2, i2), q(3, i3), q(4, i4), at(1), at(2), at(3)]),
where the environment q ∈ r4×n×k is a scratch-pad that contains four rows (the first input number, the second input number, the carry bits, and the output) and n columns",0
" qcolor(dag[vactive][childlist[vactive]]) refers to the color of the next child of vactive, used in the evaluation of the condition in the if branch in line 9 of algorithm 2",1
" in this version, there is a dependence on length: bstep and lshift are called a number of times equivalent to one less than the length of the input array, in bubble and reset respectively",1
" also the dag can have potentially more than one connected component, meaning it is necessary to transition between these components appropriately",1
"
the operations we need to be concerned most about are carry and lshift, which induce partial environment states spanning two columns",1
" we note that an earlier version of quicksort we tried lacked primitive operations to directly move a pointer to another, and therefore needed more functions and observations",0
" in fact, we argue that recursion is an essential element for neural programs to generalize, and makes it tractable to prove the generalization of neural programs",0
" however, our concept of recursion for neural programs is general",0
 this paper is the first (to our knowledge) to investigate the important problem of provable generalization properties of neural programs,0
" in this section, we give a brief review of the npi architecture from reed & de freitas (2016) as background",0
", the trace corresponding to the problem “109 + 101”)",0
" for purposes of verification, we replace the domain-specific encoder with the following:
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1) ≤ q(1, i2), 1 ≤ i1 ≤ length, 1 ≤ i2 ≤ length, i3 == length, at(1), at(2), at(3)]),which directly exposes which of the two values pointed to is larger",0
" in order to claim a proof, we must verify the model produces correct behavior over all base cases and reductions, as described in section 2",0
"
algorithm 2 shows the topological sort task of interest",1
" in practice, we can additionally use tail recursion optimization to avoid problems with the call stack growing too deep",0
" k is set to 11, to represent the range of 10 possible digits, along with a token representing the end of input",0
" in particular, we construct new training traces which explicitly contain recursive elements and show that with this type of trace, npi easily learns recursive programs",0
" if a white vertex is found, set pstart to point to it; this signifies the start of a traversal of a new connected component",0
"
arg 2 (increment or decrement): up, down1 // top level topological sort call 2 toposort() { 3 while (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 } 0 11 traverse() { 12 check_child() 13 explore() 14 } 15 16 check_child() { 17 while (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 18 move(childlist[vactive], up) 19 } 20 21 explore() { 22 do 23 if (qcolor(dag[vactive][childlist[vactive]]) is white): 24 write(color_next, color_grey) 25 stack(push) 26 write(save) 27 write(active_neighb) 28 move(childlist[vsave], up) 29 else: 30 write(color_curr, color_black) 31 write(result) 32 move(presult, up) 33 if(pstack == 1): 34 break 35 else: 36 stack(pop) 37 check_child() 38 while (true) 39 } 40 41 stack(op) { 42 if (op == push): 43 write(stack_push) 44 move(pstack, up) 45 46 if (op == pop): 47 write(active_stack) 48 write(stack_pop) 49 move(pstack, down) 50 } 51 52 next_start() { 53 while(qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 54 move(pstart, up) 55 }altered recursive functions  // top level topological sort call 2 toposort() { 3 if (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 toposort() // recursive call 0 } 11 12 check_child() { 13 if (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 14 move(childlist[vactive], up) 15 check_child() // recursive call 16 } 17 18 explore() { 19 if (qcolor(dag[vactive][childlist[vactive]]) is white): 20 write(color_next, color_grey) 21 stack(push) 22 write(save) 23 write(active_neighb) 24 move(childlist[vsave], up) 25 else: 26 write(color_curr, color_black) 27 write(result) 28 move(presult, up) 29 if(pstack == 1): 30 return 31 else: 32 stack(pop) 33 check_child() 34 explore() // recursive call 35 } 36 37 next_start() { 38 if (qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 39 move(pstart, up) 40 next_start() // recursive call 41 }algorithm 4 iterative quicksort 1: initialize an array a to sort and two empty stacks slo and shi",0
 pstacklo and pstackhi point to the top (empty) positions in qstacklo and qstackhi,0
 this algorithm is a variant of depth first search,1
g,1
" we hypothesize that the reason for this is that the neural network learns to spuriously depend on specific characteristics of the training examples that are irrelevant to the true program semantics, such as length of the training inputs, and thus fails to generalize to more complex inputs",1
"
we note that for tasks with very large input domains, such as ones involving mnist digits or speech samples, the state space of base cases and reduction rules could be prohibitively large, possibly infinite",0
" given verification sets that cover all the base cases and reduction rules, we can provide proofs that these learned programs generalize perfectly",0
" furthermore, given a verification input set that covers all base cases and reduction rules, we can formally prove that the learned neural programs achieve perfect generalization after verifying its behavior on the verification input set",1
 we find that recursion makes it easier for the network to learn the right program and generalize to unknown situations,1
"
under the assumption that there are no leading 0’s (except in the case of single digits) and the two numbers to be added have the same number of digits, the verification set for addition contains 20,181 input problems",0
" even though the earlier version also generalized just as well in practice, relatively small differences in the formulation of the traces and the environment observations can drastically change the difficulty of verification",0
" in this section, we give a brief review of the npi architecture from reed & de freitas (2016) as background",0
"
the training data for the neural programmer-interpreter consists of full execution traces for the program of interest",0
 we choose to implement a topological sort task for graphs,0
" in order to claim a proof, we must verify the model produces correct behavior over all base cases and reductions, as described in section 2",0
g,1
" in particular, in the explore function, adding a tail recursive call resets and stores the hidden states associated with vertices in a stack-like fashion",0
" each step during an execution of the program does one of three things: (1) another subprogram along with associated arguments is called, as in line 10, (2) the program writes to the environment if it is primitive, as in line 8, or (3) the loop is terminated if the return probability exceeds a threshold α, after which the stack frame is popped and control is returned to the caller",1
"
move",0
 indentation indicates the stack is one level deeper than before,1
" this functionality the original npi paper decodes to a program key embedding kt ∈ rk and then computes a program embedding pt+1, which we also did in our implementation, but we omit this for brevity",0
" for problems where the base cases may be extremely large or infinite, such as certain forms of motor control, recursion can still help reduce the problem of generalization to these two aspects and make the generalization problem significantly simpler to handle and reason about",0
" if the step output specifies a primitive function call, we need to reason about how it can affect the environment so as to change the observation in the next step input",0
" when the program is called, the current context including the caller’s memory state is stored on a stack; when the program returns, the stored context is popped off the stack to resume execution in the previous caller’s context",0
" the core module of the npi is an lstm controller that takes as input a slice of the current external environment, via a set of pointers, and a program and arguments to execute",1
"
quicksort",0
"
to perform the verification as described here, it is critical to construct v correctly",0
g,1
"we train using the adam optimizer and use a 2-layer lstm and task-specific state encoders for the external environments, as described in reed & de freitas (2016)",1
" training on the full recursive trace leads to perfect generalization, as shown in section 4",0
", to change the value of vactive) none described below move move a pointer (e",1
"
thus far, to evaluate the efficacy of neural models on programming tasks, the only metric that has been used is generalization of expected behavior to inputs of greater complexity (vinyals   (2015), kaiser & sutskever (2015), reed & de freitas (2016), graves   (2016), zaremba   (2016))",1
", pstart or childlist[vactive]) up or down none described belowwrite",0
" the write operation has the following arguments:
arg 1 (object to write): env stack lo, env stack hi, phi, plo
env stack lo and env stack hi represent qstacklo(pstacklo) and qstackhi(pstackhi), respectively",1
"
bubble sort",1
" at end, pointer ppivot is moved to the pivot compswap loop, move pivot lo, move j lo, swap
none
compswap loop runs the for loop inside the partition function compswap, move none compswap compares a[pivot] ≤ a[j]; if so, perform a swap and increment ppivot swap, move none set pivot lo sets ppivot to lo index none none set j lo sets pj to lo index none none set j null sets pj to −∞ none none stack pushes lo/hi states
onto stacks slo and shi according to argument (described below)
write, move described below
move moves pointer one unit up or down none described below swap swaps elements at given array indices none described below write write a value either to stack (e",0
" 9: else 10: function run(e, p2, a2)
a description of the inference procedure is given in algorithm 1",1
"
the three environment observations aid with control flow in algorithm 2",1
" however, the authors of npi did not consider explicitly the notion of recursion and as a consequence, did not learn recursive programs",0
 this modification also enables us to sort arrays containing arbitrary comparable elements,0
" if yes, then this indicates that the learned neural program achieves provably perfect generalization",0
 npi then outputs the return probability and next program and arguments to execute,0
"
we experiment with two levels of recursion—partial and full",0
" the theorem states that for the same sequence of step inputs, the model produces the exact same step output as the target program it aims to learn",1
" nevertheless, we would like the model to quickly learn to represent the right semantics of the underlying program from a small number of training examples, not an exhaustive number of them",0
" it is straightforward to deal with all other operations, which do not induce partial environment states",1
" for the recursive formulation of addition, we analytically construct the set of input problems that cover all base cases and reduction rules",0
", qstacklo or qstackhi) or to pointer (e",0
g,0
"1; and for bubble sort, appendix a",0
" this agrees with the generalization of non-recursive addition in reed & de freitas (2016), where they reported generalization up to 3000 digits",1
" any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of national science foundation and darpa",0
"
thus far, to evaluate the efficacy of neural models on programming tasks, the only metric that has been used is generalization of expected behavior to inputs of greater complexity (vinyals   (2015), kaiser & sutskever (2015), reed & de freitas (2016), graves   (2016), zaremba   (2016))",0
"
under the assumption that there are no leading 0’s (except in the case of single digits) and the two numbers to be added have the same number of digits, the verification set for addition contains 20,181 input problems",0
 ,1
" if no white vertex is found, the entire execution is terminated move none write write a value either to environment (e",1
 table 3 presents results on randomly generated arrays of varying length for the learned non-recursive and recursive programs,1
g,0
"
grade school addition",1
"
arg 2 (increment or decrement): up, down
swap",1
we emphasize the overall goal of this work is to enable the learning of a recursive program,0
" in this version, there is a dependence on length: bstep and lshift are called a number of times equivalent to one less than the length of the input array, in bubble and reset respectively",1
 the sequence of primitive operations (move and write operations) for the non-recursive and recursive versions are exactly the same,0
"we describe how models trained with recursive traces can be proven to generalize, by using the verification procedure described in section 3",0
" the move operation has the following arguments:
arg 1 (pointer): presult, pstack, pstart, childlist[vactive], childlist[vsave]
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 5 values",0
" for these programs, we re-use the appropriate program sets (the associated subprograms), and we refer the reader to the appendix of reed & de freitas (2016) for further details on the subprograms used in addition and bubble sort",0
" we further have qresult ∈ nu , a scratch-pad which contains the sorted list of vertices at the end of execution, and qstack ∈ nu , which serves the role of the stack s in algorithm 2",0
" empirically, we observe that the learned recursive programs solve all valid inputs with 100% accuracy after training on a very small number of examples, out-performing previous generalization results",1
 ,0
" a single element of an execution trace consists of a step input-step output pair, which can be synthesized from algorithm 1: this corresponds to, for a given time-step, the step input tuple (e, p, a) and step output tuple (r, p2, a2)",1
5,0
"in this paper, we propose that the key abstraction of recursion is necessary for neural programs to generalize",1
" recursion is an important concept in programming languages and a critical tool to
reduce the complexity of programs",0
"1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 while (pstacklo 6= 1): 6 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 7 stack(stack_pop) 8 else: 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 stack(stack_pop) 12 partition() 13 stack(stack_push_call2) 14 stack(stack_push_call1) 15 } 16 17 partition() { 18 set_pivot_lo() 19 set_j_lo() 20 compswap_loop() 21 swap(ppivot, phi) 22 set_j_null() 23 } 24 25 compswap_loop() { 26 while (pj 6= phi): 27 compswap() 28 move(pj, up) 29 } 30 31 compswap() { 32 if (a[pj ] ≤ a[phi]): 33 swap(ppivot, pj) 34 move(ppivot, up) 35 } 36 37 stack(op) { 38 if (op == stack_push_call1): 39 write(env_stack_lo, plo) 40 write(env_stack_hi, ppivot − 1) 41 move(pstacklo, up) 42 move(pstackhi, up) 43 44 if (op == stack_push_call2): 45 write(env_stack_lo, ppivot + 1) 46 write(env_stack_hi, phi) 47 move(pstacklo, up) 48 move(pstackhi, up) 49 50 if (op == stack_pop): 51 write(env_stack_lo, reset) 52 write(env_stack_hi, reset) 53 move(pstacklo, down) 54 move(pstackhi, down) 55 }altered recursive functions 1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 6 partition() 7 stack(stack_push_call2) 8 stack(stack_push_call1) 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 quicksort() // recursive call 12 stack(stack_pop) 13 write(phi, env_stack_hi_peek) 14 write(plo, env_stack_lo_peek) 15 quicksort() // recursive call 16 stack(stack_pop) 17 } 18 19 compswap_loop() { 20 if (pj 6= phi): 21 compswap() 22 move(pj, up) 23 compswap_loop() // recursive call 24 }in this section, we describe the space of base cases and reduction rules that must be covered for each of the four sample tasks, in order to create the verification set",0
" note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values, and arg 2 can only take one of 7 values",0
"
there are 6 pointers (plo, phi, pstacklo, pstackhi, ppivot, pj)",1
" bstep, used in bubble, compares pairs of numbers, continuously moving the bubble pointers once to the right each time until reaching the end of the array",1
" the architecture usually possesses some form of memory, which could be internal (such as the hidden state of a recurrent neural network) or external (such as a discrete “scratch pad” or a memory block with differentiable access)",0
 b1b0 where no carry operations occur,0
" in fact, we argue that recursion is an essential element for neural programs to generalize, and makes it tractable to prove the generalization of neural programs",0
" , a1a0 + b1b0} are added properly",1
"
algorithm 3 recursive quicksort 1: initialize an array a to sort",0
" while this matches the description from reed & de freitas (2016), we found that this causes an unnecessary blowup in the size of v and makes it much more difficult to construct the verification set",1
" we performed experiments on the partially recursive version in order to examine what happens when only one recursive call is implemented, when in reality three are required for perfect generalization",1
"
we refer the reader to the appendix for the non-recursive trace generating functions",0
"
it is sufficient to construct problems where every transition between two adjacent columns is covered",0
"we show that if we incorporate recursion, the learned npi programs can achieve provably perfect generalization for different tasks",0
" the write operation has the following arguments:
arg 1 (main action): color curr, color next, active start, active neighb, active stack, save, stack push, stack pop, result
color curr colors vactive, color next colors vertex dag[vactive][childlist[vactive]], active start writes pstart to vactive, active neighb writes dag[vactive][childlist[vactive]] to vactive, active stack writes qstack(pstack) to vactive, save writes vactive to vsave, stack push pushes vactive to the top of the stack, stack pop writes a null value to the top of the stack, and result writes vactive to qresult(presult)",0
" the partial recursion is not enough for perfect generalization, as will be presented later in section 4",0
", an lstm in npi’s case, but possibly other networks in different cases",0
" for example, a step input-step output pair in lines 2 and 3 of the left-hand side of figure 1 is (add1, write out 1)",0
" the move operation has the following arguments:
arg 1 (pointer): presult, pstack, pstart, childlist[vactive], childlist[vsave]
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 5 values",0
" in this version of curriculum learning, even though the inputs are gradually becoming more complex, the semantics of the program is succinct and does not change",1
"
for addition, we analytically determine the verification set",0
" provably perfect generalization implies the model will behave correctly, given any valid input",1
" the theorem states that for the same sequence of step inputs, the model produces the exact same step output as the target program it aims to learn",1
" as described in the verification procedure, it is possible to prove our learned recursive program generalizes perfectly by testing on an appropriate set of problem inputs, i",0
g,1
 the training set for addition contains 200 traces,1
"
grade school addition",0
"
given a verification set, we can then run the model on the verification set to check if the produced traces and results are correct",0
"
the non-recursive trace loops on cycles of bubble and reset, which logically represents one bubble sweep through the array and reset of the two bubble pointers to the very beginning of the array, respectively",0
" this functionality the original npi paper decodes to a program key embedding kt ∈ rk and then computes a program embedding pt+1, which we also did in our implementation, but we omit this for brevity",1
"program descriptions calls arguments quicksort run the quicksort
routine in place for the array a, for indices from lo to hi non-recursive: partition, stack, write recursive: same as non-recursive version, along with quicksort implicitly: array a to sort, lo, hi
partition runs the partition function",0
e,0
" lshift, used in reset, shifts the pointers left until reaching the start token",1
" there are 3 pointers (presult, pstack, pstart), presult points to the next empty location in qresult,
pstack points to the top of the stack in qstack, and pstart points to the candidate starting node for a connected component",0
"we train using the adam optimizer and use a 2-layer lstm and task-specific state encoders for the external environments, as described in reed & de freitas (2016)",1
" by iterating with this procedure, and then running p on the input observation set that we obtain for the entry point function, we can obtain v precisely",1
" naı̈vely, we might expect to synthesize and test an input for any sequence created by combining the four possible colors in two variables and another boolean variable for whether the stack is empty (so 32 possible observations at any point), but for various reasons, most of these combinations are impossible to occur at any given point in the execution trace",1
"
it is sufficient to construct problems where every transition between two adjacent columns is covered",0
" for brevity, we refer the reader to the appendix for further details on the program set and non-recursive and recursive trace-generating functions used for topological sort",0
"
base cases and reduction rules for topological sort",0
"
one common strategy to improve generalization is to use curriculum learning, where the model is trained on inputs of gradually increasing complexity",0
" observe that the non-recursive program’s correctness degrades for length 11 and beyond, while the recursive program can sort any given array",0
 ,1
"we show that if we incorporate recursion, the learned npi programs can achieve provably perfect generalization for different tasks",1
" full recursion, in addition to making the aforementioned tail recursive call, adds two additional recursive calls; bstep and lshift are made tail recursive",0
" we hypothesize that the reason for this is that the neural network learns to spuriously depend on specific characteristics of the training examples that are irrelevant to the true program semantics, such as length of the training inputs, and thus fails to generalize to more complex inputs",0
" furthermore, given a verification input set that covers all base cases and reduction rules, we can formally prove that the learned neural programs achieve perfect generalization after verifying its behavior on the verification input set",0
"
algorithm 3 recursive quicksort 1: initialize an array a to sort",1
"
the non-recursive trace loops on cycles of add1 and lshift",0
 this makes it difficult to reason about what the model will do when given complex inputs,1
 3: begin traversing from vertex 1 in the dag,0
"
in what follows, we describe the way in which we constructed npi training traces so as to make them contain recursive elements and thus enable npi to learn recursive programs",0
"program descriptions calls arguments quicksort run the quicksort
routine in place for the array a, for indices from lo to hi non-recursive: partition, stack, write recursive: same as non-recursive version, along with quicksort implicitly: array a to sort, lo, hi
partition runs the partition function",0
"
the operations we need to be concerned most about are carry and lshift, which induce partial environment states spanning two columns",1
"
we would also like to point out that in this paper, we provide as an example one way to train a recursive neural program, by providing a certain training execution trace to the npi model",1
" however, note that there is no provable guarantee that the non-recursive learned program will generalize to all inputs, whereas we show later that the recursive learned program has a provable guarantee of perfect generalization",1
3,1
g,0
"
grade-school addition",0
" for example, a step input-step output pair in lines 2 and 3 of the left-hand side of figure 1 is (add1, write out 1)",0
", the verification set",1
"
the non-recursive trace loops on cycles of bubble and reset, which logically represents one bubble sweep through the array and reset of the two bubble pointers to the very beginning of the array, respectively",1
"
there are 6 pointers (plo, phi, pstacklo, pstackhi, ppivot, pj)",1
" the only model, to our knowledge, that does not train on input-output pairs is the neural programmer-interpreter (reed & de freitas, 2016), which trains on synthetic execution traces",0
", to change the value of vactive) none described below move move a pointer (e",0
" in particular, recursion can be implemented as a program calling itself",1
" if it is too large, then the semantics of p might not be well-defined on some elements in v , or the spurious step input sequences may not be reachable from any valid problem input (e",0
" the single-digit multiplication task in zaremba   (2016), the bubble sort task in reed & de freitas (2016), and the graph tasks in graves   (2016))",0
"2 at any given time, the npi has access to values pointed to by four pointers in each of the four rows, represented by q(1, i1), q(2, i2), q(3, i3), and q(4, i4)",1
 figure 2 shows examples of traces for the different versions of bubble sort,0
"
the three environment observations aid with control flow in algorithm 2",0
" the outputs rt, pt+1, at+1 are used to determine the next action, as described in algorithm 1",0
" however, to be sure that the generated v is complete (covers all the cases needed), we need to check all pairs of observations seen in adjacent step inputs (in particular, those before and after a primitive function call), in a similar way as if we were constructing v from scratch",1
"
we describe the details of the npi model relevant to our contributions",1
" the space of possible programs is extremely large, and composing a program that performs robustly on the infinite space of possible inputs is difficult—in part because it is impractical to obtain enough training examples to easily disambiguate amongst all possible programs",0
" twc-1409915, darpa under grant no",0
" 2: initialize lo and hi to be 1 and n, where n is the length of a",0
" the arrow in the theorem refers to evaluation, as in big-step semantics",0
" when the program is called, the current context including the caller’s memory state is stored on a stack; when the program returns, the stored context is popped off the stack to resume execution in the previous caller’s context",1
" 3: 4: function partition(a, lo, hi) 5: pivot = lo 6: for j ∈ [lo, hi− 1] : do 7: if a[j] ≤ a[hi] then 8: swap a[pivot] with a[j] 9: pivot = pivot+ 1 0: swap a[pivot] with a[hi] 11: return pivot 12: 13: function quicksort(a, lo, hi) 14: while slo and shi are not empty: do 15: pop states off slo and shi, writing them to lo and hi",1
 ,0
" the move operation has the following arguments:
arg 1 (pointer): presult, pstack, pstart, childlist[vactive], childlist[vsave]
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 5 values",1
 some functional programming languages go so far as not to define any looping constructs but rely solely on recursion to enable repeated execution of the same code,1
"
the same set of problems was used to generate the training traces for all formulations of the task, for non-recursive and recursive versions",1
" formally, the npi is represented by the following set of equations:
st = fenc(et, at)
ht = flstm(st, pt, ht−1)
rt = fend(ht), pt+1 = fprog(ht), at+1 = farg(ht)
t is a subscript denoting the time-step; fenc is a domain-specific encoder (to be described later) that takes in the environment slice et and arguments at; flstm represents the core module, which takes in the state st generated by fenc, a program embedding pt ∈ rp , and hidden lstm state ht; fend decodes the return probability rt; fprog decodes a program key embedding pt+1;1 and farg decodes arguments at+1",0
"
as mentioned in section 1, all approaches to neural programming today fare poorly on this generalization issue",0
" table 1 presents results on randomly generated arrays of varying length for the learned non-recursive, partially recursive, and full recursive programs",1
" although the model is exposed to more and more data, it might learn spurious and overly complex representations of the program, as suggested in zaremba   (2016)",0
" we note that an earlier version of quicksort we tried lacked primitive operations to directly move a pointer to another, and therefore needed more functions and observations",0
" consequently, it is infeasible to construct a verification set that covers all cases, and the verification procedure we have described is inadequate",0
"
inside bubble and reset, there are two operations that can be made recursive",0
" furthermore, given a verification input set that covers all base cases and reduction rules, we can formally prove that the learned neural programs achieve perfect generalization after verifying its behavior on the verification input set",1
"
is critical for implementing recursion, since it permits us to restrict our attention to the currently relevant recursive call, ignoring irrelevant details about other contexts",1
"
given a verification set, we can then run the model on the verification set to check if the produced traces and results are correct",1
"
the training set for quicksort contains 4 traces, synthesized from arrays of length 5",0
" there are 3 pointers (presult, pstack, pstart), presult points to the next empty location in qresult,
pstack points to the top of the stack in qstack, and pstart points to the candidate starting node for a connected component",0
" in this architecture, we consider a core controller, e",0
" each step during an execution of the program does one of three things: (1) another subprogram along with associated arguments is called, as in line 10, (2) the program writes to the environment if it is primitive, as in line 8, or (3) the loop is terminated if the return probability exceeds a threshold α, after which the stack frame is popped and control is returned to the caller",0
" for many problems, the base cases and reduction rules usually consist of a finite (often small) number of cases",1
" more specifically, when m is trained on simpler inputs, such as inputs of a small length, the generalization metric evaluates how well m will do on more complex inputs, such as inputs of much longer length",0
" the contents ofqresult andqstack are not exposed directly through the domain-specific encoder; rather, we define primitive functions which manipulate these scratch-pads",1
" the architecture usually possesses some form of memory, which could be internal (such as the hidden state of a recurrent neural network) or external (such as a discrete “scratch pad” or a memory block with differentiable access)",1
 ,0
 the degree for any vertex in the dag is variable,1
" in contrast, each call to the recursive add always runs for a fixed number of steps, even on arbitrarily long problems in f , so we can test that it performs correctly on a small, fixed number of step input sequences",1
"
we refer the reader to the appendix for the non-recursive trace generating functions",1
" using this modification, we constructed a verification set consisting of one array of size 10",1
"
we experiment with two levels of recursion—partial and full",0
" that is to say, the network does not learn the true program semantics",1
 we use the lomuto partition scheme; the logic for the recursive trace is shown in algorithm 3,1
g,0
" thus, recursion helps decompose a problem and makes it easier to reason about a program’s behavior for previously unseen situations such as longer inputs",0
" each step during an execution of the program does one of three things: (1) another subprogram along with associated arguments is called, as in line 10, (2) the program writes to the environment if it is primitive, as in line 8, or (3) the loop is terminated if the return probability exceeds a threshold α, after which the stack frame is popped and control is returned to the caller",0
"
we note that for tasks with very large input domains, such as ones involving mnist digits or speech samples, the state space of base cases and reduction rules could be prohibitively large, possibly infinite",1
" in this architecture, we consider a core controller, e",0
"
for addition, we analytically determine the verification set",0
 there are 2 variables (vactive and vsave); vactive holds the active vertex (as in algorithm 2) and vsave holds the value of vactive before executing line 12 of algorithm 2,0
"
after finding v , we construct a set of problem inputs which, when executed on p , create exactly the step input sequences which make up v ",0
 ,1
", to change the value of phi) none described belowstack",0
" for bubble sort, the domain-specific encoder is
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1), q(1, i2), i3 == length, at(1), at(2), at(3)]),
where the environment q ∈ r1×n×k is a scratch-pad that contains 1 row, to represent the state of the array as sorting proceeds in-place, andn columns",1
" recursion is an important concept in programming languages and a critical tool to
reduce the complexity of programs",1
 we find that recursion makes it easier for the network to learn the right program and generalize to unknown situations,0
" for problems where the base cases may be extremely large or infinite, such as certain forms of motor control, recursion can still help reduce the problem of generalization to these two aspects and make the generalization problem significantly simpler to handle and reason about",1
 our result shows that the training procedure and the npi architecture is capable of generalizing from the step input-output pairs seen in the training data to the unseen ones present in the verification set,0
 ,1
 ,0
" the outputs rt, pt+1, at+1 are used to determine the next action, as described in algorithm 1",1
" for instance, for recursive addition, consider the family f of addition problems anan−1 ",0
"
in what follows, we describe the way in which we constructed npi training traces so as to make them contain recursive elements and thus enable npi to learn recursive programs",0
" in future work, we would like to decrease supervision and construct models that are capable of coming up with recursive abstractions themselves",0
" finally, the boolean pstack == 1 is used to check whether the stack is empty in line 18 of algorithm 2",1
3 to construct v and then empirically create a verification set which covers v ,1
" in particular, we construct new training traces which explicitly contain recursive elements and show that with this type of trace, npi easily learns recursive programs",1
" we make the trace recursive by adding a tail recursive call into the trace for the add program after calling add1 and lshift, the original paper uses k = 10, but we found it necessary to augment the range with an end token, in order to terminate properly",0
"
topological sort",1
" note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values, and arg 2 can only take one of 7 values",1
" using this modification, we constructed a verification set consisting of one array of size 10",1
 we describe the recursive re-formulation of traces for two tasks from the original npi paper—grade-school addition and bubble sort,1
" the partial recursion is not enough for perfect generalization, as will be presented later in section 4",1
" without using a recursive program, such a proof is not possible, because the non-recursive program runs on an arbitrarily long addition problem that creates correspondingly long sequences of step inputs; in the non-recursive formulation of addition, add calls add1 a number of times that is dependent on the length of the input",0
" the 4 environment observations aid with control flow; qstacklo(pstacklo− 1) < qstackhi(pstackhi− 1) implements the lo < hi comparison in line 5 of algorithm 3, pstacklo == 1 checks if the stacks are empty in line 18 of algorithm 4, and the other observations (all involving ppivot or pj) deal with logic in the partition function",0
" both the non-recursive and recursive learned programs generalize on all graphs we tried, up to 120 vertices",0
"
algorithm 3 recursive quicksort 1: initialize an array a to sort",0
" bstep, used in bubble, compares pairs of numbers, continuously moving the bubble pointers once to the right each time until reaching the end of the array",0
"
thus far, to evaluate the efficacy of neural models on programming tasks, the only metric that has been used is generalization of expected behavior to inputs of greater complexity (vinyals   (2015), kaiser & sutskever (2015), reed & de freitas (2016), graves   (2016), zaremba   (2016))",1
 we instantiated recursion for the neural programmerinterpreter by changing the training traces,1
we now report on generalization for the varying tasks,1
" the arrow in the theorem refers to evaluation, as in big-step semantics",0
 the environment and return probability are omitted for readability,1
" for each task, given the verification set, we check the traces and results of the learned, to-be-verified neural program (described in section 4",0
 the first is the actual model architecture,0
" this functionality the original npi paper decodes to a program key embedding kt ∈ rk and then computes a program embedding pt+1, which we also did in our implementation, but we omit this for brevity",0
"2 at any given time, the npi has access to values pointed to by four pointers in each of the four rows, represented by q(1, i1), q(2, i2), q(3, i3), and q(4, i4)",0
g,0
"
training setup",0
" we hypothesize that the reason for this is that the neural network learns to spuriously depend on specific characteristics of the training examples that are irrelevant to the true program semantics, such as length of the training inputs, and thus fails to generalize to more complex inputs",1
", to change the value of vactive) none described below move move a pointer (e",1
" by iterating with this procedure, and then running p on the input observation set that we obtain for the entry point function, we can obtain v precisely",0
 ,1
", the verification set",1
" the move operation has the following arguments:
arg 1 (pointer): pstacklo, pstackhi, pj , ppivot
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values",0
 the logic for the non-recursive trace is shown in algorithm 4 in the appendix,0
"
in order to demonstrate that recursion can help learn and generalize better, we trained a non-recursive and recursive model on just a single execution trace generated from a graph containing 5 nodes3 for the topological sort task",1
" our results show that for all learned, to-be-verified neural programs, they all produced the same traces as those produced by p on the verification set",0
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,1
 stack pop pushes −∞ values to qstacklo and qstackhi,1
"
algorithm 2 depth first search topological sort 1: color all vertices white",1
"
figure 1 shows examples of non-recursive and recursive addition traces",0
" when the program is called, the current context including the caller’s memory state is stored on a stack; when the program returns, the stored context is popped off the stack to resume execution in the previous caller’s context",1
" the architecture usually possesses some form of memory, which could be internal (such as the hidden state of a recurrent neural network) or external (such as a discrete “scratch pad” or a memory block with differentiable access)",0
"1, we hypothesize the non-recursive programs do not generalize well because they have learned spurious dependencies specific to the training set, such as length of the input problems",0
 ,0
"
the same set of problems was used to generate the training traces for all formulations of the task, for non-recursive and recursive versions",1
"
base cases and reduction rules for addition",1
" we implement a quicksort task, in order to demonstrate that recursion helps with learning divide-and-conquer algorithms",0
" m , as described in algorithm 1, processes the sequence of step inputs by using an lstm",1
"
the npi accesses an external environment, q, which varies according to the task",1
 table 4 also demonstrates that the (verified) recursive program generalizes perfectly,0
g,0
"
note that v should also contain the necessary reductions, which corresponds to making the recursive calls at the correct time, as indicated by p ",1
 the degree for any vertex in the dag is variable,1
" the partial recursion is not enough for perfect generalization, as will be presented later in section 4",0
" bstep, used in bubble, compares pairs of numbers, continuously moving the bubble pointers once to the right each time until reaching the end of the array",0
"1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 while (pstacklo 6= 1): 6 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 7 stack(stack_pop) 8 else: 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 stack(stack_pop) 12 partition() 13 stack(stack_push_call2) 14 stack(stack_push_call1) 15 } 16 17 partition() { 18 set_pivot_lo() 19 set_j_lo() 20 compswap_loop() 21 swap(ppivot, phi) 22 set_j_null() 23 } 24 25 compswap_loop() { 26 while (pj 6= phi): 27 compswap() 28 move(pj, up) 29 } 30 31 compswap() { 32 if (a[pj ] ≤ a[phi]): 33 swap(ppivot, pj) 34 move(ppivot, up) 35 } 36 37 stack(op) { 38 if (op == stack_push_call1): 39 write(env_stack_lo, plo) 40 write(env_stack_hi, ppivot − 1) 41 move(pstacklo, up) 42 move(pstackhi, up) 43 44 if (op == stack_push_call2): 45 write(env_stack_lo, ppivot + 1) 46 write(env_stack_hi, phi) 47 move(pstacklo, up) 48 move(pstackhi, up) 49 50 if (op == stack_pop): 51 write(env_stack_lo, reset) 52 write(env_stack_hi, reset) 53 move(pstacklo, down) 54 move(pstackhi, down) 55 }altered recursive functions 1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 6 partition() 7 stack(stack_push_call2) 8 stack(stack_push_call1) 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 quicksort() // recursive call 12 stack(stack_pop) 13 write(phi, env_stack_hi_peek) 14 write(plo, env_stack_lo_peek) 15 quicksort() // recursive call 16 stack(stack_pop) 17 } 18 19 compswap_loop() { 20 if (pj 6= phi): 21 compswap() 22 move(pj, up) 23 compswap_loop() // recursive call 24 }in this section, we describe the space of base cases and reduction rules that must be covered for each of the four sample tasks, in order to create the verification set",0
"
given a verification set, we can then run the model on the verification set to check if the produced traces and results are correct",0
" for bubble sort, the domain-specific encoder is
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1), q(1, i2), i3 == length, at(1), at(2), at(3)]),
where the environment q ∈ r1×n×k is a scratch-pad that contains 1 row, to represent the state of the array as sorting proceeds in-place, andn columns",0
"
there are 6 pointers (plo, phi, pstacklo, pstackhi, ppivot, pj)",1
" bstep, used in bubble, compares pairs of numbers, continuously moving the bubble pointers once to the right each time until reaching the end of the array",0
" qcolor(pstart) contains the color of the current start vertex, used in the evaluation of the condition in the while loop in line 5 of algorithm 2",0
" we have three observations: the color of the start node, the color of the active node’s next child to be considered, and whether the stack is empty",0
" however, the authors of npi did not consider explicitly the notion of recursion and as a consequence, did not learn recursive programs",0
"
the training set for topological sort contains 6 traces, with one synthesized from a graph of size 5 and the rest synthesized from graphs of size 7",0
" if it is too small, then execution of the program on some input might require evaluation of m(i) on some i /∈ v , and so the behavior ofm(i) might deviate from p (i)",0
" at end, pointer ppivot is moved to the pivot compswap loop, move pivot lo, move j lo, swap
none
compswap loop runs the for loop inside the partition function compswap, move none compswap compares a[pivot] ≤ a[j]; if so, perform a swap and increment ppivot swap, move none set pivot lo sets ppivot to lo index none none set j lo sets pj to lo index none none set j null sets pj to −∞ none none stack pushes lo/hi states
onto stacks slo and shi according to argument (described below)
write, move described below
move moves pointer one unit up or down none described below swap swaps elements at given array indices none described below write write a value either to stack (e",1
" 3: 4: function quicksort(a, lo, hi) 5: if lo < hi: then 6: p = partition(a, lo, hi) 7: quicksort(a, lo, p− 1) 8: quicksort(a, p+ 1, hi) 9: 10: function partition(a, lo, hi) 11: pivot = lo 12: for j ∈ [lo, hi− 1] : do 13: if a[j] ≤ a[hi] then 14: swap a[pivot] with a[j] 15: pivot = pivot+ 1 16: swap a[pivot] with a[hi] 17: return pivot
for quicksort, the domain-specific encoder is
fenc(qarray, qstacklo, qstackhi, plo, phi, pstacklo, pstackhi, ppivot, pj , at) =
mlp ([qarray(pj) ≤ qarray(phi), pj == phi, qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1), pstacklo == 1, at(1), at(2), at(3)]),
whereqarray ∈ ru×11 is a scratch-pad that contains u rows, each containing one of 11 values (one of the numbers 0 through 9 or an invalid state)",1
" the non-recursive and partially recursive versions are unable to sort long arrays, beyond length 8",0
"
grade-school addition",0
" both the non-recursive and recursive learned programs generalize on all input lengths we tried, up to 5000 digits",1
 we created a program set that reflects the semantics of algorithm 2,1
" for example, for the addition task, the model is trained on short inputs and then tested on its ability to sum inputs with much longer numbers of digits",0
" provably perfect generalization implies the model will behave correctly, given any valid input",1
" both the non-recursive and recursive learned programs generalize on all input lengths we tried, up to 5000 digits",0
" the move operation has the following arguments:
arg 1 (pointer): pstacklo, pstackhi, pj , ppivot
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values",1
" the 4 environment observations aid with control flow; qstacklo(pstacklo− 1) < qstackhi(pstackhi− 1) implements the lo < hi comparison in line 5 of algorithm 3, pstacklo == 1 checks if the stacks are empty in line 18 of algorithm 4, and the other observations (all involving ppivot or pj) deal with logic in the partition function",0
" in practice, we can additionally use tail recursion optimization to avoid problems with the call stack growing too deep",1
" each time a subprogram is called, the stack depth increases",1
"
one common strategy to improve generalization is to use curriculum learning, where the model is trained on inputs of gradually increasing complexity",0
"when constructing a neural network for the purpose of learning a program, there are two orthogonal aspects to consider",1
"as discussed in section 2, the neural programmer-interpreter (npi) is an instance of a neural programmer architecture and hence it naturally supports recursion",1
"1, we hypothesize the non-recursive programs do not generalize well because they have learned spurious dependencies specific to the training set, such as length of the input problems",0
"
base cases and reduction rules for quicksort",0
" twc-1409915, darpa under grant no",1
"
recursion drastically reduces the number of configurations we need to consider during the verification phase and makes the proof tractable, because it introduces structure that eliminates infinitely long sequences of step inputs that would otherwise need to be considered",1
"
an alternative method is to run p on many different program inputs and then observe step input sequences which occur, to create v ",0
" the pointers at index i1 and i2 are used to compare the pair of numbers considered during the bubble sweep, swapping them if the number at i1 is greater than that in i2",1
 we adapt machinery from the original paper slightly to fit our needs,1
" in this architecture, we consider a core controller, e",1
" the non-recursive and partially recursive versions are unable to sort long arrays, beyond length 8",1
 we created a program set that reflects the semantics of algorithm 2,0
" however, note that there is no provable guarantee that the non-recursive learned program will generalize to all inputs, whereas we show later that the recursive learned program has a provable guarantee of perfect generalization",1
g,0
" empirically, we observe that the learned recursive programs solve all valid inputs with 100% accuracy after training on a very small number of examples, out-performing previous generalization results",0
" given verification sets that cover all the base cases and reduction rules, we can provide proofs that these learned programs generalize perfectly",0
"
by nature, recursion reduces the complexity of a problem to simpler instances",0
" if yes, then this indicates that the learned neural program achieves provably perfect generalization",0
" we call this set of inputs the verification set, sv ",1
"
base cases and reduction rules for topological sort",1
"
the training set for quicksort contains 4 traces, synthesized from arrays of length 5",1
" m is considered to have perfect generalization if m can give the right answer for any input, such as inputs of arbitrary length",0
3,1
 npi then outputs the return probability and next program and arguments to execute,1
 the logic for the non-recursive trace is shown in algorithm 4 in the appendix,1
" we hypothesize that the reason for this is that the neural network learns to spuriously depend on specific characteristics of the training examples that are irrelevant to the true program semantics, such as length of the training inputs, and thus fails to generalize to more complex inputs",0
 we find that recursion makes it easier for the network to learn the right program and generalize to unknown situations,0
" the learned recursive program is different from neural programs learned in all previous work in an important aspect: previous approaches do not explicitly incorporate this abstraction, and hence generalize poorly, whereas our learned neural programs incorporate recursion and achieve perfect generalization",1
3,1
" when the program is called, the current context including the caller’s memory state is stored on a stack; when the program returns, the stored context is popped off the stack to resume execution in the previous caller’s context",0
" if the step output specifies a primitive function call, we need to reason about how it can affect the environment so as to change the observation in the next step input",0
5,0
" the write operation has the following arguments:
arg 1 (object to write): env stack lo, env stack hi, phi, plo
env stack lo and env stack hi represent qstacklo(pstacklo) and qstackhi(pstackhi), respectively",0
" for these programs, we re-use the appropriate program sets (the associated subprograms), and we refer the reader to the appendix of reed & de freitas (2016) for further details on the subprograms used in addition and bubble sort",0
" in particular, recursion can be implemented as a program calling itself",0
"
arg 2 (increment or decrement): up, down
swap",0
" for non-primitive subprograms, we can update the observation-to-observation mapping currently associated with the subprogram and then apply that mapping to the current set",0
"
the non-recursive trace loops on cycles of bubble and reset, which logically represents one bubble sweep through the array and reset of the two bubble pointers to the very beginning of the array, respectively",0
" numerous models have been proposed for learning programs; to name a few, this includes the differentiable neural computer (graves  , 2016), neural turing machine (graves  , 2014), neural gpu (kaiser & sutskever, 2015), neural programmer (neelakantan  , 2015), pointer network (vinyals  , 2015), hierarchical attentive memory (andrychowicz & kurach, 2016), and neural random access machine (kurach  , 2016)",0
 table 3 presents results on randomly generated arrays of varying length for the learned non-recursive and recursive programs,0
"
we experiment with two levels of recursion—partial and full",1
" at end, pointer ppivot is moved to the pivot compswap loop, move pivot lo, move j lo, swap
none
compswap loop runs the for loop inside the partition function compswap, move none compswap compares a[pivot] ≤ a[j]; if so, perform a swap and increment ppivot swap, move none set pivot lo sets ppivot to lo index none none set j lo sets pj to lo index none none set j null sets pj to −∞ none none stack pushes lo/hi states
onto stacks slo and shi according to argument (described below)
write, move described below
move moves pointer one unit up or down none described below swap swaps elements at given array indices none described below write write a value either to stack (e",0
 table 4 also demonstrates that the (verified) recursive program generalizes perfectly,0
" for non-primitive subprograms, we can update the observation-to-observation mapping currently associated with the subprogram and then apply that mapping to the current set",1
" the swap operation has the following arguments:
arg 1 (swap object 1): ppivot
arg 2 (swap object 2): phi, pj
write",0
" for these models, table 2 presents results on randomly generated dags of varying graph sizes (varying in the number of vertices)",0
"
it is sufficient to construct problems where every transition between two adjacent columns is covered",0
"
algorithm 3 recursive quicksort 1: initialize an array a to sort",0
" observe that the non-recursive program’s correctness degrades for length 11 and beyond, while the recursive program can sort any given array",0
" m is considered to have perfect generalization if m can give the right answer for any input, such as inputs of arbitrary length",0
"1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 while (pstacklo 6= 1): 6 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 7 stack(stack_pop) 8 else: 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 stack(stack_pop) 12 partition() 13 stack(stack_push_call2) 14 stack(stack_push_call1) 15 } 16 17 partition() { 18 set_pivot_lo() 19 set_j_lo() 20 compswap_loop() 21 swap(ppivot, phi) 22 set_j_null() 23 } 24 25 compswap_loop() { 26 while (pj 6= phi): 27 compswap() 28 move(pj, up) 29 } 30 31 compswap() { 32 if (a[pj ] ≤ a[phi]): 33 swap(ppivot, pj) 34 move(ppivot, up) 35 } 36 37 stack(op) { 38 if (op == stack_push_call1): 39 write(env_stack_lo, plo) 40 write(env_stack_hi, ppivot − 1) 41 move(pstacklo, up) 42 move(pstackhi, up) 43 44 if (op == stack_push_call2): 45 write(env_stack_lo, ppivot + 1) 46 write(env_stack_hi, phi) 47 move(pstacklo, up) 48 move(pstackhi, up) 49 50 if (op == stack_pop): 51 write(env_stack_lo, reset) 52 write(env_stack_hi, reset) 53 move(pstacklo, down) 54 move(pstackhi, down) 55 }altered recursive functions 1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 6 partition() 7 stack(stack_push_call2) 8 stack(stack_push_call1) 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 quicksort() // recursive call 12 stack(stack_pop) 13 write(phi, env_stack_hi_peek) 14 write(plo, env_stack_lo_peek) 15 quicksort() // recursive call 16 stack(stack_pop) 17 } 18 19 compswap_loop() { 20 if (pj 6= phi): 21 compswap() 22 move(pj, up) 23 compswap_loop() // recursive call 24 }in this section, we describe the space of base cases and reduction rules that must be covered for each of the four sample tasks, in order to create the verification set",0
" for each length, we test each program on 30 randomly generated problems",1
e,0
 ,0
" in practice, we can additionally use tail recursion optimization to avoid problems with the call stack growing too deep",0
"
formally, verification consists of proving the following theorem:
∀i ∈ v,m(i) ⇓ p (i) where i denotes a sequence of step inputs (within one function call), v denotes the set of valid sequences of step inputs, m denotes the neural network model, p denotes the correct program, and p (i) denotes the next step output from the correct program",1
 the maximum problem length in this training set is 3 (e,1
" however, note that there is no provable guarantee that the non-recursive learned program will generalize to all inputs, whereas we show later that the recursive learned program has a provable guarantee of perfect generalization",1
" for each function we use to implement the recursive version of topological sort, we need to consider the set of possible environment observation sequences we can create from all valid inputs and test that the learned program produces the correct behavior on each of these inputs",1
" it is straightforward to deal with all other operations, which do not induce partial environment states",0
" table 1 presents results on randomly generated arrays of varying length for the learned non-recursive, partially recursive, and full recursive programs",1
" the only model, to our knowledge, that does not train on input-output pairs is the neural programmer-interpreter (reed & de freitas, 2016), which trains on synthetic execution traces",1
g,0
" the swap operation has the following arguments:
arg 1 (swap object 1): ppivot
arg 2 (swap object 2): phi, pj
write",1
 we outline how to construct this set,0
"
the training data for the neural programmer-interpreter consists of full execution traces for the program of interest",0
" k is set to 11, to represent the range of 10 possible digits, along with a token representing the end of input",1
"
training setup",1
" twc-1409915, darpa under grant no",0
" in computer science, recursion (as opposed to iteration) involves solving a larger problem by combining solutions to smaller instances of the same problem",0
this material is in part based upon work supported by the national science foundation under grant no,0
 this verification phase only needs to be performed once after training,0
 the training set for addition contains 200 traces,0
" in this version, there is a dependence on length: bstep and lshift are called a number of times equivalent to one less than the length of the input array, in bubble and reset respectively",0
"
arg 2 (increment or decrement): up, down1 // top level topological sort call 2 toposort() { 3 while (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 } 0 11 traverse() { 12 check_child() 13 explore() 14 } 15 16 check_child() { 17 while (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 18 move(childlist[vactive], up) 19 } 20 21 explore() { 22 do 23 if (qcolor(dag[vactive][childlist[vactive]]) is white): 24 write(color_next, color_grey) 25 stack(push) 26 write(save) 27 write(active_neighb) 28 move(childlist[vsave], up) 29 else: 30 write(color_curr, color_black) 31 write(result) 32 move(presult, up) 33 if(pstack == 1): 34 break 35 else: 36 stack(pop) 37 check_child() 38 while (true) 39 } 40 41 stack(op) { 42 if (op == push): 43 write(stack_push) 44 move(pstack, up) 45 46 if (op == pop): 47 write(active_stack) 48 write(stack_pop) 49 move(pstack, down) 50 } 51 52 next_start() { 53 while(qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 54 move(pstart, up) 55 }altered recursive functions  // top level topological sort call 2 toposort() { 3 if (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 toposort() // recursive call 0 } 11 12 check_child() { 13 if (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 14 move(childlist[vactive], up) 15 check_child() // recursive call 16 } 17 18 explore() { 19 if (qcolor(dag[vactive][childlist[vactive]]) is white): 20 write(color_next, color_grey) 21 stack(push) 22 write(save) 23 write(active_neighb) 24 move(childlist[vsave], up) 25 else: 26 write(color_curr, color_black) 27 write(result) 28 move(presult, up) 29 if(pstack == 1): 30 return 31 else: 32 stack(pop) 33 check_child() 34 explore() // recursive call 35 } 36 37 next_start() { 38 if (qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 39 move(pstart, up) 40 next_start() // recursive call 41 }algorithm 4 iterative quicksort 1: initialize an array a to sort and two empty stacks slo and shi",1
" we performed experiments on the partially recursive version in order to examine what happens when only one recursive call is implemented, when in reality three are required for perfect generalization",1
"
we construct input problems by splitting into two cases: one case in which the left column contains a null value and another in which the left column does not contain any null values",0
 this algorithm is a variant of depth first search,1
" in practice, we can additionally use tail recursion optimization to avoid problems with the call stack growing too deep",0
" in particular, we construct new training traces which explicitly contain recursive elements and show that with this type of trace, npi easily learns recursive programs",1
"
the dag is represented as an adjacency list where dag[i][j] refers to the j-th child of vertex i",1
" we then construct problem configurations that span all possible valid environment states (for instance, in order to force the carry bit in a column to be 1, one can add the sum “1+9” in the column to the right)",0
" thus, we demonstrate that recursion enables provably perfect generalization for different tasks, including addition, topological sort, quicksort, and a variant of bubble sort",1
"
the non-recursive trace loops on cycles of add1 and lshift",0
" 2: initialize lo and hi to be 1 and n, where n is the length of a",0
"
move",1
" in particular, in the explore function, adding a tail recursive call resets and stores the hidden states associated with vertices in a stack-like fashion",1
" at end, pointer ppivot is moved to the pivot compswap loop, move pivot lo, move j lo, swap
none
compswap loop runs the for loop inside the partition function compswap, move none compswap compares a[pivot] ≤ a[j]; if so, perform a swap and increment ppivot swap, move none set pivot lo sets ppivot to lo index none none set j lo sets pj to lo index none none set j null sets pj to −∞ none none stack pushes lo/hi states
onto stacks slo and shi according to argument (described below)
write, move described below
move moves pointer one unit up or down none described below swap swaps elements at given array indices none described below write write a value either to stack (e",1
" 3: 4: function quicksort(a, lo, hi) 5: if lo < hi: then 6: p = partition(a, lo, hi) 7: quicksort(a, lo, p− 1) 8: quicksort(a, p+ 1, hi) 9: 10: function partition(a, lo, hi) 11: pivot = lo 12: for j ∈ [lo, hi− 1] : do 13: if a[j] ≤ a[hi] then 14: swap a[pivot] with a[j] 15: pivot = pivot+ 1 16: swap a[pivot] with a[hi] 17: return pivot
for quicksort, the domain-specific encoder is
fenc(qarray, qstacklo, qstackhi, plo, phi, pstacklo, pstackhi, ppivot, pj , at) =
mlp ([qarray(pj) ≤ qarray(phi), pj == phi, qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1), pstacklo == 1, at(1), at(2), at(3)]),
whereqarray ∈ ru×11 is a scratch-pad that contains u rows, each containing one of 11 values (one of the numbers 0 through 9 or an invalid state)",0
 almost all architectures train on program input/output pairs,0
 this paper is the first (to our knowledge) to investigate the important problem of provable generalization properties of neural programs,1
" our results show that for all learned, to-be-verified neural programs, they all produced the same traces as those produced by p on the verification set",0
"when constructing a neural network for the purpose of learning a program, there are two orthogonal aspects to consider",0
 our result shows that the training procedure and the npi architecture is capable of generalizing from the step input-output pairs seen in the training data to the unseen ones present in the verification set,0
"
base cases and reduction rules for addition",1
", an lstm in npi’s case, but possibly other networks in different cases",0
 the model described in the training setup of section 4 (trained on 6 traces) was verified to be correct via the matching procedure described in section 4,0
", the trace corresponding to the problem “109 + 101”)",0
we emphasize that the notion of a neural recursive program has not been presented in the literature before: this is our main contribution,1
" in particular, this condition is satisfied by npi, and thus the npi model naturally supports recursion (even though the authors of npi did not consider this aspect explicitly)",0
" observe that partially recursive does slightly better than non-recursive for the setting in which the length of the array is 3, and that the fully recursive version is able to sort every array given to it",0
" the architecture usually possesses some form of memory, which could be internal (such as the hidden state of a recurrent neural network) or external (such as a discrete “scratch pad” or a memory block with differentiable access)",1
"
note that v should also contain the necessary reductions, which corresponds to making the recursive calls at the correct time, as indicated by p ",0
"
to construct this set, by using the reference implementation of each subprogram, we construct a mapping between two sets of environment observations: the first set consists of all observations that can occur at the beginning of a particular subprogram’s invocation, and the second set contains the observations at the end of that subprogram",0
 ,1
"
base cases and reduction rules for quicksort",0
"
for addition, we analytically determine the verification set",0
" in this version, there is a dependence on length: bstep and lshift are called a number of times equivalent to one less than the length of the input array, in bubble and reset respectively",1
" childlist ∈ nu is a vector of pointers, where childlist[i] points to the next child under consideration for vertex i",1
" in order to compensate for this, ppivot and pj require special functions (move pivot lo and move j lo) to properly set them to lo in lines 11 and 12 of the partition function in algorithm 3",1
3,0
 ,1
" ppivot and pj point to the pivot and j indices of the array, used in the partition function in algorithm 3",0
"
bubble sort",1
" while this matches the description from reed & de freitas (2016), we found that this causes an unnecessary blowup in the size of v and makes it much more difficult to construct the verification set",0
" also the dag can have potentially more than one connected component, meaning it is necessary to transition between these components appropriately",1
" this agrees with the generalization of non-recursive addition in reed & de freitas (2016), where they reported generalization up to 3000 digits",0
 recursion enables provable guarantees on neural programs’ behavior without needing to exhaustively enumerate all possible inputs to the programs,0
" for example, a step input-step output pair in lines 2 and 3 of the left-hand side of figure 1 is (add1, write out 1)",0
" 9: else 10: function run(e, p2, a2)
a description of the inference procedure is given in algorithm 1",0
"we describe how models trained with recursive traces can be proven to generalize, by using the verification procedure described in section 3",0
" it is straightforward to deal with all other operations, which do not induce partial environment states",1
" however, the authors of npi did not consider explicitly the notion of recursion and as a consequence, did not learn recursive programs",0
" full recursion, in addition to making the aforementioned tail recursive call, adds two additional recursive calls; bstep and lshift are made tail recursive",1
" provably perfect generalization implies the model will behave correctly, given any valid input",0
" 3: 4: function partition(a, lo, hi) 5: pivot = lo 6: for j ∈ [lo, hi− 1] : do 7: if a[j] ≤ a[hi] then 8: swap a[pivot] with a[j] 9: pivot = pivot+ 1 0: swap a[pivot] with a[hi] 11: return pivot 12: 13: function quicksort(a, lo, hi) 14: while slo and shi are not empty: do 15: pop states off slo and shi, writing them to lo and hi",1
" for problems where the base cases may be extremely large or infinite, such as certain forms of motor control, recursion can still help reduce the problem of generalization to these two aspects and make the generalization problem significantly simpler to handle and reason about",0
" our results show that for all learned, to-be-verified neural programs, they all produced the same traces as those produced by p on the verification set",0
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,0
"
we would also like to point out that in this paper, we provide as an example one way to train a recursive neural program, by providing a certain training execution trace to the npi model",0
" k is set to 11, to denote the range of possible numbers (0 through 9), along with the start/end token (represented with the same encoding) which is observed when a pointer reaches beyond the bounds of the input",1
" we show that by modifying the training procedure, we enable the npi model to learn recursive neural programs",0
2,1
"
we propose and describe our verification procedure",1
"
as in line 13 of the right-hand side of figure 1",0
", the trace corresponding to the problem “109 + 101”)",0
 the program terminates when seeing no numbers in the current column,0
", the trace corresponding to the array [3,2])",0
" the outputs rt, pt+1, at+1 are used to determine the next action, as described in algorithm 1",0
 we instantiated recursion for the neural programmerinterpreter by changing the training traces,0
" empirically, we observe that the learned recursive programs solve all valid inputs with 100% accuracy after training on a very small number of examples, out-performing previous generalization results",1
 18: push lo and p− 1 to slo and shi,0
"
algorithm 2 depth first search topological sort 1: color all vertices white",0
 the first is the actual model architecture,1
" empirically, existing models suffer from a common limitation—generalization becomes poor beyond a threshold level of complexity",0
", qstacklo or qstackhi) or to pointer (e",1
"
algorithm 1 neural programming inference 1: inputs: environment observation e, program p, arguments a, stop threshold α 2: function run(e, p, a) 3: h← 0, r ← 0 4: while r < α do 5: s← fenc(e, a), h← flstm(s, p, h) 6: r ← fend(h), p2 ← fprog(h), a2 ← farg(h) 7: if p is a primitive function then 8: e← fenv(e, p, a)",0
" the outputs rt, pt+1, at+1 are used to determine the next action, as described in algorithm 1",0
" in fact, it is one of our future directions to explore new ways to train a recursive neural program without providing explicit training execution traces or with only partial or non-recursive traces",0
" , a1a0 + b1b0} are added properly",1
" an example of part of an addition task trace, written in shorthand, is given in figure 1",1
 the core controller acts as a dispatcher for the programs,0
"
for addition, we analytically determine the verification set",1
3,1
6,1
" any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of national science foundation and darpa",0
" the core lstm module’s hidden state is preserved over all these add1 calls, and it is difficult to interpret with certainty what happens over longer timesteps without concretely evaluating the lstm with an input of that length",1
"
arg 2 (object to copy): env stack lo peek, env stack hi peek, phi, plo, ppivot − 1, ppivot + 1, reset
env stack lo peek and env stack hi peek represent qstacklo(pstacklo − 1) and qstackhi(pstackhi − 1), respectively",0
g,1
"
to construct this set, by using the reference implementation of each subprogram, we construct a mapping between two sets of environment observations: the first set consists of all observations that can occur at the beginning of a particular subprogram’s invocation, and the second set contains the observations at the end of that subprogram",0
"1; and for bubble sort, appendix a",1
" however, the authors of npi did not consider explicitly the notion of recursion and as a consequence, did not learn recursive programs",0
" recursion is an important concept in programming languages and a critical tool to
reduce the complexity of programs",1
"
an alternative way of representing the environment slice is to expose the values of the absolute vertices to the model; however, this makes it difficult to scale the model to larger graphs, since large vertex values are not seen during training time",0
"as discussed in section 2, the neural programmer-interpreter (npi) is an instance of a neural programmer architecture and hence it naturally supports recursion",1
" we then construct problem configurations that span all possible valid environment states (for instance, in order to force the carry bit in a column to be 1, one can add the sum “1+9” in the column to the right)",1
 table 4 also demonstrates that the (verified) recursive program generalizes perfectly,0
" the stack operation has the following arguments:
arg 1 (operation): stack push call1, stack push call2, stack pop
stack push call1 pushes lo and pivot−1 toqstacklo andqstackhi",1
 2: initialize an empty stack s and a directed acyclic graph dag to traverse,0
" the non-recursive and partially recursive versions are unable to sort long arrays, beyond length 8",1
 the environment and return probability are omitted for readability,0
"
in this paper, we propose that recursion is an important concept for neural programs as well",0
" to the best of our knowledge, this is the first time verification has been applied to a neural program, providing provable guarantees about its behavior",0
"
in what follows, we describe the way in which we constructed npi training traces so as to make them contain recursive elements and thus enable npi to learn recursive programs",0
" the recursive version can generalize perfectly to long problems constructed from these components (such as the sum “822+233”, where “8+2” and “2+3” are in the training set), but the non-recursive version fails to sum these long problems properly",0
 recursion can be implemented differently for different neural programming models,1
"
in order to demonstrate that recursion can help learn and generalize better, we trained a non-recursive and recursive model on just a single execution trace generated from a graph containing 5 nodes3 for the topological sort task",0
" a topological sort is a linear ordering of vertices such that for every directed edge (u, v) from u to v, u comes before v in the ordering",1
" however, our concept of recursion for neural programs is general",1
" for each length, we test each program on 30 randomly generated problems",0
" in this pair, the step input runs a subprogram add1 that has no arguments, and the step output contains a program write that has arguments of out and 1",1
"
base cases and reduction rules for addition",0
"
in this general neural programming architecture, we show it is easy to support recursion",0
" partial recursion only adds a tail recursive call to bubblesort after bubble and reset, similar to the tail recursive call described previously for addition",1
" bstep, used in bubble, compares pairs of numbers, continuously moving the bubble pointers once to the right each time until reaching the end of the array",0
" the move operation has the following arguments:
arg 1 (pointer): pstacklo, pstackhi, pj , ppivot
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values",1
"
recursion drastically reduces the number of configurations we need to consider during the verification phase and makes the proof tractable, because it introduces structure that eliminates infinitely long sequences of step inputs that would otherwise need to be considered",1
"we show that if we incorporate recursion, the learned npi programs can achieve provably perfect generalization for different tasks",0
" m is considered to have perfect generalization if m can give the right answer for any input, such as inputs of arbitrary length",0
g,0
" consequently, there is no concept of length relevant to the problem, which has traditionally been an important focus of length-based curriculum learning",1
" while this matches the description from reed & de freitas (2016), we found that this causes an unnecessary blowup in the size of v and makes it much more difficult to construct the verification set",0
" formally, the npi is represented by the following set of equations:
st = fenc(et, at)
ht = flstm(st, pt, ht−1)
rt = fend(ht), pt+1 = fprog(ht), at+1 = farg(ht)
t is a subscript denoting the time-step; fenc is a domain-specific encoder (to be described later) that takes in the environment slice et and arguments at; flstm represents the core module, which takes in the state st generated by fenc, a program embedding pt ∈ rp , and hidden lstm state ht; fend decodes the return probability rt; fprog decodes a program key embedding pt+1;1 and farg decodes arguments at+1",0
"
we would also like to point out that in this paper, we provide as an example one way to train a recursive neural program, by providing a certain training execution trace to the npi model",1
" any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of national science foundation and darpa",0
" however, models that make use of this strategy eventually fail after a certain level of complexity (e",0
"1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 while (pstacklo 6= 1): 6 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 7 stack(stack_pop) 8 else: 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 stack(stack_pop) 12 partition() 13 stack(stack_push_call2) 14 stack(stack_push_call1) 15 } 16 17 partition() { 18 set_pivot_lo() 19 set_j_lo() 20 compswap_loop() 21 swap(ppivot, phi) 22 set_j_null() 23 } 24 25 compswap_loop() { 26 while (pj 6= phi): 27 compswap() 28 move(pj, up) 29 } 30 31 compswap() { 32 if (a[pj ] ≤ a[phi]): 33 swap(ppivot, pj) 34 move(ppivot, up) 35 } 36 37 stack(op) { 38 if (op == stack_push_call1): 39 write(env_stack_lo, plo) 40 write(env_stack_hi, ppivot − 1) 41 move(pstacklo, up) 42 move(pstackhi, up) 43 44 if (op == stack_push_call2): 45 write(env_stack_lo, ppivot + 1) 46 write(env_stack_hi, phi) 47 move(pstacklo, up) 48 move(pstackhi, up) 49 50 if (op == stack_pop): 51 write(env_stack_lo, reset) 52 write(env_stack_hi, reset) 53 move(pstacklo, down) 54 move(pstackhi, down) 55 }altered recursive functions 1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 6 partition() 7 stack(stack_push_call2) 8 stack(stack_push_call1) 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 quicksort() // recursive call 12 stack(stack_pop) 13 write(phi, env_stack_hi_peek) 14 write(plo, env_stack_lo_peek) 15 quicksort() // recursive call 16 stack(stack_pop) 17 } 18 19 compswap_loop() { 20 if (pj 6= phi): 21 compswap() 22 move(pj, up) 23 compswap_loop() // recursive call 24 }in this section, we describe the space of base cases and reduction rules that must be covered for each of the four sample tasks, in order to create the verification set",1
"
note that v should also contain the necessary reductions, which corresponds to making the recursive calls at the correct time, as indicated by p ",0
 we use the lomuto partition scheme; the logic for the recursive trace is shown in algorithm 3,1
" twc-1409915, darpa under grant no",0
"
for topological sort, the domain-specific encoder is
fenc(dag,qcolor, pstack, pstart, vactive, childlist, at)
=mlp ([qcolor(pstart), qcolor(dag[vactive][childlist[vactive]]), pstack == 1, at(1), at(2), at(3)]),
where qcolor ∈ ru×4 is a scratch-pad that contains u rows, each containing one of four colors (white, gray, black, invalid) with one-hot encoding",1
 this procedure verifies that all base cases and reductions are handled properly by the model via explicit tests,0
" we make the trace recursive by adding a tail recursive call into the trace for the add program after calling add1 and lshift, the original paper uses k = 10, but we found it necessary to augment the range with an end token, in order to terminate properly",1
 this paper is the first (to our knowledge) to investigate the important problem of provable generalization properties of neural programs,0
" consequently, there is no concept of length relevant to the problem, which has traditionally been an important focus of length-based curriculum learning",1
" to make an analogy to mdps, this procedure is analogous to how value iteration obtains the correct value for each state starting from any initialization",0
g,0
" here as a concrete and general example, we consider a general neural programming architecture (npa), similar to neural programmer-interpreter (npi) in reed & de freitas (2016)",0
" if the program is primitive, the next environmental state et+1 will be affected by pt and at, i",1
this material is in part based upon work supported by the national science foundation under grant no,0
"we describe how models trained with recursive traces can be proven to generalize, by using the verification procedure described in section 3",0
"
is critical for implementing recursion, since it permits us to restrict our attention to the currently relevant recursive call, ignoring irrelevant details about other contexts",1
" however, models that make use of this strategy eventually fail after a certain level of complexity (e",0
" as described in the verification procedure, it is possible to prove our learned recursive program generalizes perfectly by testing on an appropriate set of problem inputs, i",1
" for grade-school addition, the domain-specific encoder is
fenc(q, i1, i2, i3, i4, at) =mlp ([q(1, i1), q(2, i2), q(3, i3), q(4, i4), at(1), at(2), at(3)]),
where the environment q ∈ r4×n×k is a scratch-pad that contains four rows (the first input number, the second input number, the carry bits, and the output) and n columns",0
"
move",1
" here as a concrete and general example, we consider a general neural programming architecture (npa), similar to neural programmer-interpreter (npi) in reed & de freitas (2016)",0
" for brevity, we refer the reader to the appendix for information about the program set and non-recursive and recursive trace-generating functions for quicksort",1
" for each length, we test each program on 30 randomly generated problems",1
g,0
 errors arise due to undesirable and uninterpretable dependencies and associations the architecture learns to store in some high-dimensional hidden state,1
"
under the assumption that there are no leading 0’s (except in the case of single digits) and the two numbers to be added have the same number of digits, the verification set for addition contains 20,181 input problems",0
" in computer science, recursion (as opposed to iteration) involves solving a larger problem by combining solutions to smaller instances of the same problem",1
" for example, a step input-step output pair in lines 2 and 3 of the left-hand side of figure 1 is (add1, write out 1)",0
"
to construct this set, by using the reference implementation of each subprogram, we construct a mapping between two sets of environment observations: the first set consists of all observations that can occur at the beginning of a particular subprogram’s invocation, and the second set contains the observations at the end of that subprogram",1
" the partial recursion is not enough for perfect generalization, as will be presented later in section 4",0
"
arg 2 (auxiliary variable): color grey, color black
color grey and color black color the given vertex grey and black, respectively",1
" in particular, in the explore function, adding a tail recursive call resets and stores the hidden states associated with vertices in a stack-like fashion",0
 indentation indicates the stack is one level deeper than before,0
" more specifically, when m is trained on simpler inputs, such as inputs of a small length, the generalization metric evaluates how well m will do on more complex inputs, such as inputs of much longer length",1
" if yes, then this indicates that the learned neural program achieves provably perfect generalization",1
" for example, a step input-step output pair in lines 2 and 3 of the left-hand side of figure 1 is (add1, write out 1)",1
", to color a vertex) or variable (e",1
" in practice, we can additionally use tail recursion optimization to avoid problems with the call stack growing too deep",0
" 9: else 10: function run(e, p2, a2)
a description of the inference procedure is given in algorithm 1",0
g,0
g,0
" in particular, we construct new training traces which explicitly contain recursive elements and show that with this type of trace, npi easily learns recursive programs",0
" we also hope to decrease supervision, for example by training with only partial or non-recursive traces, and to develop novel neural programming architectures integrated directly with a notion of recursion",1
" m is considered to have perfect generalization if m can give the right answer for any input, such as inputs of arbitrary length",1
" to the best of our knowledge, this is the first time verification has been applied to a neural program, providing provable guarantees about its behavior",0
", an lstm in npi’s case, but possibly other networks in different cases",0
" each time a subprogram is called, the stack depth increases",0
" using this modification, we constructed a verification set consisting of one array of size 10",0
 ,1
" this is possible if and only if the graph has no directed cycles; that is to say, it must be a directed acyclic graph (dag)",1
" the write operation has the following arguments:
arg 1 (main action): color curr, color next, active start, active neighb, active stack, save, stack push, stack pop, result
color curr colors vactive, color next colors vertex dag[vactive][childlist[vactive]], active start writes pstart to vactive, active neighb writes dag[vactive][childlist[vactive]] to vactive, active stack writes qstack(pstack) to vactive, save writes vactive to vsave, stack push pushes vactive to the top of the stack, stack pop writes a null value to the top of the stack, and result writes vactive to qresult(presult)",0
g,1
" for non-primitive subprograms, we can update the observation-to-observation mapping currently associated with the subprogram and then apply that mapping to the current set",0
" formally, a function exhibits recursive behavior when it possesses two properties: (1) base cases— terminating scenarios that do not use recursion to produce answers; (2) a set of rules that reduces all other problems toward the base cases",0
" for example, a step input-step output pair in lines 2 and 3 of the left-hand side of figure 1 is (add1, write out 1)",1
" we implement a quicksort task, in order to demonstrate that recursion helps with learning divide-and-conquer algorithms",1
" our implementation uses two stacks qstacklo and
qstackhi, each in ru , that store the arguments to the recursive quicksort calls in algorithm 3; before each recursive call, the appropriate arguments are popped off the stack and written to plo and phi",0
"
the non-recursive trace loops on cycles of bubble and reset, which logically represents one bubble sweep through the array and reset of the two bubble pointers to the very beginning of the array, respectively",1
 there are 2 variables (vactive and vsave); vactive holds the active vertex (as in algorithm 2) and vsave holds the value of vactive before executing line 12 of algorithm 2,1
" partial recursion only adds a tail recursive call to bubblesort after bubble and reset, similar to the tail recursive call described previously for addition",0
"as there is no public implementation of npi, we implemented a version of it in keras that is as faithful to the paper as possible",0
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,0
"
directed acyclic graphs are structurally more diverse than inputs in the two tasks of grade-school addition and bubble sort",0
 a1a0 + bnbn−1 ,0
" the pointer at index i3 represents a counter internal to the environment that is incremented once after each pass of the algorithm (one cycle of bubble and reset); when incremented a number of times equal to the length of the array, the flag i3 == length becomes true and terminates the entire algorithm ",1
" thus, any general neural programming architecture supporting such a call structure can be made to support recursion",0
" even though the earlier version also generalized just as well in practice, relatively small differences in the formulation of the traces and the environment observations can drastically change the difficulty of verification",0
 recursion enables provable guarantees on neural programs’ behavior without needing to exhaustively enumerate all possible inputs to the programs,0
 we describe the recursive re-formulation of traces for two tasks from the original npi paper—grade-school addition and bubble sort,0
" it is straightforward to deal with all other operations, which do not induce partial environment states",1
" also the dag can have potentially more than one connected component, meaning it is necessary to transition between these components appropriately",0
" there are 3 pointers (presult, pstack, pstart), presult points to the next empty location in qresult,
pstack points to the top of the stack in qstack, and pstart points to the candidate starting node for a connected component",1
 ,1
 ,0
 our result shows that the training procedure and the npi architecture is capable of generalizing from the step input-output pairs seen in the training data to the unseen ones present in the verification set,0
 our experiments use a small number of training examples,0
" if it is too small, then execution of the program on some input might require evaluation of m(i) on some i /∈ v , and so the behavior ofm(i) might deviate from p (i)",0
"
in order to demonstrate that recursion can help learn and generalize better, we trained a non-recursive and recursive model on just a single execution trace generated from a graph containing 5 nodes3 for the topological sort task",0
"
base cases and reduction rules for topological sort",1
" however, the authors of npi did not consider explicitly the notion of recursion and as a consequence, did not learn recursive programs",0
"
arg 2 (increment or decrement): up, down
swap",0
"
arg 2 (object to copy): env stack lo peek, env stack hi peek, phi, plo, ppivot − 1, ppivot + 1, reset
env stack lo peek and env stack hi peek represent qstacklo(pstacklo − 1) and qstackhi(pstackhi − 1), respectively",1
"
arg 2 (increment or decrement): up, down1 // top level topological sort call 2 toposort() { 3 while (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 } 0 11 traverse() { 12 check_child() 13 explore() 14 } 15 16 check_child() { 17 while (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 18 move(childlist[vactive], up) 19 } 20 21 explore() { 22 do 23 if (qcolor(dag[vactive][childlist[vactive]]) is white): 24 write(color_next, color_grey) 25 stack(push) 26 write(save) 27 write(active_neighb) 28 move(childlist[vsave], up) 29 else: 30 write(color_curr, color_black) 31 write(result) 32 move(presult, up) 33 if(pstack == 1): 34 break 35 else: 36 stack(pop) 37 check_child() 38 while (true) 39 } 40 41 stack(op) { 42 if (op == push): 43 write(stack_push) 44 move(pstack, up) 45 46 if (op == pop): 47 write(active_stack) 48 write(stack_pop) 49 move(pstack, down) 50 } 51 52 next_start() { 53 while(qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 54 move(pstart, up) 55 }altered recursive functions  // top level topological sort call 2 toposort() { 3 if (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 toposort() // recursive call 0 } 11 12 check_child() { 13 if (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 14 move(childlist[vactive], up) 15 check_child() // recursive call 16 } 17 18 explore() { 19 if (qcolor(dag[vactive][childlist[vactive]]) is white): 20 write(color_next, color_grey) 21 stack(push) 22 write(save) 23 write(active_neighb) 24 move(childlist[vsave], up) 25 else: 26 write(color_curr, color_black) 27 write(result) 28 move(presult, up) 29 if(pstack == 1): 30 return 31 else: 32 stack(pop) 33 check_child() 34 explore() // recursive call 35 } 36 37 next_start() { 38 if (qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 39 move(pstart, up) 40 next_start() // recursive call 41 }algorithm 4 iterative quicksort 1: initialize an array a to sort and two empty stacks slo and shi",1
" both the non-recursive and recursive learned programs generalize on all input lengths we tried, up to 5000 digits",0
"
to perform the verification as described here, it is critical to construct v correctly",0
", an array for quicksort or a dag for topological sort)",0
" the pointers at index i1 and i2 are used to compare the pair of numbers considered during the bubble sweep, swapping them if the number at i1 is greater than that in i2",0
" in contrast, each call to the recursive add always runs for a fixed number of steps, even on arbitrarily long problems in f , so we can test that it performs correctly on a small, fixed number of step input sequences",1
g,0
"
there are 6 pointers (plo, phi, pstacklo, pstackhi, ppivot, pj)",1
" the outputs rt, pt+1, at+1 are used to determine the next action, as described in algorithm 1",0
"
as a concrete instantiation, we show in this paper that we can enable recursive neural programs in the npi model, and thus enable perfectly generalizable neural programs for tasks such as sorting where the original, non-recursive npi program fails",0
g,1
3,0
"
arg 2 (increment or decrement): up, down
swap",0
 ,0
 ,1
"
arg 2 (increment or decrement): up, down1 // top level topological sort call 2 toposort() { 3 while (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 } 0 11 traverse() { 12 check_child() 13 explore() 14 } 15 16 check_child() { 17 while (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 18 move(childlist[vactive], up) 19 } 20 21 explore() { 22 do 23 if (qcolor(dag[vactive][childlist[vactive]]) is white): 24 write(color_next, color_grey) 25 stack(push) 26 write(save) 27 write(active_neighb) 28 move(childlist[vsave], up) 29 else: 30 write(color_curr, color_black) 31 write(result) 32 move(presult, up) 33 if(pstack == 1): 34 break 35 else: 36 stack(pop) 37 check_child() 38 while (true) 39 } 40 41 stack(op) { 42 if (op == push): 43 write(stack_push) 44 move(pstack, up) 45 46 if (op == pop): 47 write(active_stack) 48 write(stack_pop) 49 move(pstack, down) 50 } 51 52 next_start() { 53 while(qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 54 move(pstart, up) 55 }altered recursive functions  // top level topological sort call 2 toposort() { 3 if (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 toposort() // recursive call 0 } 11 12 check_child() { 13 if (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 14 move(childlist[vactive], up) 15 check_child() // recursive call 16 } 17 18 explore() { 19 if (qcolor(dag[vactive][childlist[vactive]]) is white): 20 write(color_next, color_grey) 21 stack(push) 22 write(save) 23 write(active_neighb) 24 move(childlist[vsave], up) 25 else: 26 write(color_curr, color_black) 27 write(result) 28 move(presult, up) 29 if(pstack == 1): 30 return 31 else: 32 stack(pop) 33 check_child() 34 explore() // recursive call 35 } 36 37 next_start() { 38 if (qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 39 move(pstart, up) 40 next_start() // recursive call 41 }algorithm 4 iterative quicksort 1: initialize an array a to sort and two empty stacks slo and shi",1
" qcolor(dag[vactive][childlist[vactive]]) refers to the color of the next child of vactive, used in the evaluation of the condition in the if branch in line 9 of algorithm 2",0
" as before, the non-recursive learned program lacks a provable guarantee of generalization, whereas we show later that the recursive learned program has one",1
" the space of possible programs is extremely large, and composing a program that performs robustly on the infinite space of possible inputs is difficult—in part because it is impractical to obtain enough training examples to easily disambiguate amongst all possible programs",1
"
quicksort",0
" finally, the boolean pstack == 1 is used to check whether the stack is empty in line 18 of algorithm 2",1
" observe that the non-recursive program’s correctness degrades for length 11 and beyond, while the recursive program can sort any given array",0
"
algorithm 2 depth first search topological sort 1: color all vertices white",0
3,1
", pstart or childlist[vactive]) up or down none described belowwrite",1
e,1
" for tasks other than addition, it is difficult to analytically determine the verification set, so instead, we randomly generate input candidates until they completely cover the base cases and reduction rules",0
"
we experiment with two levels of recursion—partial and full",1
"
the dag is represented as an adjacency list where dag[i][j] refers to the j-th child of vertex i",0
" in our experiments, we only present dag’s as inputs and represent the vertices as values ranging from 1, ",0
 we outline how to construct this set,0
"
we also report on generalization results for the non-recursive and recursive versions of this variant of bubble sort",0
" this agrees with the generalization of non-recursive addition in reed & de freitas (2016), where they reported generalization up to 3000 digits",1
"
topological sort",0
" an example of part of an addition task trace, written in shorthand, is given in figure 1",1
" empirically, we observe that the learned recursive programs solve all valid inputs with 100% accuracy after training on a very small number of examples, out-performing previous generalization results",0
" our implementation uses two stacks qstacklo and
qstackhi, each in ru , that store the arguments to the recursive quicksort calls in algorithm 3; before each recursive call, the appropriate arguments are popped off the stack and written to plo and phi",0
"
we describe the base cases, reduction rules, and the verification set for each task in appendix a",1
" for these programs, we re-use the appropriate program sets (the associated subprograms), and we refer the reader to the appendix of reed & de freitas (2016) for further details on the subprograms used in addition and bubble sort",0
" twc-1409915, darpa under grant no",0
" the contents ofqresult andqstack are not exposed directly through the domain-specific encoder; rather, we define primitive functions which manipulate these scratch-pads",1
" for each graph size, we test the learned programs on 30 randomly generated dags",1
"
as in line 13 of the right-hand side of figure 1",1
" we call this set of inputs the verification set, sv ",0
" we prove every member of f is added properly, given that subproblems s = {anan−1 + bnbn−1, an−1an−2 + bn−1bn−2, ",0
"
in this paper, we propose that recursion is an important concept for neural programs as well",1
" as a consequence, our learned neural programs empirically achieve perfect generalization from a very small number of training examples",1
" in our experiments, we only present dag’s as inputs and represent the vertices as values ranging from 1, ",1
g,1
" table 1 presents results on randomly generated arrays of varying length for the learned non-recursive, partially recursive, and full recursive programs",1
" as before, the non-recursive learned program lacks a provable guarantee of generalization, whereas we show later that the recursive learned program has one",0
" also, compared to the bubble pointers in bubble sort, the pointers that perform the comparison for quicksort (the compswap function) are usually not adjacent to each other, making quicksort less local than bubble sort",1
"program descriptions calls arguments toposort perform topological
sort on graph traverse, next start, write, move
none
traverse traverse graph until stack is empty check child, explore none check child check if a white child exists; if so, set childlist[vactive] to point to it move none explore repeatedly traverse subgraphs until stack is empty stack, check child, write, move none stack interact with stack, either pushing or popping write, move push, pop next start move pstart until reaching a white vertex",0
" for example, for the addition task, the model is trained on short inputs and then tested on its ability to sum inputs with much longer numbers of digits",0
 the memory updates of these neural programs are so complex and interdependent that it is difficult to reason about the behaviors of the learned neural program under previously unseen situations (such as problems with longer inputs),0
" numerous models have been proposed for learning programs; to name a few, this includes the differentiable neural computer (graves  , 2016), neural turing machine (graves  , 2014), neural gpu (kaiser & sutskever, 2015), neural programmer (neelakantan  , 2015), pointer network (vinyals  , 2015), hierarchical attentive memory (andrychowicz & kurach, 2016), and neural random access machine (kurach  , 2016)",1
 this algorithm is a variant of depth first search,0
"2 at any given time, the npi has access to values pointed to by four pointers in each of the four rows, represented by q(1, i1), q(2, i2), q(3, i3), and q(4, i4)",0
" the general notion of recursion has been an important concept in many domains, including mathematics and computer science",1
" if the step output specifies a primitive function call, we need to reason about how it can affect the environment so as to change the observation in the next step input",1
" there are 3 pointers (presult, pstack, pstart), presult points to the next empty location in qresult,
pstack points to the top of the stack in qstack, and pstart points to the candidate starting node for a connected component",0
" both the non-recursive and recursive learned programs generalize on all input lengths we tried, up to 5000 digits",0
" we prove every member of f is added properly, given that subproblems s = {anan−1 + bnbn−1, an−1an−2 + bn−1bn−2, ",0
 a1a0 + bnbn−1 ,1
" we further have qresult ∈ nu , a scratch-pad which contains the sorted list of vertices at the end of execution, and qstack ∈ nu , which serves the role of the stack s in algorithm 2",0
"
the operations we need to be concerned most about are carry and lshift, which induce partial environment states spanning two columns",0
" given verification sets that cover all the base cases and reduction rules, we can provide proofs that these learned programs generalize perfectly",1
" for example, a step input-step output pair in lines 2 and 3 of the left-hand side of figure 1 is (add1, write out 1)",1
as mentioned in section 2,0
"
thus far, to evaluate the efficacy of neural models on programming tasks, the only metric that has been used is generalization of expected behavior to inputs of greater complexity (vinyals   (2015), kaiser & sutskever (2015), reed & de freitas (2016), graves   (2016), zaremba   (2016))",0
 we adapt machinery from the original paper slightly to fit our needs,0
 the program terminates when seeing no numbers in the current column,0
 we instantiated recursion for the neural programmerinterpreter by changing the training traces,1
" provably perfect generalization implies the model will behave correctly, given any valid input",1
", to change the value of vactive) none described below move move a pointer (e",0
"
an alternative method is to run p on many different program inputs and then observe step input sequences which occur, to create v ",1
" the recursive version of topological sort solves all graph instances we tried, from graphs of size 5 through 70",1
"program descriptions calls arguments quicksort run the quicksort
routine in place for the array a, for indices from lo to hi non-recursive: partition, stack, write recursive: same as non-recursive version, along with quicksort implicitly: array a to sort, lo, hi
partition runs the partition function",0
3,0
" the recursive version can generalize perfectly to long problems constructed from these components (such as the sum “822+233”, where “8+2” and “2+3” are in the training set), but the non-recursive version fails to sum these long problems properly",1
 there is a (changing) list of neural programs used to accomplish a given task,1
 ,0
 reset represents a −∞ value,0
" empirically, we observe that the learned recursive programs solve all valid inputs with 100% accuracy after training on a very small number of examples, out-performing previous generalization results",0
"
training setup",0
" it is straightforward to deal with all other operations, which do not induce partial environment states",0
 figure 2 shows examples of traces for the different versions of bubble sort,1
" for purposes of verification, we replace the domain-specific encoder with the following:
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1) ≤ q(1, i2), 1 ≤ i1 ≤ length, 1 ≤ i2 ≤ length, i3 == length, at(1), at(2), at(3)]),which directly exposes which of the two values pointed to is larger",1
" we make the trace recursive by adding a tail recursive call into the trace for the add program after calling add1 and lshift, the original paper uses k = 10, but we found it necessary to augment the range with an end token, in order to terminate properly",1
" however, the authors of npi did not consider explicitly the notion of recursion and as a consequence, did not learn recursive programs",1
3 to construct v and then empirically create a verification set which covers v ,1
 we choose to implement a topological sort task for graphs,1
" in all experiments, α is set to 0",1
", the verification set",0
we now report on generalization for the varying tasks,0
" without using a recursive program, such a proof is not possible, because the non-recursive program runs on an arbitrarily long addition problem that creates correspondingly long sequences of step inputs; in the non-recursive formulation of addition, add calls add1 a number of times that is dependent on the length of the input",1
" the only model, to our knowledge, that does not train on input-output pairs is the neural programmer-interpreter (reed & de freitas, 2016), which trains on synthetic execution traces",0
6,0
e,1
" as aforementioned, the npi model naturally supports recursion",0
 there is a (changing) list of neural programs used to accomplish a given task,0
", an array for quicksort or a dag for topological sort)",0
" using this modification, we constructed a verification set consisting of one array of size 10",0
"
algorithm 1 neural programming inference 1: inputs: environment observation e, program p, arguments a, stop threshold α 2: function run(e, p, a) 3: h← 0, r ← 0 4: while r < α do 5: s← fenc(e, a), h← flstm(s, p, h) 6: r ← fend(h), p2 ← fprog(h), a2 ← farg(h) 7: if p is a primitive function then 8: e← fenv(e, p, a)",1
" in fact, it is one of our future directions to explore new ways to train a recursive neural program without providing explicit training execution traces or with only partial or non-recursive traces",0
 stack pop pushes −∞ values to qstacklo and qstackhi,0
" recall that this verification procedure cannot be performed for the non-recursive versions, since the propagation of the hidden state in the core lstm module makes reasoning difficult and so we would need to check an unbounded number of examples",0
" in practice, we can additionally use tail recursion optimization to avoid problems with the call stack growing too deep",0
" plo and phi point to the lo and hi indices of the array, as in algorithm 3",0
"
inside bubble and reset, there are two operations that can be made recursive",0
" the pointers at index i1 and i2 are used to compare the pair of numbers considered during the bubble sweep, swapping them if the number at i1 is greater than that in i2",1
 the model described in the training setup of section 4 (trained on 6 traces) was verified to be correct via the matching procedure described in section 4,0
"
the non-recursive trace loops on cycles of add1 and lshift",1
" for brevity, we refer the reader to the appendix for further details on the program set and non-recursive and recursive trace-generating functions used for topological sort",1
" in practice, we can additionally use tail recursion optimization to avoid problems with the call stack growing too deep",0
"
in addition, none of the current approaches to neural programming provide a method or even aim to enable provable guarantees about generalization",1
" in fact, we argue that recursion is an essential element for neural programs to generalize, and makes it tractable to prove the generalization of neural programs",0
" recursion is an important concept in programming languages and a critical tool to
reduce the complexity of programs",1
 errors arise due to undesirable and uninterpretable dependencies and associations the architecture learns to store in some high-dimensional hidden state,0
" we show that by modifying the training procedure, we enable the npi model to learn recursive neural programs",0
" the space of possible programs is extremely large, and composing a program that performs robustly on the infinite space of possible inputs is difficult—in part because it is impractical to obtain enough training examples to easily disambiguate amongst all possible programs",1
"
for topological sort, the domain-specific encoder is
fenc(dag,qcolor, pstack, pstart, vactive, childlist, at)
=mlp ([qcolor(pstart), qcolor(dag[vactive][childlist[vactive]]), pstack == 1, at(1), at(2), at(3)]),
where qcolor ∈ ru×4 is a scratch-pad that contains u rows, each containing one of four colors (white, gray, black, invalid) with one-hot encoding",1
" although the model is exposed to more and more data, it might learn spurious and overly complex representations of the program, as suggested in zaremba   (2016)",1
" given verification sets that cover all the base cases and reduction rules, we can provide proofs that these learned programs generalize perfectly",1
"
bubble sort",0
 we adapt machinery from the original paper slightly to fit our needs,0
"
to construct this set, by using the reference implementation of each subprogram, we construct a mapping between two sets of environment observations: the first set consists of all observations that can occur at the beginning of a particular subprogram’s invocation, and the second set contains the observations at the end of that subprogram",1
" our implementation uses two stacks qstacklo and
qstackhi, each in ru , that store the arguments to the recursive quicksort calls in algorithm 3; before each recursive call, the appropriate arguments are popped off the stack and written to plo and phi",0
", to change the value of phi) none described belowstack",1
 this procedure verifies that all base cases and reductions are handled properly by the model via explicit tests,1
"
note that the recursion for quicksort is not purely tail recursive and therefore represents a more complex kind of recursion that is harder to learn than in the previous tasks",1
" the single-digit multiplication task in zaremba   (2016), the bubble sort task in reed & de freitas (2016), and the graph tasks in graves   (2016))",0
" on the other hand, the recursive programs have learned the true program semantics",1
" in fact, we argue that recursion is an essential element for neural programs to generalize, and makes it tractable to prove the generalization of neural programs",0
"
the dag is represented as an adjacency list where dag[i][j] refers to the j-th child of vertex i",1
" 2: initialize lo and hi to be 1 and n, where n is the length of a",0
" we then construct problem configurations that span all possible valid environment states (for instance, in order to force the carry bit in a column to be 1, one can add the sum “1+9” in the column to the right)",1
" the general notion of recursion has been an important concept in many domains, including mathematics and computer science",0
" for instance, for recursive addition, consider the family f of addition problems anan−1 ",1
"
move",0
"
by nature, recursion reduces the complexity of a problem to simpler instances",0
"
we describe the details of the npi model relevant to our contributions",0
" consequently, there is no concept of length relevant to the problem, which has traditionally been an important focus of length-based curriculum learning",0
" m is considered to have perfect generalization if m can give the right answer for any input, such as inputs of arbitrary length",1
"
we propose and describe our verification procedure",0
" the npi model has three learnable components: a task-agnostic core, a program-key embedding, and domain-specific encoders that allow the npi to operate in diverse environments",1
" for these models, table 2 presents results on randomly generated dags of varying graph sizes (varying in the number of vertices)",1
" 2: initialize lo and hi to be 1 and n, where n is the length of a",0
g,1
g,1
" each step during an execution of the program does one of three things: (1) another subprogram along with associated arguments is called, as in line 10, (2) the program writes to the environment if it is primitive, as in line 8, or (3) the loop is terminated if the return probability exceeds a threshold α, after which the stack frame is popped and control is returned to the caller",0
 table 4 demonstrates that the accuracy of the non-recursive program degrades sharply when moving from arrays of length 7 to arrays of length 8,0
"
in order to demonstrate that recursion can help learn and generalize better, we trained a non-recursive and recursive model on just a single execution trace generated from a graph containing 5 nodes3 for the topological sort task",1
" if yes, then this indicates that the learned neural program achieves provably perfect generalization",1
 table 4 demonstrates that the accuracy of the non-recursive program degrades sharply when moving from arrays of length 7 to arrays of length 8,0
" for many problems, the base cases and reduction rules usually consist of a finite (often small) number of cases",1
" for each task, given the verification set, we check the traces and results of the learned, to-be-verified neural program (described in section 4",0
" when the program is called, the current context including the caller’s memory state is stored on a stack; when the program returns, the stored context is popped off the stack to resume execution in the previous caller’s context",0
" the core module of the npi is an lstm controller that takes as input a slice of the current external environment, via a set of pointers, and a program and arguments to execute",0
"
we describe the details of the npi model relevant to our contributions",1
" each time a subprogram is called, the stack depth increases",1
" in future work, we seek to enable more tasks with recursive structure",1
 table 4 demonstrates that the accuracy of the non-recursive program degrades sharply when moving from arrays of length 7 to arrays of length 8,0
", to color a vertex) or variable (e",0
" the 4 environment observations aid with control flow; qstacklo(pstacklo− 1) < qstackhi(pstackhi− 1) implements the lo < hi comparison in line 5 of algorithm 3, pstacklo == 1 checks if the stacks are empty in line 18 of algorithm 4, and the other observations (all involving ppivot or pj) deal with logic in the partition function",1
"
we note that for tasks with very large input domains, such as ones involving mnist digits or speech samples, the state space of base cases and reduction rules could be prohibitively large, possibly infinite",0
" provably perfect generalization implies the model will behave correctly, given any valid input",0
g,0
 the core controller acts as a dispatcher for the programs,0
" however, the authors of npi did not consider explicitly the notion of recursion and as a consequence, did not learn recursive programs",1
 the core controller acts as a dispatcher for the programs,0
 table 4 also demonstrates that the (verified) recursive program generalizes perfectly,1
" the recursive version of topological sort solves all graph instances we tried, from graphs of size 5 through 70",1
"
we experiment with two levels of recursion—partial and full",0
" on the other hand, the non-recursive version has low accuracy, beginning from size 5, and fails completely for graphs of size 8 and beyond",0
" we implement a quicksort task, in order to demonstrate that recursion helps with learning divide-and-conquer algorithms",1
"
by nature, recursion reduces the complexity of a problem to simpler instances",0
"
quicksort",0
" the stack operation has the following arguments:
arg 1 (operation): stack push call1, stack push call2, stack pop
stack push call1 pushes lo and pivot−1 toqstacklo andqstackhi",0
 figure 2 shows examples of traces for the different versions of bubble sort,0
"
an alternative way of representing the environment slice is to expose the values of the absolute vertices to the model; however, this makes it difficult to scale the model to larger graphs, since large vertex values are not seen during training time",0
" for purposes of verification, we replace the domain-specific encoder with the following:
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1) ≤ q(1, i2), 1 ≤ i1 ≤ length, 1 ≤ i2 ≤ length, i3 == length, at(1), at(2), at(3)]),which directly exposes which of the two values pointed to is larger",1
" ppivot and pj point to the pivot and j indices of the array, used in the partition function in algorithm 3",1
"program descriptions calls arguments quicksort run the quicksort
routine in place for the array a, for indices from lo to hi non-recursive: partition, stack, write recursive: same as non-recursive version, along with quicksort implicitly: array a to sort, lo, hi
partition runs the partition function",1
 the logic for the non-recursive trace is shown in algorithm 4 in the appendix,0
" table 1 presents results on randomly generated arrays of varying length for the learned non-recursive, partially recursive, and full recursive programs",0
" we then construct problem configurations that span all possible valid environment states (for instance, in order to force the carry bit in a column to be 1, one can add the sum “1+9” in the column to the right)",1
" ppivot and pj point to the pivot and j indices of the array, used in the partition function in algorithm 3",0
 stack pop pushes −∞ values to qstacklo and qstackhi,0
" the write operation has the following arguments:
arg 1 (main action): color curr, color next, active start, active neighb, active stack, save, stack push, stack pop, result
color curr colors vactive, color next colors vertex dag[vactive][childlist[vactive]], active start writes pstart to vactive, active neighb writes dag[vactive][childlist[vactive]] to vactive, active stack writes qstack(pstack) to vactive, save writes vactive to vsave, stack push pushes vactive to the top of the stack, stack pop writes a null value to the top of the stack, and result writes vactive to qresult(presult)",0
" more specifically, when m is trained on simpler inputs, such as inputs of a small length, the generalization metric evaluates how well m will do on more complex inputs, such as inputs of much longer length",0
" the move operation has the following arguments:
arg 1 (pointer): presult, pstack, pstart, childlist[vactive], childlist[vsave]
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 5 values",1
" plo and phi point to the lo and hi indices of the array, as in algorithm 3",1
"
the non-recursive trace loops on cycles of add1 and lshift",1
" the general notion of recursion has been an important concept in many domains, including mathematics and computer science",1
" the arrow in the theorem refers to evaluation, as in big-step semantics",0
"
quicksort",0
" we also hope to decrease supervision, for example by training with only partial or non-recursive traces, and to develop novel neural programming architectures integrated directly with a notion of recursion",0
"
algorithm 2 shows the topological sort task of interest",0
" consequently, it is infeasible to construct a verification set that covers all cases, and the verification procedure we have described is inadequate",1
 the training set for addition contains 200 traces,0
" as before, the non-recursive learned program lacks a provable guarantee of generalization, whereas we show later that the recursive learned program has one",0
"
one common strategy to improve generalization is to use curriculum learning, where the model is trained on inputs of gradually increasing complexity",1
" empirically, existing models suffer from a common limitation—generalization becomes poor beyond a threshold level of complexity",0
"
inside bubble and reset, there are two operations that can be made recursive",0
"as there is no public implementation of npi, we implemented a version of it in keras that is as faithful to the paper as possible",1
 we found that changing the npi training traces is a simple way to enable this,0
" as this complexity interfered with determining the base cases and reductions, we changed the algorithm to its current form",1
" in the non-recursive trace, there are four functions that can be made recursive—toposort, check child, explore, and next start, and we add a tail recursive call to each of these functions in order to make the recursive trace",0
" at each time step, the core controller can decide to select one of the programs to call with certain arguments",0
" because the context of the caller is stored on a stack when it calls another program and the callee starts in a fresh context, this enables recursion simply by allowing a program to call itself",1
"when constructing a neural network for the purpose of learning a program, there are two orthogonal aspects to consider",0
" using this modification, we constructed a verification set consisting of one array of size 10",0
"program descriptions calls arguments quicksort run the quicksort
routine in place for the array a, for indices from lo to hi non-recursive: partition, stack, write recursive: same as non-recursive version, along with quicksort implicitly: array a to sort, lo, hi
partition runs the partition function",1
" the architecture usually possesses some form of memory, which could be internal (such as the hidden state of a recurrent neural network) or external (such as a discrete “scratch pad” or a memory block with differentiable access)",0
" for problems where the base cases may be extremely large or infinite, such as certain forms of motor control, recursion can still help reduce the problem of generalization to these two aspects and make the generalization problem significantly simpler to handle and reason about",0
5,0
" the arrow in the theorem refers to evaluation, as in big-step semantics",1
"
we experiment with two levels of recursion—partial and full",0
 the first is the actual model architecture,0
"
bubble sort",0
"
grade-school addition",1
" if it is too large, then the semantics of p might not be well-defined on some elements in v , or the spurious step input sequences may not be reachable from any valid problem input (e",0
", qstacklo or qstackhi) or to pointer (e",0
" that is to say, the network does not learn the true program semantics",1
 the first is the actual model architecture,0
 ,1
" naı̈vely, we might expect to synthesize and test an input for any sequence created by combining the four possible colors in two variables and another boolean variable for whether the stack is empty (so 32 possible observations at any point), but for various reasons, most of these combinations are impossible to occur at any given point in the execution trace",0
" qcolor(pstart) contains the color of the current start vertex, used in the evaluation of the condition in the while loop in line 5 of algorithm 2",1
 we describe the recursive re-formulation of traces for two tasks from the original npi paper—grade-school addition and bubble sort,0
" for bubble sort, the domain-specific encoder is
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1), q(1, i2), i3 == length, at(1), at(2), at(3)]),
where the environment q ∈ r1×n×k is a scratch-pad that contains 1 row, to represent the state of the array as sorting proceeds in-place, andn columns",0
" if the step output specifies a primitive function call, we need to reason about how it can affect the environment so as to change the observation in the next step input",1
"
the same set of problems was used to generate the training traces for all formulations of the task, for non-recursive and recursive versions",0
 the sequence of primitive operations (move and write operations) for the non-recursive and recursive versions are exactly the same,0
" we then construct problem configurations that span all possible valid environment states (for instance, in order to force the carry bit in a column to be 1, one can add the sum “1+9” in the column to the right)",1
" nevertheless, we would like the model to quickly learn to represent the right semantics of the underlying program from a small number of training examples, not an exhaustive number of them",0
" for these models, table 2 presents results on randomly generated dags of varying graph sizes (varying in the number of vertices)",1
" the outputs rt, pt+1, at+1 are used to determine the next action, as described in algorithm 1",1
 ,0
" however, the authors of npi did not consider explicitly the notion of recursion and as a consequence, did not learn recursive programs",1
" also, compared to the bubble pointers in bubble sort, the pointers that perform the comparison for quicksort (the compswap function) are usually not adjacent to each other, making quicksort less local than bubble sort",0
" the move operation has the following arguments:
arg 1 (pointer): presult, pstack, pstart, childlist[vactive], childlist[vsave]
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 5 values",0
" our results show that for all learned, to-be-verified neural programs, they all produced the same traces as those produced by p on the verification set",0
 3: begin traversing from vertex 1 in the dag,0
"
in order to demonstrate that recursion can help learn and generalize better, we trained a non-recursive and recursive model on just a single execution trace generated from a graph containing 5 nodes3 for the topological sort task",1
" each time a subprogram is called, the stack depth increases",0
" we then construct problem configurations that span all possible valid environment states (for instance, in order to force the carry bit in a column to be 1, one can add the sum “1+9” in the column to the right)",0
" for the recursive formulation of addition, we analytically construct the set of input problems that cover all base cases and reduction rules",0
"
we describe the base cases, reduction rules, and the verification set for each task in appendix a",0
"
in this paper, we propose that recursion is an important concept for neural programs as well",0
" bstep, used in bubble, compares pairs of numbers, continuously moving the bubble pointers once to the right each time until reaching the end of the array",1
" we made the assumption of equivalent lengths in order to parametrize the input format with respect to length, but this assumption can be removed as well",0
" the write operation has the following arguments:
arg 1 (main action): color curr, color next, active start, active neighb, active stack, save, stack push, stack pop, result
color curr colors vactive, color next colors vertex dag[vactive][childlist[vactive]], active start writes pstart to vactive, active neighb writes dag[vactive][childlist[vactive]] to vactive, active stack writes qstack(pstack) to vactive, save writes vactive to vsave, stack push pushes vactive to the top of the stack, stack pop writes a null value to the top of the stack, and result writes vactive to qresult(presult)",0
" on the other hand, the recursive programs have learned the true program semantics",1
" in fact, it is one of our future directions to explore new ways to train a recursive neural program without providing explicit training execution traces or with only partial or non-recursive traces",1
" in this version, there is a dependence on length: bstep and lshift are called a number of times equivalent to one less than the length of the input array, in bubble and reset respectively",1
" if a white vertex is found, set pstart to point to it; this signifies the start of a traversal of a new connected component",0
"
to perform the verification as described here, it is critical to construct v correctly",0
" , n , where the dag contains n vertices",0
" at end, pointer ppivot is moved to the pivot compswap loop, move pivot lo, move j lo, swap
none
compswap loop runs the for loop inside the partition function compswap, move none compswap compares a[pivot] ≤ a[j]; if so, perform a swap and increment ppivot swap, move none set pivot lo sets ppivot to lo index none none set j lo sets pj to lo index none none set j null sets pj to −∞ none none stack pushes lo/hi states
onto stacks slo and shi according to argument (described below)
write, move described below
move moves pointer one unit up or down none described below swap swaps elements at given array indices none described below write write a value either to stack (e",1
" consequently, it is infeasible to construct a verification set that covers all cases, and the verification procedure we have described is inadequate",0
" m is considered to have perfect generalization if m can give the right answer for any input, such as inputs of arbitrary length",0
" that is to say, the network does not learn the true program semantics",0
"program descriptions calls arguments quicksort run the quicksort
routine in place for the array a, for indices from lo to hi non-recursive: partition, stack, write recursive: same as non-recursive version, along with quicksort implicitly: array a to sort, lo, hi
partition runs the partition function",0
"we show that if we incorporate recursion, the learned npi programs can achieve provably perfect generalization for different tasks",1
" if the program is primitive, the next environmental state et+1 will be affected by pt and at, i",1
" m is considered to have perfect generalization if m can give the right answer for any input, such as inputs of arbitrary length",0
"
in order to demonstrate that recursion can help learn and generalize better, for addition, we trained only on traces for 5 arbitrarily chosen 1-digit addition sum examples",1
" in the non-recursive trace, there are four functions that can be made recursive—toposort, check child, explore, and next start, and we add a tail recursive call to each of these functions in order to make the recursive trace",1
" we performed experiments on the partially recursive version in order to examine what happens when only one recursive call is implemented, when in reality three are required for perfect generalization",0
 we then construct a verification set of size 73 by ensuring that randomly generated graphs cover the analytically derived v ,0
" because the context of the caller is stored on a stack when it calls another program and the callee starts in a fresh context, this enables recursion simply by allowing a program to call itself",0
"
for addition, we analytically determine the verification set",0
" 2: initialize lo and hi to be 1 and n, where n is the length of a",1
 we choose to implement a topological sort task for graphs,1
" we show that by modifying the training procedure, we enable the npi model to learn recursive neural programs",0
" 3: 4: function partition(a, lo, hi) 5: pivot = lo 6: for j ∈ [lo, hi− 1] : do 7: if a[j] ≤ a[hi] then 8: swap a[pivot] with a[j] 9: pivot = pivot+ 1 0: swap a[pivot] with a[hi] 11: return pivot 12: 13: function quicksort(a, lo, hi) 14: while slo and shi are not empty: do 15: pop states off slo and shi, writing them to lo and hi",0
"we train using the adam optimizer and use a 2-layer lstm and task-specific state encoders for the external environments, as described in reed & de freitas (2016)",0
" at any given time, the npi has access to the values referred to by two pointers, represented by q(1, i1) and q(1, i2),",0
" as with the original npi architecture, the experiments for this paper always used a 3-tuple of integers at = (at(1), at(2), at(3))",1
e,0
" by iterating with this procedure, and then running p on the input observation set that we obtain for the entry point function, we can obtain v precisely",1
"6) on the verification set, and ensure they match the traces produced by the true program p ",0
" in computer science, recursion (as opposed to iteration) involves solving a larger problem by combining solutions to smaller instances of the same problem",1
"as there is no public implementation of npi, we implemented a version of it in keras that is as faithful to the paper as possible",0
" the 4 environment observations aid with control flow; qstacklo(pstacklo− 1) < qstackhi(pstackhi− 1) implements the lo < hi comparison in line 5 of algorithm 3, pstacklo == 1 checks if the stacks are empty in line 18 of algorithm 4, and the other observations (all involving ppivot or pj) deal with logic in the partition function",0
"
it is sufficient to construct problems where every transition between two adjacent columns is covered",0
 some functional programming languages go so far as not to define any looping constructs but rely solely on recursion to enable repeated execution of the same code,0
 ,0
"
quicksort",0
"
recursion drastically reduces the number of configurations we need to consider during the verification phase and makes the proof tractable, because it introduces structure that eliminates infinitely long sequences of step inputs that would otherwise need to be considered",1
2,0
 we choose to implement a topological sort task for graphs,1
 this verification phase only needs to be performed once after training,1
" in contrast, each call to the recursive add always runs for a fixed number of steps, even on arbitrarily long problems in f , so we can test that it performs correctly on a small, fixed number of step input sequences",1
" if no white vertex is found, the entire execution is terminated move none write write a value either to environment (e",0
5,0
" for each graph size, we test the learned programs on 30 randomly generated dags",0
" because the context of the caller is stored on a stack when it calls another program and the callee starts in a fresh context, this enables recursion simply by allowing a program to call itself",1
" the core lstm module’s hidden state is preserved over all these add1 calls, and it is difficult to interpret with certainty what happens over longer timesteps without concretely evaluating the lstm with an input of that length",0
" naı̈vely, we might expect to synthesize and test an input for any sequence created by combining the four possible colors in two variables and another boolean variable for whether the stack is empty (so 32 possible observations at any point), but for various reasons, most of these combinations are impossible to occur at any given point in the execution trace",0
" we made the assumption of equivalent lengths in order to parametrize the input format with respect to length, but this assumption can be removed as well",0
 npi then outputs the return probability and next program and arguments to execute,0
" lshift moves the four pointers to the left, to move to the next column",1
" the recursive version can generalize perfectly to long problems constructed from these components (such as the sum “822+233”, where “8+2” and “2+3” are in the training set), but the non-recursive version fails to sum these long problems properly",0
"
move",0
", an array for quicksort or a dag for topological sort)",0
 ,1
"
base cases and reduction rules for addition",0
 ,0
" if it is too large, then the semantics of p might not be well-defined on some elements in v , or the spurious step input sequences may not be reachable from any valid problem input (e",1
"
we describe the details of the npi model relevant to our contributions",0
"
as mentioned in section 1, all approaches to neural programming today fare poorly on this generalization issue",0
" the 4 environment observations aid with control flow; qstacklo(pstacklo− 1) < qstackhi(pstackhi− 1) implements the lo < hi comparison in line 5 of algorithm 3, pstacklo == 1 checks if the stacks are empty in line 18 of algorithm 4, and the other observations (all involving ppivot or pj) deal with logic in the partition function",0
" in this version of curriculum learning, even though the inputs are gradually becoming more complex, the semantics of the program is succinct and does not change",0
" for the recursive formulation of addition, we analytically construct the set of input problems that cover all base cases and reduction rules",1
" on the other hand, the recursive programs have learned the true program semantics",0
g,1
" in particular, given that a recursion is defined by two properties as mentioned before, the base cases and the set of reduction rules, we can prove a recursive neural program generalizes perfectly if we can prove that (1) it performs correctly on the base cases; (2) it learns the reduction rules correctly",1
" 16: p = partition(a, lo, hi) 17: push p+ 1 and hi to slo and shi",1
" childlist ∈ nu is a vector of pointers, where childlist[i] points to the next child under consideration for vertex i",0
 recursion enables provably perfect generalization,0
"
the operations we need to be concerned most about are carry and lshift, which induce partial environment states spanning two columns",1
 we adapt machinery from the original paper slightly to fit our needs,1
" this agrees with the generalization of non-recursive addition in reed & de freitas (2016), where they reported generalization up to 3000 digits",1
" this is highly undesirable, since being able to provide the correct answer in all possible settings is one of the most important aspects of any learned neural program",0
" note that recursion helps make this process tractable, because we only need to test a finite number of inputs to show that the model will work correctly on inputs of unbounded complexity",0
" thus, recursion helps decompose a problem and makes it easier to reason about a program’s behavior for previously unseen situations such as longer inputs",0
"1; and for bubble sort, appendix a",0
"6) on the verification set, and ensure they match the traces produced by the true program p ",0
"
in this general neural programming architecture, we show it is easy to support recursion",1
 table 4 demonstrates that the accuracy of the non-recursive program degrades sharply when moving from arrays of length 7 to arrays of length 8,1
" for each length, we test each program on 30 randomly generated problems",0
"
the training set for topological sort contains 6 traces, with one synthesized from a graph of size 5 and the rest synthesized from graphs of size 7",1
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,1
" furthermore, given a verification input set that covers all base cases and reduction rules, we can formally prove that the learned neural programs achieve perfect generalization after verifying its behavior on the verification input set",0
" the arrow in the theorem refers to evaluation, as in big-step semantics",0
 2: initialize an empty stack s and a directed acyclic graph dag to traverse,0
"program descriptions calls arguments toposort perform topological
sort on graph traverse, next start, write, move
none
traverse traverse graph until stack is empty check child, explore none check child check if a white child exists; if so, set childlist[vactive] to point to it move none explore repeatedly traverse subgraphs until stack is empty stack, check child, write, move none stack interact with stack, either pushing or popping write, move push, pop next start move pstart until reaching a white vertex",0
" we then construct problem configurations that span all possible valid environment states (for instance, in order to force the carry bit in a column to be 1, one can add the sum “1+9” in the column to the right)",0
", to change the value of phi) none described belowstack",1
 recursion enables provably perfect generalization,1
 u varies with the number of vertices in the graph,0
" at each time step, the core controller can decide to select one of the programs to call with certain arguments",0
" observe that partially recursive does slightly better than non-recursive for the setting in which the length of the array is 3, and that the fully recursive version is able to sort every array given to it",0
" the arrow in the theorem refers to evaluation, as in big-step semantics",0
3,0
" an example of part of an addition task trace, written in shorthand, is given in figure 1",0
" when the program is called, the current context including the caller’s memory state is stored on a stack; when the program returns, the stored context is popped off the stack to resume execution in the previous caller’s context",0
" 2: initialize lo and hi to be 1 and n, where n is the length of a",0
" note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values, and arg 2 can only take one of 7 values",1
" the only model, to our knowledge, that does not train on input-output pairs is the neural programmer-interpreter (reed & de freitas, 2016), which trains on synthetic execution traces",0
5,1
" consequently, there is no concept of length relevant to the problem, which has traditionally been an important focus of length-based curriculum learning",0
" there are 3 pointers (presult, pstack, pstart), presult points to the next empty location in qresult,
pstack points to the top of the stack in qstack, and pstart points to the candidate starting node for a connected component",0
"
the operations we need to be concerned most about are carry and lshift, which induce partial environment states spanning two columns",0
" the arrow in the theorem refers to evaluation, as in big-step semantics",1
" empirically, we observe that the learned recursive programs solve all valid inputs with 100% accuracy after training on a very small number of examples, out-performing previous generalization results",0
g,1
we emphasize that the notion of a neural recursive program has not been presented in the literature before: this is our main contribution,0
"
the dag is represented as an adjacency list where dag[i][j] refers to the j-th child of vertex i",1
"
topological sort",0
" bstep, used in bubble, compares pairs of numbers, continuously moving the bubble pointers once to the right each time until reaching the end of the array",1
", to change the value of vactive) none described below move move a pointer (e",1
 figure 2 shows examples of traces for the different versions of bubble sort,0
" to the best of our knowledge, this is the first time verification has been applied to a neural program, providing provable guarantees about its behavior",0
" note that recursion helps make this process tractable, because we only need to test a finite number of inputs to show that the model will work correctly on inputs of unbounded complexity",0
" to the best of our knowledge, this is the first time verification has been applied to a neural program, providing provable guarantees about its behavior",0
"1; and for bubble sort, appendix a",0
"
move",1
" for grade-school addition, the domain-specific encoder is
fenc(q, i1, i2, i3, i4, at) =mlp ([q(1, i1), q(2, i2), q(3, i3), q(4, i4), at(1), at(2), at(3)]),
where the environment q ∈ r4×n×k is a scratch-pad that contains four rows (the first input number, the second input number, the carry bits, and the output) and n columns",0
" this makes it so that we only need to consider the vertices in the subgraph that are currently relevant for computing the sort, allowing simpler reasoning about behavior for large graphs",0
"
algorithm 2 shows the topological sort task of interest",1
 we outline how to construct this set,0
" without using a recursive program, such a proof is not possible, because the non-recursive program runs on an arbitrarily long addition problem that creates correspondingly long sequences of step inputs; in the non-recursive formulation of addition, add calls add1 a number of times that is dependent on the length of the input",0
" there are 3 pointers (presult, pstack, pstart), presult points to the next empty location in qresult,
pstack points to the top of the stack in qstack, and pstart points to the candidate starting node for a connected component",1
" via the recursive call, we effectively forget that the column just added exists, since the recursive call to add starts with a new hidden state for the lstm controller",1
" however, to be sure that the generated v is complete (covers all the cases needed), we need to check all pairs of observations seen in adjacent step inputs (in particular, those before and after a primitive function call), in a similar way as if we were constructing v from scratch",1
 a1a0 + bnbn−1 ,1
"
the operations we need to be concerned most about are carry and lshift, which induce partial environment states spanning two columns",0
"
the dag is represented as an adjacency list where dag[i][j] refers to the j-th child of vertex i",0
" table 1 presents results on randomly generated arrays of varying length for the learned non-recursive, partially recursive, and full recursive programs",0
" the write operation has the following arguments:
arg 1 (object to write): env stack lo, env stack hi, phi, plo
env stack lo and env stack hi represent qstacklo(pstacklo) and qstackhi(pstackhi), respectively",0
 we then construct a verification set of size 73 by ensuring that randomly generated graphs cover the analytically derived v ,0
 3: begin traversing from vertex 1 in the dag,0
" for each length, we test each program on 30 randomly generated problems",1
" we show that by modifying the training procedure, we enable the npi model to learn recursive neural programs",0
"
under the assumption that there are no leading 0’s (except in the case of single digits) and the two numbers to be added have the same number of digits, the verification set for addition contains 20,181 input problems",0
"
arg 2 (auxiliary variable): color grey, color black
color grey and color black color the given vertex grey and black, respectively",0
g,1
"in this paper, we propose that the key abstraction of recursion is necessary for neural programs to generalize",0
" in this section, we give a brief review of the npi architecture from reed & de freitas (2016) as background",0
" the theorem states that for the same sequence of step inputs, the model produces the exact same step output as the target program it aims to learn",0
" in this version, there is a dependence on length: bstep and lshift are called a number of times equivalent to one less than the length of the input array, in bubble and reset respectively",1
"
base cases and reduction rules for addition",0
" by iterating with this procedure, and then running p on the input observation set that we obtain for the entry point function, we can obtain v precisely",0
"1; and for bubble sort, appendix a",1
"
grade-school addition",1
" lshift, used in reset, shifts the pointers left until reaching the start token",0
" finally, the boolean pstack == 1 is used to check whether the stack is empty in line 18 of algorithm 2",0
 the logic for the non-recursive trace is shown in algorithm 4 in the appendix,0
 stack pop pushes −∞ values to qstacklo and qstackhi,0
"program descriptions calls arguments quicksort run the quicksort
routine in place for the array a, for indices from lo to hi non-recursive: partition, stack, write recursive: same as non-recursive version, along with quicksort implicitly: array a to sort, lo, hi
partition runs the partition function",1
" also, compared to the bubble pointers in bubble sort, the pointers that perform the comparison for quicksort (the compswap function) are usually not adjacent to each other, making quicksort less local than bubble sort",1
 this is due to the properties of the training set – we trained on 2 traces synthesized from arrays of length 7 and 1 trace synthesized from an array of length 6,0
"
the npi accesses an external environment, q, which varies according to the task",0
 2: initialize an empty stack s and a directed acyclic graph dag to traverse,1
" if it is too large, then the semantics of p might not be well-defined on some elements in v , or the spurious step input sequences may not be reachable from any valid problem input (e",1
" note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values, and arg 2 can only take one of 7 values",0
"
in order to demonstrate that recursion can help learn and generalize better, for addition, we trained only on traces for 5 arbitrarily chosen 1-digit addition sum examples",0
" this guarantees that the step input sequences considered during verification contain all step input sequences which arise during execution of an unseen problem in f , leading to generalization to any problem in f ",0
 the core controller acts as a dispatcher for the programs,1
"2 at any given time, the npi has access to values pointed to by four pointers in each of the four rows, represented by q(1, i1), q(2, i2), q(3, i3), and q(4, i4)",1
" for example, for the addition task, the model is trained on short inputs and then tested on its ability to sum inputs with much longer numbers of digits",1
" for non-primitive subprograms, we can update the observation-to-observation mapping currently associated with the subprogram and then apply that mapping to the current set",1
" a single element of an execution trace consists of a step input-step output pair, which can be synthesized from algorithm 1: this corresponds to, for a given time-step, the step input tuple (e, p, a) and step output tuple (r, p2, a2)",1
 stack pop pushes −∞ values to qstacklo and qstackhi,1
" any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of national science foundation and darpa",1
" as with the original npi architecture, the experiments for this paper always used a 3-tuple of integers at = (at(1), at(2), at(3))",1
 ,1
" we can obtain this mapping by first considering the possible observations that can arise at the beginning of the entry function (add, bubblesort, toposort, and quicksort) for some valid program input, and iteratively applying the observation-to-observation mapping implied by the reference implementation’s step output at that point in the execution",1
" both the non-recursive and recursive learned programs generalize on all graphs we tried, up to 120 vertices",1
 ,1
", to change the value of vactive) none described below move move a pointer (e",0
" furthermore, given a verification input set that covers all base cases and reduction rules, we can formally prove that the learned neural programs achieve perfect generalization after verifying its behavior on the verification input set",0
"
we describe the base cases, reduction rules, and the verification set for each task in appendix a",1
 the core controller acts as a dispatcher for the programs,0
"
as mentioned in section 1, all approaches to neural programming today fare poorly on this generalization issue",0
" for problems where the base cases may be extremely large or infinite, such as certain forms of motor control, recursion can still help reduce the problem of generalization to these two aspects and make the generalization problem significantly simpler to handle and reason about",0
" we note that an earlier version of quicksort we tried lacked primitive operations to directly move a pointer to another, and therefore needed more functions and observations",0
"
the same set of problems was used to generate the training traces for all formulations of the task, for non-recursive and recursive versions",0
" the second is the training procedure, which consists of the form of the training data and the optimization process",0
"
since npi naturally supports the notion of recursion, a key question is how to enable npi to learn recursive programs",0
3,0
 we choose to implement a topological sort task for graphs,0
" hence, if all subproblems in s are added correctly, we have proven that any member of f will be added correctly, thus eliminating an infinite family of inputs that need to be tested",0
" the arrow in the theorem refers to evaluation, as in big-step semantics",0
" the verification set can be very small, as we found a 10-element array ([8,2,1,2,0,8,5,8,3,7]) is sufficient to cover all of v ",1
 pstacklo and pstackhi point to the top (empty) positions in qstacklo and qstackhi,1
" given a precise definition of p , it may be possible to automate the generation of v from p in future work",0
" hence, if all subproblems in s are added correctly, we have proven that any member of f will be added correctly, thus eliminating an infinite family of inputs that need to be tested",1
" we call this set of inputs the verification set, sv ",0
we emphasize the overall goal of this work is to enable the learning of a recursive program,1
" the 4 environment observations aid with control flow; qstacklo(pstacklo− 1) < qstackhi(pstackhi− 1) implements the lo < hi comparison in line 5 of algorithm 3, pstacklo == 1 checks if the stacks are empty in line 18 of algorithm 4, and the other observations (all involving ppivot or pj) deal with logic in the partition function",0
 we adapt machinery from the original paper slightly to fit our needs,1
"
we experiment with two levels of recursion—partial and full",0
" if yes, then this indicates that the learned neural program achieves provably perfect generalization",0
"
algorithm 2 shows the topological sort task of interest",1
"when constructing a neural network for the purpose of learning a program, there are two orthogonal aspects to consider",0
" the assumption of leading 0’s can be easily removed, at the cost of slightly increasing the size of the verification set",1
" in all experiments, α is set to 0",0
" this is possible if and only if the graph has no directed cycles; that is to say, it must be a directed acyclic graph (dag)",0
"
base cases and reduction rules for bubble sort",0
" on the other hand, the recursive programs have learned the true program semantics",0
" in fact, it is one of our future directions to explore new ways to train a recursive neural program without providing explicit training execution traces or with only partial or non-recursive traces",0
"2 at any given time, the npi has access to values pointed to by four pointers in each of the four rows, represented by q(1, i1), q(2, i2), q(3, i3), and q(4, i4)",1
" in particular, this condition is satisfied by npi, and thus the npi model naturally supports recursion (even though the authors of npi did not consider this aspect explicitly)",1
" a topological sort is a linear ordering of vertices such that for every directed edge (u, v) from u to v, u comes before v in the ordering",1
 2: initialize an empty stack s and a directed acyclic graph dag to traverse,0
"
directed acyclic graphs are structurally more diverse than inputs in the two tasks of grade-school addition and bubble sort",1
"1, we hypothesize the non-recursive programs do not generalize well because they have learned spurious dependencies specific to the training set, such as length of the input problems",0
 2: initialize an empty stack s and a directed acyclic graph dag to traverse,0
" for many problems, the base cases and reduction rules usually consist of a finite (often small) number of cases",0
" given a precise definition of p , it may be possible to automate the generation of v from p in future work",0
" also the dag can have potentially more than one connected component, meaning it is necessary to transition between these components appropriately",1
" in particular, in the explore function, adding a tail recursive call resets and stores the hidden states associated with vertices in a stack-like fashion",0
" as with the others, we apply the procedure described in section 3",0
"2 at any given time, the npi has access to values pointed to by four pointers in each of the four rows, represented by q(1, i1), q(2, i2), q(3, i3), and q(4, i4)",1
" we call this set of inputs the verification set, sv ",1
" the assumption of leading 0’s can be easily removed, at the cost of slightly increasing the size of the verification set",0
"as discussed in section 2, the neural programmer-interpreter (npi) is an instance of a neural programmer architecture and hence it naturally supports recursion",0
"
we note that for tasks with very large input domains, such as ones involving mnist digits or speech samples, the state space of base cases and reduction rules could be prohibitively large, possibly infinite",1
" 3: 4: function partition(a, lo, hi) 5: pivot = lo 6: for j ∈ [lo, hi− 1] : do 7: if a[j] ≤ a[hi] then 8: swap a[pivot] with a[j] 9: pivot = pivot+ 1 0: swap a[pivot] with a[hi] 11: return pivot 12: 13: function quicksort(a, lo, hi) 14: while slo and shi are not empty: do 15: pop states off slo and shi, writing them to lo and hi",0
" for brevity, we refer the reader to the appendix for further details on the program set and non-recursive and recursive trace-generating functions used for topological sort",0
g,1
"
the non-recursive trace loops on cycles of add1 and lshift",0
"program descriptions calls arguments quicksort run the quicksort
routine in place for the array a, for indices from lo to hi non-recursive: partition, stack, write recursive: same as non-recursive version, along with quicksort implicitly: array a to sort, lo, hi
partition runs the partition function",0
" the arrow in the theorem refers to evaluation, as in big-step semantics",0
" also, compared to the bubble pointers in bubble sort, the pointers that perform the comparison for quicksort (the compswap function) are usually not adjacent to each other, making quicksort less local than bubble sort",0
" recall that this verification procedure cannot be performed for the non-recursive versions, since the propagation of the hidden state in the core lstm module makes reasoning difficult and so we would need to check an unbounded number of examples",0
" recursion is an important concept in programming languages and a critical tool to
reduce the complexity of programs",1
" if it is too small, then execution of the program on some input might require evaluation of m(i) on some i /∈ v , and so the behavior ofm(i) might deviate from p (i)",0
"6) on the verification set, and ensure they match the traces produced by the true program p ",1
 stack push call2 pushes pivot + 1 and hi to qstacklo and qstackhi,0
" an example of part of an addition task trace, written in shorthand, is given in figure 1",1
" in particular, in the explore function, adding a tail recursive call resets and stores the hidden states associated with vertices in a stack-like fashion",1
" we make the trace recursive by adding a tail recursive call into the trace for the add program after calling add1 and lshift, the original paper uses k = 10, but we found it necessary to augment the range with an end token, in order to terminate properly",0
"
formally, verification consists of proving the following theorem:
∀i ∈ v,m(i) ⇓ p (i) where i denotes a sequence of step inputs (within one function call), v denotes the set of valid sequences of step inputs, m denotes the neural network model, p denotes the correct program, and p (i) denotes the next step output from the correct program",0
" the general notion of recursion has been an important concept in many domains, including mathematics and computer science",0
" in the non-recursive trace, there are four functions that can be made recursive—toposort, check child, explore, and next start, and we add a tail recursive call to each of these functions in order to make the recursive trace",1
" the contents ofqresult andqstack are not exposed directly through the domain-specific encoder; rather, we define primitive functions which manipulate these scratch-pads",1
" we made the assumption of equivalent lengths in order to parametrize the input format with respect to length, but this assumption can be removed as well",1
"
thus far, to evaluate the efficacy of neural models on programming tasks, the only metric that has been used is generalization of expected behavior to inputs of greater complexity (vinyals   (2015), kaiser & sutskever (2015), reed & de freitas (2016), graves   (2016), zaremba   (2016))",0
" for example, for the addition task, the model is trained on short inputs and then tested on its ability to sum inputs with much longer numbers of digits",0
" in this version of curriculum learning, even though the inputs are gradually becoming more complex, the semantics of the program is succinct and does not change",1
"in this paper, we propose that the key abstraction of recursion is necessary for neural programs to generalize",1
we emphasize that the notion of a neural recursive program has not been presented in the literature before: this is our main contribution,0
" for each graph size, we test the learned programs on 30 randomly generated dags",0
" the learned recursive program is different from neural programs learned in all previous work in an important aspect: previous approaches do not explicitly incorporate this abstraction, and hence generalize poorly, whereas our learned neural programs incorporate recursion and achieve perfect generalization",0
" naı̈vely, we might expect to synthesize and test an input for any sequence created by combining the four possible colors in two variables and another boolean variable for whether the stack is empty (so 32 possible observations at any point), but for various reasons, most of these combinations are impossible to occur at any given point in the execution trace",1
g,0
"
the same set of problems was used to generate the training traces for all formulations of the task, for non-recursive and recursive versions",1
 the core controller acts as a dispatcher for the programs,1
" for brevity, we refer the reader to the appendix for further details on the program set and non-recursive and recursive trace-generating functions used for topological sort",0
" we make the trace recursive by adding a tail recursive call into the trace for the add program after calling add1 and lshift, the original paper uses k = 10, but we found it necessary to augment the range with an end token, in order to terminate properly",1
"we train using the adam optimizer and use a 2-layer lstm and task-specific state encoders for the external environments, as described in reed & de freitas (2016)",0
"
algorithm 3 recursive quicksort 1: initialize an array a to sort",0
" we prove every member of f is added properly, given that subproblems s = {anan−1 + bnbn−1, an−1an−2 + bn−1bn−2, ",0
"
to evaluate a neural network that learns a neural program to accomplish a certain task, one common evaluation metric is how well the learned model m generalizes",0
"
the training set for quicksort contains 4 traces, synthesized from arrays of length 5",0
" in future work, we seek to enable more tasks with recursive structure",1
" also, compared to the bubble pointers in bubble sort, the pointers that perform the comparison for quicksort (the compswap function) are usually not adjacent to each other, making quicksort less local than bubble sort",1
 2: initialize an empty stack s and a directed acyclic graph dag to traverse,0
"
grade school addition",0
" the swap operation has the following arguments:
arg 1 (swap object 1): ppivot
arg 2 (swap object 2): phi, pj
write",1
g,0
 some functional programming languages go so far as not to define any looping constructs but rely solely on recursion to enable repeated execution of the same code,0
"
to evaluate a neural network that learns a neural program to accomplish a certain task, one common evaluation metric is how well the learned model m generalizes",1
" bstep, used in bubble, compares pairs of numbers, continuously moving the bubble pointers once to the right each time until reaching the end of the array",0
", qstacklo or qstackhi) or to pointer (e",1
"
the operations we need to be concerned most about are carry and lshift, which induce partial environment states spanning two columns",0
"
we experiment with two levels of recursion—partial and full",1
"
in addition, none of the current approaches to neural programming provide a method or even aim to enable provable guarantees about generalization",1
"
bubble sort",1
", the verification set",0
 table 4 demonstrates that the accuracy of the non-recursive program degrades sharply when moving from arrays of length 7 to arrays of length 8,1
"we describe how models trained with recursive traces can be proven to generalize, by using the verification procedure described in section 3",0
" when the program is called, the current context including the caller’s memory state is stored on a stack; when the program returns, the stored context is popped off the stack to resume execution in the previous caller’s context",0
" nevertheless, we would like the model to quickly learn to represent the right semantics of the underlying program from a small number of training examples, not an exhaustive number of them",0
 we leave this as future work to devise a verification procedure more appropriate to this setting,0
"
after finding v , we construct a set of problem inputs which, when executed on p , create exactly the step input sequences which make up v ",0
" there are 3 pointers (presult, pstack, pstart), presult points to the next empty location in qresult,
pstack points to the top of the stack in qstack, and pstart points to the candidate starting node for a connected component",0
"we show that if we incorporate recursion, the learned npi programs can achieve provably perfect generalization for different tasks",1
" consequently, there is no concept of length relevant to the problem, which has traditionally been an important focus of length-based curriculum learning",0
" we made the assumption of equivalent lengths in order to parametrize the input format with respect to length, but this assumption can be removed as well",1
"
move",0
" the second is the training procedure, which consists of the form of the training data and the optimization process",1
"
recursion drastically reduces the number of configurations we need to consider during the verification phase and makes the proof tractable, because it introduces structure that eliminates infinitely long sequences of step inputs that would otherwise need to be considered",0
" as aforementioned, the npi model naturally supports recursion",1
" recall that this verification procedure cannot be performed for the non-recursive versions, since the propagation of the hidden state in the core lstm module makes reasoning difficult and so we would need to check an unbounded number of examples",0
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,1
" however, our concept of recursion for neural programs is general",0
" in all experiments, α is set to 0",0
" however, note that there is no provable guarantee that the non-recursive learned program will generalize to all inputs, whereas we show later that the recursive learned program has a provable guarantee of perfect generalization",1
" both the non-recursive and recursive learned programs generalize on all graphs we tried, up to 120 vertices",1
" lshift, used in reset, shifts the pointers left until reaching the start token",1
" without using a recursive program, such a proof is not possible, because the non-recursive program runs on an arbitrarily long addition problem that creates correspondingly long sequences of step inputs; in the non-recursive formulation of addition, add calls add1 a number of times that is dependent on the length of the input",0
" for the recursive formulation of addition, we analytically construct the set of input problems that cover all base cases and reduction rules",0
" numerous models have been proposed for learning programs; to name a few, this includes the differentiable neural computer (graves  , 2016), neural turing machine (graves  , 2014), neural gpu (kaiser & sutskever, 2015), neural programmer (neelakantan  , 2015), pointer network (vinyals  , 2015), hierarchical attentive memory (andrychowicz & kurach, 2016), and neural random access machine (kurach  , 2016)",1
"
grade-school addition",0
" to the best of our knowledge, this is the first time verification has been applied to a neural program, providing provable guarantees about its behavior",1
" the move operation has the following arguments:
arg 1 (pointer): pstacklo, pstackhi, pj , ppivot
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values",1
 the maximum problem length in this training set is 3 (e,1
 18: push lo and p− 1 to slo and shi,1
"
arg 2 (increment or decrement): up, down1 // top level topological sort call 2 toposort() { 3 while (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 } 0 11 traverse() { 12 check_child() 13 explore() 14 } 15 16 check_child() { 17 while (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 18 move(childlist[vactive], up) 19 } 20 21 explore() { 22 do 23 if (qcolor(dag[vactive][childlist[vactive]]) is white): 24 write(color_next, color_grey) 25 stack(push) 26 write(save) 27 write(active_neighb) 28 move(childlist[vsave], up) 29 else: 30 write(color_curr, color_black) 31 write(result) 32 move(presult, up) 33 if(pstack == 1): 34 break 35 else: 36 stack(pop) 37 check_child() 38 while (true) 39 } 40 41 stack(op) { 42 if (op == push): 43 write(stack_push) 44 move(pstack, up) 45 46 if (op == pop): 47 write(active_stack) 48 write(stack_pop) 49 move(pstack, down) 50 } 51 52 next_start() { 53 while(qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 54 move(pstart, up) 55 }altered recursive functions  // top level topological sort call 2 toposort() { 3 if (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 toposort() // recursive call 0 } 11 12 check_child() { 13 if (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 14 move(childlist[vactive], up) 15 check_child() // recursive call 16 } 17 18 explore() { 19 if (qcolor(dag[vactive][childlist[vactive]]) is white): 20 write(color_next, color_grey) 21 stack(push) 22 write(save) 23 write(active_neighb) 24 move(childlist[vsave], up) 25 else: 26 write(color_curr, color_black) 27 write(result) 28 move(presult, up) 29 if(pstack == 1): 30 return 31 else: 32 stack(pop) 33 check_child() 34 explore() // recursive call 35 } 36 37 next_start() { 38 if (qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 39 move(pstart, up) 40 next_start() // recursive call 41 }algorithm 4 iterative quicksort 1: initialize an array a to sort and two empty stacks slo and shi",0
" note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values, and arg 2 can only take one of 7 values",0
e,1
" given a precise definition of p , it may be possible to automate the generation of v from p in future work",0
 18: push lo and p− 1 to slo and shi,0
"
inside bubble and reset, there are two operations that can be made recursive",1
"
in order to demonstrate that recursion can help learn and generalize better, we trained a non-recursive and recursive model on just a single execution trace generated from a graph containing 5 nodes3 for the topological sort task",1
" naı̈vely, we might expect to synthesize and test an input for any sequence created by combining the four possible colors in two variables and another boolean variable for whether the stack is empty (so 32 possible observations at any point), but for various reasons, most of these combinations are impossible to occur at any given point in the execution trace",1
" we then construct problem configurations that span all possible valid environment states (for instance, in order to force the carry bit in a column to be 1, one can add the sum “1+9” in the column to the right)",0
 there are 2 variables (vactive and vsave); vactive holds the active vertex (as in algorithm 2) and vsave holds the value of vactive before executing line 12 of algorithm 2,1
"
algorithm 1 neural programming inference 1: inputs: environment observation e, program p, arguments a, stop threshold α 2: function run(e, p, a) 3: h← 0, r ← 0 4: while r < α do 5: s← fenc(e, a), h← flstm(s, p, h) 6: r ← fend(h), p2 ← fprog(h), a2 ← farg(h) 7: if p is a primitive function then 8: e← fenv(e, p, a)",0
 we then construct a verification set of size 73 by ensuring that randomly generated graphs cover the analytically derived v ,1
" the 4 environment observations aid with control flow; qstacklo(pstacklo− 1) < qstackhi(pstackhi− 1) implements the lo < hi comparison in line 5 of algorithm 3, pstacklo == 1 checks if the stacks are empty in line 18 of algorithm 4, and the other observations (all involving ppivot or pj) deal with logic in the partition function",1
" fa8750-15-2-0104, and berkeley deep drive",1
" ppivot and pj point to the pivot and j indices of the array, used in the partition function in algorithm 3",0
"
given a verification set, we can then run the model on the verification set to check if the produced traces and results are correct",1
 the memory updates of these neural programs are so complex and interdependent that it is difficult to reason about the behaviors of the learned neural program under previously unseen situations (such as problems with longer inputs),1
" if yes, then this indicates that the learned neural program achieves provably perfect generalization",0
" for instance, for recursive addition, consider the family f of addition problems anan−1 ",0
" the pointer at index i3 represents a counter internal to the environment that is incremented once after each pass of the algorithm (one cycle of bubble and reset); when incremented a number of times equal to the length of the array, the flag i3 == length becomes true and terminates the entire algorithm ",0
 this procedure verifies that all base cases and reductions are handled properly by the model via explicit tests,1
"
arg 2 (auxiliary variable): color grey, color black
color grey and color black color the given vertex grey and black, respectively",0
 there is a (changing) list of neural programs used to accomplish a given task,0
"
in order to demonstrate that recursion can help learn and generalize better, for addition, we trained only on traces for 5 arbitrarily chosen 1-digit addition sum examples",1
" for example, a step input-step output pair in lines 2 and 3 of the left-hand side of figure 1 is (add1, write out 1)",0
" also, compared to the bubble pointers in bubble sort, the pointers that perform the comparison for quicksort (the compswap function) are usually not adjacent to each other, making quicksort less local than bubble sort",0
", the trace corresponding to the problem “109 + 101”)",1
" for non-primitive subprograms, we can update the observation-to-observation mapping currently associated with the subprogram and then apply that mapping to the current set",1
" we can obtain this mapping by first considering the possible observations that can arise at the beginning of the entry function (add, bubblesort, toposort, and quicksort) for some valid program input, and iteratively applying the observation-to-observation mapping implied by the reference implementation’s step output at that point in the execution",1
"
arg 2 (auxiliary variable): color grey, color black
color grey and color black color the given vertex grey and black, respectively",0
"1; and for bubble sort, appendix a",0
"
the operations we need to be concerned most about are carry and lshift, which induce partial environment states spanning two columns",0
" in computer science, recursion (as opposed to iteration) involves solving a larger problem by combining solutions to smaller instances of the same problem",0
"
in this general neural programming architecture, we show it is easy to support recursion",1
" observe that partially recursive does slightly better than non-recursive for the setting in which the length of the array is 3, and that the fully recursive version is able to sort every array given to it",0
g,0
" for each graph size, we test the learned programs on 30 randomly generated dags",0
" the add reduction rule ensures that each call to add only covers two adjacent columns, and so the lstm only ever runs for a fixed number of steps necessary to process these two columns",0
 2: initialize an empty stack s and a directed acyclic graph dag to traverse,1
g,0
" any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of national science foundation and darpa",1
 our experiments use a small number of training examples,1
" in this version, there is a dependence on length: bstep and lshift are called a number of times equivalent to one less than the length of the input array, in bubble and reset respectively",0
" hence, if all subproblems in s are added correctly, we have proven that any member of f will be added correctly, thus eliminating an infinite family of inputs that need to be tested",0
" using this modification, we constructed a verification set consisting of one array of size 10",0
 indentation indicates the stack is one level deeper than before,0
" using this modification, we constructed a verification set consisting of one array of size 10",0
"
in order to demonstrate that recursion can help learn and generalize better, for addition, we trained only on traces for 5 arbitrarily chosen 1-digit addition sum examples",0
" naı̈vely, we might expect to synthesize and test an input for any sequence created by combining the four possible colors in two variables and another boolean variable for whether the stack is empty (so 32 possible observations at any point), but for various reasons, most of these combinations are impossible to occur at any given point in the execution trace",0
" without using a recursive program, such a proof is not possible, because the non-recursive program runs on an arbitrarily long addition problem that creates correspondingly long sequences of step inputs; in the non-recursive formulation of addition, add calls add1 a number of times that is dependent on the length of the input",1
" the write operation has the following arguments:
arg 1 (object to write): env stack lo, env stack hi, phi, plo
env stack lo and env stack hi represent qstacklo(pstacklo) and qstackhi(pstackhi), respectively",0
" in this version of curriculum learning, even though the inputs are gradually becoming more complex, the semantics of the program is succinct and does not change",1
"
we experiment with two levels of recursion—partial and full",0
 the environment and return probability are omitted for readability,1
 we choose to implement a topological sort task for graphs,0
" in contrast, each call to the recursive add always runs for a fixed number of steps, even on arbitrarily long problems in f , so we can test that it performs correctly on a small, fixed number of step input sequences",0
 ,0
" we hypothesize that the reason for this is that the neural network learns to spuriously depend on specific characteristics of the training examples that are irrelevant to the true program semantics, such as length of the training inputs, and thus fails to generalize to more complex inputs",0
" partial recursion only adds a tail recursive call to bubblesort after bubble and reset, similar to the tail recursive call described previously for addition",0
" we performed experiments on the partially recursive version in order to examine what happens when only one recursive call is implemented, when in reality three are required for perfect generalization",1
" the assumption of leading 0’s can be easily removed, at the cost of slightly increasing the size of the verification set",0
" as with the others, we apply the procedure described in section 3",1
" in computer science, recursion (as opposed to iteration) involves solving a larger problem by combining solutions to smaller instances of the same problem",1
 18: push lo and p− 1 to slo and shi,1
" the recursive version of topological sort solves all graph instances we tried, from graphs of size 5 through 70",0
" as a consequence, our learned neural programs empirically achieve perfect generalization from a very small number of training examples",0
" 9: else 10: function run(e, p2, a2)
a description of the inference procedure is given in algorithm 1",1
"
we experiment with two levels of recursion—partial and full",0
" finally, the boolean pstack == 1 is used to check whether the stack is empty in line 18 of algorithm 2",1
" in particular, this condition is satisfied by npi, and thus the npi model naturally supports recursion (even though the authors of npi did not consider this aspect explicitly)",0
" in all experiments, α is set to 0",0
" 16: p = partition(a, lo, hi) 17: push p+ 1 and hi to slo and shi",1
"
note that v should also contain the necessary reductions, which corresponds to making the recursive calls at the correct time, as indicated by p ",1
 ,0
"1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 while (pstacklo 6= 1): 6 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 7 stack(stack_pop) 8 else: 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 stack(stack_pop) 12 partition() 13 stack(stack_push_call2) 14 stack(stack_push_call1) 15 } 16 17 partition() { 18 set_pivot_lo() 19 set_j_lo() 20 compswap_loop() 21 swap(ppivot, phi) 22 set_j_null() 23 } 24 25 compswap_loop() { 26 while (pj 6= phi): 27 compswap() 28 move(pj, up) 29 } 30 31 compswap() { 32 if (a[pj ] ≤ a[phi]): 33 swap(ppivot, pj) 34 move(ppivot, up) 35 } 36 37 stack(op) { 38 if (op == stack_push_call1): 39 write(env_stack_lo, plo) 40 write(env_stack_hi, ppivot − 1) 41 move(pstacklo, up) 42 move(pstackhi, up) 43 44 if (op == stack_push_call2): 45 write(env_stack_lo, ppivot + 1) 46 write(env_stack_hi, phi) 47 move(pstacklo, up) 48 move(pstackhi, up) 49 50 if (op == stack_pop): 51 write(env_stack_lo, reset) 52 write(env_stack_hi, reset) 53 move(pstacklo, down) 54 move(pstackhi, down) 55 }altered recursive functions 1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 6 partition() 7 stack(stack_push_call2) 8 stack(stack_push_call1) 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 quicksort() // recursive call 12 stack(stack_pop) 13 write(phi, env_stack_hi_peek) 14 write(plo, env_stack_lo_peek) 15 quicksort() // recursive call 16 stack(stack_pop) 17 } 18 19 compswap_loop() { 20 if (pj 6= phi): 21 compswap() 22 move(pj, up) 23 compswap_loop() // recursive call 24 }in this section, we describe the space of base cases and reduction rules that must be covered for each of the four sample tasks, in order to create the verification set",0
" thus, recursion helps decompose a problem and makes it easier to reason about a program’s behavior for previously unseen situations such as longer inputs",0
" we note that an earlier version of quicksort we tried lacked primitive operations to directly move a pointer to another, and therefore needed more functions and observations",0
" we further have qresult ∈ nu , a scratch-pad which contains the sorted list of vertices at the end of execution, and qstack ∈ nu , which serves the role of the stack s in algorithm 2",0
" the non-recursive and partially recursive versions are unable to sort long arrays, beyond length 8",0
" the swap operation has the following arguments:
arg 1 (swap object 1): ppivot
arg 2 (swap object 2): phi, pj
write",1
 the training set for addition contains 200 traces,0
" as before, the non-recursive learned program lacks a provable guarantee of generalization, whereas we show later that the recursive learned program has one",0
"
inside bubble and reset, there are two operations that can be made recursive",0
 ,0
" in contrast, each call to the recursive add always runs for a fixed number of steps, even on arbitrarily long problems in f , so we can test that it performs correctly on a small, fixed number of step input sequences",0
we now report on generalization for the varying tasks,0
" for purposes of verification, we replace the domain-specific encoder with the following:
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1) ≤ q(1, i2), 1 ≤ i1 ≤ length, 1 ≤ i2 ≤ length, i3 == length, at(1), at(2), at(3)]),which directly exposes which of the two values pointed to is larger",0
 this is due to the properties of the training set – we trained on 2 traces synthesized from arrays of length 7 and 1 trace synthesized from an array of length 6,0
 ,0
 the core controller acts as a dispatcher for the programs,1
" we can obtain this mapping by first considering the possible observations that can arise at the beginning of the entry function (add, bubblesort, toposort, and quicksort) for some valid program input, and iteratively applying the observation-to-observation mapping implied by the reference implementation’s step output at that point in the execution",0
g,0
" in computer science, recursion (as opposed to iteration) involves solving a larger problem by combining solutions to smaller instances of the same problem",0
"
an alternative way of representing the environment slice is to expose the values of the absolute vertices to the model; however, this makes it difficult to scale the model to larger graphs, since large vertex values are not seen during training time",0
" we performed experiments on the partially recursive version in order to examine what happens when only one recursive call is implemented, when in reality three are required for perfect generalization",0
" to make an analogy to mdps, this procedure is analogous to how value iteration obtains the correct value for each state starting from any initialization",0
"
the training set for bubble sort contains 100 traces, with maximum problem length of 2 (e",0
" consequently, it is infeasible to construct a verification set that covers all cases, and the verification procedure we have described is inadequate",0
" we note that an earlier version of quicksort we tried lacked primitive operations to directly move a pointer to another, and therefore needed more functions and observations",0
" more specifically, when m is trained on simpler inputs, such as inputs of a small length, the generalization metric evaluates how well m will do on more complex inputs, such as inputs of much longer length",0
" in practice, we can additionally use tail recursion optimization to avoid problems with the call stack growing too deep",0
 ,0
 we use the lomuto partition scheme; the logic for the recursive trace is shown in algorithm 3,1
" plo and phi point to the lo and hi indices of the array, as in algorithm 3",0
"
in order to demonstrate that recursion can help learn and generalize better, we trained a non-recursive and recursive model on just a single execution trace generated from a graph containing 5 nodes3 for the topological sort task",0
"
we also report on generalization results for the non-recursive and recursive versions of this variant of bubble sort",1
"
algorithm 3 recursive quicksort 1: initialize an array a to sort",0
" on the other hand, the recursive programs have learned the true program semantics",0
we emphasize the overall goal of this work is to enable the learning of a recursive program,1
"
we would also like to point out that in this paper, we provide as an example one way to train a recursive neural program, by providing a certain training execution trace to the npi model",0
" 16: p = partition(a, lo, hi) 17: push p+ 1 and hi to slo and shi",0
" we call this set of inputs the verification set, sv ",0
" any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of national science foundation and darpa",1
"1, we hypothesize the non-recursive programs do not generalize well because they have learned spurious dependencies specific to the training set, such as length of the input problems",1
we emphasize the overall goal of this work is to enable the learning of a recursive program,1
" the verification set can be very small, as we found a 10-element array ([8,2,1,2,0,8,5,8,3,7]) is sufficient to cover all of v ",0
" each time a subprogram is called, the stack depth increases",1
" at any given time, the npi has access to the values referred to by two pointers, represented by q(1, i1) and q(1, i2),",1
" the recursive version can generalize perfectly to long problems constructed from these components (such as the sum “822+233”, where “8+2” and “2+3” are in the training set), but the non-recursive version fails to sum these long problems properly",1
" qcolor(dag[vactive][childlist[vactive]]) refers to the color of the next child of vactive, used in the evaluation of the condition in the if branch in line 9 of algorithm 2",0
 a1a0 + bnbn−1 ,0
"
directed acyclic graphs are structurally more diverse than inputs in the two tasks of grade-school addition and bubble sort",0
" an example of part of an addition task trace, written in shorthand, is given in figure 1",0
"
an alternative method is to run p on many different program inputs and then observe step input sequences which occur, to create v ",1
"
an alternative way of representing the environment slice is to expose the values of the absolute vertices to the model; however, this makes it difficult to scale the model to larger graphs, since large vertex values are not seen during training time",1
 our result shows that the training procedure and the npi architecture is capable of generalizing from the step input-output pairs seen in the training data to the unseen ones present in the verification set,0
" for example, for the addition task, the model is trained on short inputs and then tested on its ability to sum inputs with much longer numbers of digits",1
" however, our concept of recursion for neural programs is general",0
 these pointers are referred to as bubble pointers,0
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,0
"
formally, verification consists of proving the following theorem:
∀i ∈ v,m(i) ⇓ p (i) where i denotes a sequence of step inputs (within one function call), v denotes the set of valid sequences of step inputs, m denotes the neural network model, p denotes the correct program, and p (i) denotes the next step output from the correct program",0
"we train using the adam optimizer and use a 2-layer lstm and task-specific state encoders for the external environments, as described in reed & de freitas (2016)",1
" thus, any general neural programming architecture supporting such a call structure can be made to support recursion",1
 the training set for addition contains 200 traces,0
" for the recursive formulation of addition, we analytically construct the set of input problems that cover all base cases and reduction rules",0
 the program terminates when seeing no numbers in the current column,0
g,0
"
base cases and reduction rules for addition",1
" by iterating with this procedure, and then running p on the input observation set that we obtain for the entry point function, we can obtain v precisely",0
" an example of part of an addition task trace, written in shorthand, is given in figure 1",0
" an example of part of an addition task trace, written in shorthand, is given in figure 1",1
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,1
 the model described in the training setup of section 4 (trained on 6 traces) was verified to be correct via the matching procedure described in section 4,1
" at any given time, the npi has access to the values referred to by two pointers, represented by q(1, i1) and q(1, i2),",1
"
algorithm 2 shows the topological sort task of interest",0
" for these programs, we re-use the appropriate program sets (the associated subprograms), and we refer the reader to the appendix of reed & de freitas (2016) for further details on the subprograms used in addition and bubble sort",0
" this is possible if and only if the graph has no directed cycles; that is to say, it must be a directed acyclic graph (dag)",1
" k is set to 11, to denote the range of possible numbers (0 through 9), along with the start/end token (represented with the same encoding) which is observed when a pointer reaches beyond the bounds of the input",0
"
the training set for quicksort contains 4 traces, synthesized from arrays of length 5",1
"
is critical for implementing recursion, since it permits us to restrict our attention to the currently relevant recursive call, ignoring irrelevant details about other contexts",1
" for many problems, the base cases and reduction rules usually consist of a finite (often small) number of cases",0
6,0
" in this version, there is a dependence on length: bstep and lshift are called a number of times equivalent to one less than the length of the input array, in bubble and reset respectively",0
as mentioned in section 2,0
"
bubble sort",1
"
it is sufficient to construct problems where every transition between two adjacent columns is covered",1
" thus, recursion helps decompose a problem and makes it easier to reason about a program’s behavior for previously unseen situations such as longer inputs",0
"
note that v should also contain the necessary reductions, which corresponds to making the recursive calls at the correct time, as indicated by p ",0
"
an alternative method is to run p on many different program inputs and then observe step input sequences which occur, to create v ",1
" recursion is an important concept in programming languages and a critical tool to
reduce the complexity of programs",1
" using this modification, we constructed a verification set consisting of one array of size 10",1
" the theorem states that for the same sequence of step inputs, the model produces the exact same step output as the target program it aims to learn",1
"
directed acyclic graphs are structurally more diverse than inputs in the two tasks of grade-school addition and bubble sort",0
"as discussed in section 2, the neural programmer-interpreter (npi) is an instance of a neural programmer architecture and hence it naturally supports recursion",1
"
formally, verification consists of proving the following theorem:
∀i ∈ v,m(i) ⇓ p (i) where i denotes a sequence of step inputs (within one function call), v denotes the set of valid sequences of step inputs, m denotes the neural network model, p denotes the correct program, and p (i) denotes the next step output from the correct program",0
"
the operations we need to be concerned most about are carry and lshift, which induce partial environment states spanning two columns",0
 errors arise due to undesirable and uninterpretable dependencies and associations the architecture learns to store in some high-dimensional hidden state,1
 3: begin traversing from vertex 1 in the dag,1
" we prove every member of f is added properly, given that subproblems s = {anan−1 + bnbn−1, an−1an−2 + bn−1bn−2, ",0
" the swap operation has the following arguments:
arg 1 (swap object 1): ppivot
arg 2 (swap object 2): phi, pj
write",0
 table 3 presents results on randomly generated arrays of varying length for the learned non-recursive and recursive programs,1
we emphasize the overall goal of this work is to enable the learning of a recursive program,0
"
algorithm 1 neural programming inference 1: inputs: environment observation e, program p, arguments a, stop threshold α 2: function run(e, p, a) 3: h← 0, r ← 0 4: while r < α do 5: s← fenc(e, a), h← flstm(s, p, h) 6: r ← fend(h), p2 ← fprog(h), a2 ← farg(h) 7: if p is a primitive function then 8: e← fenv(e, p, a)",0
" as this complexity interfered with determining the base cases and reductions, we changed the algorithm to its current form",0
 we outline how to construct this set,0
 recursion enables provably perfect generalization,0
 there are 2 variables (vactive and vsave); vactive holds the active vertex (as in algorithm 2) and vsave holds the value of vactive before executing line 12 of algorithm 2,0
" qcolor(pstart) contains the color of the current start vertex, used in the evaluation of the condition in the while loop in line 5 of algorithm 2",1
" childlist ∈ nu is a vector of pointers, where childlist[i] points to the next child under consideration for vertex i",1
" both the non-recursive and recursive learned programs generalize on all graphs we tried, up to 120 vertices",0
 ,1
" for each length, we test each program on 30 randomly generated problems",0
" ppivot and pj point to the pivot and j indices of the array, used in the partition function in algorithm 3",0
"
we describe the base cases, reduction rules, and the verification set for each task in appendix a",0
"
in this general neural programming architecture, we show it is easy to support recursion",0
"
we would also like to point out that in this paper, we provide as an example one way to train a recursive neural program, by providing a certain training execution trace to the npi model",1
" the outputs rt, pt+1, at+1 are used to determine the next action, as described in algorithm 1",1
"
to perform the verification as described here, it is critical to construct v correctly",1
" twc-1409915, darpa under grant no",1
g,1
" 2: initialize lo and hi to be 1 and n, where n is the length of a",1
" observe that partially recursive does slightly better than non-recursive for the setting in which the length of the array is 3, and that the fully recursive version is able to sort every array given to it",1
" the second is the training procedure, which consists of the form of the training data and the optimization process",0
" thus, recursion helps decompose a problem and makes it easier to reason about a program’s behavior for previously unseen situations such as longer inputs",1
" however, models that make use of this strategy eventually fail after a certain level of complexity (e",0
 pstacklo and pstackhi point to the top (empty) positions in qstacklo and qstackhi,0
" m , as described in algorithm 1, processes the sequence of step inputs by using an lstm",0
" the core module of the npi is an lstm controller that takes as input a slice of the current external environment, via a set of pointers, and a program and arguments to execute",1
3,1
" in practice, we can additionally use tail recursion optimization to avoid problems with the call stack growing too deep",0
" however, note that there is no provable guarantee that the non-recursive learned program will generalize to all inputs, whereas we show later that the recursive learned program has a provable guarantee of perfect generalization",0
 some functional programming languages go so far as not to define any looping constructs but rely solely on recursion to enable repeated execution of the same code,0
"
the training data for the neural programmer-interpreter consists of full execution traces for the program of interest",1
" in contrast, each call to the recursive add always runs for a fixed number of steps, even on arbitrarily long problems in f , so we can test that it performs correctly on a small, fixed number of step input sequences",0
" however, to be sure that the generated v is complete (covers all the cases needed), we need to check all pairs of observations seen in adjacent step inputs (in particular, those before and after a primitive function call), in a similar way as if we were constructing v from scratch",0
"program descriptions calls arguments toposort perform topological
sort on graph traverse, next start, write, move
none
traverse traverse graph until stack is empty check child, explore none check child check if a white child exists; if so, set childlist[vactive] to point to it move none explore repeatedly traverse subgraphs until stack is empty stack, check child, write, move none stack interact with stack, either pushing or popping write, move push, pop next start move pstart until reaching a white vertex",1
 our result shows that the training procedure and the npi architecture is capable of generalizing from the step input-output pairs seen in the training data to the unseen ones present in the verification set,0
" in future work, we would like to decrease supervision and construct models that are capable of coming up with recursive abstractions themselves",0
" , n , where the dag contains n vertices",0
" we can obtain this mapping by first considering the possible observations that can arise at the beginning of the entry function (add, bubblesort, toposort, and quicksort) for some valid program input, and iteratively applying the observation-to-observation mapping implied by the reference implementation’s step output at that point in the execution",0
 ,1
" a topological sort is a linear ordering of vertices such that for every directed edge (u, v) from u to v, u comes before v in the ordering",1
" more specifically, when m is trained on simpler inputs, such as inputs of a small length, the generalization metric evaluates how well m will do on more complex inputs, such as inputs of much longer length",0
 this paper is the first (to our knowledge) to investigate the important problem of provable generalization properties of neural programs,0
"
as mentioned in section 1, all approaches to neural programming today fare poorly on this generalization issue",0
"
to construct this set, by using the reference implementation of each subprogram, we construct a mapping between two sets of environment observations: the first set consists of all observations that can occur at the beginning of a particular subprogram’s invocation, and the second set contains the observations at the end of that subprogram",1
"in this paper, we propose that the key abstraction of recursion is necessary for neural programs to generalize",1
"
algorithm 2 depth first search topological sort 1: color all vertices white",0
"
we refer the reader to the appendix for the non-recursive trace generating functions",0
g,0
"
arg 2 (object to copy): env stack lo peek, env stack hi peek, phi, plo, ppivot − 1, ppivot + 1, reset
env stack lo peek and env stack hi peek represent qstacklo(pstacklo − 1) and qstackhi(pstackhi − 1), respectively",0
" however, our concept of recursion for neural programs is general",1
" we show that by modifying the training procedure, we enable the npi model to learn recursive neural programs",1
"
in this paper, we propose to resolve these issues by explicitly incorporating recursion into neural architectures",1
 we describe the recursive re-formulation of traces for two tasks from the original npi paper—grade-school addition and bubble sort,0
" we hypothesize that the reason for this is that the neural network learns to spuriously depend on specific characteristics of the training examples that are irrelevant to the true program semantics, such as length of the training inputs, and thus fails to generalize to more complex inputs",1
" the swap operation has the following arguments:
arg 1 (swap object 1): ppivot
arg 2 (swap object 2): phi, pj
write",1
" empirically, existing models suffer from a common limitation—generalization becomes poor beyond a threshold level of complexity",1
" 2: initialize lo and hi to be 1 and n, where n is the length of a",0
" observe that the non-recursive program’s correctness degrades for length 11 and beyond, while the recursive program can sort any given array",1
"
the training set for quicksort contains 4 traces, synthesized from arrays of length 5",1
 3: begin traversing from vertex 1 in the dag,1
", an array for quicksort or a dag for topological sort)",1
"
the three environment observations aid with control flow in algorithm 2",0
 table 4 demonstrates that the accuracy of the non-recursive program degrades sharply when moving from arrays of length 7 to arrays of length 8,0
" , a1a0 + b1b0} are added properly",0
 npi then outputs the return probability and next program and arguments to execute,1
" the pointer at index i3 represents a counter internal to the environment that is incremented once after each pass of the algorithm (one cycle of bubble and reset); when incremented a number of times equal to the length of the array, the flag i3 == length becomes true and terminates the entire algorithm ",0
" 2: initialize lo and hi to be 1 and n, where n is the length of a",0
" as before, the non-recursive learned program lacks a provable guarantee of generalization, whereas we show later that the recursive learned program has one",0
"
we describe the base cases, reduction rules, and the verification set for each task in appendix a",1
"1; and for bubble sort, appendix a",0
" the contents ofqresult andqstack are not exposed directly through the domain-specific encoder; rather, we define primitive functions which manipulate these scratch-pads",0
" we have three observations: the color of the start node, the color of the active node’s next child to be considered, and whether the stack is empty",1
" we hypothesize that the reason for this is that the neural network learns to spuriously depend on specific characteristics of the training examples that are irrelevant to the true program semantics, such as length of the training inputs, and thus fails to generalize to more complex inputs",1
 ,0
"
given a verification set, we can then run the model on the verification set to check if the produced traces and results are correct",0
"
bubble sort",1
" the 4 environment observations aid with control flow; qstacklo(pstacklo− 1) < qstackhi(pstackhi− 1) implements the lo < hi comparison in line 5 of algorithm 3, pstacklo == 1 checks if the stacks are empty in line 18 of algorithm 4, and the other observations (all involving ppivot or pj) deal with logic in the partition function",0
 we describe the recursive re-formulation of traces for two tasks from the original npi paper—grade-school addition and bubble sort,1
"
algorithm 2 shows the topological sort task of interest",0
" the contents ofqresult andqstack are not exposed directly through the domain-specific encoder; rather, we define primitive functions which manipulate these scratch-pads",0
" 2: initialize lo and hi to be 1 and n, where n is the length of a",1
"
topological sort",0
" numerous models have been proposed for learning programs; to name a few, this includes the differentiable neural computer (graves  , 2016), neural turing machine (graves  , 2014), neural gpu (kaiser & sutskever, 2015), neural programmer (neelakantan  , 2015), pointer network (vinyals  , 2015), hierarchical attentive memory (andrychowicz & kurach, 2016), and neural random access machine (kurach  , 2016)",0
" childlist ∈ nu is a vector of pointers, where childlist[i] points to the next child under consideration for vertex i",0
as mentioned in section 2,1
" although the model is exposed to more and more data, it might learn spurious and overly complex representations of the program, as suggested in zaremba   (2016)",0
" if a white vertex is found, set pstart to point to it; this signifies the start of a traversal of a new connected component",0
"
the npi accesses an external environment, q, which varies according to the task",1
", the trace corresponding to the problem “109 + 101”)",0
"
grade-school addition",1
g,1
"
an alternative method is to run p on many different program inputs and then observe step input sequences which occur, to create v ",0
 b1b0 where no carry operations occur,1
 the first is the actual model architecture,1
" for bubble sort, the domain-specific encoder is
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1), q(1, i2), i3 == length, at(1), at(2), at(3)]),
where the environment q ∈ r1×n×k is a scratch-pad that contains 1 row, to represent the state of the array as sorting proceeds in-place, andn columns",0
" we further have qresult ∈ nu , a scratch-pad which contains the sorted list of vertices at the end of execution, and qstack ∈ nu , which serves the role of the stack s in algorithm 2",1
"
by nature, recursion reduces the complexity of a problem to simpler instances",0
" finally, we implement recursive traces for our own topological sort and quicksort tasks",0
 stack pop pushes −∞ values to qstacklo and qstackhi,1
"program descriptions calls arguments quicksort run the quicksort
routine in place for the array a, for indices from lo to hi non-recursive: partition, stack, write recursive: same as non-recursive version, along with quicksort implicitly: array a to sort, lo, hi
partition runs the partition function",0
"
the same set of problems was used to generate the training traces for all formulations of the task, for non-recursive and recursive versions",1
" in fact, it is one of our future directions to explore new ways to train a recursive neural program without providing explicit training execution traces or with only partial or non-recursive traces",0
 there is a (changing) list of neural programs used to accomplish a given task,1
" as aforementioned, the npi model naturally supports recursion",0
" in fact, we argue that recursion is an essential element for neural programs to generalize, and makes it tractable to prove the generalization of neural programs",1
3,0
" thus, recursion helps decompose a problem and makes it easier to reason about a program’s behavior for previously unseen situations such as longer inputs",1
 table 4 demonstrates that the accuracy of the non-recursive program degrades sharply when moving from arrays of length 7 to arrays of length 8,0
"program descriptions calls arguments toposort perform topological
sort on graph traverse, next start, write, move
none
traverse traverse graph until stack is empty check child, explore none check child check if a white child exists; if so, set childlist[vactive] to point to it move none explore repeatedly traverse subgraphs until stack is empty stack, check child, write, move none stack interact with stack, either pushing or popping write, move push, pop next start move pstart until reaching a white vertex",0
" as before, the non-recursive learned program lacks a provable guarantee of generalization, whereas we show later that the recursive learned program has one",0
"
algorithm 2 shows the topological sort task of interest",0
" each time a subprogram is called, the stack depth increases",1
"
thus far, to evaluate the efficacy of neural models on programming tasks, the only metric that has been used is generalization of expected behavior to inputs of greater complexity (vinyals   (2015), kaiser & sutskever (2015), reed & de freitas (2016), graves   (2016), zaremba   (2016))",0
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,1
training neural networks to synthesize robust programs from a small number of examples is a challenging task,1
" thus, we demonstrate that recursion enables provably perfect generalization for different tasks, including addition, topological sort, quicksort, and a variant of bubble sort",0
" however, note that there is no provable guarantee that the non-recursive learned program will generalize to all inputs, whereas we show later that the recursive learned program has a provable guarantee of perfect generalization",0
" thus, we demonstrate that recursion enables provably perfect generalization for different tasks, including addition, topological sort, quicksort, and a variant of bubble sort",1
" formally, the npi is represented by the following set of equations:
st = fenc(et, at)
ht = flstm(st, pt, ht−1)
rt = fend(ht), pt+1 = fprog(ht), at+1 = farg(ht)
t is a subscript denoting the time-step; fenc is a domain-specific encoder (to be described later) that takes in the environment slice et and arguments at; flstm represents the core module, which takes in the state st generated by fenc, a program embedding pt ∈ rp , and hidden lstm state ht; fend decodes the return probability rt; fprog decodes a program key embedding pt+1;1 and farg decodes arguments at+1",1
 the logic for the non-recursive trace is shown in algorithm 4 in the appendix,0
" even though the earlier version also generalized just as well in practice, relatively small differences in the formulation of the traces and the environment observations can drastically change the difficulty of verification",0
"
algorithm 2 shows the topological sort task of interest",0
training neural networks to synthesize robust programs from a small number of examples is a challenging task,0
" as with the original npi architecture, the experiments for this paper always used a 3-tuple of integers at = (at(1), at(2), at(3))",1
"
the operations we need to be concerned most about are carry and lshift, which induce partial environment states spanning two columns",1
 these pointers are referred to as bubble pointers,0
" et+1 ∼ fenv(et, pt, at)",0
"
the non-recursive trace loops on cycles of bubble and reset, which logically represents one bubble sweep through the array and reset of the two bubble pointers to the very beginning of the array, respectively",0
 table 4 also demonstrates that the (verified) recursive program generalizes perfectly,1
"
we refer the reader to the appendix for the non-recursive trace generating functions",1
" for each task, given the verification set, we check the traces and results of the learned, to-be-verified neural program (described in section 4",0
" more specifically, when m is trained on simpler inputs, such as inputs of a small length, the generalization metric evaluates how well m will do on more complex inputs, such as inputs of much longer length",0
" if the program is primitive, the next environmental state et+1 will be affected by pt and at, i",1
"
thus far, to evaluate the efficacy of neural models on programming tasks, the only metric that has been used is generalization of expected behavior to inputs of greater complexity (vinyals   (2015), kaiser & sutskever (2015), reed & de freitas (2016), graves   (2016), zaremba   (2016))",1
" table 1 presents results on randomly generated arrays of varying length for the learned non-recursive, partially recursive, and full recursive programs",0
 our result shows that the training procedure and the npi architecture is capable of generalizing from the step input-output pairs seen in the training data to the unseen ones present in the verification set,1
" the assumption of leading 0’s can be easily removed, at the cost of slightly increasing the size of the verification set",0
g,0
g,0
3 to construct v and then empirically create a verification set which covers v ,0
" any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of national science foundation and darpa",0
" full recursion, in addition to making the aforementioned tail recursive call, adds two additional recursive calls; bstep and lshift are made tail recursive",1
" an example of part of an addition task trace, written in shorthand, is given in figure 1",1
" 16: p = partition(a, lo, hi) 17: push p+ 1 and hi to slo and shi",1
" for brevity, we refer the reader to the appendix for further details on the program set and non-recursive and recursive trace-generating functions used for topological sort",0
 these pointers are referred to as bubble pointers,1
 we then construct a verification set of size 73 by ensuring that randomly generated graphs cover the analytically derived v ,1
" however, our concept of recursion for neural programs is general",1
" bstep, used in bubble, compares pairs of numbers, continuously moving the bubble pointers once to the right each time until reaching the end of the array",0
" in this version, there is a dependence on length: bstep and lshift are called a number of times equivalent to one less than the length of the input array, in bubble and reset respectively",0
e,0
" via the recursive call, we effectively forget that the column just added exists, since the recursive call to add starts with a new hidden state for the lstm controller",0
" table 1 presents results on randomly generated arrays of varying length for the learned non-recursive, partially recursive, and full recursive programs",0
" the contents ofqresult andqstack are not exposed directly through the domain-specific encoder; rather, we define primitive functions which manipulate these scratch-pads",0
" each time a subprogram is called, the stack depth increases",0
e,0
" empirically, existing models suffer from a common limitation—generalization becomes poor beyond a threshold level of complexity",0
" in particular, we construct new training traces which explicitly contain recursive elements and show that with this type of trace, npi easily learns recursive programs",1
" we also hope to decrease supervision, for example by training with only partial or non-recursive traces, and to develop novel neural programming architectures integrated directly with a notion of recursion",0
" the partial recursion is not enough for perfect generalization, as will be presented later in section 4",1
 our result shows that the training procedure and the npi architecture is capable of generalizing from the step input-output pairs seen in the training data to the unseen ones present in the verification set,1
g,0
" numerous models have been proposed for learning programs; to name a few, this includes the differentiable neural computer (graves  , 2016), neural turing machine (graves  , 2014), neural gpu (kaiser & sutskever, 2015), neural programmer (neelakantan  , 2015), pointer network (vinyals  , 2015), hierarchical attentive memory (andrychowicz & kurach, 2016), and neural random access machine (kurach  , 2016)",1
" this guarantees that the step input sequences considered during verification contain all step input sequences which arise during execution of an unseen problem in f , leading to generalization to any problem in f ",0
" for instance, for recursive addition, consider the family f of addition problems anan−1 ",0
"
arg 2 (increment or decrement): up, down1 // top level topological sort call 2 toposort() { 3 while (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 } 0 11 traverse() { 12 check_child() 13 explore() 14 } 15 16 check_child() { 17 while (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 18 move(childlist[vactive], up) 19 } 20 21 explore() { 22 do 23 if (qcolor(dag[vactive][childlist[vactive]]) is white): 24 write(color_next, color_grey) 25 stack(push) 26 write(save) 27 write(active_neighb) 28 move(childlist[vsave], up) 29 else: 30 write(color_curr, color_black) 31 write(result) 32 move(presult, up) 33 if(pstack == 1): 34 break 35 else: 36 stack(pop) 37 check_child() 38 while (true) 39 } 40 41 stack(op) { 42 if (op == push): 43 write(stack_push) 44 move(pstack, up) 45 46 if (op == pop): 47 write(active_stack) 48 write(stack_pop) 49 move(pstack, down) 50 } 51 52 next_start() { 53 while(qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 54 move(pstart, up) 55 }altered recursive functions  // top level topological sort call 2 toposort() { 3 if (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 toposort() // recursive call 0 } 11 12 check_child() { 13 if (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 14 move(childlist[vactive], up) 15 check_child() // recursive call 16 } 17 18 explore() { 19 if (qcolor(dag[vactive][childlist[vactive]]) is white): 20 write(color_next, color_grey) 21 stack(push) 22 write(save) 23 write(active_neighb) 24 move(childlist[vsave], up) 25 else: 26 write(color_curr, color_black) 27 write(result) 28 move(presult, up) 29 if(pstack == 1): 30 return 31 else: 32 stack(pop) 33 check_child() 34 explore() // recursive call 35 } 36 37 next_start() { 38 if (qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 39 move(pstart, up) 40 next_start() // recursive call 41 }algorithm 4 iterative quicksort 1: initialize an array a to sort and two empty stacks slo and shi",0
" the move operation has the following arguments:
arg 1 (pointer): presult, pstack, pstart, childlist[vactive], childlist[vsave]
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 5 values",1
 recursion enables provable guarantees on neural programs’ behavior without needing to exhaustively enumerate all possible inputs to the programs,1
" the theorem states that for the same sequence of step inputs, the model produces the exact same step output as the target program it aims to learn",0
 npi then outputs the return probability and next program and arguments to execute,1
"
the non-recursive trace loops on cycles of bubble and reset, which logically represents one bubble sweep through the array and reset of the two bubble pointers to the very beginning of the array, respectively",0
" for brevity, we refer the reader to the appendix for information about the program set and non-recursive and recursive trace-generating functions for quicksort",0
 we outline how to construct this set,0
 our experiments use a small number of training examples,1
we now report on generalization for the varying tasks,0
" more specifically, when m is trained on simpler inputs, such as inputs of a small length, the generalization metric evaluates how well m will do on more complex inputs, such as inputs of much longer length",0
"
in order to demonstrate that recursion can help learn and generalize better, we trained a non-recursive and recursive model on just a single execution trace generated from a graph containing 5 nodes3 for the topological sort task",0
" formally, the npi is represented by the following set of equations:
st = fenc(et, at)
ht = flstm(st, pt, ht−1)
rt = fend(ht), pt+1 = fprog(ht), at+1 = farg(ht)
t is a subscript denoting the time-step; fenc is a domain-specific encoder (to be described later) that takes in the environment slice et and arguments at; flstm represents the core module, which takes in the state st generated by fenc, a program embedding pt ∈ rp , and hidden lstm state ht; fend decodes the return probability rt; fprog decodes a program key embedding pt+1;1 and farg decodes arguments at+1",0
" if the program is primitive, the next environmental state et+1 will be affected by pt and at, i",0
" k is set to 11, to denote the range of possible numbers (0 through 9), along with the start/end token (represented with the same encoding) which is observed when a pointer reaches beyond the bounds of the input",0
" when the program is called, the current context including the caller’s memory state is stored on a stack; when the program returns, the stored context is popped off the stack to resume execution in the previous caller’s context",0
" both the non-recursive and recursive learned programs generalize on all input lengths we tried, up to 5000 digits",0
" for each graph size, we test the learned programs on 30 randomly generated dags",0
" the add reduction rule ensures that each call to add only covers two adjacent columns, and so the lstm only ever runs for a fixed number of steps necessary to process these two columns",0
"as discussed in section 2, the neural programmer-interpreter (npi) is an instance of a neural programmer architecture and hence it naturally supports recursion",0
 this verification phase only needs to be performed once after training,0
"
note that the recursion for quicksort is not purely tail recursive and therefore represents a more complex kind of recursion that is harder to learn than in the previous tasks",0
"
there are 6 pointers (plo, phi, pstacklo, pstackhi, ppivot, pj)",1
" bstep, used in bubble, compares pairs of numbers, continuously moving the bubble pointers once to the right each time until reaching the end of the array",0
" observe that the non-recursive program’s correctness degrades for length 11 and beyond, while the recursive program can sort any given array",1
", qstacklo or qstackhi) or to pointer (e",1
"
for topological sort, the domain-specific encoder is
fenc(dag,qcolor, pstack, pstart, vactive, childlist, at)
=mlp ([qcolor(pstart), qcolor(dag[vactive][childlist[vactive]]), pstack == 1, at(1), at(2), at(3)]),
where qcolor ∈ ru×4 is a scratch-pad that contains u rows, each containing one of four colors (white, gray, black, invalid) with one-hot encoding",0
 2: initialize an empty stack s and a directed acyclic graph dag to traverse,0
" ppivot and pj point to the pivot and j indices of the array, used in the partition function in algorithm 3",1
" if it is too small, then execution of the program on some input might require evaluation of m(i) on some i /∈ v , and so the behavior ofm(i) might deviate from p (i)",1
"
in this paper, we propose that recursion is an important concept for neural programs as well",0
 the model described in the training setup of section 4 (trained on 6 traces) was verified to be correct via the matching procedure described in section 4,0
" the add reduction rule ensures that each call to add only covers two adjacent columns, and so the lstm only ever runs for a fixed number of steps necessary to process these two columns",1
" using this modification, we constructed a verification set consisting of one array of size 10",0
g,0
" in particular, in the explore function, adding a tail recursive call resets and stores the hidden states associated with vertices in a stack-like fashion",1
" however, our concept of recursion for neural programs is general",1
" even though the earlier version also generalized just as well in practice, relatively small differences in the formulation of the traces and the environment observations can drastically change the difficulty of verification",1
" as aforementioned, the npi model naturally supports recursion",0
"
we would also like to point out that in this paper, we provide as an example one way to train a recursive neural program, by providing a certain training execution trace to the npi model",1
" for the recursive formulation of addition, we analytically construct the set of input problems that cover all base cases and reduction rules",0
 we adapt machinery from the original paper slightly to fit our needs,1
 recursion can be implemented differently for different neural programming models,0
" observe that the non-recursive program’s correctness degrades for length 11 and beyond, while the recursive program can sort any given array",0
", qstacklo or qstackhi) or to pointer (e",0
" the verification set can be very small, as we found a 10-element array ([8,2,1,2,0,8,5,8,3,7]) is sufficient to cover all of v ",0
"
it is important to emphasize that at inference time in the npi, the hidden state of the lstm controller is reset (to zero) at each subprogram call, as in line 3 of algorithm 1 (h ← 0)",1
" for non-primitive subprograms, we can update the observation-to-observation mapping currently associated with the subprogram and then apply that mapping to the current set",0
"as there is no public implementation of npi, we implemented a version of it in keras that is as faithful to the paper as possible",0
 18: push lo and p− 1 to slo and shi,1
" thus, recursion helps decompose a problem and makes it easier to reason about a program’s behavior for previously unseen situations such as longer inputs",1
 stack pop pushes −∞ values to qstacklo and qstackhi,1
" on the other hand, the non-recursive version has low accuracy, beginning from size 5, and fails completely for graphs of size 8 and beyond",1
"
grade school addition",0
" furthermore, given a verification input set that covers all base cases and reduction rules, we can formally prove that the learned neural programs achieve perfect generalization after verifying its behavior on the verification input set",1
 indentation indicates the stack is one level deeper than before,0
" in the non-recursive trace, there are four functions that can be made recursive—toposort, check child, explore, and next start, and we add a tail recursive call to each of these functions in order to make the recursive trace",0
3 to construct v and then empirically create a verification set which covers v ,0
" thus, we demonstrate that recursion enables provably perfect generalization for different tasks, including addition, topological sort, quicksort, and a variant of bubble sort",0
5,0
"
arg 2 (increment or decrement): up, down
swap",1
" the move operation has the following arguments:
arg 1 (pointer): pstacklo, pstackhi, pj , ppivot
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values",1
" as an application, we incorporate recursion into the neural programmer-interpreter architecture and consider four sample tasks: grade-school addition, bubble sort, topological sort, and quicksort",0
" the add reduction rule ensures that each call to add only covers two adjacent columns, and so the lstm only ever runs for a fixed number of steps necessary to process these two columns",1
" in future work, we would like to decrease supervision and construct models that are capable of coming up with recursive abstractions themselves",1
2,0
"1; and for bubble sort, appendix a",0
"
one common strategy to improve generalization is to use curriculum learning, where the model is trained on inputs of gradually increasing complexity",0
"
for addition, we analytically determine the verification set",0
" a single element of an execution trace consists of a step input-step output pair, which can be synthesized from algorithm 1: this corresponds to, for a given time-step, the step input tuple (e, p, a) and step output tuple (r, p2, a2)",1
" m is considered to have perfect generalization if m can give the right answer for any input, such as inputs of arbitrary length",0
"when constructing a neural network for the purpose of learning a program, there are two orthogonal aspects to consider",1
" for each function we use to implement the recursive version of topological sort, we need to consider the set of possible environment observation sequences we can create from all valid inputs and test that the learned program produces the correct behavior on each of these inputs",0
"
base cases and reduction rules for topological sort",0
 the sequence of primitive operations (move and write operations) for the non-recursive and recursive versions are exactly the same,0
" to the best of our knowledge, this is the first time verification has been applied to a neural program, providing provable guarantees about its behavior",1
 the first is the actual model architecture,1
"
the training data for the neural programmer-interpreter consists of full execution traces for the program of interest",1
", to change the value of phi) none described belowstack",0
" furthermore, given a verification input set that covers all base cases and reduction rules, we can formally prove that the learned neural programs achieve perfect generalization after verifying its behavior on the verification input set",1
"
the npi accesses an external environment, q, which varies according to the task",1
" this makes it so that we only need to consider the vertices in the subgraph that are currently relevant for computing the sort, allowing simpler reasoning about behavior for large graphs",1
"
the training set for topological sort contains 6 traces, with one synthesized from a graph of size 5 and the rest synthesized from graphs of size 7",0
 pstacklo and pstackhi point to the top (empty) positions in qstacklo and qstackhi,0
"
after finding v , we construct a set of problem inputs which, when executed on p , create exactly the step input sequences which make up v ",0
", to change the value of phi) none described belowstack",0
" thus, any general neural programming architecture supporting such a call structure can be made to support recursion",1
"
is critical for implementing recursion, since it permits us to restrict our attention to the currently relevant recursive call, ignoring irrelevant details about other contexts",1
 ,1
"
base cases and reduction rules for bubble sort",0
g,0
"
in this paper, we propose to resolve these issues by explicitly incorporating recursion into neural architectures",0
 ,1
"
in this paper, we propose to resolve these issues by explicitly incorporating recursion into neural architectures",1
"
arg 2 (object to copy): env stack lo peek, env stack hi peek, phi, plo, ppivot − 1, ppivot + 1, reset
env stack lo peek and env stack hi peek represent qstacklo(pstacklo − 1) and qstackhi(pstackhi − 1), respectively",1
"
under the assumption that there are no leading 0’s (except in the case of single digits) and the two numbers to be added have the same number of digits, the verification set for addition contains 20,181 input problems",1
" as this complexity interfered with determining the base cases and reductions, we changed the algorithm to its current form",0
 this paper is the first (to our knowledge) to investigate the important problem of provable generalization properties of neural programs,0
3,0
"we train using the adam optimizer and use a 2-layer lstm and task-specific state encoders for the external environments, as described in reed & de freitas (2016)",1
" thus, we demonstrate that recursion enables provably perfect generalization for different tasks, including addition, topological sort, quicksort, and a variant of bubble sort",1
" the recursive version can generalize perfectly to long problems constructed from these components (such as the sum “822+233”, where “8+2” and “2+3” are in the training set), but the non-recursive version fails to sum these long problems properly",0
" the pointers at index i1 and i2 are used to compare the pair of numbers considered during the bubble sweep, swapping them if the number at i1 is greater than that in i2",1
" for each function we use to implement the recursive version of topological sort, we need to consider the set of possible environment observation sequences we can create from all valid inputs and test that the learned program produces the correct behavior on each of these inputs",1
", to change the value of phi) none described belowstack",1
" this guarantees that the step input sequences considered during verification contain all step input sequences which arise during execution of an unseen problem in f , leading to generalization to any problem in f ",0
" note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values, and arg 2 can only take one of 7 values",0
"
note that the training set can often be considerably smaller than the verification set, and despite this, the learned model can still pass the entire verification set",1
"
base cases and reduction rules for bubble sort",0
" in particular, recursion can be implemented as a program calling itself",0
"we describe how models trained with recursive traces can be proven to generalize, by using the verification procedure described in section 3",1
" via the recursive call, we effectively forget that the column just added exists, since the recursive call to add starts with a new hidden state for the lstm controller",0
2,0
"
in addition, none of the current approaches to neural programming provide a method or even aim to enable provable guarantees about generalization",0
" the move operation has the following arguments:
arg 1 (pointer): pstacklo, pstackhi, pj , ppivot
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values",1
" in practice, we can additionally use tail recursion optimization to avoid problems with the call stack growing too deep",0
" fa8750-15-2-0104, and berkeley deep drive",1
" the second is the training procedure, which consists of the form of the training data and the optimization process",0
"
we note that for tasks with very large input domains, such as ones involving mnist digits or speech samples, the state space of base cases and reduction rules could be prohibitively large, possibly infinite",0
"
it is sufficient to construct problems where every transition between two adjacent columns is covered",1
" we make the trace recursive by adding a tail recursive call into the trace for the add program after calling add1 and lshift, the original paper uses k = 10, but we found it necessary to augment the range with an end token, in order to terminate properly",0
"
the training set for bubble sort contains 100 traces, with maximum problem length of 2 (e",0
" recursion is an important concept in programming languages and a critical tool to
reduce the complexity of programs",1
" numerous models have been proposed for learning programs; to name a few, this includes the differentiable neural computer (graves  , 2016), neural turing machine (graves  , 2014), neural gpu (kaiser & sutskever, 2015), neural programmer (neelakantan  , 2015), pointer network (vinyals  , 2015), hierarchical attentive memory (andrychowicz & kurach, 2016), and neural random access machine (kurach  , 2016)",1
5,1
" in future work, we would like to decrease supervision and construct models that are capable of coming up with recursive abstractions themselves",1
" 2: initialize lo and hi to be 1 and n, where n is the length of a",0
" for each length, we test each program on 30 randomly generated problems",0
" however, models that make use of this strategy eventually fail after a certain level of complexity (e",0
" this makes it so that we only need to consider the vertices in the subgraph that are currently relevant for computing the sort, allowing simpler reasoning about behavior for large graphs",1
" the verification set can be very small, as we found a 10-element array ([8,2,1,2,0,8,5,8,3,7]) is sufficient to cover all of v ",1
 we adapt machinery from the original paper slightly to fit our needs,0
" the write operation has the following arguments:
arg 1 (object to write): env stack lo, env stack hi, phi, plo
env stack lo and env stack hi represent qstacklo(pstacklo) and qstackhi(pstackhi), respectively",0
"
given a verification set, we can then run the model on the verification set to check if the produced traces and results are correct",0
"
quicksort",0
" that is to say, the network does not learn the true program semantics",1
" if it is too large, then the semantics of p might not be well-defined on some elements in v , or the spurious step input sequences may not be reachable from any valid problem input (e",1
 table 4 demonstrates that the accuracy of the non-recursive program degrades sharply when moving from arrays of length 7 to arrays of length 8,1
" given verification sets that cover all the base cases and reduction rules, we can provide proofs that these learned programs generalize perfectly",0
" if the step output specifies a primitive function call, we need to reason about how it can affect the environment so as to change the observation in the next step input",0
" as a consequence, our learned neural programs empirically achieve perfect generalization from a very small number of training examples",1
" for brevity, we refer the reader to the appendix for information about the program set and non-recursive and recursive trace-generating functions for quicksort",1
3,0
 the training set for addition contains 200 traces,1
" we show that by modifying the training procedure, we enable the npi model to learn recursive neural programs",1
 we then construct a verification set of size 73 by ensuring that randomly generated graphs cover the analytically derived v ,0
" for these programs, we re-use the appropriate program sets (the associated subprograms), and we refer the reader to the appendix of reed & de freitas (2016) for further details on the subprograms used in addition and bubble sort",1
" et+1 ∼ fenv(et, pt, at)",0
" on the other hand, the non-recursive version has low accuracy, beginning from size 5, and fails completely for graphs of size 8 and beyond",1
"
the npi accesses an external environment, q, which varies according to the task",0
"
the dag is represented as an adjacency list where dag[i][j] refers to the j-th child of vertex i",1
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,0
" partial recursion only adds a tail recursive call to bubblesort after bubble and reset, similar to the tail recursive call described previously for addition",0
" for these programs, we re-use the appropriate program sets (the associated subprograms), and we refer the reader to the appendix of reed & de freitas (2016) for further details on the subprograms used in addition and bubble sort",0
"
we would also like to point out that in this paper, we provide as an example one way to train a recursive neural program, by providing a certain training execution trace to the npi model",0
" using this modification, we constructed a verification set consisting of one array of size 10",1
3,1
" we show that by modifying the training procedure, we enable the npi model to learn recursive neural programs",0
"
for addition, we analytically determine the verification set",1
"
we describe the base cases, reduction rules, and the verification set for each task in appendix a",0
" thus, recursion helps decompose a problem and makes it easier to reason about a program’s behavior for previously unseen situations such as longer inputs",0
" this makes it so that we only need to consider the vertices in the subgraph that are currently relevant for computing the sort, allowing simpler reasoning about behavior for large graphs",1
", an array for quicksort or a dag for topological sort)",1
"
it is important to emphasize that at inference time in the npi, the hidden state of the lstm controller is reset (to zero) at each subprogram call, as in line 3 of algorithm 1 (h ← 0)",0
 the logic for the non-recursive trace is shown in algorithm 4 in the appendix,1
 ,0
" we have three observations: the color of the start node, the color of the active node’s next child to be considered, and whether the stack is empty",1
" our implementation uses two stacks qstacklo and
qstackhi, each in ru , that store the arguments to the recursive quicksort calls in algorithm 3; before each recursive call, the appropriate arguments are popped off the stack and written to plo and phi",0
3,0
"
it is important to emphasize that at inference time in the npi, the hidden state of the lstm controller is reset (to zero) at each subprogram call, as in line 3 of algorithm 1 (h ← 0)",0
" we have three observations: the color of the start node, the color of the active node’s next child to be considered, and whether the stack is empty",1
"
the npi accesses an external environment, q, which varies according to the task",1
", the trace corresponding to the problem “109 + 101”)",1
" this makes it so that we only need to consider the vertices in the subgraph that are currently relevant for computing the sort, allowing simpler reasoning about behavior for large graphs",0
" a topological sort is a linear ordering of vertices such that for every directed edge (u, v) from u to v, u comes before v in the ordering",0
"
for topological sort, the domain-specific encoder is
fenc(dag,qcolor, pstack, pstart, vactive, childlist, at)
=mlp ([qcolor(pstart), qcolor(dag[vactive][childlist[vactive]]), pstack == 1, at(1), at(2), at(3)]),
where qcolor ∈ ru×4 is a scratch-pad that contains u rows, each containing one of four colors (white, gray, black, invalid) with one-hot encoding",0
" for each length, we test each program on 30 randomly generated problems",0
" , n , where the dag contains n vertices",1
" the stack operation has the following arguments:
arg 1 (operation): stack push call1, stack push call2, stack pop
stack push call1 pushes lo and pivot−1 toqstacklo andqstackhi",0
 ,0
" the move operation has the following arguments:
arg 1 (pointer): pstacklo, pstackhi, pj , ppivot
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values",0
" on the other hand, the non-recursive version has low accuracy, beginning from size 5, and fails completely for graphs of size 8 and beyond",0
" in future work, we seek to enable more tasks with recursive structure",1
3,0
" we also hope to decrease supervision, for example by training with only partial or non-recursive traces, and to develop novel neural programming architectures integrated directly with a notion of recursion",1
" empirically, we observe that the learned recursive programs solve all valid inputs with 100% accuracy after training on a very small number of examples, out-performing previous generalization results",0
 we use the lomuto partition scheme; the logic for the recursive trace is shown in algorithm 3,0
" however, note that there is no provable guarantee that the non-recursive learned program will generalize to all inputs, whereas we show later that the recursive learned program has a provable guarantee of perfect generalization",1
 ,1
" m is considered to have perfect generalization if m can give the right answer for any input, such as inputs of arbitrary length",0
2,0
" qcolor(dag[vactive][childlist[vactive]]) refers to the color of the next child of vactive, used in the evaluation of the condition in the if branch in line 9 of algorithm 2",1
" in all experiments, α is set to 0",1
" if a white vertex is found, set pstart to point to it; this signifies the start of a traversal of a new connected component",1
" training on the full recursive trace leads to perfect generalization, as shown in section 4",0
" in particular, given that a recursion is defined by two properties as mentioned before, the base cases and the set of reduction rules, we can prove a recursive neural program generalizes perfectly if we can prove that (1) it performs correctly on the base cases; (2) it learns the reduction rules correctly",0
 table 3 presents results on randomly generated arrays of varying length for the learned non-recursive and recursive programs,1
" observe that the non-recursive program’s correctness degrades for length 11 and beyond, while the recursive program can sort any given array",1
3 to construct v and then empirically create a verification set which covers v ,0
"
an alternative method is to run p on many different program inputs and then observe step input sequences which occur, to create v ",0
"
arg 2 (increment or decrement): up, down
swap",0
as mentioned in section 2,1
training neural networks to synthesize robust programs from a small number of examples is a challenging task,0
", to change the value of phi) none described belowstack",0
 we choose to implement a topological sort task for graphs,0
" childlist ∈ nu is a vector of pointers, where childlist[i] points to the next child under consideration for vertex i",0
" for brevity, we refer the reader to the appendix for information about the program set and non-recursive and recursive trace-generating functions for quicksort",0
" it is straightforward to deal with all other operations, which do not induce partial environment states",0
" recursion is an important concept in programming languages and a critical tool to
reduce the complexity of programs",0
" in practice, we can additionally use tail recursion optimization to avoid problems with the call stack growing too deep",0
" k is set to 11, to represent the range of 10 possible digits, along with a token representing the end of input",1
" each step during an execution of the program does one of three things: (1) another subprogram along with associated arguments is called, as in line 10, (2) the program writes to the environment if it is primitive, as in line 8, or (3) the loop is terminated if the return probability exceeds a threshold α, after which the stack frame is popped and control is returned to the caller",1
 we find that recursion makes it easier for the network to learn the right program and generalize to unknown situations,1
"
move",0
" if the step output specifies a primitive function call, we need to reason about how it can affect the environment so as to change the observation in the next step input",0
" we implement a quicksort task, in order to demonstrate that recursion helps with learning divide-and-conquer algorithms",0
2,1
"we describe how models trained with recursive traces can be proven to generalize, by using the verification procedure described in section 3",0
e,0
e,0
 this is due to the properties of the training set – we trained on 2 traces synthesized from arrays of length 7 and 1 trace synthesized from an array of length 6,0
" each step during an execution of the program does one of three things: (1) another subprogram along with associated arguments is called, as in line 10, (2) the program writes to the environment if it is primitive, as in line 8, or (3) the loop is terminated if the return probability exceeds a threshold α, after which the stack frame is popped and control is returned to the caller",0
" however, our concept of recursion for neural programs is general",0
"
we propose and describe our verification procedure",1
" nevertheless, we would like the model to quickly learn to represent the right semantics of the underlying program from a small number of training examples, not an exhaustive number of them",0
"
the training set for bubble sort contains 100 traces, with maximum problem length of 2 (e",0
", qstacklo or qstackhi) or to pointer (e",0
" consequently, it is infeasible to construct a verification set that covers all cases, and the verification procedure we have described is inadequate",0
" fa8750-15-2-0104, and berkeley deep drive",0
" for each graph size, we test the learned programs on 30 randomly generated dags",1
"
it is sufficient to construct problems where every transition between two adjacent columns is covered",1
" any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of national science foundation and darpa",0
" the pointer at index i3 represents a counter internal to the environment that is incremented once after each pass of the algorithm (one cycle of bubble and reset); when incremented a number of times equal to the length of the array, the flag i3 == length becomes true and terminates the entire algorithm ",1
g,1
 almost all architectures train on program input/output pairs,0
"
bubble sort",0
" because the context of the caller is stored on a stack when it calls another program and the callee starts in a fresh context, this enables recursion simply by allowing a program to call itself",0
 this verification phase only needs to be performed once after training,0
"program descriptions calls arguments toposort perform topological
sort on graph traverse, next start, write, move
none
traverse traverse graph until stack is empty check child, explore none check child check if a white child exists; if so, set childlist[vactive] to point to it move none explore repeatedly traverse subgraphs until stack is empty stack, check child, write, move none stack interact with stack, either pushing or popping write, move push, pop next start move pstart until reaching a white vertex",1
 the environment and return probability are omitted for readability,0
 there are 2 variables (vactive and vsave); vactive holds the active vertex (as in algorithm 2) and vsave holds the value of vactive before executing line 12 of algorithm 2,0
" 3: 4: function partition(a, lo, hi) 5: pivot = lo 6: for j ∈ [lo, hi− 1] : do 7: if a[j] ≤ a[hi] then 8: swap a[pivot] with a[j] 9: pivot = pivot+ 1 0: swap a[pivot] with a[hi] 11: return pivot 12: 13: function quicksort(a, lo, hi) 14: while slo and shi are not empty: do 15: pop states off slo and shi, writing them to lo and hi",0
"
we describe the base cases, reduction rules, and the verification set for each task in appendix a",0
" this guarantees that the step input sequences considered during verification contain all step input sequences which arise during execution of an unseen problem in f , leading to generalization to any problem in f ",0
" our implementation uses two stacks qstacklo and
qstackhi, each in ru , that store the arguments to the recursive quicksort calls in algorithm 3; before each recursive call, the appropriate arguments are popped off the stack and written to plo and phi",0
" in particular, we construct new training traces which explicitly contain recursive elements and show that with this type of trace, npi easily learns recursive programs",0
"
an alternative way of representing the environment slice is to expose the values of the absolute vertices to the model; however, this makes it difficult to scale the model to larger graphs, since large vertex values are not seen during training time",0
" the second is the training procedure, which consists of the form of the training data and the optimization process",0
 ,0
" in this version, there is a dependence on length: bstep and lshift are called a number of times equivalent to one less than the length of the input array, in bubble and reset respectively",0
" m is considered to have perfect generalization if m can give the right answer for any input, such as inputs of arbitrary length",0
" note that recursion helps make this process tractable, because we only need to test a finite number of inputs to show that the model will work correctly on inputs of unbounded complexity",0
"
as in line 13 of the right-hand side of figure 1",0
" in fact, it is one of our future directions to explore new ways to train a recursive neural program without providing explicit training execution traces or with only partial or non-recursive traces",1
"
the training set for bubble sort contains 100 traces, with maximum problem length of 2 (e",0
" the partial recursion is not enough for perfect generalization, as will be presented later in section 4",1
" a topological sort is a linear ordering of vertices such that for every directed edge (u, v) from u to v, u comes before v in the ordering",0
"
we refer the reader to the appendix for the non-recursive trace generating functions",0
" finally, the boolean pstack == 1 is used to check whether the stack is empty in line 18 of algorithm 2",1
", the verification set",0
 ,0
 2: initialize an empty stack s and a directed acyclic graph dag to traverse,0
" et+1 ∼ fenv(et, pt, at)",1
 3: begin traversing from vertex 1 in the dag,0
" each time a subprogram is called, the stack depth increases",1
 recursion enables provably perfect generalization,0
 a1a0 + bnbn−1 ,1
" training on the full recursive trace leads to perfect generalization, as shown in section 4",0
", to change the value of vactive) none described below move move a pointer (e",1
g,0
 we choose to implement a topological sort task for graphs,0
" a topological sort is a linear ordering of vertices such that for every directed edge (u, v) from u to v, u comes before v in the ordering",0
" qcolor(pstart) contains the color of the current start vertex, used in the evaluation of the condition in the while loop in line 5 of algorithm 2",0
"
the npi accesses an external environment, q, which varies according to the task",1
"
the three environment observations aid with control flow in algorithm 2",1
" consequently, it is infeasible to construct a verification set that covers all cases, and the verification procedure we have described is inadequate",0
we emphasize that the notion of a neural recursive program has not been presented in the literature before: this is our main contribution,0
"
inside bubble and reset, there are two operations that can be made recursive",1
3,1
 the maximum problem length in this training set is 3 (e,0
"
is critical for implementing recursion, since it permits us to restrict our attention to the currently relevant recursive call, ignoring irrelevant details about other contexts",0
" consequently, it is infeasible to construct a verification set that covers all cases, and the verification procedure we have described is inadequate",0
we emphasize the overall goal of this work is to enable the learning of a recursive program,0
we emphasize that the notion of a neural recursive program has not been presented in the literature before: this is our main contribution,1
"we describe how models trained with recursive traces can be proven to generalize, by using the verification procedure described in section 3",0
"
move",0
" recall that this verification procedure cannot be performed for the non-recursive versions, since the propagation of the hidden state in the core lstm module makes reasoning difficult and so we would need to check an unbounded number of examples",1
"
as mentioned in section 1, all approaches to neural programming today fare poorly on this generalization issue",1
"
by nature, recursion reduces the complexity of a problem to simpler instances",0
3,0
"
as a concrete instantiation, we show in this paper that we can enable recursive neural programs in the npi model, and thus enable perfectly generalizable neural programs for tasks such as sorting where the original, non-recursive npi program fails",0
" in this section, we give a brief review of the npi architecture from reed & de freitas (2016) as background",0
" that is to say, the network does not learn the true program semantics",1
" our implementation uses two stacks qstacklo and
qstackhi, each in ru , that store the arguments to the recursive quicksort calls in algorithm 3; before each recursive call, the appropriate arguments are popped off the stack and written to plo and phi",0
"
note that the recursion for quicksort is not purely tail recursive and therefore represents a more complex kind of recursion that is harder to learn than in the previous tasks",0
 add1 is a subprogram that adds the current column (writing the appropriate digit to the output row and carrying a bit to the next column if needed),1
"
thus far, to evaluate the efficacy of neural models on programming tasks, the only metric that has been used is generalization of expected behavior to inputs of greater complexity (vinyals   (2015), kaiser & sutskever (2015), reed & de freitas (2016), graves   (2016), zaremba   (2016))",0
"
inside bubble and reset, there are two operations that can be made recursive",0
" an example of part of an addition task trace, written in shorthand, is given in figure 1",0
" numerous models have been proposed for learning programs; to name a few, this includes the differentiable neural computer (graves  , 2016), neural turing machine (graves  , 2014), neural gpu (kaiser & sutskever, 2015), neural programmer (neelakantan  , 2015), pointer network (vinyals  , 2015), hierarchical attentive memory (andrychowicz & kurach, 2016), and neural random access machine (kurach  , 2016)",0
" the core lstm module’s hidden state is preserved over all these add1 calls, and it is difficult to interpret with certainty what happens over longer timesteps without concretely evaluating the lstm with an input of that length",1
" 16: p = partition(a, lo, hi) 17: push p+ 1 and hi to slo and shi",0
" we also hope to decrease supervision, for example by training with only partial or non-recursive traces, and to develop novel neural programming architectures integrated directly with a notion of recursion",1
"
the dag is represented as an adjacency list where dag[i][j] refers to the j-th child of vertex i",0
"1; and for bubble sort, appendix a",0
"in this paper, we propose that the key abstraction of recursion is necessary for neural programs to generalize",1
 the maximum problem length in this training set is 3 (e,0
"
it is sufficient to construct problems where every transition between two adjacent columns is covered",1
" the space of possible programs is extremely large, and composing a program that performs robustly on the infinite space of possible inputs is difficult—in part because it is impractical to obtain enough training examples to easily disambiguate amongst all possible programs",1
"
thus far, to evaluate the efficacy of neural models on programming tasks, the only metric that has been used is generalization of expected behavior to inputs of greater complexity (vinyals   (2015), kaiser & sutskever (2015), reed & de freitas (2016), graves   (2016), zaremba   (2016))",1
 ,0
" however, the authors of npi did not consider explicitly the notion of recursion and as a consequence, did not learn recursive programs",0
" fa8750-15-2-0104, and berkeley deep drive",1
" while this matches the description from reed & de freitas (2016), we found that this causes an unnecessary blowup in the size of v and makes it much more difficult to construct the verification set",0
 reset represents a −∞ value,1
" it is straightforward to deal with all other operations, which do not induce partial environment states",0
"
topological sort",0
" in practice, we can additionally use tail recursion optimization to avoid problems with the call stack growing too deep",1
"
the same set of problems was used to generate the training traces for all formulations of the task, for non-recursive and recursive versions",1
" the move operation has the following arguments:
arg 1 (pointer): presult, pstack, pstart, childlist[vactive], childlist[vsave]
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 5 values",0
" formally, the npi is represented by the following set of equations:
st = fenc(et, at)
ht = flstm(st, pt, ht−1)
rt = fend(ht), pt+1 = fprog(ht), at+1 = farg(ht)
t is a subscript denoting the time-step; fenc is a domain-specific encoder (to be described later) that takes in the environment slice et and arguments at; flstm represents the core module, which takes in the state st generated by fenc, a program embedding pt ∈ rp , and hidden lstm state ht; fend decodes the return probability rt; fprog decodes a program key embedding pt+1;1 and farg decodes arguments at+1",1
" this is highly undesirable, since being able to provide the correct answer in all possible settings is one of the most important aspects of any learned neural program",1
" on the other hand, the non-recursive version has low accuracy, beginning from size 5, and fails completely for graphs of size 8 and beyond",0
" as this complexity interfered with determining the base cases and reductions, we changed the algorithm to its current form",0
" we then construct problem configurations that span all possible valid environment states (for instance, in order to force the carry bit in a column to be 1, one can add the sum “1+9” in the column to the right)",0
 the program terminates when seeing no numbers in the current column,0
" for each task, given the verification set, we check the traces and results of the learned, to-be-verified neural program (described in section 4",1
"1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 while (pstacklo 6= 1): 6 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 7 stack(stack_pop) 8 else: 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 stack(stack_pop) 12 partition() 13 stack(stack_push_call2) 14 stack(stack_push_call1) 15 } 16 17 partition() { 18 set_pivot_lo() 19 set_j_lo() 20 compswap_loop() 21 swap(ppivot, phi) 22 set_j_null() 23 } 24 25 compswap_loop() { 26 while (pj 6= phi): 27 compswap() 28 move(pj, up) 29 } 30 31 compswap() { 32 if (a[pj ] ≤ a[phi]): 33 swap(ppivot, pj) 34 move(ppivot, up) 35 } 36 37 stack(op) { 38 if (op == stack_push_call1): 39 write(env_stack_lo, plo) 40 write(env_stack_hi, ppivot − 1) 41 move(pstacklo, up) 42 move(pstackhi, up) 43 44 if (op == stack_push_call2): 45 write(env_stack_lo, ppivot + 1) 46 write(env_stack_hi, phi) 47 move(pstacklo, up) 48 move(pstackhi, up) 49 50 if (op == stack_pop): 51 write(env_stack_lo, reset) 52 write(env_stack_hi, reset) 53 move(pstacklo, down) 54 move(pstackhi, down) 55 }altered recursive functions 1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 6 partition() 7 stack(stack_push_call2) 8 stack(stack_push_call1) 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 quicksort() // recursive call 12 stack(stack_pop) 13 write(phi, env_stack_hi_peek) 14 write(plo, env_stack_lo_peek) 15 quicksort() // recursive call 16 stack(stack_pop) 17 } 18 19 compswap_loop() { 20 if (pj 6= phi): 21 compswap() 22 move(pj, up) 23 compswap_loop() // recursive call 24 }in this section, we describe the space of base cases and reduction rules that must be covered for each of the four sample tasks, in order to create the verification set",1
" in future work, we seek to enable more tasks with recursive structure",0
" the 4 environment observations aid with control flow; qstacklo(pstacklo− 1) < qstackhi(pstackhi− 1) implements the lo < hi comparison in line 5 of algorithm 3, pstacklo == 1 checks if the stacks are empty in line 18 of algorithm 4, and the other observations (all involving ppivot or pj) deal with logic in the partition function",0
" if yes, then this indicates that the learned neural program achieves provably perfect generalization",1
" we show that by modifying the training procedure, we enable the npi model to learn recursive neural programs",0
g,0
" in order to compensate for this, ppivot and pj require special functions (move pivot lo and move j lo) to properly set them to lo in lines 11 and 12 of the partition function in algorithm 3",1
"
bubble sort",0
" in future work, we seek to enable more tasks with recursive structure",1
" because the context of the caller is stored on a stack when it calls another program and the callee starts in a fresh context, this enables recursion simply by allowing a program to call itself",1
5,0
" in this version of curriculum learning, even though the inputs are gradually becoming more complex, the semantics of the program is succinct and does not change",0
 u varies with the number of vertices in the graph,0
", an lstm in npi’s case, but possibly other networks in different cases",0
" the space of possible programs is extremely large, and composing a program that performs robustly on the infinite space of possible inputs is difficult—in part because it is impractical to obtain enough training examples to easily disambiguate amongst all possible programs",1
 almost all architectures train on program input/output pairs,1
 this verification phase only needs to be performed once after training,0
"
in this paper, we propose that recursion is an important concept for neural programs as well",0
" for each task, given the verification set, we check the traces and results of the learned, to-be-verified neural program (described in section 4",1
" 2: initialize lo and hi to be 1 and n, where n is the length of a",1
" the space of possible programs is extremely large, and composing a program that performs robustly on the infinite space of possible inputs is difficult—in part because it is impractical to obtain enough training examples to easily disambiguate amongst all possible programs",1
"
we also report on generalization results for the non-recursive and recursive versions of this variant of bubble sort",1
" for bubble sort, the domain-specific encoder is
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1), q(1, i2), i3 == length, at(1), at(2), at(3)]),
where the environment q ∈ r1×n×k is a scratch-pad that contains 1 row, to represent the state of the array as sorting proceeds in-place, andn columns",0
" in particular, in the explore function, adding a tail recursive call resets and stores the hidden states associated with vertices in a stack-like fashion",1
 u varies with the number of vertices in the graph,0
" 16: p = partition(a, lo, hi) 17: push p+ 1 and hi to slo and shi",0
" note that recursion helps make this process tractable, because we only need to test a finite number of inputs to show that the model will work correctly on inputs of unbounded complexity",0
"we describe how models trained with recursive traces can be proven to generalize, by using the verification procedure described in section 3",0
 we describe the recursive re-formulation of traces for two tasks from the original npi paper—grade-school addition and bubble sort,1
" for each length, we test each program on 30 randomly generated problems",0
"
move",1
" the single-digit multiplication task in zaremba   (2016), the bubble sort task in reed & de freitas (2016), and the graph tasks in graves   (2016))",1
" for these programs, we re-use the appropriate program sets (the associated subprograms), and we refer the reader to the appendix of reed & de freitas (2016) for further details on the subprograms used in addition and bubble sort",1
 the logic for the non-recursive trace is shown in algorithm 4 in the appendix,0
 almost all architectures train on program input/output pairs,1
" for bubble sort, the domain-specific encoder is
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1), q(1, i2), i3 == length, at(1), at(2), at(3)]),
where the environment q ∈ r1×n×k is a scratch-pad that contains 1 row, to represent the state of the array as sorting proceeds in-place, andn columns",0
" also, compared to the bubble pointers in bubble sort, the pointers that perform the comparison for quicksort (the compswap function) are usually not adjacent to each other, making quicksort less local than bubble sort",0
as mentioned in section 2,1
2,0
"
algorithm 2 shows the topological sort task of interest",1
" partial recursion only adds a tail recursive call to bubblesort after bubble and reset, similar to the tail recursive call described previously for addition",0
" by iterating with this procedure, and then running p on the input observation set that we obtain for the entry point function, we can obtain v precisely",1
" when the program is called, the current context including the caller’s memory state is stored on a stack; when the program returns, the stored context is popped off the stack to resume execution in the previous caller’s context",0
" in particular, this condition is satisfied by npi, and thus the npi model naturally supports recursion (even though the authors of npi did not consider this aspect explicitly)",0
" any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of national science foundation and darpa",1
 ,1
" formally, the npi is represented by the following set of equations:
st = fenc(et, at)
ht = flstm(st, pt, ht−1)
rt = fend(ht), pt+1 = fprog(ht), at+1 = farg(ht)
t is a subscript denoting the time-step; fenc is a domain-specific encoder (to be described later) that takes in the environment slice et and arguments at; flstm represents the core module, which takes in the state st generated by fenc, a program embedding pt ∈ rp , and hidden lstm state ht; fend decodes the return probability rt; fprog decodes a program key embedding pt+1;1 and farg decodes arguments at+1",1
" in all experiments, α is set to 0",1
"
for addition, we analytically determine the verification set",0
 the model described in the training setup of section 4 (trained on 6 traces) was verified to be correct via the matching procedure described in section 4,0
" furthermore, given a verification input set that covers all base cases and reduction rules, we can formally prove that the learned neural programs achieve perfect generalization after verifying its behavior on the verification input set",1
 the sequence of primitive operations (move and write operations) for the non-recursive and recursive versions are exactly the same,1
"
for topological sort, the domain-specific encoder is
fenc(dag,qcolor, pstack, pstart, vactive, childlist, at)
=mlp ([qcolor(pstart), qcolor(dag[vactive][childlist[vactive]]), pstack == 1, at(1), at(2), at(3)]),
where qcolor ∈ ru×4 is a scratch-pad that contains u rows, each containing one of four colors (white, gray, black, invalid) with one-hot encoding",1
" this agrees with the generalization of non-recursive addition in reed & de freitas (2016), where they reported generalization up to 3000 digits",1
" the architecture usually possesses some form of memory, which could be internal (such as the hidden state of a recurrent neural network) or external (such as a discrete “scratch pad” or a memory block with differentiable access)",0
" qcolor(dag[vactive][childlist[vactive]]) refers to the color of the next child of vactive, used in the evaluation of the condition in the if branch in line 9 of algorithm 2",0
3,0
"
to perform the verification as described here, it is critical to construct v correctly",1
" if no white vertex is found, the entire execution is terminated move none write write a value either to environment (e",1
"
given a verification set, we can then run the model on the verification set to check if the produced traces and results are correct",0
" the core lstm module’s hidden state is preserved over all these add1 calls, and it is difficult to interpret with certainty what happens over longer timesteps without concretely evaluating the lstm with an input of that length",0
" finally, we implement recursive traces for our own topological sort and quicksort tasks",0
" empirically, existing models suffer from a common limitation—generalization becomes poor beyond a threshold level of complexity",1
" a single element of an execution trace consists of a step input-step output pair, which can be synthesized from algorithm 1: this corresponds to, for a given time-step, the step input tuple (e, p, a) and step output tuple (r, p2, a2)",0
 we outline how to construct this set,0
" the core lstm module’s hidden state is preserved over all these add1 calls, and it is difficult to interpret with certainty what happens over longer timesteps without concretely evaluating the lstm with an input of that length",0
" the pointer at index i3 represents a counter internal to the environment that is incremented once after each pass of the algorithm (one cycle of bubble and reset); when incremented a number of times equal to the length of the array, the flag i3 == length becomes true and terminates the entire algorithm ",0
"
an alternative method is to run p on many different program inputs and then observe step input sequences which occur, to create v ",0
"
bubble sort",0
"
topological sort",1
"
in this paper, we propose to resolve these issues by explicitly incorporating recursion into neural architectures",0
" naı̈vely, we might expect to synthesize and test an input for any sequence created by combining the four possible colors in two variables and another boolean variable for whether the stack is empty (so 32 possible observations at any point), but for various reasons, most of these combinations are impossible to occur at any given point in the execution trace",1
 the core controller acts as a dispatcher for the programs,0
" the partial recursion is not enough for perfect generalization, as will be presented later in section 4",1
" lshift moves the four pointers to the left, to move to the next column",0
" observe that the non-recursive program’s correctness degrades for length 11 and beyond, while the recursive program can sort any given array",1
" the 4 environment observations aid with control flow; qstacklo(pstacklo− 1) < qstackhi(pstackhi− 1) implements the lo < hi comparison in line 5 of algorithm 3, pstacklo == 1 checks if the stacks are empty in line 18 of algorithm 4, and the other observations (all involving ppivot or pj) deal with logic in the partition function",0
 add1 is a subprogram that adds the current column (writing the appropriate digit to the output row and carrying a bit to the next column if needed),1
" the partial recursion is not enough for perfect generalization, as will be presented later in section 4",1
 reset represents a −∞ value,0
" naı̈vely, we might expect to synthesize and test an input for any sequence created by combining the four possible colors in two variables and another boolean variable for whether the stack is empty (so 32 possible observations at any point), but for various reasons, most of these combinations are impossible to occur at any given point in the execution trace",1
" both the non-recursive and recursive learned programs generalize on all input lengths we tried, up to 5000 digits",0
" for the recursive formulation of addition, we analytically construct the set of input problems that cover all base cases and reduction rules",0
 indentation indicates the stack is one level deeper than before,0
" in order to claim a proof, we must verify the model produces correct behavior over all base cases and reductions, as described in section 2",1
" observe that partially recursive does slightly better than non-recursive for the setting in which the length of the array is 3, and that the fully recursive version is able to sort every array given to it",1
" for example, a step input-step output pair in lines 2 and 3 of the left-hand side of figure 1 is (add1, write out 1)",0
" 4: function toposort(dag) 5: while there is still a white vertex u: do 6: color[u] = grey 7: vactive = u 8: do 9: if vactive has a white child v then 10: color[v] = grey 11: push vactive onto s 12: vactive = v 13: else 14: color[vactive] = black 15: write vactive to result 16: if s is empty then pass 17: else pop the top vertex off s and set it to vactive 18: while s is not empty
topological sort",1
" we also hope to decrease supervision, for example by training with only partial or non-recursive traces, and to develop novel neural programming architectures integrated directly with a notion of recursion",0
" in order to compensate for this, ppivot and pj require special functions (move pivot lo and move j lo) to properly set them to lo in lines 11 and 12 of the partition function in algorithm 3",0
g,0
 stack push call2 pushes pivot + 1 and hi to qstacklo and qstackhi,1
" it is straightforward to deal with all other operations, which do not induce partial environment states",1
"
in this paper, we propose to resolve these issues by explicitly incorporating recursion into neural architectures",1
" however, our concept of recursion for neural programs is general",0
" as described in the verification procedure, it is possible to prove our learned recursive program generalizes perfectly by testing on an appropriate set of problem inputs, i",0
"we describe how models trained with recursive traces can be proven to generalize, by using the verification procedure described in section 3",0
" in particular, in the explore function, adding a tail recursive call resets and stores the hidden states associated with vertices in a stack-like fashion",0
"
directed acyclic graphs are structurally more diverse than inputs in the two tasks of grade-school addition and bubble sort",0
 our result shows that the training procedure and the npi architecture is capable of generalizing from the step input-output pairs seen in the training data to the unseen ones present in the verification set,0
" the add reduction rule ensures that each call to add only covers two adjacent columns, and so the lstm only ever runs for a fixed number of steps necessary to process these two columns",1
" because the context of the caller is stored on a stack when it calls another program and the callee starts in a fresh context, this enables recursion simply by allowing a program to call itself",0
 we then construct a verification set of size 73 by ensuring that randomly generated graphs cover the analytically derived v ,0
3,0
"
given a verification set, we can then run the model on the verification set to check if the produced traces and results are correct",0
" because the context of the caller is stored on a stack when it calls another program and the callee starts in a fresh context, this enables recursion simply by allowing a program to call itself",0
 npi then outputs the return probability and next program and arguments to execute,1
" by iterating with this procedure, and then running p on the input observation set that we obtain for the entry point function, we can obtain v precisely",0
"
algorithm 2 shows the topological sort task of interest",1
 3: begin traversing from vertex 1 in the dag,1
"
we describe the base cases, reduction rules, and the verification set for each task in appendix a",0
" while this matches the description from reed & de freitas (2016), we found that this causes an unnecessary blowup in the size of v and makes it much more difficult to construct the verification set",0
" we can obtain this mapping by first considering the possible observations that can arise at the beginning of the entry function (add, bubblesort, toposort, and quicksort) for some valid program input, and iteratively applying the observation-to-observation mapping implied by the reference implementation’s step output at that point in the execution",0
"
as in line 13 of the right-hand side of figure 1",1
" the swap operation has the following arguments:
arg 1 (swap object 1): ppivot
arg 2 (swap object 2): phi, pj
write",0
" that is to say, the network does not learn the true program semantics",1
"
algorithm 3 recursive quicksort 1: initialize an array a to sort",1
"when constructing a neural network for the purpose of learning a program, there are two orthogonal aspects to consider",1
"
is critical for implementing recursion, since it permits us to restrict our attention to the currently relevant recursive call, ignoring irrelevant details about other contexts",1
" for problems where the base cases may be extremely large or infinite, such as certain forms of motor control, recursion can still help reduce the problem of generalization to these two aspects and make the generalization problem significantly simpler to handle and reason about",1
"
we note that for tasks with very large input domains, such as ones involving mnist digits or speech samples, the state space of base cases and reduction rules could be prohibitively large, possibly infinite",1
" naı̈vely, we might expect to synthesize and test an input for any sequence created by combining the four possible colors in two variables and another boolean variable for whether the stack is empty (so 32 possible observations at any point), but for various reasons, most of these combinations are impossible to occur at any given point in the execution trace",0
" hence, if all subproblems in s are added correctly, we have proven that any member of f will be added correctly, thus eliminating an infinite family of inputs that need to be tested",1
" we also hope to decrease supervision, for example by training with only partial or non-recursive traces, and to develop novel neural programming architectures integrated directly with a notion of recursion",1
" for purposes of verification, we replace the domain-specific encoder with the following:
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1) ≤ q(1, i2), 1 ≤ i1 ≤ length, 1 ≤ i2 ≤ length, i3 == length, at(1), at(2), at(3)]),which directly exposes which of the two values pointed to is larger",1
g,0
" et+1 ∼ fenv(et, pt, at)",0
" a topological sort is a linear ordering of vertices such that for every directed edge (u, v) from u to v, u comes before v in the ordering",1
 stack push call2 pushes pivot + 1 and hi to qstacklo and qstackhi,1
"
one common strategy to improve generalization is to use curriculum learning, where the model is trained on inputs of gradually increasing complexity",0
" we also hope to decrease supervision, for example by training with only partial or non-recursive traces, and to develop novel neural programming architectures integrated directly with a notion of recursion",0
"
algorithm 2 shows the topological sort task of interest",0
" using this modification, we constructed a verification set consisting of one array of size 10",1
"
we also report on generalization results for the non-recursive and recursive versions of this variant of bubble sort",0
" hence, if all subproblems in s are added correctly, we have proven that any member of f will be added correctly, thus eliminating an infinite family of inputs that need to be tested",0
" because the context of the caller is stored on a stack when it calls another program and the callee starts in a fresh context, this enables recursion simply by allowing a program to call itself",1
" in contrast, each call to the recursive add always runs for a fixed number of steps, even on arbitrarily long problems in f , so we can test that it performs correctly on a small, fixed number of step input sequences",0
"6) on the verification set, and ensure they match the traces produced by the true program p ",1
"
there are 6 pointers (plo, phi, pstacklo, pstackhi, ppivot, pj)",0
"
we construct input problems by splitting into two cases: one case in which the left column contains a null value and another in which the left column does not contain any null values",1
", pstart or childlist[vactive]) up or down none described belowwrite",1
 stack pop pushes −∞ values to qstacklo and qstackhi,0
" k is set to 11, to denote the range of possible numbers (0 through 9), along with the start/end token (represented with the same encoding) which is observed when a pointer reaches beyond the bounds of the input",1
"
in this general neural programming architecture, we show it is easy to support recursion",0
" as aforementioned, the npi model naturally supports recursion",0
" if the step output specifies a primitive function call, we need to reason about how it can affect the environment so as to change the observation in the next step input",1
" we further have qresult ∈ nu , a scratch-pad which contains the sorted list of vertices at the end of execution, and qstack ∈ nu , which serves the role of the stack s in algorithm 2",0
" fa8750-15-2-0104, and berkeley deep drive",1
", an array for quicksort or a dag for topological sort)",1
3 to construct v and then empirically create a verification set which covers v ,0
g,1
"program descriptions calls arguments toposort perform topological
sort on graph traverse, next start, write, move
none
traverse traverse graph until stack is empty check child, explore none check child check if a white child exists; if so, set childlist[vactive] to point to it move none explore repeatedly traverse subgraphs until stack is empty stack, check child, write, move none stack interact with stack, either pushing or popping write, move push, pop next start move pstart until reaching a white vertex",0
" k is set to 11, to denote the range of possible numbers (0 through 9), along with the start/end token (represented with the same encoding) which is observed when a pointer reaches beyond the bounds of the input",0
"
formally, verification consists of proving the following theorem:
∀i ∈ v,m(i) ⇓ p (i) where i denotes a sequence of step inputs (within one function call), v denotes the set of valid sequences of step inputs, m denotes the neural network model, p denotes the correct program, and p (i) denotes the next step output from the correct program",0
 these pointers are referred to as bubble pointers,1
"
the dag is represented as an adjacency list where dag[i][j] refers to the j-th child of vertex i",0
 3: begin traversing from vertex 1 in the dag,0
" at end, pointer ppivot is moved to the pivot compswap loop, move pivot lo, move j lo, swap
none
compswap loop runs the for loop inside the partition function compswap, move none compswap compares a[pivot] ≤ a[j]; if so, perform a swap and increment ppivot swap, move none set pivot lo sets ppivot to lo index none none set j lo sets pj to lo index none none set j null sets pj to −∞ none none stack pushes lo/hi states
onto stacks slo and shi according to argument (described below)
write, move described below
move moves pointer one unit up or down none described below swap swaps elements at given array indices none described below write write a value either to stack (e",0
"
algorithm 2 shows the topological sort task of interest",0
" for the recursive formulation of addition, we analytically construct the set of input problems that cover all base cases and reduction rules",0
"
thus far, to evaluate the efficacy of neural models on programming tasks, the only metric that has been used is generalization of expected behavior to inputs of greater complexity (vinyals   (2015), kaiser & sutskever (2015), reed & de freitas (2016), graves   (2016), zaremba   (2016))",1
" we also hope to decrease supervision, for example by training with only partial or non-recursive traces, and to develop novel neural programming architectures integrated directly with a notion of recursion",1
as mentioned in section 2,0
", the verification set",0
" using this modification, we constructed a verification set consisting of one array of size 10",0
" thus, we demonstrate that recursion enables provably perfect generalization for different tasks, including addition, topological sort, quicksort, and a variant of bubble sort",0
"
in this paper, we propose that recursion is an important concept for neural programs as well",1
" 3: 4: function quicksort(a, lo, hi) 5: if lo < hi: then 6: p = partition(a, lo, hi) 7: quicksort(a, lo, p− 1) 8: quicksort(a, p+ 1, hi) 9: 10: function partition(a, lo, hi) 11: pivot = lo 12: for j ∈ [lo, hi− 1] : do 13: if a[j] ≤ a[hi] then 14: swap a[pivot] with a[j] 15: pivot = pivot+ 1 16: swap a[pivot] with a[hi] 17: return pivot
for quicksort, the domain-specific encoder is
fenc(qarray, qstacklo, qstackhi, plo, phi, pstacklo, pstackhi, ppivot, pj , at) =
mlp ([qarray(pj) ≤ qarray(phi), pj == phi, qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1), pstacklo == 1, at(1), at(2), at(3)]),
whereqarray ∈ ru×11 is a scratch-pad that contains u rows, each containing one of 11 values (one of the numbers 0 through 9 or an invalid state)",0
 stack push call2 pushes pivot + 1 and hi to qstacklo and qstackhi,0
 we created a program set that reflects the semantics of algorithm 2,0
" this guarantees that the step input sequences considered during verification contain all step input sequences which arise during execution of an unseen problem in f , leading to generalization to any problem in f ",1
" lshift moves the four pointers to the left, to move to the next column",1
" in this section, we give a brief review of the npi architecture from reed & de freitas (2016) as background",1
" here as a concrete and general example, we consider a general neural programming architecture (npa), similar to neural programmer-interpreter (npi) in reed & de freitas (2016)",1
" this is highly undesirable, since being able to provide the correct answer in all possible settings is one of the most important aspects of any learned neural program",0
 we found that changing the npi training traces is a simple way to enable this,1
5,1
training neural networks to synthesize robust programs from a small number of examples is a challenging task,1
"
move",1
" we performed experiments on the partially recursive version in order to examine what happens when only one recursive call is implemented, when in reality three are required for perfect generalization",1
" both the non-recursive and recursive learned programs generalize on all graphs we tried, up to 120 vertices",0
" 2: initialize lo and hi to be 1 and n, where n is the length of a",0
" observe that partially recursive does slightly better than non-recursive for the setting in which the length of the array is 3, and that the fully recursive version is able to sort every array given to it",0
" in fact, it is one of our future directions to explore new ways to train a recursive neural program without providing explicit training execution traces or with only partial or non-recursive traces",1
"
quicksort",0
" qcolor(pstart) contains the color of the current start vertex, used in the evaluation of the condition in the while loop in line 5 of algorithm 2",0
" the pointers at index i1 and i2 are used to compare the pair of numbers considered during the bubble sweep, swapping them if the number at i1 is greater than that in i2",0
"
directed acyclic graphs are structurally more diverse than inputs in the two tasks of grade-school addition and bubble sort",1
" for grade-school addition, the domain-specific encoder is
fenc(q, i1, i2, i3, i4, at) =mlp ([q(1, i1), q(2, i2), q(3, i3), q(4, i4), at(1), at(2), at(3)]),
where the environment q ∈ r4×n×k is a scratch-pad that contains four rows (the first input number, the second input number, the carry bits, and the output) and n columns",0
 table 4 also demonstrates that the (verified) recursive program generalizes perfectly,1
" the recursive version can generalize perfectly to long problems constructed from these components (such as the sum “822+233”, where “8+2” and “2+3” are in the training set), but the non-recursive version fails to sum these long problems properly",1
" for many problems, the base cases and reduction rules usually consist of a finite (often small) number of cases",0
"6) on the verification set, and ensure they match the traces produced by the true program p ",1
" the core lstm module’s hidden state is preserved over all these add1 calls, and it is difficult to interpret with certainty what happens over longer timesteps without concretely evaluating the lstm with an input of that length",1
 this paper is the first (to our knowledge) to investigate the important problem of provable generalization properties of neural programs,0
" the second is the training procedure, which consists of the form of the training data and the optimization process",0
" in practice, we can additionally use tail recursion optimization to avoid problems with the call stack growing too deep",0
 the model described in the training setup of section 4 (trained on 6 traces) was verified to be correct via the matching procedure described in section 4,1
"
as in line 13 of the right-hand side of figure 1",1
 ,0
"
in addition, none of the current approaches to neural programming provide a method or even aim to enable provable guarantees about generalization",0
" ppivot and pj point to the pivot and j indices of the array, used in the partition function in algorithm 3",0
 we created a program set that reflects the semantics of algorithm 2,0
" at each time step, the core controller can decide to select one of the programs to call with certain arguments",0
 our result shows that the training procedure and the npi architecture is capable of generalizing from the step input-output pairs seen in the training data to the unseen ones present in the verification set,1
 the logic for the non-recursive trace is shown in algorithm 4 in the appendix,1
"program descriptions calls arguments toposort perform topological
sort on graph traverse, next start, write, move
none
traverse traverse graph until stack is empty check child, explore none check child check if a white child exists; if so, set childlist[vactive] to point to it move none explore repeatedly traverse subgraphs until stack is empty stack, check child, write, move none stack interact with stack, either pushing or popping write, move push, pop next start move pstart until reaching a white vertex",1
this material is in part based upon work supported by the national science foundation under grant no,0
"
in this general neural programming architecture, we show it is easy to support recursion",0
" the swap operation has the following arguments:
arg 1 (swap object 1): ppivot
arg 2 (swap object 2): phi, pj
write",1
 ,1
" formally, the npi is represented by the following set of equations:
st = fenc(et, at)
ht = flstm(st, pt, ht−1)
rt = fend(ht), pt+1 = fprog(ht), at+1 = farg(ht)
t is a subscript denoting the time-step; fenc is a domain-specific encoder (to be described later) that takes in the environment slice et and arguments at; flstm represents the core module, which takes in the state st generated by fenc, a program embedding pt ∈ rp , and hidden lstm state ht; fend decodes the return probability rt; fprog decodes a program key embedding pt+1;1 and farg decodes arguments at+1",1
" childlist ∈ nu is a vector of pointers, where childlist[i] points to the next child under consideration for vertex i",1
 this verification phase only needs to be performed once after training,0
" for many problems, the base cases and reduction rules usually consist of a finite (often small) number of cases",1
"
we refer the reader to the appendix for the non-recursive trace generating functions",0
" for example, a step input-step output pair in lines 2 and 3 of the left-hand side of figure 1 is (add1, write out 1)",0
"
recursion drastically reduces the number of configurations we need to consider during the verification phase and makes the proof tractable, because it introduces structure that eliminates infinitely long sequences of step inputs that would otherwise need to be considered",0
g,0
 stack push call2 pushes pivot + 1 and hi to qstacklo and qstackhi,0
" both the non-recursive and recursive learned programs generalize on all graphs we tried, up to 120 vertices",0
"
it is sufficient to construct problems where every transition between two adjacent columns is covered",0
" in the non-recursive trace, there are four functions that can be made recursive—toposort, check child, explore, and next start, and we add a tail recursive call to each of these functions in order to make the recursive trace",0
" bstep, used in bubble, compares pairs of numbers, continuously moving the bubble pointers once to the right each time until reaching the end of the array",0
e,0
e,0
" thus, any general neural programming architecture supporting such a call structure can be made to support recursion",1
" empirically, existing models suffer from a common limitation—generalization becomes poor beyond a threshold level of complexity",0
"
we describe the base cases, reduction rules, and the verification set for each task in appendix a",1
" the general notion of recursion has been an important concept in many domains, including mathematics and computer science",0
" the contents ofqresult andqstack are not exposed directly through the domain-specific encoder; rather, we define primitive functions which manipulate these scratch-pads",0
" as with the others, we apply the procedure described in section 3",0
 a1a0 + bnbn−1 ,1
5,0
 this modification also enables us to sort arrays containing arbitrary comparable elements,0
 the maximum problem length in this training set is 3 (e,0
"
for topological sort, the domain-specific encoder is
fenc(dag,qcolor, pstack, pstart, vactive, childlist, at)
=mlp ([qcolor(pstart), qcolor(dag[vactive][childlist[vactive]]), pstack == 1, at(1), at(2), at(3)]),
where qcolor ∈ ru×4 is a scratch-pad that contains u rows, each containing one of four colors (white, gray, black, invalid) with one-hot encoding",0
 a1a0 + bnbn−1 ,0
"
note that the recursion for quicksort is not purely tail recursive and therefore represents a more complex kind of recursion that is harder to learn than in the previous tasks",1
"
move",0
" ppivot and pj point to the pivot and j indices of the array, used in the partition function in algorithm 3",1
"6) on the verification set, and ensure they match the traces produced by the true program p ",0
" in computer science, recursion (as opposed to iteration) involves solving a larger problem by combining solutions to smaller instances of the same problem",1
" we implement a quicksort task, in order to demonstrate that recursion helps with learning divide-and-conquer algorithms",1
" in particular, in the explore function, adding a tail recursive call resets and stores the hidden states associated with vertices in a stack-like fashion",0
" we hypothesize that the reason for this is that the neural network learns to spuriously depend on specific characteristics of the training examples that are irrelevant to the true program semantics, such as length of the training inputs, and thus fails to generalize to more complex inputs",1
" there are 3 pointers (presult, pstack, pstart), presult points to the next empty location in qresult,
pstack points to the top of the stack in qstack, and pstart points to the candidate starting node for a connected component",0
"
since npi naturally supports the notion of recursion, a key question is how to enable npi to learn recursive programs",0
"
algorithm 2 depth first search topological sort 1: color all vertices white",0
" m , as described in algorithm 1, processes the sequence of step inputs by using an lstm",1
" it is straightforward to deal with all other operations, which do not induce partial environment states",1
"
grade-school addition",1
we now report on generalization for the varying tasks,0
"
inside bubble and reset, there are two operations that can be made recursive",0
" fa8750-15-2-0104, and berkeley deep drive",0
 there are 2 variables (vactive and vsave); vactive holds the active vertex (as in algorithm 2) and vsave holds the value of vactive before executing line 12 of algorithm 2,1
 a1a0 + bnbn−1 ,0
" bstep, used in bubble, compares pairs of numbers, continuously moving the bubble pointers once to the right each time until reaching the end of the array",0
"program descriptions calls arguments quicksort run the quicksort
routine in place for the array a, for indices from lo to hi non-recursive: partition, stack, write recursive: same as non-recursive version, along with quicksort implicitly: array a to sort, lo, hi
partition runs the partition function",0
" the core lstm module’s hidden state is preserved over all these add1 calls, and it is difficult to interpret with certainty what happens over longer timesteps without concretely evaluating the lstm with an input of that length",0
" each step during an execution of the program does one of three things: (1) another subprogram along with associated arguments is called, as in line 10, (2) the program writes to the environment if it is primitive, as in line 8, or (3) the loop is terminated if the return probability exceeds a threshold α, after which the stack frame is popped and control is returned to the caller",0
" we then construct problem configurations that span all possible valid environment states (for instance, in order to force the carry bit in a column to be 1, one can add the sum “1+9” in the column to the right)",0
g,1
" we make the trace recursive by adding a tail recursive call into the trace for the add program after calling add1 and lshift, the original paper uses k = 10, but we found it necessary to augment the range with an end token, in order to terminate properly",0
 ,1
"1, we hypothesize the non-recursive programs do not generalize well because they have learned spurious dependencies specific to the training set, such as length of the input problems",0
" the move operation has the following arguments:
arg 1 (pointer): pstacklo, pstackhi, pj , ppivot
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values",0
"
base cases and reduction rules for topological sort",1
"
quicksort",0
 3: begin traversing from vertex 1 in the dag,0
"
directed acyclic graphs are structurally more diverse than inputs in the two tasks of grade-school addition and bubble sort",0
" the space of possible programs is extremely large, and composing a program that performs robustly on the infinite space of possible inputs is difficult—in part because it is impractical to obtain enough training examples to easily disambiguate amongst all possible programs",0
" for these programs, we re-use the appropriate program sets (the associated subprograms), and we refer the reader to the appendix of reed & de freitas (2016) for further details on the subprograms used in addition and bubble sort",0
" for bubble sort, the domain-specific encoder is
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1), q(1, i2), i3 == length, at(1), at(2), at(3)]),
where the environment q ∈ r1×n×k is a scratch-pad that contains 1 row, to represent the state of the array as sorting proceeds in-place, andn columns",1
 table 4 demonstrates that the accuracy of the non-recursive program degrades sharply when moving from arrays of length 7 to arrays of length 8,1
" in fact, it is one of our future directions to explore new ways to train a recursive neural program without providing explicit training execution traces or with only partial or non-recursive traces",1
"
note that the training set can often be considerably smaller than the verification set, and despite this, the learned model can still pass the entire verification set",0
" , n , where the dag contains n vertices",0
" at any given time, the npi has access to the values referred to by two pointers, represented by q(1, i1) and q(1, i2),",0
"
in what follows, we describe the way in which we constructed npi training traces so as to make them contain recursive elements and thus enable npi to learn recursive programs",0
 our experiments use a small number of training examples,0
" by iterating with this procedure, and then running p on the input observation set that we obtain for the entry point function, we can obtain v precisely",1
"
in this paper, we propose that recursion is an important concept for neural programs as well",1
"
we construct input problems by splitting into two cases: one case in which the left column contains a null value and another in which the left column does not contain any null values",0
"
arg 2 (increment or decrement): up, down1 // top level topological sort call 2 toposort() { 3 while (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 } 0 11 traverse() { 12 check_child() 13 explore() 14 } 15 16 check_child() { 17 while (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 18 move(childlist[vactive], up) 19 } 20 21 explore() { 22 do 23 if (qcolor(dag[vactive][childlist[vactive]]) is white): 24 write(color_next, color_grey) 25 stack(push) 26 write(save) 27 write(active_neighb) 28 move(childlist[vsave], up) 29 else: 30 write(color_curr, color_black) 31 write(result) 32 move(presult, up) 33 if(pstack == 1): 34 break 35 else: 36 stack(pop) 37 check_child() 38 while (true) 39 } 40 41 stack(op) { 42 if (op == push): 43 write(stack_push) 44 move(pstack, up) 45 46 if (op == pop): 47 write(active_stack) 48 write(stack_pop) 49 move(pstack, down) 50 } 51 52 next_start() { 53 while(qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 54 move(pstart, up) 55 }altered recursive functions  // top level topological sort call 2 toposort() { 3 if (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 toposort() // recursive call 0 } 11 12 check_child() { 13 if (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 14 move(childlist[vactive], up) 15 check_child() // recursive call 16 } 17 18 explore() { 19 if (qcolor(dag[vactive][childlist[vactive]]) is white): 20 write(color_next, color_grey) 21 stack(push) 22 write(save) 23 write(active_neighb) 24 move(childlist[vsave], up) 25 else: 26 write(color_curr, color_black) 27 write(result) 28 move(presult, up) 29 if(pstack == 1): 30 return 31 else: 32 stack(pop) 33 check_child() 34 explore() // recursive call 35 } 36 37 next_start() { 38 if (qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 39 move(pstart, up) 40 next_start() // recursive call 41 }algorithm 4 iterative quicksort 1: initialize an array a to sort and two empty stacks slo and shi",1
" the 4 environment observations aid with control flow; qstacklo(pstacklo− 1) < qstackhi(pstackhi− 1) implements the lo < hi comparison in line 5 of algorithm 3, pstacklo == 1 checks if the stacks are empty in line 18 of algorithm 4, and the other observations (all involving ppivot or pj) deal with logic in the partition function",0
 the logic for the non-recursive trace is shown in algorithm 4 in the appendix,1
" for each graph size, we test the learned programs on 30 randomly generated dags",1
"
we propose and describe our verification procedure",0
" the partial recursion is not enough for perfect generalization, as will be presented later in section 4",1
" however, the authors of npi did not consider explicitly the notion of recursion and as a consequence, did not learn recursive programs",0
"
under the assumption that there are no leading 0’s (except in the case of single digits) and the two numbers to be added have the same number of digits, the verification set for addition contains 20,181 input problems",0
" at end, pointer ppivot is moved to the pivot compswap loop, move pivot lo, move j lo, swap
none
compswap loop runs the for loop inside the partition function compswap, move none compswap compares a[pivot] ≤ a[j]; if so, perform a swap and increment ppivot swap, move none set pivot lo sets ppivot to lo index none none set j lo sets pj to lo index none none set j null sets pj to −∞ none none stack pushes lo/hi states
onto stacks slo and shi according to argument (described below)
write, move described below
move moves pointer one unit up or down none described below swap swaps elements at given array indices none described below write write a value either to stack (e",0
" k is set to 11, to represent the range of 10 possible digits, along with a token representing the end of input",1
"
note that the training set can often be considerably smaller than the verification set, and despite this, the learned model can still pass the entire verification set",0
" in particular, we construct new training traces which explicitly contain recursive elements and show that with this type of trace, npi easily learns recursive programs",0
training neural networks to synthesize robust programs from a small number of examples is a challenging task,1
" our results show that for all learned, to-be-verified neural programs, they all produced the same traces as those produced by p on the verification set",0
 this paper is the first (to our knowledge) to investigate the important problem of provable generalization properties of neural programs,0
 our experiments use a small number of training examples,0
" the second is the training procedure, which consists of the form of the training data and the optimization process",0
"
we refer the reader to the appendix for the non-recursive trace generating functions",0
" as described in the verification procedure, it is possible to prove our learned recursive program generalizes perfectly by testing on an appropriate set of problem inputs, i",0
 ,0
"
in what follows, we describe the way in which we constructed npi training traces so as to make them contain recursive elements and thus enable npi to learn recursive programs",0
" to the best of our knowledge, this is the first time verification has been applied to a neural program, providing provable guarantees about its behavior",0
"
base cases and reduction rules for addition",0
" the pointers at index i1 and i2 are used to compare the pair of numbers considered during the bubble sweep, swapping them if the number at i1 is greater than that in i2",0
 this verification phase only needs to be performed once after training,0
" given a precise definition of p , it may be possible to automate the generation of v from p in future work",1
we emphasize that the notion of a neural recursive program has not been presented in the literature before: this is our main contribution,0
 npi then outputs the return probability and next program and arguments to execute,0
"program descriptions calls arguments quicksort run the quicksort
routine in place for the array a, for indices from lo to hi non-recursive: partition, stack, write recursive: same as non-recursive version, along with quicksort implicitly: array a to sort, lo, hi
partition runs the partition function",1
 errors arise due to undesirable and uninterpretable dependencies and associations the architecture learns to store in some high-dimensional hidden state,1
"
grade school addition",0
" the architecture usually possesses some form of memory, which could be internal (such as the hidden state of a recurrent neural network) or external (such as a discrete “scratch pad” or a memory block with differentiable access)",0
" the corresponding edge list is [(1, 2), (1, 5), (2, 4), (2, 5), (3, 5)]",1
" we note that an earlier version of quicksort we tried lacked primitive operations to directly move a pointer to another, and therefore needed more functions and observations",0
"
the non-recursive trace loops on cycles of bubble and reset, which logically represents one bubble sweep through the array and reset of the two bubble pointers to the very beginning of the array, respectively",1
", to change the value of phi) none described belowstack",1
"
one common strategy to improve generalization is to use curriculum learning, where the model is trained on inputs of gradually increasing complexity",1
" in future work, we seek to enable more tasks with recursive structure",1
" for instance, for recursive addition, consider the family f of addition problems anan−1 ",1
 this is due to the properties of the training set – we trained on 2 traces synthesized from arrays of length 7 and 1 trace synthesized from an array of length 6,1
" however, models that make use of this strategy eventually fail after a certain level of complexity (e",0
" at each time step, the core controller can decide to select one of the programs to call with certain arguments",1
"
the non-recursive trace loops on cycles of add1 and lshift",1
" the 4 environment observations aid with control flow; qstacklo(pstacklo− 1) < qstackhi(pstackhi− 1) implements the lo < hi comparison in line 5 of algorithm 3, pstacklo == 1 checks if the stacks are empty in line 18 of algorithm 4, and the other observations (all involving ppivot or pj) deal with logic in the partition function",1
" without using a recursive program, such a proof is not possible, because the non-recursive program runs on an arbitrarily long addition problem that creates correspondingly long sequences of step inputs; in the non-recursive formulation of addition, add calls add1 a number of times that is dependent on the length of the input",1
"
topological sort",0
"
the training data for the neural programmer-interpreter consists of full execution traces for the program of interest",1
" in future work, we would like to decrease supervision and construct models that are capable of coming up with recursive abstractions themselves",0
" the add reduction rule ensures that each call to add only covers two adjacent columns, and so the lstm only ever runs for a fixed number of steps necessary to process these two columns",1
" for each task, given the verification set, we check the traces and results of the learned, to-be-verified neural program (described in section 4",0
" furthermore, given a verification input set that covers all base cases and reduction rules, we can formally prove that the learned neural programs achieve perfect generalization after verifying its behavior on the verification input set",0
" this agrees with the generalization of non-recursive addition in reed & de freitas (2016), where they reported generalization up to 3000 digits",1
this material is in part based upon work supported by the national science foundation under grant no,0
 the environment and return probability are omitted for readability,1
"1; and for bubble sort, appendix a",0
" our implementation uses two stacks qstacklo and
qstackhi, each in ru , that store the arguments to the recursive quicksort calls in algorithm 3; before each recursive call, the appropriate arguments are popped off the stack and written to plo and phi",1
" we performed experiments on the partially recursive version in order to examine what happens when only one recursive call is implemented, when in reality three are required for perfect generalization",1
" an example of part of an addition task trace, written in shorthand, is given in figure 1",0
" plo and phi point to the lo and hi indices of the array, as in algorithm 3",1
"
quicksort",0
" lshift moves the four pointers to the left, to move to the next column",0
g,1
6,0
"
the non-recursive trace loops on cycles of bubble and reset, which logically represents one bubble sweep through the array and reset of the two bubble pointers to the very beginning of the array, respectively",0
" given a precise definition of p , it may be possible to automate the generation of v from p in future work",1
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,0
", an lstm in npi’s case, but possibly other networks in different cases",0
 u varies with the number of vertices in the graph,0
" for problems where the base cases may be extremely large or infinite, such as certain forms of motor control, recursion can still help reduce the problem of generalization to these two aspects and make the generalization problem significantly simpler to handle and reason about",0
 pstacklo and pstackhi point to the top (empty) positions in qstacklo and qstackhi,0
" our results show that for all learned, to-be-verified neural programs, they all produced the same traces as those produced by p on the verification set",0
3 to construct v and then empirically create a verification set which covers v ,1
6,1
"1, we hypothesize the non-recursive programs do not generalize well because they have learned spurious dependencies specific to the training set, such as length of the input problems",1
 recursion can be implemented differently for different neural programming models,1
" nevertheless, we would like the model to quickly learn to represent the right semantics of the underlying program from a small number of training examples, not an exhaustive number of them",1
"
for addition, we analytically determine the verification set",0
" naı̈vely, we might expect to synthesize and test an input for any sequence created by combining the four possible colors in two variables and another boolean variable for whether the stack is empty (so 32 possible observations at any point), but for various reasons, most of these combinations are impossible to occur at any given point in the execution trace",0
" thus, we demonstrate that recursion enables provably perfect generalization for different tasks, including addition, topological sort, quicksort, and a variant of bubble sort",0
" plo and phi point to the lo and hi indices of the array, as in algorithm 3",0
3,1
5,1
" note that recursion helps make this process tractable, because we only need to test a finite number of inputs to show that the model will work correctly on inputs of unbounded complexity",1
" the add reduction rule ensures that each call to add only covers two adjacent columns, and so the lstm only ever runs for a fixed number of steps necessary to process these two columns",0
 the model described in the training setup of section 4 (trained on 6 traces) was verified to be correct via the matching procedure described in section 4,1
" lshift moves the four pointers to the left, to move to the next column",0
g,0
" that is to say, the network does not learn the true program semantics",1
" the pointer at index i3 represents a counter internal to the environment that is incremented once after each pass of the algorithm (one cycle of bubble and reset); when incremented a number of times equal to the length of the array, the flag i3 == length becomes true and terminates the entire algorithm ",0
" our implementation uses two stacks qstacklo and
qstackhi, each in ru , that store the arguments to the recursive quicksort calls in algorithm 3; before each recursive call, the appropriate arguments are popped off the stack and written to plo and phi",1
" as a consequence, our learned neural programs empirically achieve perfect generalization from a very small number of training examples",0
 indentation indicates the stack is one level deeper than before,1
 table 3 presents results on randomly generated arrays of varying length for the learned non-recursive and recursive programs,0
 our result shows that the training procedure and the npi architecture is capable of generalizing from the step input-output pairs seen in the training data to the unseen ones present in the verification set,0
" our implementation uses two stacks qstacklo and
qstackhi, each in ru , that store the arguments to the recursive quicksort calls in algorithm 3; before each recursive call, the appropriate arguments are popped off the stack and written to plo and phi",0
" when the program is called, the current context including the caller’s memory state is stored on a stack; when the program returns, the stored context is popped off the stack to resume execution in the previous caller’s context",0
" our results show that for all learned, to-be-verified neural programs, they all produced the same traces as those produced by p on the verification set",0
" furthermore, given a verification input set that covers all base cases and reduction rules, we can formally prove that the learned neural programs achieve perfect generalization after verifying its behavior on the verification input set",1
" naı̈vely, we might expect to synthesize and test an input for any sequence created by combining the four possible colors in two variables and another boolean variable for whether the stack is empty (so 32 possible observations at any point), but for various reasons, most of these combinations are impossible to occur at any given point in the execution trace",1
"
in what follows, we describe the way in which we constructed npi training traces so as to make them contain recursive elements and thus enable npi to learn recursive programs",1
" this functionality the original npi paper decodes to a program key embedding kt ∈ rk and then computes a program embedding pt+1, which we also did in our implementation, but we omit this for brevity",1
 ,0
" for non-primitive subprograms, we can update the observation-to-observation mapping currently associated with the subprogram and then apply that mapping to the current set",0
" the contents ofqresult andqstack are not exposed directly through the domain-specific encoder; rather, we define primitive functions which manipulate these scratch-pads",0
" note that recursion helps make this process tractable, because we only need to test a finite number of inputs to show that the model will work correctly on inputs of unbounded complexity",0
" if yes, then this indicates that the learned neural program achieves provably perfect generalization",1
 the maximum problem length in this training set is 3 (e,1
"
through careful reasoning about the possible set of environment observations created by all valid inputs, and how each of the operations in the execution trace affects the environment, we can construct v using the procedure described in section 3",0
"
base cases and reduction rules for bubble sort",0
"program descriptions calls arguments toposort perform topological
sort on graph traverse, next start, write, move
none
traverse traverse graph until stack is empty check child, explore none check child check if a white child exists; if so, set childlist[vactive] to point to it move none explore repeatedly traverse subgraphs until stack is empty stack, check child, write, move none stack interact with stack, either pushing or popping write, move push, pop next start move pstart until reaching a white vertex",0
" for each length, we test each program on 30 randomly generated problems",0
"
algorithm 2 depth first search topological sort 1: color all vertices white",1
g,0
" if a white vertex is found, set pstart to point to it; this signifies the start of a traversal of a new connected component",0
"
since npi naturally supports the notion of recursion, a key question is how to enable npi to learn recursive programs",1
" each time a subprogram is called, the stack depth increases",0
 ,0
"
an alternative method is to run p on many different program inputs and then observe step input sequences which occur, to create v ",1
"we train using the adam optimizer and use a 2-layer lstm and task-specific state encoders for the external environments, as described in reed & de freitas (2016)",0
"
we experiment with two levels of recursion—partial and full",1
" in our experiments, we only present dag’s as inputs and represent the vertices as values ranging from 1, ",1
"
an alternative way of representing the environment slice is to expose the values of the absolute vertices to the model; however, this makes it difficult to scale the model to larger graphs, since large vertex values are not seen during training time",0
" for instance, for recursive addition, consider the family f of addition problems anan−1 ",0
" finally, we implement recursive traces for our own topological sort and quicksort tasks",1
" m , as described in algorithm 1, processes the sequence of step inputs by using an lstm",1
 npi then outputs the return probability and next program and arguments to execute,1
" 4: function toposort(dag) 5: while there is still a white vertex u: do 6: color[u] = grey 7: vactive = u 8: do 9: if vactive has a white child v then 10: color[v] = grey 11: push vactive onto s 12: vactive = v 13: else 14: color[vactive] = black 15: write vactive to result 16: if s is empty then pass 17: else pop the top vertex off s and set it to vactive 18: while s is not empty
topological sort",1
 ,0
"
the dag is represented as an adjacency list where dag[i][j] refers to the j-th child of vertex i",1
" in the non-recursive trace, there are four functions that can be made recursive—toposort, check child, explore, and next start, and we add a tail recursive call to each of these functions in order to make the recursive trace",0
" in particular, in the explore function, adding a tail recursive call resets and stores the hidden states associated with vertices in a stack-like fashion",1
 pstacklo and pstackhi point to the top (empty) positions in qstacklo and qstackhi,1
" in order to claim a proof, we must verify the model produces correct behavior over all base cases and reductions, as described in section 2",0
"
for topological sort, the domain-specific encoder is
fenc(dag,qcolor, pstack, pstart, vactive, childlist, at)
=mlp ([qcolor(pstart), qcolor(dag[vactive][childlist[vactive]]), pstack == 1, at(1), at(2), at(3)]),
where qcolor ∈ ru×4 is a scratch-pad that contains u rows, each containing one of four colors (white, gray, black, invalid) with one-hot encoding",1
", an lstm in npi’s case, but possibly other networks in different cases",1
"
figure 1 shows examples of non-recursive and recursive addition traces",0
" this is possible if and only if the graph has no directed cycles; that is to say, it must be a directed acyclic graph (dag)",0
"
directed acyclic graphs are structurally more diverse than inputs in the two tasks of grade-school addition and bubble sort",0
g,0
"
there are 6 pointers (plo, phi, pstacklo, pstackhi, ppivot, pj)",0
" for tasks other than addition, it is difficult to analytically determine the verification set, so instead, we randomly generate input candidates until they completely cover the base cases and reduction rules",0
g,1
 ,1
" furthermore, given a verification input set that covers all base cases and reduction rules, we can formally prove that the learned neural programs achieve perfect generalization after verifying its behavior on the verification input set",0
"we train using the adam optimizer and use a 2-layer lstm and task-specific state encoders for the external environments, as described in reed & de freitas (2016)",0
"
the non-recursive trace loops on cycles of add1 and lshift",1
 the degree for any vertex in the dag is variable,0
" k is set to 11, to denote the range of possible numbers (0 through 9), along with the start/end token (represented with the same encoding) which is observed when a pointer reaches beyond the bounds of the input",1
"
one common strategy to improve generalization is to use curriculum learning, where the model is trained on inputs of gradually increasing complexity",0
" for purposes of verification, we replace the domain-specific encoder with the following:
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1) ≤ q(1, i2), 1 ≤ i1 ≤ length, 1 ≤ i2 ≤ length, i3 == length, at(1), at(2), at(3)]),which directly exposes which of the two values pointed to is larger",1
" thus, recursion helps decompose a problem and makes it easier to reason about a program’s behavior for previously unseen situations such as longer inputs",0
" in order to claim a proof, we must verify the model produces correct behavior over all base cases and reductions, as described in section 2",0
" although the model is exposed to more and more data, it might learn spurious and overly complex representations of the program, as suggested in zaremba   (2016)",0
 the degree for any vertex in the dag is variable,1
" this is possible if and only if the graph has no directed cycles; that is to say, it must be a directed acyclic graph (dag)",0
" in computer science, recursion (as opposed to iteration) involves solving a larger problem by combining solutions to smaller instances of the same problem",0
"
we describe the base cases, reduction rules, and the verification set for each task in appendix a",0
"
base cases and reduction rules for quicksort",0
" more specifically, when m is trained on simpler inputs, such as inputs of a small length, the generalization metric evaluates how well m will do on more complex inputs, such as inputs of much longer length",0
" also, compared to the bubble pointers in bubble sort, the pointers that perform the comparison for quicksort (the compswap function) are usually not adjacent to each other, making quicksort less local than bubble sort",0
" k is set to 11, to denote the range of possible numbers (0 through 9), along with the start/end token (represented with the same encoding) which is observed when a pointer reaches beyond the bounds of the input",0
"
the non-recursive trace loops on cycles of bubble and reset, which logically represents one bubble sweep through the array and reset of the two bubble pointers to the very beginning of the array, respectively",0
" we further have qresult ∈ nu , a scratch-pad which contains the sorted list of vertices at the end of execution, and qstack ∈ nu , which serves the role of the stack s in algorithm 2",0
"
in this general neural programming architecture, we show it is easy to support recursion",0
"
we propose and describe our verification procedure",0
" both the non-recursive and recursive learned programs generalize on all input lengths we tried, up to 5000 digits",1
" this functionality the original npi paper decodes to a program key embedding kt ∈ rk and then computes a program embedding pt+1, which we also did in our implementation, but we omit this for brevity",0
"1; and for bubble sort, appendix a",0
" for problems where the base cases may be extremely large or infinite, such as certain forms of motor control, recursion can still help reduce the problem of generalization to these two aspects and make the generalization problem significantly simpler to handle and reason about",0
" furthermore, given a verification input set that covers all base cases and reduction rules, we can formally prove that the learned neural programs achieve perfect generalization after verifying its behavior on the verification input set",0
" given a precise definition of p , it may be possible to automate the generation of v from p in future work",0
"
note that the training set can often be considerably smaller than the verification set, and despite this, the learned model can still pass the entire verification set",0
"
to evaluate a neural network that learns a neural program to accomplish a certain task, one common evaluation metric is how well the learned model m generalizes",1
 ,1
" for purposes of verification, we replace the domain-specific encoder with the following:
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1) ≤ q(1, i2), 1 ≤ i1 ≤ length, 1 ≤ i2 ≤ length, i3 == length, at(1), at(2), at(3)]),which directly exposes which of the two values pointed to is larger",0
"6) on the verification set, and ensure they match the traces produced by the true program p ",1
" in future work, we seek to enable more tasks with recursive structure",0
" the learned recursive program is different from neural programs learned in all previous work in an important aspect: previous approaches do not explicitly incorporate this abstraction, and hence generalize poorly, whereas our learned neural programs incorporate recursion and achieve perfect generalization",0
" in our experiments, we only present dag’s as inputs and represent the vertices as values ranging from 1, ",1
" m is considered to have perfect generalization if m can give the right answer for any input, such as inputs of arbitrary length",0
"
we experiment with two levels of recursion—partial and full",1
" full recursion, in addition to making the aforementioned tail recursive call, adds two additional recursive calls; bstep and lshift are made tail recursive",1
 we found that changing the npi training traces is a simple way to enable this,0
" the learned recursive program is different from neural programs learned in all previous work in an important aspect: previous approaches do not explicitly incorporate this abstraction, and hence generalize poorly, whereas our learned neural programs incorporate recursion and achieve perfect generalization",0
" the npi model has three learnable components: a task-agnostic core, a program-key embedding, and domain-specific encoders that allow the npi to operate in diverse environments",0
" more specifically, when m is trained on simpler inputs, such as inputs of a small length, the generalization metric evaluates how well m will do on more complex inputs, such as inputs of much longer length",0
" the recursive version can generalize perfectly to long problems constructed from these components (such as the sum “822+233”, where “8+2” and “2+3” are in the training set), but the non-recursive version fails to sum these long problems properly",0
" as aforementioned, the npi model naturally supports recursion",1
" however, the authors of npi did not consider explicitly the notion of recursion and as a consequence, did not learn recursive programs",1
3 to construct v and then empirically create a verification set which covers v ,1
" we further have qresult ∈ nu , a scratch-pad which contains the sorted list of vertices at the end of execution, and qstack ∈ nu , which serves the role of the stack s in algorithm 2",0
"
by reasoning about the possible set of environment observations created by all valid inputs, we construct v using the procedure described in section 3",0
"
under the assumption that there are no leading 0’s (except in the case of single digits) and the two numbers to be added have the same number of digits, the verification set for addition contains 20,181 input problems",0
"
base cases and reduction rules for addition",1
g,1
" observe that partially recursive does slightly better than non-recursive for the setting in which the length of the array is 3, and that the fully recursive version is able to sort every array given to it",1
" for these models, table 2 presents results on randomly generated dags of varying graph sizes (varying in the number of vertices)",1
" there are 3 pointers (presult, pstack, pstart), presult points to the next empty location in qresult,
pstack points to the top of the stack in qstack, and pstart points to the candidate starting node for a connected component",1
"
after finding v , we construct a set of problem inputs which, when executed on p , create exactly the step input sequences which make up v ",1
"2 at any given time, the npi has access to values pointed to by four pointers in each of the four rows, represented by q(1, i1), q(2, i2), q(3, i3), and q(4, i4)",0
"
grade-school addition",0
 the sequence of primitive operations (move and write operations) for the non-recursive and recursive versions are exactly the same,1
" given verification sets that cover all the base cases and reduction rules, we can provide proofs that these learned programs generalize perfectly",1
"
to perform the verification as described here, it is critical to construct v correctly",0
" k is set to 11, to denote the range of possible numbers (0 through 9), along with the start/end token (represented with the same encoding) which is observed when a pointer reaches beyond the bounds of the input",0
 these pointers are referred to as bubble pointers,0
" the core module of the npi is an lstm controller that takes as input a slice of the current external environment, via a set of pointers, and a program and arguments to execute",1
" in future work, we seek to enable more tasks with recursive structure",0
"
we describe the details of the npi model relevant to our contributions",0
", the verification set",0
as mentioned in section 2,1
e,0
" also the dag can have potentially more than one connected component, meaning it is necessary to transition between these components appropriately",1
" even though the earlier version also generalized just as well in practice, relatively small differences in the formulation of the traces and the environment observations can drastically change the difficulty of verification",0
"
in order to demonstrate that recursion can help learn and generalize better, for addition, we trained only on traces for 5 arbitrarily chosen 1-digit addition sum examples",0
" the swap operation has the following arguments:
arg 1 (swap object 1): ppivot
arg 2 (swap object 2): phi, pj
write",1
" both the non-recursive and recursive learned programs generalize on all graphs we tried, up to 120 vertices",1
g,1
"
after finding v , we construct a set of problem inputs which, when executed on p , create exactly the step input sequences which make up v ",1
"
as mentioned in section 1, all approaches to neural programming today fare poorly on this generalization issue",0
" for each graph size, we test the learned programs on 30 randomly generated dags",1
 ,0
" in order to claim a proof, we must verify the model produces correct behavior over all base cases and reductions, as described in section 2",0
 we outline how to construct this set,1
"
move",1
" note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values, and arg 2 can only take one of 7 values",0
" for each length, we test each program on 30 randomly generated problems",0
" the verification set can be very small, as we found a 10-element array ([8,2,1,2,0,8,5,8,3,7]) is sufficient to cover all of v ",0
" if it is too small, then execution of the program on some input might require evaluation of m(i) on some i /∈ v , and so the behavior ofm(i) might deviate from p (i)",1
 table 4 demonstrates that the accuracy of the non-recursive program degrades sharply when moving from arrays of length 7 to arrays of length 8,0
"
quicksort",0
"
we experiment with two levels of recursion—partial and full",1
 the model described in the training setup of section 4 (trained on 6 traces) was verified to be correct via the matching procedure described in section 4,0
" in future work, we seek to enable more tasks with recursive structure",0
 ,0
", to change the value of phi) none described belowstack",1
" 3: 4: function quicksort(a, lo, hi) 5: if lo < hi: then 6: p = partition(a, lo, hi) 7: quicksort(a, lo, p− 1) 8: quicksort(a, p+ 1, hi) 9: 10: function partition(a, lo, hi) 11: pivot = lo 12: for j ∈ [lo, hi− 1] : do 13: if a[j] ≤ a[hi] then 14: swap a[pivot] with a[j] 15: pivot = pivot+ 1 16: swap a[pivot] with a[hi] 17: return pivot
for quicksort, the domain-specific encoder is
fenc(qarray, qstacklo, qstackhi, plo, phi, pstacklo, pstackhi, ppivot, pj , at) =
mlp ([qarray(pj) ≤ qarray(phi), pj == phi, qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1), pstacklo == 1, at(1), at(2), at(3)]),
whereqarray ∈ ru×11 is a scratch-pad that contains u rows, each containing one of 11 values (one of the numbers 0 through 9 or an invalid state)",0
", an array for quicksort or a dag for topological sort)",0
" observe that partially recursive does slightly better than non-recursive for the setting in which the length of the array is 3, and that the fully recursive version is able to sort every array given to it",0
" the 4 environment observations aid with control flow; qstacklo(pstacklo− 1) < qstackhi(pstackhi− 1) implements the lo < hi comparison in line 5 of algorithm 3, pstacklo == 1 checks if the stacks are empty in line 18 of algorithm 4, and the other observations (all involving ppivot or pj) deal with logic in the partition function",1
" in this version of curriculum learning, even though the inputs are gradually becoming more complex, the semantics of the program is succinct and does not change",0
"
move",0
 we use the lomuto partition scheme; the logic for the recursive trace is shown in algorithm 3,0
" recursion is an important concept in programming languages and a critical tool to
reduce the complexity of programs",1
" the recursive version of topological sort solves all graph instances we tried, from graphs of size 5 through 70",0
"we train using the adam optimizer and use a 2-layer lstm and task-specific state encoders for the external environments, as described in reed & de freitas (2016)",1
" the write operation has the following arguments:
arg 1 (object to write): env stack lo, env stack hi, phi, plo
env stack lo and env stack hi represent qstacklo(pstacklo) and qstackhi(pstackhi), respectively",0
"
note that v should also contain the necessary reductions, which corresponds to making the recursive calls at the correct time, as indicated by p ",0
"
algorithm 1 neural programming inference 1: inputs: environment observation e, program p, arguments a, stop threshold α 2: function run(e, p, a) 3: h← 0, r ← 0 4: while r < α do 5: s← fenc(e, a), h← flstm(s, p, h) 6: r ← fend(h), p2 ← fprog(h), a2 ← farg(h) 7: if p is a primitive function then 8: e← fenv(e, p, a)",1
"
in addition, none of the current approaches to neural programming provide a method or even aim to enable provable guarantees about generalization",1
"program descriptions calls arguments toposort perform topological
sort on graph traverse, next start, write, move
none
traverse traverse graph until stack is empty check child, explore none check child check if a white child exists; if so, set childlist[vactive] to point to it move none explore repeatedly traverse subgraphs until stack is empty stack, check child, write, move none stack interact with stack, either pushing or popping write, move push, pop next start move pstart until reaching a white vertex",0
"
the three environment observations aid with control flow in algorithm 2",0
"6) on the verification set, and ensure they match the traces produced by the true program p ",0
 almost all architectures train on program input/output pairs,0
 we then construct a verification set of size 73 by ensuring that randomly generated graphs cover the analytically derived v ,1
"
note that the training set can often be considerably smaller than the verification set, and despite this, the learned model can still pass the entire verification set",1
"
we construct input problems by splitting into two cases: one case in which the left column contains a null value and another in which the left column does not contain any null values",0
" we further have qresult ∈ nu , a scratch-pad which contains the sorted list of vertices at the end of execution, and qstack ∈ nu , which serves the role of the stack s in algorithm 2",1
", the trace corresponding to the problem “109 + 101”)",0
"
arg 2 (increment or decrement): up, down
swap",1
" also, compared to the bubble pointers in bubble sort, the pointers that perform the comparison for quicksort (the compswap function) are usually not adjacent to each other, making quicksort less local than bubble sort",1
" without using a recursive program, such a proof is not possible, because the non-recursive program runs on an arbitrarily long addition problem that creates correspondingly long sequences of step inputs; in the non-recursive formulation of addition, add calls add1 a number of times that is dependent on the length of the input",0
" if it is too small, then execution of the program on some input might require evaluation of m(i) on some i /∈ v , and so the behavior ofm(i) might deviate from p (i)",0
3,0
" in particular, given that a recursion is defined by two properties as mentioned before, the base cases and the set of reduction rules, we can prove a recursive neural program generalizes perfectly if we can prove that (1) it performs correctly on the base cases; (2) it learns the reduction rules correctly",0
"
the non-recursive trace loops on cycles of bubble and reset, which logically represents one bubble sweep through the array and reset of the two bubble pointers to the very beginning of the array, respectively",0
" this guarantees that the step input sequences considered during verification contain all step input sequences which arise during execution of an unseen problem in f , leading to generalization to any problem in f ",0
" the non-recursive and partially recursive versions are unable to sort long arrays, beyond length 8",1
" empirically, we observe that the learned recursive programs solve all valid inputs with 100% accuracy after training on a very small number of examples, out-performing previous generalization results",0
" formally, a function exhibits recursive behavior when it possesses two properties: (1) base cases— terminating scenarios that do not use recursion to produce answers; (2) a set of rules that reduces all other problems toward the base cases",0
" for each length, we test each program on 30 randomly generated problems",0
" for these programs, we re-use the appropriate program sets (the associated subprograms), and we refer the reader to the appendix of reed & de freitas (2016) for further details on the subprograms used in addition and bubble sort",0
" recall that this verification procedure cannot be performed for the non-recursive versions, since the propagation of the hidden state in the core lstm module makes reasoning difficult and so we would need to check an unbounded number of examples",1
 some functional programming languages go so far as not to define any looping constructs but rely solely on recursion to enable repeated execution of the same code,0
 some functional programming languages go so far as not to define any looping constructs but rely solely on recursion to enable repeated execution of the same code,1
" for example, for the addition task, the model is trained on short inputs and then tested on its ability to sum inputs with much longer numbers of digits",1
" the swap operation has the following arguments:
arg 1 (swap object 1): ppivot
arg 2 (swap object 2): phi, pj
write",0
" as with the others, we apply the procedure described in section 3",1
" in this section, we give a brief review of the npi architecture from reed & de freitas (2016) as background",1
" observe that partially recursive does slightly better than non-recursive for the setting in which the length of the array is 3, and that the fully recursive version is able to sort every array given to it",0
" we made the assumption of equivalent lengths in order to parametrize the input format with respect to length, but this assumption can be removed as well",0
" the 4 environment observations aid with control flow; qstacklo(pstacklo− 1) < qstackhi(pstackhi− 1) implements the lo < hi comparison in line 5 of algorithm 3, pstacklo == 1 checks if the stacks are empty in line 18 of algorithm 4, and the other observations (all involving ppivot or pj) deal with logic in the partition function",1
" in future work, we would like to decrease supervision and construct models that are capable of coming up with recursive abstractions themselves",1
 the environment and return probability are omitted for readability,0
 reset represents a −∞ value,0
" at any given time, the npi has access to the values referred to by two pointers, represented by q(1, i1) and q(1, i2),",0
 stack push call2 pushes pivot + 1 and hi to qstacklo and qstackhi,0
 table 4 also demonstrates that the (verified) recursive program generalizes perfectly,1
" k is set to 11, to represent the range of 10 possible digits, along with a token representing the end of input",0
"
the training set for bubble sort contains 100 traces, with maximum problem length of 2 (e",0
" in future work, we seek to enable more tasks with recursive structure",1
g,0
" hence, if all subproblems in s are added correctly, we have proven that any member of f will be added correctly, thus eliminating an infinite family of inputs that need to be tested",0
"
arg 2 (increment or decrement): up, down1 // top level topological sort call 2 toposort() { 3 while (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 } 0 11 traverse() { 12 check_child() 13 explore() 14 } 15 16 check_child() { 17 while (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 18 move(childlist[vactive], up) 19 } 20 21 explore() { 22 do 23 if (qcolor(dag[vactive][childlist[vactive]]) is white): 24 write(color_next, color_grey) 25 stack(push) 26 write(save) 27 write(active_neighb) 28 move(childlist[vsave], up) 29 else: 30 write(color_curr, color_black) 31 write(result) 32 move(presult, up) 33 if(pstack == 1): 34 break 35 else: 36 stack(pop) 37 check_child() 38 while (true) 39 } 40 41 stack(op) { 42 if (op == push): 43 write(stack_push) 44 move(pstack, up) 45 46 if (op == pop): 47 write(active_stack) 48 write(stack_pop) 49 move(pstack, down) 50 } 51 52 next_start() { 53 while(qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 54 move(pstart, up) 55 }altered recursive functions  // top level topological sort call 2 toposort() { 3 if (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 toposort() // recursive call 0 } 11 12 check_child() { 13 if (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 14 move(childlist[vactive], up) 15 check_child() // recursive call 16 } 17 18 explore() { 19 if (qcolor(dag[vactive][childlist[vactive]]) is white): 20 write(color_next, color_grey) 21 stack(push) 22 write(save) 23 write(active_neighb) 24 move(childlist[vsave], up) 25 else: 26 write(color_curr, color_black) 27 write(result) 28 move(presult, up) 29 if(pstack == 1): 30 return 31 else: 32 stack(pop) 33 check_child() 34 explore() // recursive call 35 } 36 37 next_start() { 38 if (qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 39 move(pstart, up) 40 next_start() // recursive call 41 }algorithm 4 iterative quicksort 1: initialize an array a to sort and two empty stacks slo and shi",0
 our experiments use a small number of training examples,1
"
arg 2 (increment or decrement): up, down1 // top level topological sort call 2 toposort() { 3 while (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 } 0 11 traverse() { 12 check_child() 13 explore() 14 } 15 16 check_child() { 17 while (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 18 move(childlist[vactive], up) 19 } 20 21 explore() { 22 do 23 if (qcolor(dag[vactive][childlist[vactive]]) is white): 24 write(color_next, color_grey) 25 stack(push) 26 write(save) 27 write(active_neighb) 28 move(childlist[vsave], up) 29 else: 30 write(color_curr, color_black) 31 write(result) 32 move(presult, up) 33 if(pstack == 1): 34 break 35 else: 36 stack(pop) 37 check_child() 38 while (true) 39 } 40 41 stack(op) { 42 if (op == push): 43 write(stack_push) 44 move(pstack, up) 45 46 if (op == pop): 47 write(active_stack) 48 write(stack_pop) 49 move(pstack, down) 50 } 51 52 next_start() { 53 while(qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 54 move(pstart, up) 55 }altered recursive functions  // top level topological sort call 2 toposort() { 3 if (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 toposort() // recursive call 0 } 11 12 check_child() { 13 if (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 14 move(childlist[vactive], up) 15 check_child() // recursive call 16 } 17 18 explore() { 19 if (qcolor(dag[vactive][childlist[vactive]]) is white): 20 write(color_next, color_grey) 21 stack(push) 22 write(save) 23 write(active_neighb) 24 move(childlist[vsave], up) 25 else: 26 write(color_curr, color_black) 27 write(result) 28 move(presult, up) 29 if(pstack == 1): 30 return 31 else: 32 stack(pop) 33 check_child() 34 explore() // recursive call 35 } 36 37 next_start() { 38 if (qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 39 move(pstart, up) 40 next_start() // recursive call 41 }algorithm 4 iterative quicksort 1: initialize an array a to sort and two empty stacks slo and shi",0
" thus, we demonstrate that recursion enables provably perfect generalization for different tasks, including addition, topological sort, quicksort, and a variant of bubble sort",1
"
quicksort",0
" we have three observations: the color of the start node, the color of the active node’s next child to be considered, and whether the stack is empty",1
3,1
" the verification set can be very small, as we found a 10-element array ([8,2,1,2,0,8,5,8,3,7]) is sufficient to cover all of v ",0
" in future work, we would like to decrease supervision and construct models that are capable of coming up with recursive abstractions themselves",1
" consequently, there is no concept of length relevant to the problem, which has traditionally been an important focus of length-based curriculum learning",0
"
given a verification set, we can then run the model on the verification set to check if the produced traces and results are correct",0
" note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values, and arg 2 can only take one of 7 values",0
" using this modification, we constructed a verification set consisting of one array of size 10",0
" empirically, we observe that the learned recursive programs solve all valid inputs with 100% accuracy after training on a very small number of examples, out-performing previous generalization results",1
", the trace corresponding to the problem “109 + 101”)",1
 ,1
" observe that partially recursive does slightly better than non-recursive for the setting in which the length of the array is 3, and that the fully recursive version is able to sort every array given to it",0
" for grade-school addition, the domain-specific encoder is
fenc(q, i1, i2, i3, i4, at) =mlp ([q(1, i1), q(2, i2), q(3, i3), q(4, i4), at(1), at(2), at(3)]),
where the environment q ∈ r4×n×k is a scratch-pad that contains four rows (the first input number, the second input number, the carry bits, and the output) and n columns",0
"
in this general neural programming architecture, we show it is easy to support recursion",0
" the arrow in the theorem refers to evaluation, as in big-step semantics",1
" as aforementioned, the npi model naturally supports recursion",1
" 2: initialize lo and hi to be 1 and n, where n is the length of a",1
 the program terminates when seeing no numbers in the current column,0
"2 at any given time, the npi has access to values pointed to by four pointers in each of the four rows, represented by q(1, i1), q(2, i2), q(3, i3), and q(4, i4)",0
"
as a concrete instantiation, we show in this paper that we can enable recursive neural programs in the npi model, and thus enable perfectly generalizable neural programs for tasks such as sorting where the original, non-recursive npi program fails",1
 npi then outputs the return probability and next program and arguments to execute,0
" the pointer at index i3 represents a counter internal to the environment that is incremented once after each pass of the algorithm (one cycle of bubble and reset); when incremented a number of times equal to the length of the array, the flag i3 == length becomes true and terminates the entire algorithm ",1
"
we also report on generalization results for the non-recursive and recursive versions of this variant of bubble sort",1
"
training setup",0
" we further have qresult ∈ nu , a scratch-pad which contains the sorted list of vertices at the end of execution, and qstack ∈ nu , which serves the role of the stack s in algorithm 2",1
"
under the assumption that there are no leading 0’s (except in the case of single digits) and the two numbers to be added have the same number of digits, the verification set for addition contains 20,181 input problems",1
" we call this set of inputs the verification set, sv ",0
", to change the value of phi) none described belowstack",0
" 4: function toposort(dag) 5: while there is still a white vertex u: do 6: color[u] = grey 7: vactive = u 8: do 9: if vactive has a white child v then 10: color[v] = grey 11: push vactive onto s 12: vactive = v 13: else 14: color[vactive] = black 15: write vactive to result 16: if s is empty then pass 17: else pop the top vertex off s and set it to vactive 18: while s is not empty
topological sort",0
 figure 2 shows examples of traces for the different versions of bubble sort,0
" for many problems, the base cases and reduction rules usually consist of a finite (often small) number of cases",1
" in this version, there is a dependence on length: bstep and lshift are called a number of times equivalent to one less than the length of the input array, in bubble and reset respectively",0
"
we propose and describe our verification procedure",0
" for each graph size, we test the learned programs on 30 randomly generated dags",0
"
move",0
" as an application, we incorporate recursion into the neural programmer-interpreter architecture and consider four sample tasks: grade-school addition, bubble sort, topological sort, and quicksort",0
 the environment and return probability are omitted for readability,0
 18: push lo and p− 1 to slo and shi,1
"
to perform the verification as described here, it is critical to construct v correctly",0
"
arg 2 (auxiliary variable): color grey, color black
color grey and color black color the given vertex grey and black, respectively",1
" 2: initialize lo and hi to be 1 and n, where n is the length of a",0
"in this paper, we propose that the key abstraction of recursion is necessary for neural programs to generalize",0
 recursion enables provably perfect generalization,0
" the corresponding edge list is [(1, 2), (1, 5), (2, 4), (2, 5), (3, 5)]",1
" for each length, we test each program on 30 randomly generated problems",1
" in fact, we argue that recursion is an essential element for neural programs to generalize, and makes it tractable to prove the generalization of neural programs",0
"1, we hypothesize the non-recursive programs do not generalize well because they have learned spurious dependencies specific to the training set, such as length of the input problems",1
 the training set for addition contains 200 traces,1
"
we propose and describe our verification procedure",0
" at each time step, the core controller can decide to select one of the programs to call with certain arguments",0
 2: initialize an empty stack s and a directed acyclic graph dag to traverse,1
"
is critical for implementing recursion, since it permits us to restrict our attention to the currently relevant recursive call, ignoring irrelevant details about other contexts",1
" given a precise definition of p , it may be possible to automate the generation of v from p in future work",0
" for brevity, we refer the reader to the appendix for further details on the program set and non-recursive and recursive trace-generating functions used for topological sort",1
" to make an analogy to mdps, this procedure is analogous to how value iteration obtains the correct value for each state starting from any initialization",0
"we train using the adam optimizer and use a 2-layer lstm and task-specific state encoders for the external environments, as described in reed & de freitas (2016)",0
"
note that v should also contain the necessary reductions, which corresponds to making the recursive calls at the correct time, as indicated by p ",1
" in fact, we argue that recursion is an essential element for neural programs to generalize, and makes it tractable to prove the generalization of neural programs",1
"
algorithm 3 recursive quicksort 1: initialize an array a to sort",1
" in order to compensate for this, ppivot and pj require special functions (move pivot lo and move j lo) to properly set them to lo in lines 11 and 12 of the partition function in algorithm 3",0
", qstacklo or qstackhi) or to pointer (e",0
" in this architecture, we consider a core controller, e",1
 this verification phase only needs to be performed once after training,1
 indentation indicates the stack is one level deeper than before,1
" for many problems, the base cases and reduction rules usually consist of a finite (often small) number of cases",1
"
by reasoning about the possible set of environment observations created by all valid inputs, we construct v using the procedure described in section 3",0
"6) on the verification set, and ensure they match the traces produced by the true program p ",0
" in this architecture, we consider a core controller, e",0
" at end, pointer ppivot is moved to the pivot compswap loop, move pivot lo, move j lo, swap
none
compswap loop runs the for loop inside the partition function compswap, move none compswap compares a[pivot] ≤ a[j]; if so, perform a swap and increment ppivot swap, move none set pivot lo sets ppivot to lo index none none set j lo sets pj to lo index none none set j null sets pj to −∞ none none stack pushes lo/hi states
onto stacks slo and shi according to argument (described below)
write, move described below
move moves pointer one unit up or down none described below swap swaps elements at given array indices none described below write write a value either to stack (e",0
" a single element of an execution trace consists of a step input-step output pair, which can be synthesized from algorithm 1: this corresponds to, for a given time-step, the step input tuple (e, p, a) and step output tuple (r, p2, a2)",1
 the memory updates of these neural programs are so complex and interdependent that it is difficult to reason about the behaviors of the learned neural program under previously unseen situations (such as problems with longer inputs),1
" for brevity, we refer the reader to the appendix for information about the program set and non-recursive and recursive trace-generating functions for quicksort",1
 u varies with the number of vertices in the graph,0
"6) on the verification set, and ensure they match the traces produced by the true program p ",0
 the sequence of primitive operations (move and write operations) for the non-recursive and recursive versions are exactly the same,0
training neural networks to synthesize robust programs from a small number of examples is a challenging task,1
"
grade school addition",0
" the write operation has the following arguments:
arg 1 (object to write): env stack lo, env stack hi, phi, plo
env stack lo and env stack hi represent qstacklo(pstacklo) and qstackhi(pstackhi), respectively",0
" the single-digit multiplication task in zaremba   (2016), the bubble sort task in reed & de freitas (2016), and the graph tasks in graves   (2016))",0
3 to construct v and then empirically create a verification set which covers v ,0
"
the operations we need to be concerned most about are carry and lshift, which induce partial environment states spanning two columns",0
" however, to be sure that the generated v is complete (covers all the cases needed), we need to check all pairs of observations seen in adjacent step inputs (in particular, those before and after a primitive function call), in a similar way as if we were constructing v from scratch",0
" in fact, it is one of our future directions to explore new ways to train a recursive neural program without providing explicit training execution traces or with only partial or non-recursive traces",1
" childlist ∈ nu is a vector of pointers, where childlist[i] points to the next child under consideration for vertex i",0
 the program terminates when seeing no numbers in the current column,1
 we leave this as future work to devise a verification procedure more appropriate to this setting,0
" the theorem states that for the same sequence of step inputs, the model produces the exact same step output as the target program it aims to learn",1
" the space of possible programs is extremely large, and composing a program that performs robustly on the infinite space of possible inputs is difficult—in part because it is impractical to obtain enough training examples to easily disambiguate amongst all possible programs",1
" however, to be sure that the generated v is complete (covers all the cases needed), we need to check all pairs of observations seen in adjacent step inputs (in particular, those before and after a primitive function call), in a similar way as if we were constructing v from scratch",1
" 16: p = partition(a, lo, hi) 17: push p+ 1 and hi to slo and shi",1
"
one common strategy to improve generalization is to use curriculum learning, where the model is trained on inputs of gradually increasing complexity",0
"
thus far, to evaluate the efficacy of neural models on programming tasks, the only metric that has been used is generalization of expected behavior to inputs of greater complexity (vinyals   (2015), kaiser & sutskever (2015), reed & de freitas (2016), graves   (2016), zaremba   (2016))",1
" in this pair, the step input runs a subprogram add1 that has no arguments, and the step output contains a program write that has arguments of out and 1",1
" the theorem states that for the same sequence of step inputs, the model produces the exact same step output as the target program it aims to learn",1
"
in this paper, we propose to resolve these issues by explicitly incorporating recursion into neural architectures",1
" table 1 presents results on randomly generated arrays of varying length for the learned non-recursive, partially recursive, and full recursive programs",0
"
directed acyclic graphs are structurally more diverse than inputs in the two tasks of grade-school addition and bubble sort",1
" m is considered to have perfect generalization if m can give the right answer for any input, such as inputs of arbitrary length",1
 the logic for the non-recursive trace is shown in algorithm 4 in the appendix,0
"
the training set for bubble sort contains 100 traces, with maximum problem length of 2 (e",0
" for brevity, we refer the reader to the appendix for further details on the program set and non-recursive and recursive trace-generating functions used for topological sort",0
" in particular, we construct new training traces which explicitly contain recursive elements and show that with this type of trace, npi easily learns recursive programs",0
", the trace corresponding to the problem “109 + 101”)",1
" the write operation has the following arguments:
arg 1 (main action): color curr, color next, active start, active neighb, active stack, save, stack push, stack pop, result
color curr colors vactive, color next colors vertex dag[vactive][childlist[vactive]], active start writes pstart to vactive, active neighb writes dag[vactive][childlist[vactive]] to vactive, active stack writes qstack(pstack) to vactive, save writes vactive to vsave, stack push pushes vactive to the top of the stack, stack pop writes a null value to the top of the stack, and result writes vactive to qresult(presult)",0
" however, our concept of recursion for neural programs is general",1
"
the npi accesses an external environment, q, which varies according to the task",0
"
directed acyclic graphs are structurally more diverse than inputs in the two tasks of grade-school addition and bubble sort",1
" numerous models have been proposed for learning programs; to name a few, this includes the differentiable neural computer (graves  , 2016), neural turing machine (graves  , 2014), neural gpu (kaiser & sutskever, 2015), neural programmer (neelakantan  , 2015), pointer network (vinyals  , 2015), hierarchical attentive memory (andrychowicz & kurach, 2016), and neural random access machine (kurach  , 2016)",1
as mentioned in section 2,0
" the move operation has the following arguments:
arg 1 (pointer): presult, pstack, pstart, childlist[vactive], childlist[vsave]
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 5 values",0
" the core module of the npi is an lstm controller that takes as input a slice of the current external environment, via a set of pointers, and a program and arguments to execute",0
"
figure 1 shows examples of non-recursive and recursive addition traces",0
"
to perform the verification as described here, it is critical to construct v correctly",0
"
since npi naturally supports the notion of recursion, a key question is how to enable npi to learn recursive programs",0
"
arg 2 (auxiliary variable): color grey, color black
color grey and color black color the given vertex grey and black, respectively",1
 the environment and return probability are omitted for readability,1
 this is due to the properties of the training set – we trained on 2 traces synthesized from arrays of length 7 and 1 trace synthesized from an array of length 6,0
g,0
 this is due to the properties of the training set – we trained on 2 traces synthesized from arrays of length 7 and 1 trace synthesized from an array of length 6,1
"
arg 2 (auxiliary variable): color grey, color black
color grey and color black color the given vertex grey and black, respectively",0
" for each graph size, we test the learned programs on 30 randomly generated dags",0
 the training set for addition contains 200 traces,0
" the recursive version of topological sort solves all graph instances we tried, from graphs of size 5 through 70",1
" full recursion, in addition to making the aforementioned tail recursive call, adds two additional recursive calls; bstep and lshift are made tail recursive",1
"we train using the adam optimizer and use a 2-layer lstm and task-specific state encoders for the external environments, as described in reed & de freitas (2016)",1
" 3: 4: function quicksort(a, lo, hi) 5: if lo < hi: then 6: p = partition(a, lo, hi) 7: quicksort(a, lo, p− 1) 8: quicksort(a, p+ 1, hi) 9: 10: function partition(a, lo, hi) 11: pivot = lo 12: for j ∈ [lo, hi− 1] : do 13: if a[j] ≤ a[hi] then 14: swap a[pivot] with a[j] 15: pivot = pivot+ 1 16: swap a[pivot] with a[hi] 17: return pivot
for quicksort, the domain-specific encoder is
fenc(qarray, qstacklo, qstackhi, plo, phi, pstacklo, pstackhi, ppivot, pj , at) =
mlp ([qarray(pj) ≤ qarray(phi), pj == phi, qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1), pstacklo == 1, at(1), at(2), at(3)]),
whereqarray ∈ ru×11 is a scratch-pad that contains u rows, each containing one of 11 values (one of the numbers 0 through 9 or an invalid state)",0
 table 4 demonstrates that the accuracy of the non-recursive program degrades sharply when moving from arrays of length 7 to arrays of length 8,0
" 2: initialize lo and hi to be 1 and n, where n is the length of a",0
"
for addition, we analytically determine the verification set",1
" observe that partially recursive does slightly better than non-recursive for the setting in which the length of the array is 3, and that the fully recursive version is able to sort every array given to it",0
"
base cases and reduction rules for topological sort",0
" ppivot and pj point to the pivot and j indices of the array, used in the partition function in algorithm 3",0
" without using a recursive program, such a proof is not possible, because the non-recursive program runs on an arbitrarily long addition problem that creates correspondingly long sequences of step inputs; in the non-recursive formulation of addition, add calls add1 a number of times that is dependent on the length of the input",0
"
in this paper, we propose to resolve these issues by explicitly incorporating recursion into neural architectures",1
" empirically, existing models suffer from a common limitation—generalization becomes poor beyond a threshold level of complexity",1
 this is due to the properties of the training set – we trained on 2 traces synthesized from arrays of length 7 and 1 trace synthesized from an array of length 6,1
" et+1 ∼ fenv(et, pt, at)",0
g,0
 recursion can be implemented differently for different neural programming models,0
 pstacklo and pstackhi point to the top (empty) positions in qstacklo and qstackhi,0
" the assumption of leading 0’s can be easily removed, at the cost of slightly increasing the size of the verification set",0
" , n , where the dag contains n vertices",0
"
in order to demonstrate that recursion can help learn and generalize better, for addition, we trained only on traces for 5 arbitrarily chosen 1-digit addition sum examples",0
 the first is the actual model architecture,0
 the memory updates of these neural programs are so complex and interdependent that it is difficult to reason about the behaviors of the learned neural program under previously unseen situations (such as problems with longer inputs),0
" partial recursion only adds a tail recursive call to bubblesort after bubble and reset, similar to the tail recursive call described previously for addition",0
"
in this paper, we propose that recursion is an important concept for neural programs as well",0
" the core module of the npi is an lstm controller that takes as input a slice of the current external environment, via a set of pointers, and a program and arguments to execute",0
"
bubble sort",0
"
we propose and describe our verification procedure",0
" to the best of our knowledge, this is the first time verification has been applied to a neural program, providing provable guarantees about its behavior",1
" in particular, we construct new training traces which explicitly contain recursive elements and show that with this type of trace, npi easily learns recursive programs",1
" the stack operation has the following arguments:
arg 1 (operation): stack push call1, stack push call2, stack pop
stack push call1 pushes lo and pivot−1 toqstacklo andqstackhi",0
"
arg 2 (auxiliary variable): color grey, color black
color grey and color black color the given vertex grey and black, respectively",0
"
base cases and reduction rules for bubble sort",0
" in contrast, each call to the recursive add always runs for a fixed number of steps, even on arbitrarily long problems in f , so we can test that it performs correctly on a small, fixed number of step input sequences",0
"
move",0
"
in this general neural programming architecture, we show it is easy to support recursion",1
"
to evaluate a neural network that learns a neural program to accomplish a certain task, one common evaluation metric is how well the learned model m generalizes",0
" we can obtain this mapping by first considering the possible observations that can arise at the beginning of the entry function (add, bubblesort, toposort, and quicksort) for some valid program input, and iteratively applying the observation-to-observation mapping implied by the reference implementation’s step output at that point in the execution",0
 a1a0 + bnbn−1 ,0
"
through careful reasoning about the possible set of environment observations created by all valid inputs, and how each of the operations in the execution trace affects the environment, we can construct v using the procedure described in section 3",0
"
the dag is represented as an adjacency list where dag[i][j] refers to the j-th child of vertex i",1
" we implement a quicksort task, in order to demonstrate that recursion helps with learning divide-and-conquer algorithms",0
" also the dag can have potentially more than one connected component, meaning it is necessary to transition between these components appropriately",0
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,1
" et+1 ∼ fenv(et, pt, at)",0
" there are 3 pointers (presult, pstack, pstart), presult points to the next empty location in qresult,
pstack points to the top of the stack in qstack, and pstart points to the candidate starting node for a connected component",0
"
the non-recursive trace loops on cycles of bubble and reset, which logically represents one bubble sweep through the array and reset of the two bubble pointers to the very beginning of the array, respectively",0
"
arg 2 (auxiliary variable): color grey, color black
color grey and color black color the given vertex grey and black, respectively",1
"program descriptions calls arguments quicksort run the quicksort
routine in place for the array a, for indices from lo to hi non-recursive: partition, stack, write recursive: same as non-recursive version, along with quicksort implicitly: array a to sort, lo, hi
partition runs the partition function",0
" recursion is an important concept in programming languages and a critical tool to
reduce the complexity of programs",1
" both the non-recursive and recursive learned programs generalize on all graphs we tried, up to 120 vertices",1
we now report on generalization for the varying tasks,0
g,0
" we have three observations: the color of the start node, the color of the active node’s next child to be considered, and whether the stack is empty",0
" lshift, used in reset, shifts the pointers left until reaching the start token",1
" bstep, used in bubble, compares pairs of numbers, continuously moving the bubble pointers once to the right each time until reaching the end of the array",0
" in particular, recursion can be implemented as a program calling itself",1
" finally, the boolean pstack == 1 is used to check whether the stack is empty in line 18 of algorithm 2",1
"
in addition, none of the current approaches to neural programming provide a method or even aim to enable provable guarantees about generalization",1
g,1
e,0
 this is due to the properties of the training set – we trained on 2 traces synthesized from arrays of length 7 and 1 trace synthesized from an array of length 6,0
" the recursive version of topological sort solves all graph instances we tried, from graphs of size 5 through 70",1
" hence, if all subproblems in s are added correctly, we have proven that any member of f will be added correctly, thus eliminating an infinite family of inputs that need to be tested",0
 ,0
" provably perfect generalization implies the model will behave correctly, given any valid input",0
this material is in part based upon work supported by the national science foundation under grant no,1
 errors arise due to undesirable and uninterpretable dependencies and associations the architecture learns to store in some high-dimensional hidden state,0
" thus, recursion helps decompose a problem and makes it easier to reason about a program’s behavior for previously unseen situations such as longer inputs",0
 a1a0 + bnbn−1 ,1
" the recursive version can generalize perfectly to long problems constructed from these components (such as the sum “822+233”, where “8+2” and “2+3” are in the training set), but the non-recursive version fails to sum these long problems properly",0
 the memory updates of these neural programs are so complex and interdependent that it is difficult to reason about the behaviors of the learned neural program under previously unseen situations (such as problems with longer inputs),0
"
by nature, recursion reduces the complexity of a problem to simpler instances",0
" in contrast, each call to the recursive add always runs for a fixed number of steps, even on arbitrarily long problems in f , so we can test that it performs correctly on a small, fixed number of step input sequences",0
" as aforementioned, the npi model naturally supports recursion",0
" observe that partially recursive does slightly better than non-recursive for the setting in which the length of the array is 3, and that the fully recursive version is able to sort every array given to it",1
" both the non-recursive and recursive learned programs generalize on all input lengths we tried, up to 5000 digits",1
as mentioned in section 2,0
" we made the assumption of equivalent lengths in order to parametrize the input format with respect to length, but this assumption can be removed as well",0
", the trace corresponding to the array [3,2])",0
" plo and phi point to the lo and hi indices of the array, as in algorithm 3",0
"
recursion drastically reduces the number of configurations we need to consider during the verification phase and makes the proof tractable, because it introduces structure that eliminates infinitely long sequences of step inputs that would otherwise need to be considered",0
"
the operations we need to be concerned most about are carry and lshift, which induce partial environment states spanning two columns",0
" more specifically, when m is trained on simpler inputs, such as inputs of a small length, the generalization metric evaluates how well m will do on more complex inputs, such as inputs of much longer length",0
" our results show that for all learned, to-be-verified neural programs, they all produced the same traces as those produced by p on the verification set",0
" in our experiments, we only present dag’s as inputs and represent the vertices as values ranging from 1, ",0
" in order to claim a proof, we must verify the model produces correct behavior over all base cases and reductions, as described in section 2",1
 the sequence of primitive operations (move and write operations) for the non-recursive and recursive versions are exactly the same,0
" the single-digit multiplication task in zaremba   (2016), the bubble sort task in reed & de freitas (2016), and the graph tasks in graves   (2016))",1
" it is straightforward to deal with all other operations, which do not induce partial environment states",0
" we further have qresult ∈ nu , a scratch-pad which contains the sorted list of vertices at the end of execution, and qstack ∈ nu , which serves the role of the stack s in algorithm 2",1
" the only model, to our knowledge, that does not train on input-output pairs is the neural programmer-interpreter (reed & de freitas, 2016), which trains on synthetic execution traces",0
" in the non-recursive trace, there are four functions that can be made recursive—toposort, check child, explore, and next start, and we add a tail recursive call to each of these functions in order to make the recursive trace",0
 errors arise due to undesirable and uninterpretable dependencies and associations the architecture learns to store in some high-dimensional hidden state,0
" the add reduction rule ensures that each call to add only covers two adjacent columns, and so the lstm only ever runs for a fixed number of steps necessary to process these two columns",1
 these pointers are referred to as bubble pointers,0
" observe that the non-recursive program’s correctness degrades for length 11 and beyond, while the recursive program can sort any given array",0
 ,1
"
algorithm 1 neural programming inference 1: inputs: environment observation e, program p, arguments a, stop threshold α 2: function run(e, p, a) 3: h← 0, r ← 0 4: while r < α do 5: s← fenc(e, a), h← flstm(s, p, h) 6: r ← fend(h), p2 ← fprog(h), a2 ← farg(h) 7: if p is a primitive function then 8: e← fenv(e, p, a)",0
" however, to be sure that the generated v is complete (covers all the cases needed), we need to check all pairs of observations seen in adjacent step inputs (in particular, those before and after a primitive function call), in a similar way as if we were constructing v from scratch",0
" furthermore, given a verification input set that covers all base cases and reduction rules, we can formally prove that the learned neural programs achieve perfect generalization after verifying its behavior on the verification input set",0
5,0
" bstep, used in bubble, compares pairs of numbers, continuously moving the bubble pointers once to the right each time until reaching the end of the array",1
"
an alternative way of representing the environment slice is to expose the values of the absolute vertices to the model; however, this makes it difficult to scale the model to larger graphs, since large vertex values are not seen during training time",1
" the outputs rt, pt+1, at+1 are used to determine the next action, as described in algorithm 1",1
" consequently, it is infeasible to construct a verification set that covers all cases, and the verification procedure we have described is inadequate",0
"
we propose and describe our verification procedure",1
"
given a verification set, we can then run the model on the verification set to check if the produced traces and results are correct",0
" this agrees with the generalization of non-recursive addition in reed & de freitas (2016), where they reported generalization up to 3000 digits",0
 we choose to implement a topological sort task for graphs,1
" however, note that there is no provable guarantee that the non-recursive learned program will generalize to all inputs, whereas we show later that the recursive learned program has a provable guarantee of perfect generalization",1
" for the recursive formulation of addition, we analytically construct the set of input problems that cover all base cases and reduction rules",1
 the training set for addition contains 200 traces,1
" the stack operation has the following arguments:
arg 1 (operation): stack push call1, stack push call2, stack pop
stack push call1 pushes lo and pivot−1 toqstacklo andqstackhi",0
 ,1
" we prove every member of f is added properly, given that subproblems s = {anan−1 + bnbn−1, an−1an−2 + bn−1bn−2, ",0
" this guarantees that the step input sequences considered during verification contain all step input sequences which arise during execution of an unseen problem in f , leading to generalization to any problem in f ",1
"
we describe the details of the npi model relevant to our contributions",1
"
it is sufficient to construct problems where every transition between two adjacent columns is covered",0
" we hypothesize that the reason for this is that the neural network learns to spuriously depend on specific characteristics of the training examples that are irrelevant to the true program semantics, such as length of the training inputs, and thus fails to generalize to more complex inputs",1
" fa8750-15-2-0104, and berkeley deep drive",0
" as before, the non-recursive learned program lacks a provable guarantee of generalization, whereas we show later that the recursive learned program has one",0
"
to evaluate a neural network that learns a neural program to accomplish a certain task, one common evaluation metric is how well the learned model m generalizes",0
" however, to be sure that the generated v is complete (covers all the cases needed), we need to check all pairs of observations seen in adjacent step inputs (in particular, those before and after a primitive function call), in a similar way as if we were constructing v from scratch",1
 this procedure verifies that all base cases and reductions are handled properly by the model via explicit tests,0
" as described in the verification procedure, it is possible to prove our learned recursive program generalizes perfectly by testing on an appropriate set of problem inputs, i",0
 we adapt machinery from the original paper slightly to fit our needs,0
" in fact, we argue that recursion is an essential element for neural programs to generalize, and makes it tractable to prove the generalization of neural programs",0
"
the same set of problems was used to generate the training traces for all formulations of the task, for non-recursive and recursive versions",0
" twc-1409915, darpa under grant no",0
" for example, for the addition task, the model is trained on short inputs and then tested on its ability to sum inputs with much longer numbers of digits",0
" finally, we implement recursive traces for our own topological sort and quicksort tasks",0
 recursion enables provable guarantees on neural programs’ behavior without needing to exhaustively enumerate all possible inputs to the programs,1
 this algorithm is a variant of depth first search,0
3,1
" the core lstm module’s hidden state is preserved over all these add1 calls, and it is difficult to interpret with certainty what happens over longer timesteps without concretely evaluating the lstm with an input of that length",1
" in fact, it is one of our future directions to explore new ways to train a recursive neural program without providing explicit training execution traces or with only partial or non-recursive traces",0
" in particular, this condition is satisfied by npi, and thus the npi model naturally supports recursion (even though the authors of npi did not consider this aspect explicitly)",1
"
the npi accesses an external environment, q, which varies according to the task",0
" training on the full recursive trace leads to perfect generalization, as shown in section 4",0
" 2: initialize lo and hi to be 1 and n, where n is the length of a",0
" if the step output specifies a primitive function call, we need to reason about how it can affect the environment so as to change the observation in the next step input",0
" more specifically, when m is trained on simpler inputs, such as inputs of a small length, the generalization metric evaluates how well m will do on more complex inputs, such as inputs of much longer length",1
"
given a verification set, we can then run the model on the verification set to check if the produced traces and results are correct",1
"
arg 2 (increment or decrement): up, down
swap",0
 b1b0 where no carry operations occur,0
 we leave this as future work to devise a verification procedure more appropriate to this setting,0
 we describe the recursive re-formulation of traces for two tasks from the original npi paper—grade-school addition and bubble sort,0
"1; and for bubble sort, appendix a",0
"
in this general neural programming architecture, we show it is easy to support recursion",1
 indentation indicates the stack is one level deeper than before,0
"
thus far, to evaluate the efficacy of neural models on programming tasks, the only metric that has been used is generalization of expected behavior to inputs of greater complexity (vinyals   (2015), kaiser & sutskever (2015), reed & de freitas (2016), graves   (2016), zaremba   (2016))",0
" in particular, in the explore function, adding a tail recursive call resets and stores the hidden states associated with vertices in a stack-like fashion",0
3,1
" in all experiments, α is set to 0",1
" at any given time, the npi has access to the values referred to by two pointers, represented by q(1, i1) and q(1, i2),",0
 we choose to implement a topological sort task for graphs,1
" the verification set can be very small, as we found a 10-element array ([8,2,1,2,0,8,5,8,3,7]) is sufficient to cover all of v ",1
"
grade school addition",0
" childlist ∈ nu is a vector of pointers, where childlist[i] points to the next child under consideration for vertex i",0
" our results show that for all learned, to-be-verified neural programs, they all produced the same traces as those produced by p on the verification set",1
" given verification sets that cover all the base cases and reduction rules, we can provide proofs that these learned programs generalize perfectly",1
"
algorithm 1 neural programming inference 1: inputs: environment observation e, program p, arguments a, stop threshold α 2: function run(e, p, a) 3: h← 0, r ← 0 4: while r < α do 5: s← fenc(e, a), h← flstm(s, p, h) 6: r ← fend(h), p2 ← fprog(h), a2 ← farg(h) 7: if p is a primitive function then 8: e← fenv(e, p, a)",1
 the degree for any vertex in the dag is variable,0
" in fact, it is one of our future directions to explore new ways to train a recursive neural program without providing explicit training execution traces or with only partial or non-recursive traces",1
we now report on generalization for the varying tasks,1
" we make the trace recursive by adding a tail recursive call into the trace for the add program after calling add1 and lshift, the original paper uses k = 10, but we found it necessary to augment the range with an end token, in order to terminate properly",1
" empirically, existing models suffer from a common limitation—generalization becomes poor beyond a threshold level of complexity",0
" as aforementioned, the npi model naturally supports recursion",0
 the memory updates of these neural programs are so complex and interdependent that it is difficult to reason about the behaviors of the learned neural program under previously unseen situations (such as problems with longer inputs),0
"program descriptions calls arguments toposort perform topological
sort on graph traverse, next start, write, move
none
traverse traverse graph until stack is empty check child, explore none check child check if a white child exists; if so, set childlist[vactive] to point to it move none explore repeatedly traverse subgraphs until stack is empty stack, check child, write, move none stack interact with stack, either pushing or popping write, move push, pop next start move pstart until reaching a white vertex",1
"
note that the recursion for quicksort is not purely tail recursive and therefore represents a more complex kind of recursion that is harder to learn than in the previous tasks",1
" for these models, table 2 presents results on randomly generated dags of varying graph sizes (varying in the number of vertices)",0
 the program terminates when seeing no numbers in the current column,1
" we performed experiments on the partially recursive version in order to examine what happens when only one recursive call is implemented, when in reality three are required for perfect generalization",0
" in order to claim a proof, we must verify the model produces correct behavior over all base cases and reductions, as described in section 2",1
" the space of possible programs is extremely large, and composing a program that performs robustly on the infinite space of possible inputs is difficult—in part because it is impractical to obtain enough training examples to easily disambiguate amongst all possible programs",0
" the pointers at index i1 and i2 are used to compare the pair of numbers considered during the bubble sweep, swapping them if the number at i1 is greater than that in i2",0
" if no white vertex is found, the entire execution is terminated move none write write a value either to environment (e",0
" in all experiments, α is set to 0",0
"
by nature, recursion reduces the complexity of a problem to simpler instances",1
 b1b0 where no carry operations occur,0
" for tasks other than addition, it is difficult to analytically determine the verification set, so instead, we randomly generate input candidates until they completely cover the base cases and reduction rules",1
"
in this paper, we propose that recursion is an important concept for neural programs as well",1
"
as a concrete instantiation, we show in this paper that we can enable recursive neural programs in the npi model, and thus enable perfectly generalizable neural programs for tasks such as sorting where the original, non-recursive npi program fails",1
 the model described in the training setup of section 4 (trained on 6 traces) was verified to be correct via the matching procedure described in section 4,0
"
to construct this set, by using the reference implementation of each subprogram, we construct a mapping between two sets of environment observations: the first set consists of all observations that can occur at the beginning of a particular subprogram’s invocation, and the second set contains the observations at the end of that subprogram",0
g,1
"
since npi naturally supports the notion of recursion, a key question is how to enable npi to learn recursive programs",0
" we made the assumption of equivalent lengths in order to parametrize the input format with respect to length, but this assumption can be removed as well",1
" the non-recursive and partially recursive versions are unable to sort long arrays, beyond length 8",0
"
the three environment observations aid with control flow in algorithm 2",0
3,1
" recursion is an important concept in programming languages and a critical tool to
reduce the complexity of programs",0
"
in order to demonstrate that recursion can help learn and generalize better, for addition, we trained only on traces for 5 arbitrarily chosen 1-digit addition sum examples",1
" this is possible if and only if the graph has no directed cycles; that is to say, it must be a directed acyclic graph (dag)",1
"
base cases and reduction rules for quicksort",0
" the pointers at index i1 and i2 are used to compare the pair of numbers considered during the bubble sweep, swapping them if the number at i1 is greater than that in i2",1
"
is critical for implementing recursion, since it permits us to restrict our attention to the currently relevant recursive call, ignoring irrelevant details about other contexts",0
 recursion can be implemented differently for different neural programming models,1
" we then construct problem configurations that span all possible valid environment states (for instance, in order to force the carry bit in a column to be 1, one can add the sum “1+9” in the column to the right)",0
" the move operation has the following arguments:
arg 1 (pointer): presult, pstack, pstart, childlist[vactive], childlist[vsave]
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 5 values",0
"
move",0
 stack push call2 pushes pivot + 1 and hi to qstacklo and qstackhi,1
" for each length, we test each program on 30 randomly generated problems",1
" as described in the verification procedure, it is possible to prove our learned recursive program generalizes perfectly by testing on an appropriate set of problem inputs, i",1
 the first is the actual model architecture,1
" in particular, in the explore function, adding a tail recursive call resets and stores the hidden states associated with vertices in a stack-like fashion",0
" here as a concrete and general example, we consider a general neural programming architecture (npa), similar to neural programmer-interpreter (npi) in reed & de freitas (2016)",0
g,1
 recursion can be implemented differently for different neural programming models,0
" formally, the npi is represented by the following set of equations:
st = fenc(et, at)
ht = flstm(st, pt, ht−1)
rt = fend(ht), pt+1 = fprog(ht), at+1 = farg(ht)
t is a subscript denoting the time-step; fenc is a domain-specific encoder (to be described later) that takes in the environment slice et and arguments at; flstm represents the core module, which takes in the state st generated by fenc, a program embedding pt ∈ rp , and hidden lstm state ht; fend decodes the return probability rt; fprog decodes a program key embedding pt+1;1 and farg decodes arguments at+1",1
" in order to compensate for this, ppivot and pj require special functions (move pivot lo and move j lo) to properly set them to lo in lines 11 and 12 of the partition function in algorithm 3",1
" for grade-school addition, the domain-specific encoder is
fenc(q, i1, i2, i3, i4, at) =mlp ([q(1, i1), q(2, i2), q(3, i3), q(4, i4), at(1), at(2), at(3)]),
where the environment q ∈ r4×n×k is a scratch-pad that contains four rows (the first input number, the second input number, the carry bits, and the output) and n columns",1
 the model described in the training setup of section 4 (trained on 6 traces) was verified to be correct via the matching procedure described in section 4,0
"
by nature, recursion reduces the complexity of a problem to simpler instances",0
"2 at any given time, the npi has access to values pointed to by four pointers in each of the four rows, represented by q(1, i1), q(2, i2), q(3, i3), and q(4, i4)",0
" this guarantees that the step input sequences considered during verification contain all step input sequences which arise during execution of an unseen problem in f , leading to generalization to any problem in f ",0
" using this modification, we constructed a verification set consisting of one array of size 10",1
" lshift, used in reset, shifts the pointers left until reaching the start token",0
" consequently, it is infeasible to construct a verification set that covers all cases, and the verification procedure we have described is inadequate",1
" we implement a quicksort task, in order to demonstrate that recursion helps with learning divide-and-conquer algorithms",0
"
algorithm 1 neural programming inference 1: inputs: environment observation e, program p, arguments a, stop threshold α 2: function run(e, p, a) 3: h← 0, r ← 0 4: while r < α do 5: s← fenc(e, a), h← flstm(s, p, h) 6: r ← fend(h), p2 ← fprog(h), a2 ← farg(h) 7: if p is a primitive function then 8: e← fenv(e, p, a)",1
 we describe the recursive re-formulation of traces for two tasks from the original npi paper—grade-school addition and bubble sort,1
"
for addition, we analytically determine the verification set",0
" 3: 4: function quicksort(a, lo, hi) 5: if lo < hi: then 6: p = partition(a, lo, hi) 7: quicksort(a, lo, p− 1) 8: quicksort(a, p+ 1, hi) 9: 10: function partition(a, lo, hi) 11: pivot = lo 12: for j ∈ [lo, hi− 1] : do 13: if a[j] ≤ a[hi] then 14: swap a[pivot] with a[j] 15: pivot = pivot+ 1 16: swap a[pivot] with a[hi] 17: return pivot
for quicksort, the domain-specific encoder is
fenc(qarray, qstacklo, qstackhi, plo, phi, pstacklo, pstackhi, ppivot, pj , at) =
mlp ([qarray(pj) ≤ qarray(phi), pj == phi, qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1), pstacklo == 1, at(1), at(2), at(3)]),
whereqarray ∈ ru×11 is a scratch-pad that contains u rows, each containing one of 11 values (one of the numbers 0 through 9 or an invalid state)",0
" partial recursion only adds a tail recursive call to bubblesort after bubble and reset, similar to the tail recursive call described previously for addition",0
" a single element of an execution trace consists of a step input-step output pair, which can be synthesized from algorithm 1: this corresponds to, for a given time-step, the step input tuple (e, p, a) and step output tuple (r, p2, a2)",0
"
by reasoning about the possible set of environment observations created by all valid inputs, we construct v using the procedure described in section 3",1
g,0
" as an application, we incorporate recursion into the neural programmer-interpreter architecture and consider four sample tasks: grade-school addition, bubble sort, topological sort, and quicksort",1
" lshift moves the four pointers to the left, to move to the next column",0
 b1b0 where no carry operations occur,0
" the only model, to our knowledge, that does not train on input-output pairs is the neural programmer-interpreter (reed & de freitas, 2016), which trains on synthetic execution traces",0
" training on the full recursive trace leads to perfect generalization, as shown in section 4",1
", to color a vertex) or variable (e",0
 we instantiated recursion for the neural programmerinterpreter by changing the training traces,0
" if a white vertex is found, set pstart to point to it; this signifies the start of a traversal of a new connected component",1
" on the other hand, the recursive programs have learned the true program semantics",0
"
bubble sort",0
"
to perform the verification as described here, it is critical to construct v correctly",0
" to make an analogy to mdps, this procedure is analogous to how value iteration obtains the correct value for each state starting from any initialization",1
" as with the original npi architecture, the experiments for this paper always used a 3-tuple of integers at = (at(1), at(2), at(3))",0
" qcolor(dag[vactive][childlist[vactive]]) refers to the color of the next child of vactive, used in the evaluation of the condition in the if branch in line 9 of algorithm 2",0
"
the npi accesses an external environment, q, which varies according to the task",0
" the second is the training procedure, which consists of the form of the training data and the optimization process",0
" if it is too large, then the semantics of p might not be well-defined on some elements in v , or the spurious step input sequences may not be reachable from any valid problem input (e",1
" the swap operation has the following arguments:
arg 1 (swap object 1): ppivot
arg 2 (swap object 2): phi, pj
write",0
" if no white vertex is found, the entire execution is terminated move none write write a value either to environment (e",0
" this functionality the original npi paper decodes to a program key embedding kt ∈ rk and then computes a program embedding pt+1, which we also did in our implementation, but we omit this for brevity",1
"
figure 1 shows examples of non-recursive and recursive addition traces",1
" for the recursive formulation of addition, we analytically construct the set of input problems that cover all base cases and reduction rules",0
"
bubble sort",1
" both the non-recursive and recursive learned programs generalize on all input lengths we tried, up to 5000 digits",0
" however, note that there is no provable guarantee that the non-recursive learned program will generalize to all inputs, whereas we show later that the recursive learned program has a provable guarantee of perfect generalization",1
" the partial recursion is not enough for perfect generalization, as will be presented later in section 4",0
"we train using the adam optimizer and use a 2-layer lstm and task-specific state encoders for the external environments, as described in reed & de freitas (2016)",1
" in all experiments, α is set to 0",1
g,1
 this makes it difficult to reason about what the model will do when given complex inputs,0
" for tasks other than addition, it is difficult to analytically determine the verification set, so instead, we randomly generate input candidates until they completely cover the base cases and reduction rules",0
 there is a (changing) list of neural programs used to accomplish a given task,1
"
base cases and reduction rules for bubble sort",0
" 2: initialize lo and hi to be 1 and n, where n is the length of a",1
 we found that changing the npi training traces is a simple way to enable this,1
", an array for quicksort or a dag for topological sort)",0
 figure 2 shows examples of traces for the different versions of bubble sort,0
" et+1 ∼ fenv(et, pt, at)",1
2,1
 we created a program set that reflects the semantics of algorithm 2,1
"
formally, verification consists of proving the following theorem:
∀i ∈ v,m(i) ⇓ p (i) where i denotes a sequence of step inputs (within one function call), v denotes the set of valid sequences of step inputs, m denotes the neural network model, p denotes the correct program, and p (i) denotes the next step output from the correct program",1
" if a white vertex is found, set pstart to point to it; this signifies the start of a traversal of a new connected component",1
" full recursion, in addition to making the aforementioned tail recursive call, adds two additional recursive calls; bstep and lshift are made tail recursive",0
" full recursion, in addition to making the aforementioned tail recursive call, adds two additional recursive calls; bstep and lshift are made tail recursive",0
"
given a verification set, we can then run the model on the verification set to check if the produced traces and results are correct",1
" the write operation has the following arguments:
arg 1 (main action): color curr, color next, active start, active neighb, active stack, save, stack push, stack pop, result
color curr colors vactive, color next colors vertex dag[vactive][childlist[vactive]], active start writes pstart to vactive, active neighb writes dag[vactive][childlist[vactive]] to vactive, active stack writes qstack(pstack) to vactive, save writes vactive to vsave, stack push pushes vactive to the top of the stack, stack pop writes a null value to the top of the stack, and result writes vactive to qresult(presult)",0
 there is a (changing) list of neural programs used to accomplish a given task,0
", qstacklo or qstackhi) or to pointer (e",1
"1, we hypothesize the non-recursive programs do not generalize well because they have learned spurious dependencies specific to the training set, such as length of the input problems",0
" in particular, in the explore function, adding a tail recursive call resets and stores the hidden states associated with vertices in a stack-like fashion",1
" the space of possible programs is extremely large, and composing a program that performs robustly on the infinite space of possible inputs is difficult—in part because it is impractical to obtain enough training examples to easily disambiguate amongst all possible programs",0
" as with the original npi architecture, the experiments for this paper always used a 3-tuple of integers at = (at(1), at(2), at(3))",1
" this is highly undesirable, since being able to provide the correct answer in all possible settings is one of the most important aspects of any learned neural program",1
 table 4 demonstrates that the accuracy of the non-recursive program degrades sharply when moving from arrays of length 7 to arrays of length 8,0
" this guarantees that the step input sequences considered during verification contain all step input sequences which arise during execution of an unseen problem in f , leading to generalization to any problem in f ",1
 this paper is the first (to our knowledge) to investigate the important problem of provable generalization properties of neural programs,1
"1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 while (pstacklo 6= 1): 6 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 7 stack(stack_pop) 8 else: 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 stack(stack_pop) 12 partition() 13 stack(stack_push_call2) 14 stack(stack_push_call1) 15 } 16 17 partition() { 18 set_pivot_lo() 19 set_j_lo() 20 compswap_loop() 21 swap(ppivot, phi) 22 set_j_null() 23 } 24 25 compswap_loop() { 26 while (pj 6= phi): 27 compswap() 28 move(pj, up) 29 } 30 31 compswap() { 32 if (a[pj ] ≤ a[phi]): 33 swap(ppivot, pj) 34 move(ppivot, up) 35 } 36 37 stack(op) { 38 if (op == stack_push_call1): 39 write(env_stack_lo, plo) 40 write(env_stack_hi, ppivot − 1) 41 move(pstacklo, up) 42 move(pstackhi, up) 43 44 if (op == stack_push_call2): 45 write(env_stack_lo, ppivot + 1) 46 write(env_stack_hi, phi) 47 move(pstacklo, up) 48 move(pstackhi, up) 49 50 if (op == stack_pop): 51 write(env_stack_lo, reset) 52 write(env_stack_hi, reset) 53 move(pstacklo, down) 54 move(pstackhi, down) 55 }altered recursive functions 1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 6 partition() 7 stack(stack_push_call2) 8 stack(stack_push_call1) 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 quicksort() // recursive call 12 stack(stack_pop) 13 write(phi, env_stack_hi_peek) 14 write(plo, env_stack_lo_peek) 15 quicksort() // recursive call 16 stack(stack_pop) 17 } 18 19 compswap_loop() { 20 if (pj 6= phi): 21 compswap() 22 move(pj, up) 23 compswap_loop() // recursive call 24 }in this section, we describe the space of base cases and reduction rules that must be covered for each of the four sample tasks, in order to create the verification set",0
"
note that v should also contain the necessary reductions, which corresponds to making the recursive calls at the correct time, as indicated by p ",1
 we then construct a verification set of size 73 by ensuring that randomly generated graphs cover the analytically derived v ,0
e,0
" in this pair, the step input runs a subprogram add1 that has no arguments, and the step output contains a program write that has arguments of out and 1",1
" provably perfect generalization implies the model will behave correctly, given any valid input",0
" via the recursive call, we effectively forget that the column just added exists, since the recursive call to add starts with a new hidden state for the lstm controller",1
"
under the assumption that there are no leading 0’s (except in the case of single digits) and the two numbers to be added have the same number of digits, the verification set for addition contains 20,181 input problems",0
" table 1 presents results on randomly generated arrays of varying length for the learned non-recursive, partially recursive, and full recursive programs",0
" consequently, it is infeasible to construct a verification set that covers all cases, and the verification procedure we have described is inadequate",0
"
we propose and describe our verification procedure",0
"
an alternative method is to run p on many different program inputs and then observe step input sequences which occur, to create v ",0
" the write operation has the following arguments:
arg 1 (object to write): env stack lo, env stack hi, phi, plo
env stack lo and env stack hi represent qstacklo(pstacklo) and qstackhi(pstackhi), respectively",1
" if it is too small, then execution of the program on some input might require evaluation of m(i) on some i /∈ v , and so the behavior ofm(i) might deviate from p (i)",0
"
thus far, to evaluate the efficacy of neural models on programming tasks, the only metric that has been used is generalization of expected behavior to inputs of greater complexity (vinyals   (2015), kaiser & sutskever (2015), reed & de freitas (2016), graves   (2016), zaremba   (2016))",0
" the add reduction rule ensures that each call to add only covers two adjacent columns, and so the lstm only ever runs for a fixed number of steps necessary to process these two columns",0
3 to construct v and then empirically create a verification set which covers v ,0
" our implementation uses two stacks qstacklo and
qstackhi, each in ru , that store the arguments to the recursive quicksort calls in algorithm 3; before each recursive call, the appropriate arguments are popped off the stack and written to plo and phi",1
"as there is no public implementation of npi, we implemented a version of it in keras that is as faithful to the paper as possible",0
" the contents ofqresult andqstack are not exposed directly through the domain-specific encoder; rather, we define primitive functions which manipulate these scratch-pads",1
"
arg 2 (object to copy): env stack lo peek, env stack hi peek, phi, plo, ppivot − 1, ppivot + 1, reset
env stack lo peek and env stack hi peek represent qstacklo(pstacklo − 1) and qstackhi(pstackhi − 1), respectively",0
" in order to compensate for this, ppivot and pj require special functions (move pivot lo and move j lo) to properly set them to lo in lines 11 and 12 of the partition function in algorithm 3",0
" in our experiments, we only present dag’s as inputs and represent the vertices as values ranging from 1, ",1
" k is set to 11, to represent the range of 10 possible digits, along with a token representing the end of input",1
 ,1
" the recursive version of topological sort solves all graph instances we tried, from graphs of size 5 through 70",0
" however, note that there is no provable guarantee that the non-recursive learned program will generalize to all inputs, whereas we show later that the recursive learned program has a provable guarantee of perfect generalization",0
"as discussed in section 2, the neural programmer-interpreter (npi) is an instance of a neural programmer architecture and hence it naturally supports recursion",0
" 3: 4: function partition(a, lo, hi) 5: pivot = lo 6: for j ∈ [lo, hi− 1] : do 7: if a[j] ≤ a[hi] then 8: swap a[pivot] with a[j] 9: pivot = pivot+ 1 0: swap a[pivot] with a[hi] 11: return pivot 12: 13: function quicksort(a, lo, hi) 14: while slo and shi are not empty: do 15: pop states off slo and shi, writing them to lo and hi",0
" our results show that for all learned, to-be-verified neural programs, they all produced the same traces as those produced by p on the verification set",0
" to the best of our knowledge, this is the first time verification has been applied to a neural program, providing provable guarantees about its behavior",0
" when the program is called, the current context including the caller’s memory state is stored on a stack; when the program returns, the stored context is popped off the stack to resume execution in the previous caller’s context",0
" formally, a function exhibits recursive behavior when it possesses two properties: (1) base cases— terminating scenarios that do not use recursion to produce answers; (2) a set of rules that reduces all other problems toward the base cases",1
" we make the trace recursive by adding a tail recursive call into the trace for the add program after calling add1 and lshift, the original paper uses k = 10, but we found it necessary to augment the range with an end token, in order to terminate properly",0
"
in this paper, we propose that recursion is an important concept for neural programs as well",1
" in particular, given that a recursion is defined by two properties as mentioned before, the base cases and the set of reduction rules, we can prove a recursive neural program generalizes perfectly if we can prove that (1) it performs correctly on the base cases; (2) it learns the reduction rules correctly",1
" the npi model has three learnable components: a task-agnostic core, a program-key embedding, and domain-specific encoders that allow the npi to operate in diverse environments",0
" we have three observations: the color of the start node, the color of the active node’s next child to be considered, and whether the stack is empty",0
" as described in the verification procedure, it is possible to prove our learned recursive program generalizes perfectly by testing on an appropriate set of problem inputs, i",1
" we performed experiments on the partially recursive version in order to examine what happens when only one recursive call is implemented, when in reality three are required for perfect generalization",0
 the maximum problem length in this training set is 3 (e,0
" for each length, we test each program on 30 randomly generated problems",1
" thus, any general neural programming architecture supporting such a call structure can be made to support recursion",0
", to change the value of phi) none described belowstack",1
" for bubble sort, the domain-specific encoder is
fenc(q, i1, i2, i3, at) =mlp ([q(1, i1), q(1, i2), i3 == length, at(1), at(2), at(3)]),
where the environment q ∈ r1×n×k is a scratch-pad that contains 1 row, to represent the state of the array as sorting proceeds in-place, andn columns",0
 the maximum problem length in this training set is 3 (e,0
"
the training set for topological sort contains 6 traces, with one synthesized from a graph of size 5 and the rest synthesized from graphs of size 7",0
 the sequence of primitive operations (move and write operations) for the non-recursive and recursive versions are exactly the same,0
we emphasize the overall goal of this work is to enable the learning of a recursive program,0
" given verification sets that cover all the base cases and reduction rules, we can provide proofs that these learned programs generalize perfectly",1
" the write operation has the following arguments:
arg 1 (object to write): env stack lo, env stack hi, phi, plo
env stack lo and env stack hi represent qstacklo(pstacklo) and qstackhi(pstackhi), respectively",0
" in contrast, each call to the recursive add always runs for a fixed number of steps, even on arbitrarily long problems in f , so we can test that it performs correctly on a small, fixed number of step input sequences",0
 ,0
"
as mentioned in section 1, all approaches to neural programming today fare poorly on this generalization issue",0
5,0
"as there is no public implementation of npi, we implemented a version of it in keras that is as faithful to the paper as possible",0
 u varies with the number of vertices in the graph,0
" in this architecture, we consider a core controller, e",0
" 3: 4: function partition(a, lo, hi) 5: pivot = lo 6: for j ∈ [lo, hi− 1] : do 7: if a[j] ≤ a[hi] then 8: swap a[pivot] with a[j] 9: pivot = pivot+ 1 0: swap a[pivot] with a[hi] 11: return pivot 12: 13: function quicksort(a, lo, hi) 14: while slo and shi are not empty: do 15: pop states off slo and shi, writing them to lo and hi",0
" finally, the boolean pstack == 1 is used to check whether the stack is empty in line 18 of algorithm 2",0
 this paper is the first (to our knowledge) to investigate the important problem of provable generalization properties of neural programs,0
" this makes it so that we only need to consider the vertices in the subgraph that are currently relevant for computing the sort, allowing simpler reasoning about behavior for large graphs",0
"program descriptions calls arguments quicksort run the quicksort
routine in place for the array a, for indices from lo to hi non-recursive: partition, stack, write recursive: same as non-recursive version, along with quicksort implicitly: array a to sort, lo, hi
partition runs the partition function",0
" the second is the training procedure, which consists of the form of the training data and the optimization process",0
e,0
 recursion enables provably perfect generalization,0
" this makes it so that we only need to consider the vertices in the subgraph that are currently relevant for computing the sort, allowing simpler reasoning about behavior for large graphs",0
"1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 while (pstacklo 6= 1): 6 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 7 stack(stack_pop) 8 else: 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 stack(stack_pop) 12 partition() 13 stack(stack_push_call2) 14 stack(stack_push_call1) 15 } 16 17 partition() { 18 set_pivot_lo() 19 set_j_lo() 20 compswap_loop() 21 swap(ppivot, phi) 22 set_j_null() 23 } 24 25 compswap_loop() { 26 while (pj 6= phi): 27 compswap() 28 move(pj, up) 29 } 30 31 compswap() { 32 if (a[pj ] ≤ a[phi]): 33 swap(ppivot, pj) 34 move(ppivot, up) 35 } 36 37 stack(op) { 38 if (op == stack_push_call1): 39 write(env_stack_lo, plo) 40 write(env_stack_hi, ppivot − 1) 41 move(pstacklo, up) 42 move(pstackhi, up) 43 44 if (op == stack_push_call2): 45 write(env_stack_lo, ppivot + 1) 46 write(env_stack_hi, phi) 47 move(pstacklo, up) 48 move(pstackhi, up) 49 50 if (op == stack_pop): 51 write(env_stack_lo, reset) 52 write(env_stack_hi, reset) 53 move(pstacklo, down) 54 move(pstackhi, down) 55 }altered recursive functions 1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 6 partition() 7 stack(stack_push_call2) 8 stack(stack_push_call1) 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 quicksort() // recursive call 12 stack(stack_pop) 13 write(phi, env_stack_hi_peek) 14 write(plo, env_stack_lo_peek) 15 quicksort() // recursive call 16 stack(stack_pop) 17 } 18 19 compswap_loop() { 20 if (pj 6= phi): 21 compswap() 22 move(pj, up) 23 compswap_loop() // recursive call 24 }in this section, we describe the space of base cases and reduction rules that must be covered for each of the four sample tasks, in order to create the verification set",0
e,0
" each time a subprogram is called, the stack depth increases",0
 there are 2 variables (vactive and vsave); vactive holds the active vertex (as in algorithm 2) and vsave holds the value of vactive before executing line 12 of algorithm 2,0
"
it is important to emphasize that at inference time in the npi, the hidden state of the lstm controller is reset (to zero) at each subprogram call, as in line 3 of algorithm 1 (h ← 0)",0
" the recursive version of topological sort solves all graph instances we tried, from graphs of size 5 through 70",0
"
the training data for the neural programmer-interpreter consists of full execution traces for the program of interest",0
" , a1a0 + b1b0} are added properly",0
" 3: 4: function quicksort(a, lo, hi) 5: if lo < hi: then 6: p = partition(a, lo, hi) 7: quicksort(a, lo, p− 1) 8: quicksort(a, p+ 1, hi) 9: 10: function partition(a, lo, hi) 11: pivot = lo 12: for j ∈ [lo, hi− 1] : do 13: if a[j] ≤ a[hi] then 14: swap a[pivot] with a[j] 15: pivot = pivot+ 1 16: swap a[pivot] with a[hi] 17: return pivot
for quicksort, the domain-specific encoder is
fenc(qarray, qstacklo, qstackhi, plo, phi, pstacklo, pstackhi, ppivot, pj , at) =
mlp ([qarray(pj) ≤ qarray(phi), pj == phi, qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1), pstacklo == 1, at(1), at(2), at(3)]),
whereqarray ∈ ru×11 is a scratch-pad that contains u rows, each containing one of 11 values (one of the numbers 0 through 9 or an invalid state)",0
" for example, for the addition task, the model is trained on short inputs and then tested on its ability to sum inputs with much longer numbers of digits",0
"program descriptions calls arguments quicksort run the quicksort
routine in place for the array a, for indices from lo to hi non-recursive: partition, stack, write recursive: same as non-recursive version, along with quicksort implicitly: array a to sort, lo, hi
partition runs the partition function",1
 table 4 demonstrates that the accuracy of the non-recursive program degrades sharply when moving from arrays of length 7 to arrays of length 8,0
" our implementation uses two stacks qstacklo and
qstackhi, each in ru , that store the arguments to the recursive quicksort calls in algorithm 3; before each recursive call, the appropriate arguments are popped off the stack and written to plo and phi",0
", an array for quicksort or a dag for topological sort)",1
" by iterating with this procedure, and then running p on the input observation set that we obtain for the entry point function, we can obtain v precisely",0
" for these programs, we re-use the appropriate program sets (the associated subprograms), and we refer the reader to the appendix of reed & de freitas (2016) for further details on the subprograms used in addition and bubble sort",0
 add1 is a subprogram that adds the current column (writing the appropriate digit to the output row and carrying a bit to the next column if needed),1
"
we construct input problems by splitting into two cases: one case in which the left column contains a null value and another in which the left column does not contain any null values",0
" as before, the non-recursive learned program lacks a provable guarantee of generalization, whereas we show later that the recursive learned program has one",0
 this algorithm is a variant of depth first search,0
 there is a (changing) list of neural programs used to accomplish a given task,0
" we implement a quicksort task, in order to demonstrate that recursion helps with learning divide-and-conquer algorithms",0
 ,1
" the contents ofqresult andqstack are not exposed directly through the domain-specific encoder; rather, we define primitive functions which manipulate these scratch-pads",1
" this guarantees that the step input sequences considered during verification contain all step input sequences which arise during execution of an unseen problem in f , leading to generalization to any problem in f ",0
" both the non-recursive and recursive learned programs generalize on all graphs we tried, up to 120 vertices",1
" for these programs, we re-use the appropriate program sets (the associated subprograms), and we refer the reader to the appendix of reed & de freitas (2016) for further details on the subprograms used in addition and bubble sort",0
 ,1
" however, note that there is no provable guarantee that the non-recursive learned program will generalize to all inputs, whereas we show later that the recursive learned program has a provable guarantee of perfect generalization",0
" for brevity, we refer the reader to the appendix for information about the program set and non-recursive and recursive trace-generating functions for quicksort",0
 we found that changing the npi training traces is a simple way to enable this,0
", to change the value of vactive) none described below move move a pointer (e",0
" thus, recursion helps decompose a problem and makes it easier to reason about a program’s behavior for previously unseen situations such as longer inputs",0
" given verification sets that cover all the base cases and reduction rules, we can provide proofs that these learned programs generalize perfectly",0
" , n , where the dag contains n vertices",0
" training on the full recursive trace leads to perfect generalization, as shown in section 4",0
" however, models that make use of this strategy eventually fail after a certain level of complexity (e",0
 this makes it difficult to reason about what the model will do when given complex inputs,0
 almost all architectures train on program input/output pairs,0
" m , as described in algorithm 1, processes the sequence of step inputs by using an lstm",0
"
in this paper, we propose to resolve these issues by explicitly incorporating recursion into neural architectures",0
" , n , where the dag contains n vertices",1
" we note that an earlier version of quicksort we tried lacked primitive operations to directly move a pointer to another, and therefore needed more functions and observations",1
" both the non-recursive and recursive learned programs generalize on all input lengths we tried, up to 5000 digits",0
" the corresponding edge list is [(1, 2), (1, 5), (2, 4), (2, 5), (3, 5)]",0
 we choose to implement a topological sort task for graphs,0
" recall that this verification procedure cannot be performed for the non-recursive versions, since the propagation of the hidden state in the core lstm module makes reasoning difficult and so we would need to check an unbounded number of examples",0
" lshift, used in reset, shifts the pointers left until reaching the start token",0
" as a consequence, our learned neural programs empirically achieve perfect generalization from a very small number of training examples",1
" the corresponding edge list is [(1, 2), (1, 5), (2, 4), (2, 5), (3, 5)]",0
"
to evaluate a neural network that learns a neural program to accomplish a certain task, one common evaluation metric is how well the learned model m generalizes",1
" we show that by modifying the training procedure, we enable the npi model to learn recursive neural programs",1
" provably perfect generalization implies the model will behave correctly, given any valid input",1
 we find that recursion makes it easier for the network to learn the right program and generalize to unknown situations,0
"
it is sufficient to construct problems where every transition between two adjacent columns is covered",1
 the core controller acts as a dispatcher for the programs,0
 the sequence of primitive operations (move and write operations) for the non-recursive and recursive versions are exactly the same,0
"6) on the verification set, and ensure they match the traces produced by the true program p ",1
" for grade-school addition, the domain-specific encoder is
fenc(q, i1, i2, i3, i4, at) =mlp ([q(1, i1), q(2, i2), q(3, i3), q(4, i4), at(1), at(2), at(3)]),
where the environment q ∈ r4×n×k is a scratch-pad that contains four rows (the first input number, the second input number, the carry bits, and the output) and n columns",1
"as discussed in section 2, the neural programmer-interpreter (npi) is an instance of a neural programmer architecture and hence it naturally supports recursion",0
"
under the assumption that there are no leading 0’s (except in the case of single digits) and the two numbers to be added have the same number of digits, the verification set for addition contains 20,181 input problems",1
" the assumption of leading 0’s can be easily removed, at the cost of slightly increasing the size of the verification set",0
" for each function we use to implement the recursive version of topological sort, we need to consider the set of possible environment observation sequences we can create from all valid inputs and test that the learned program produces the correct behavior on each of these inputs",1
" consequently, there is no concept of length relevant to the problem, which has traditionally been an important focus of length-based curriculum learning",0
 there is a (changing) list of neural programs used to accomplish a given task,0
" finally, the boolean pstack == 1 is used to check whether the stack is empty in line 18 of algorithm 2",1
" the space of possible programs is extremely large, and composing a program that performs robustly on the infinite space of possible inputs is difficult—in part because it is impractical to obtain enough training examples to easily disambiguate amongst all possible programs",0
" k is set to 11, to denote the range of possible numbers (0 through 9), along with the start/end token (represented with the same encoding) which is observed when a pointer reaches beyond the bounds of the input",0
" this agrees with the generalization of non-recursive addition in reed & de freitas (2016), where they reported generalization up to 3000 digits",0
" for example, a step input-step output pair in lines 2 and 3 of the left-hand side of figure 1 is (add1, write out 1)",1
" at end, pointer ppivot is moved to the pivot compswap loop, move pivot lo, move j lo, swap
none
compswap loop runs the for loop inside the partition function compswap, move none compswap compares a[pivot] ≤ a[j]; if so, perform a swap and increment ppivot swap, move none set pivot lo sets ppivot to lo index none none set j lo sets pj to lo index none none set j null sets pj to −∞ none none stack pushes lo/hi states
onto stacks slo and shi according to argument (described below)
write, move described below
move moves pointer one unit up or down none described below swap swaps elements at given array indices none described below write write a value either to stack (e",0
" via the recursive call, we effectively forget that the column just added exists, since the recursive call to add starts with a new hidden state for the lstm controller",1
" each time a subprogram is called, the stack depth increases",0
"
move",0
"
as in line 13 of the right-hand side of figure 1",0
" ppivot and pj point to the pivot and j indices of the array, used in the partition function in algorithm 3",0
" the single-digit multiplication task in zaremba   (2016), the bubble sort task in reed & de freitas (2016), and the graph tasks in graves   (2016))",1
" for example, a step input-step output pair in lines 2 and 3 of the left-hand side of figure 1 is (add1, write out 1)",0
" the pointer at index i3 represents a counter internal to the environment that is incremented once after each pass of the algorithm (one cycle of bubble and reset); when incremented a number of times equal to the length of the array, the flag i3 == length becomes true and terminates the entire algorithm ",1
" in practice, we can additionally use tail recursion optimization to avoid problems with the call stack growing too deep",0
" this makes it so that we only need to consider the vertices in the subgraph that are currently relevant for computing the sort, allowing simpler reasoning about behavior for large graphs",1
"
base cases and reduction rules for quicksort",0
" the contents ofqresult andqstack are not exposed directly through the domain-specific encoder; rather, we define primitive functions which manipulate these scratch-pads",0
 the program terminates when seeing no numbers in the current column,0
 ,0
"
to perform the verification as described here, it is critical to construct v correctly",1
g,0
"1; and for bubble sort, appendix a",1
" also the dag can have potentially more than one connected component, meaning it is necessary to transition between these components appropriately",0
 ,0
 we find that recursion makes it easier for the network to learn the right program and generalize to unknown situations,0
"
by nature, recursion reduces the complexity of a problem to simpler instances",0
"
directed acyclic graphs are structurally more diverse than inputs in the two tasks of grade-school addition and bubble sort",0
training neural networks to synthesize robust programs from a small number of examples is a challenging task,0
"
note that v should also contain the necessary reductions, which corresponds to making the recursive calls at the correct time, as indicated by p ",0
", the trace corresponding to the problem “109 + 101”)",0
" we make the trace recursive by adding a tail recursive call into the trace for the add program after calling add1 and lshift, the original paper uses k = 10, but we found it necessary to augment the range with an end token, in order to terminate properly",0
 stack pop pushes −∞ values to qstacklo and qstackhi,1
 errors arise due to undesirable and uninterpretable dependencies and associations the architecture learns to store in some high-dimensional hidden state,0
", qstacklo or qstackhi) or to pointer (e",0
" k is set to 11, to represent the range of 10 possible digits, along with a token representing the end of input",0
" in particular, recursion can be implemented as a program calling itself",0
we emphasize the overall goal of this work is to enable the learning of a recursive program,0
" also, compared to the bubble pointers in bubble sort, the pointers that perform the comparison for quicksort (the compswap function) are usually not adjacent to each other, making quicksort less local than bubble sort",0
3 to construct v and then empirically create a verification set which covers v ,0
"
algorithm 3 recursive quicksort 1: initialize an array a to sort",0
" full recursion, in addition to making the aforementioned tail recursive call, adds two additional recursive calls; bstep and lshift are made tail recursive",0
" the arrow in the theorem refers to evaluation, as in big-step semantics",1
 the original version of the bubblesort implementation exposes the values within the array,1
" we make the trace recursive by adding a tail recursive call into the trace for the add program after calling add1 and lshift, the original paper uses k = 10, but we found it necessary to augment the range with an end token, in order to terminate properly",0
"
we construct input problems by splitting into two cases: one case in which the left column contains a null value and another in which the left column does not contain any null values",0
"
we describe the details of the npi model relevant to our contributions",0
" given verification sets that cover all the base cases and reduction rules, we can provide proofs that these learned programs generalize perfectly",0
" we performed experiments on the partially recursive version in order to examine what happens when only one recursive call is implemented, when in reality three are required for perfect generalization",0
 this procedure verifies that all base cases and reductions are handled properly by the model via explicit tests,0
" observe that the non-recursive program’s correctness degrades for length 11 and beyond, while the recursive program can sort any given array",0
2,1
" if it is too large, then the semantics of p might not be well-defined on some elements in v , or the spurious step input sequences may not be reachable from any valid problem input (e",0
" table 1 presents results on randomly generated arrays of varying length for the learned non-recursive, partially recursive, and full recursive programs",1
 the first is the actual model architecture,0
" both the non-recursive and recursive learned programs generalize on all graphs we tried, up to 120 vertices",0
"
we would also like to point out that in this paper, we provide as an example one way to train a recursive neural program, by providing a certain training execution trace to the npi model",1
"
in what follows, we describe the way in which we constructed npi training traces so as to make them contain recursive elements and thus enable npi to learn recursive programs",0
 recursion enables provably perfect generalization,0
3,0
" as with the others, we apply the procedure described in section 3",0
" that is to say, the network does not learn the true program semantics",1
"
bubble sort",1
 b1b0 where no carry operations occur,0
" for many problems, the base cases and reduction rules usually consist of a finite (often small) number of cases",1
"
base cases and reduction rules for addition",1
", pstart or childlist[vactive]) up or down none described belowwrite",0
"
since npi naturally supports the notion of recursion, a key question is how to enable npi to learn recursive programs",1
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,1
 we outline how to construct this set,0
 stack push call2 pushes pivot + 1 and hi to qstacklo and qstackhi,0
"
the dag is represented as an adjacency list where dag[i][j] refers to the j-th child of vertex i",0
" consequently, there is no concept of length relevant to the problem, which has traditionally been an important focus of length-based curriculum learning",1
" a topological sort is a linear ordering of vertices such that for every directed edge (u, v) from u to v, u comes before v in the ordering",0
" the only model, to our knowledge, that does not train on input-output pairs is the neural programmer-interpreter (reed & de freitas, 2016), which trains on synthetic execution traces",0
" the theorem states that for the same sequence of step inputs, the model produces the exact same step output as the target program it aims to learn",0
" for brevity, we refer the reader to the appendix for further details on the program set and non-recursive and recursive trace-generating functions used for topological sort",0
" if yes, then this indicates that the learned neural program achieves provably perfect generalization",0
" 4: function toposort(dag) 5: while there is still a white vertex u: do 6: color[u] = grey 7: vactive = u 8: do 9: if vactive has a white child v then 10: color[v] = grey 11: push vactive onto s 12: vactive = v 13: else 14: color[vactive] = black 15: write vactive to result 16: if s is empty then pass 17: else pop the top vertex off s and set it to vactive 18: while s is not empty
topological sort",0
"
for topological sort, the domain-specific encoder is
fenc(dag,qcolor, pstack, pstart, vactive, childlist, at)
=mlp ([qcolor(pstart), qcolor(dag[vactive][childlist[vactive]]), pstack == 1, at(1), at(2), at(3)]),
where qcolor ∈ ru×4 is a scratch-pad that contains u rows, each containing one of four colors (white, gray, black, invalid) with one-hot encoding",0
 2: initialize an empty stack s and a directed acyclic graph dag to traverse,1
"1; and for bubble sort, appendix a",1
" consequently, there is no concept of length relevant to the problem, which has traditionally been an important focus of length-based curriculum learning",0
 these pointers are referred to as bubble pointers,0
"
we describe the details of the npi model relevant to our contributions",0
"
we describe the base cases, reduction rules, and the verification set for each task in appendix a",0
" hence, if all subproblems in s are added correctly, we have proven that any member of f will be added correctly, thus eliminating an infinite family of inputs that need to be tested",0
 the program terminates when seeing no numbers in the current column,1
" we implement a quicksort task, in order to demonstrate that recursion helps with learning divide-and-conquer algorithms",0
 this makes it difficult to reason about what the model will do when given complex inputs,0
" as with the others, we apply the procedure described in section 3",0
" although the model is exposed to more and more data, it might learn spurious and overly complex representations of the program, as suggested in zaremba   (2016)",0
" the move operation has the following arguments:
arg 1 (pointer): presult, pstack, pstart, childlist[vactive], childlist[vsave]
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 5 values",0
 these pointers are referred to as bubble pointers,0
"
in what follows, we describe the way in which we constructed npi training traces so as to make them contain recursive elements and thus enable npi to learn recursive programs",0
" this agrees with the generalization of non-recursive addition in reed & de freitas (2016), where they reported generalization up to 3000 digits",0
" formally, the npi is represented by the following set of equations:
st = fenc(et, at)
ht = flstm(st, pt, ht−1)
rt = fend(ht), pt+1 = fprog(ht), at+1 = farg(ht)
t is a subscript denoting the time-step; fenc is a domain-specific encoder (to be described later) that takes in the environment slice et and arguments at; flstm represents the core module, which takes in the state st generated by fenc, a program embedding pt ∈ rp , and hidden lstm state ht; fend decodes the return probability rt; fprog decodes a program key embedding pt+1;1 and farg decodes arguments at+1",1
 we choose to implement a topological sort task for graphs,0
" the add reduction rule ensures that each call to add only covers two adjacent columns, and so the lstm only ever runs for a fixed number of steps necessary to process these two columns",0
" the npi model has three learnable components: a task-agnostic core, a program-key embedding, and domain-specific encoders that allow the npi to operate in diverse environments",1
"
to construct this set, by using the reference implementation of each subprogram, we construct a mapping between two sets of environment observations: the first set consists of all observations that can occur at the beginning of a particular subprogram’s invocation, and the second set contains the observations at the end of that subprogram",0
" we note that an earlier version of quicksort we tried lacked primitive operations to directly move a pointer to another, and therefore needed more functions and observations",1
 the degree for any vertex in the dag is variable,1
"
as a concrete instantiation, we show in this paper that we can enable recursive neural programs in the npi model, and thus enable perfectly generalizable neural programs for tasks such as sorting where the original, non-recursive npi program fails",0
" m is considered to have perfect generalization if m can give the right answer for any input, such as inputs of arbitrary length",0
 this modification also enables us to sort arrays containing arbitrary comparable elements,0
"1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 while (pstacklo 6= 1): 6 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 7 stack(stack_pop) 8 else: 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 stack(stack_pop) 12 partition() 13 stack(stack_push_call2) 14 stack(stack_push_call1) 15 } 16 17 partition() { 18 set_pivot_lo() 19 set_j_lo() 20 compswap_loop() 21 swap(ppivot, phi) 22 set_j_null() 23 } 24 25 compswap_loop() { 26 while (pj 6= phi): 27 compswap() 28 move(pj, up) 29 } 30 31 compswap() { 32 if (a[pj ] ≤ a[phi]): 33 swap(ppivot, pj) 34 move(ppivot, up) 35 } 36 37 stack(op) { 38 if (op == stack_push_call1): 39 write(env_stack_lo, plo) 40 write(env_stack_hi, ppivot − 1) 41 move(pstacklo, up) 42 move(pstackhi, up) 43 44 if (op == stack_push_call2): 45 write(env_stack_lo, ppivot + 1) 46 write(env_stack_hi, phi) 47 move(pstacklo, up) 48 move(pstackhi, up) 49 50 if (op == stack_pop): 51 write(env_stack_lo, reset) 52 write(env_stack_hi, reset) 53 move(pstacklo, down) 54 move(pstackhi, down) 55 }altered recursive functions 1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 6 partition() 7 stack(stack_push_call2) 8 stack(stack_push_call1) 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 quicksort() // recursive call 12 stack(stack_pop) 13 write(phi, env_stack_hi_peek) 14 write(plo, env_stack_lo_peek) 15 quicksort() // recursive call 16 stack(stack_pop) 17 } 18 19 compswap_loop() { 20 if (pj 6= phi): 21 compswap() 22 move(pj, up) 23 compswap_loop() // recursive call 24 }in this section, we describe the space of base cases and reduction rules that must be covered for each of the four sample tasks, in order to create the verification set",1
"
note that the training set can often be considerably smaller than the verification set, and despite this, the learned model can still pass the entire verification set",1
" observe that the non-recursive program’s correctness degrades for length 11 and beyond, while the recursive program can sort any given array",0
" the stack operation has the following arguments:
arg 1 (operation): stack push call1, stack push call2, stack pop
stack push call1 pushes lo and pivot−1 toqstacklo andqstackhi",1
 the model described in the training setup of section 4 (trained on 6 traces) was verified to be correct via the matching procedure described in section 4,1
"
the training set for topological sort contains 6 traces, with one synthesized from a graph of size 5 and the rest synthesized from graphs of size 7",1
"
an alternative method is to run p on many different program inputs and then observe step input sequences which occur, to create v ",0
" k is set to 11, to represent the range of 10 possible digits, along with a token representing the end of input",0
"
an alternative way of representing the environment slice is to expose the values of the absolute vertices to the model; however, this makes it difficult to scale the model to larger graphs, since large vertex values are not seen during training time",1
" in our experiments, we only present dag’s as inputs and represent the vertices as values ranging from 1, ",1
" in particular, in the explore function, adding a tail recursive call resets and stores the hidden states associated with vertices in a stack-like fashion",0
" a single element of an execution trace consists of a step input-step output pair, which can be synthesized from algorithm 1: this corresponds to, for a given time-step, the step input tuple (e, p, a) and step output tuple (r, p2, a2)",1
"
the training set for quicksort contains 4 traces, synthesized from arrays of length 5",1
" in the non-recursive trace, there are four functions that can be made recursive—toposort, check child, explore, and next start, and we add a tail recursive call to each of these functions in order to make the recursive trace",0
"
thus far, to evaluate the efficacy of neural models on programming tasks, the only metric that has been used is generalization of expected behavior to inputs of greater complexity (vinyals   (2015), kaiser & sutskever (2015), reed & de freitas (2016), graves   (2016), zaremba   (2016))",0
" note that recursion helps make this process tractable, because we only need to test a finite number of inputs to show that the model will work correctly on inputs of unbounded complexity",0
" for grade-school addition, the domain-specific encoder is
fenc(q, i1, i2, i3, i4, at) =mlp ([q(1, i1), q(2, i2), q(3, i3), q(4, i4), at(1), at(2), at(3)]),
where the environment q ∈ r4×n×k is a scratch-pad that contains four rows (the first input number, the second input number, the carry bits, and the output) and n columns",0
" a topological sort is a linear ordering of vertices such that for every directed edge (u, v) from u to v, u comes before v in the ordering",0
 18: push lo and p− 1 to slo and shi,0
 u varies with the number of vertices in the graph,0
" the only model, to our knowledge, that does not train on input-output pairs is the neural programmer-interpreter (reed & de freitas, 2016), which trains on synthetic execution traces",0
 table 4 demonstrates that the accuracy of the non-recursive program degrades sharply when moving from arrays of length 7 to arrays of length 8,0
"
inside bubble and reset, there are two operations that can be made recursive",1
 ,0
" this functionality the original npi paper decodes to a program key embedding kt ∈ rk and then computes a program embedding pt+1, which we also did in our implementation, but we omit this for brevity",1
" we show that by modifying the training procedure, we enable the npi model to learn recursive neural programs",1
 some functional programming languages go so far as not to define any looping constructs but rely solely on recursion to enable repeated execution of the same code,1
"
quicksort",0
" we have three observations: the color of the start node, the color of the active node’s next child to be considered, and whether the stack is empty",0
" at each time step, the core controller can decide to select one of the programs to call with certain arguments",0
" we prove every member of f is added properly, given that subproblems s = {anan−1 + bnbn−1, an−1an−2 + bn−1bn−2, ",1
" 3: 4: function partition(a, lo, hi) 5: pivot = lo 6: for j ∈ [lo, hi− 1] : do 7: if a[j] ≤ a[hi] then 8: swap a[pivot] with a[j] 9: pivot = pivot+ 1 0: swap a[pivot] with a[hi] 11: return pivot 12: 13: function quicksort(a, lo, hi) 14: while slo and shi are not empty: do 15: pop states off slo and shi, writing them to lo and hi",0
" while this matches the description from reed & de freitas (2016), we found that this causes an unnecessary blowup in the size of v and makes it much more difficult to construct the verification set",1
 we use the lomuto partition scheme; the logic for the recursive trace is shown in algorithm 3,0
"
note that v should also contain the necessary reductions, which corresponds to making the recursive calls at the correct time, as indicated by p ",0
" numerous models have been proposed for learning programs; to name a few, this includes the differentiable neural computer (graves  , 2016), neural turing machine (graves  , 2014), neural gpu (kaiser & sutskever, 2015), neural programmer (neelakantan  , 2015), pointer network (vinyals  , 2015), hierarchical attentive memory (andrychowicz & kurach, 2016), and neural random access machine (kurach  , 2016)",1
"
quicksort",1
" here as a concrete and general example, we consider a general neural programming architecture (npa), similar to neural programmer-interpreter (npi) in reed & de freitas (2016)",1
we emphasize that the notion of a neural recursive program has not been presented in the literature before: this is our main contribution,1
"
we experiment with two levels of recursion—partial and full",1
"
to perform the verification as described here, it is critical to construct v correctly",0
3,0
" in order to compensate for this, ppivot and pj require special functions (move pivot lo and move j lo) to properly set them to lo in lines 11 and 12 of the partition function in algorithm 3",0
"
the training data for the neural programmer-interpreter consists of full execution traces for the program of interest",1
" the learned recursive program is different from neural programs learned in all previous work in an important aspect: previous approaches do not explicitly incorporate this abstraction, and hence generalize poorly, whereas our learned neural programs incorporate recursion and achieve perfect generalization",1
"
one common strategy to improve generalization is to use curriculum learning, where the model is trained on inputs of gradually increasing complexity",0
 ,0
"
base cases and reduction rules for quicksort",1
"
we also report on generalization results for the non-recursive and recursive versions of this variant of bubble sort",1
" at end, pointer ppivot is moved to the pivot compswap loop, move pivot lo, move j lo, swap
none
compswap loop runs the for loop inside the partition function compswap, move none compswap compares a[pivot] ≤ a[j]; if so, perform a swap and increment ppivot swap, move none set pivot lo sets ppivot to lo index none none set j lo sets pj to lo index none none set j null sets pj to −∞ none none stack pushes lo/hi states
onto stacks slo and shi according to argument (described below)
write, move described below
move moves pointer one unit up or down none described below swap swaps elements at given array indices none described below write write a value either to stack (e",1
 the environment and return probability are omitted for readability,0
" for instance, for recursive addition, consider the family f of addition problems anan−1 ",1
" m , as described in algorithm 1, processes the sequence of step inputs by using an lstm",0
" the theorem states that for the same sequence of step inputs, the model produces the exact same step output as the target program it aims to learn",0
" at each time step, the core controller can decide to select one of the programs to call with certain arguments",0
" observe that the non-recursive program’s correctness degrades for length 11 and beyond, while the recursive program can sort any given array",0
" observe that partially recursive does slightly better than non-recursive for the setting in which the length of the array is 3, and that the fully recursive version is able to sort every array given to it",0
" for these models, table 2 presents results on randomly generated dags of varying graph sizes (varying in the number of vertices)",0
we emphasize that the notion of a neural recursive program has not been presented in the literature before: this is our main contribution,0
" each step during an execution of the program does one of three things: (1) another subprogram along with associated arguments is called, as in line 10, (2) the program writes to the environment if it is primitive, as in line 8, or (3) the loop is terminated if the return probability exceeds a threshold α, after which the stack frame is popped and control is returned to the caller",1
" lshift moves the four pointers to the left, to move to the next column",0
" for tasks other than addition, it is difficult to analytically determine the verification set, so instead, we randomly generate input candidates until they completely cover the base cases and reduction rules",0
" at end, pointer ppivot is moved to the pivot compswap loop, move pivot lo, move j lo, swap
none
compswap loop runs the for loop inside the partition function compswap, move none compswap compares a[pivot] ≤ a[j]; if so, perform a swap and increment ppivot swap, move none set pivot lo sets ppivot to lo index none none set j lo sets pj to lo index none none set j null sets pj to −∞ none none stack pushes lo/hi states
onto stacks slo and shi according to argument (described below)
write, move described below
move moves pointer one unit up or down none described below swap swaps elements at given array indices none described below write write a value either to stack (e",1
" recursion is an important concept in programming languages and a critical tool to
reduce the complexity of programs",0
 our result shows that the training procedure and the npi architecture is capable of generalizing from the step input-output pairs seen in the training data to the unseen ones present in the verification set,1
"
move",0
" however, note that there is no provable guarantee that the non-recursive learned program will generalize to all inputs, whereas we show later that the recursive learned program has a provable guarantee of perfect generalization",0
"
arg 2 (object to copy): env stack lo peek, env stack hi peek, phi, plo, ppivot − 1, ppivot + 1, reset
env stack lo peek and env stack hi peek represent qstacklo(pstacklo − 1) and qstackhi(pstackhi − 1), respectively",0
" by iterating with this procedure, and then running p on the input observation set that we obtain for the entry point function, we can obtain v precisely",0
" the npi model has three learnable components: a task-agnostic core, a program-key embedding, and domain-specific encoders that allow the npi to operate in diverse environments",0
" this guarantees that the step input sequences considered during verification contain all step input sequences which arise during execution of an unseen problem in f , leading to generalization to any problem in f ",0
" given verification sets that cover all the base cases and reduction rules, we can provide proofs that these learned programs generalize perfectly",1
" in future work, we seek to enable more tasks with recursive structure",0
" 9: else 10: function run(e, p2, a2)
a description of the inference procedure is given in algorithm 1",1
" the npi model has three learnable components: a task-agnostic core, a program-key embedding, and domain-specific encoders that allow the npi to operate in diverse environments",0
" in particular, given that a recursion is defined by two properties as mentioned before, the base cases and the set of reduction rules, we can prove a recursive neural program generalizes perfectly if we can prove that (1) it performs correctly on the base cases; (2) it learns the reduction rules correctly",0
" recall that this verification procedure cannot be performed for the non-recursive versions, since the propagation of the hidden state in the core lstm module makes reasoning difficult and so we would need to check an unbounded number of examples",0
"
move",0
", to color a vertex) or variable (e",0
" for each task, given the verification set, we check the traces and results of the learned, to-be-verified neural program (described in section 4",0
"
by reasoning about the possible set of environment observations created by all valid inputs, we construct v using the procedure described in section 3",0
" furthermore, given a verification input set that covers all base cases and reduction rules, we can formally prove that the learned neural programs achieve perfect generalization after verifying its behavior on the verification input set",0
we emphasize the overall goal of this work is to enable the learning of a recursive program,0
3 to construct v and then empirically create a verification set which covers v ,0
"
for topological sort, the domain-specific encoder is
fenc(dag,qcolor, pstack, pstart, vactive, childlist, at)
=mlp ([qcolor(pstart), qcolor(dag[vactive][childlist[vactive]]), pstack == 1, at(1), at(2), at(3)]),
where qcolor ∈ ru×4 is a scratch-pad that contains u rows, each containing one of four colors (white, gray, black, invalid) with one-hot encoding",1
" twc-1409915, darpa under grant no",1
", to change the value of vactive) none described below move move a pointer (e",0
 recursion enables provable guarantees on neural programs’ behavior without needing to exhaustively enumerate all possible inputs to the programs,1
 the program terminates when seeing no numbers in the current column,0
" however, models that make use of this strategy eventually fail after a certain level of complexity (e",0
" the architecture usually possesses some form of memory, which could be internal (such as the hidden state of a recurrent neural network) or external (such as a discrete “scratch pad” or a memory block with differentiable access)",1
"in this paper, we propose that the key abstraction of recursion is necessary for neural programs to generalize",0
" for each task, given the verification set, we check the traces and results of the learned, to-be-verified neural program (described in section 4",0
" in this version of curriculum learning, even though the inputs are gradually becoming more complex, the semantics of the program is succinct and does not change",1
"
we refer the reader to the appendix for the non-recursive trace generating functions",0
 the logic for the non-recursive trace is shown in algorithm 4 in the appendix,0
"we train using the adam optimizer and use a 2-layer lstm and task-specific state encoders for the external environments, as described in reed & de freitas (2016)",0
"
to evaluate a neural network that learns a neural program to accomplish a certain task, one common evaluation metric is how well the learned model m generalizes",0
" for tasks other than addition, it is difficult to analytically determine the verification set, so instead, we randomly generate input candidates until they completely cover the base cases and reduction rules",1
" k is set to 11, to represent the range of 10 possible digits, along with a token representing the end of input",0
" for tasks other than addition, it is difficult to analytically determine the verification set, so instead, we randomly generate input candidates until they completely cover the base cases and reduction rules",1
" we then construct problem configurations that span all possible valid environment states (for instance, in order to force the carry bit in a column to be 1, one can add the sum “1+9” in the column to the right)",1
" in computer science, recursion (as opposed to iteration) involves solving a larger problem by combining solutions to smaller instances of the same problem",0
 the original version of the bubblesort implementation exposes the values within the array,0
" bstep, used in bubble, compares pairs of numbers, continuously moving the bubble pointers once to the right each time until reaching the end of the array",0
"
move",0
 errors arise due to undesirable and uninterpretable dependencies and associations the architecture learns to store in some high-dimensional hidden state,0
 recursion enables provable guarantees on neural programs’ behavior without needing to exhaustively enumerate all possible inputs to the programs,1
 3: begin traversing from vertex 1 in the dag,1
"
for topological sort, the domain-specific encoder is
fenc(dag,qcolor, pstack, pstart, vactive, childlist, at)
=mlp ([qcolor(pstart), qcolor(dag[vactive][childlist[vactive]]), pstack == 1, at(1), at(2), at(3)]),
where qcolor ∈ ru×4 is a scratch-pad that contains u rows, each containing one of four colors (white, gray, black, invalid) with one-hot encoding",0
"
one common strategy to improve generalization is to use curriculum learning, where the model is trained on inputs of gradually increasing complexity",1
" in particular, this condition is satisfied by npi, and thus the npi model naturally supports recursion (even though the authors of npi did not consider this aspect explicitly)",0
we now report on generalization for the varying tasks,0
" in particular, given that a recursion is defined by two properties as mentioned before, the base cases and the set of reduction rules, we can prove a recursive neural program generalizes perfectly if we can prove that (1) it performs correctly on the base cases; (2) it learns the reduction rules correctly",0
" to the best of our knowledge, this is the first time verification has been applied to a neural program, providing provable guarantees about its behavior",0
 we instantiated recursion for the neural programmerinterpreter by changing the training traces,1
" for these programs, we re-use the appropriate program sets (the associated subprograms), and we refer the reader to the appendix of reed & de freitas (2016) for further details on the subprograms used in addition and bubble sort",0
" for non-primitive subprograms, we can update the observation-to-observation mapping currently associated with the subprogram and then apply that mapping to the current set",0
 npi then outputs the return probability and next program and arguments to execute,0
" as this complexity interfered with determining the base cases and reductions, we changed the algorithm to its current form",1
" the learned recursive program is different from neural programs learned in all previous work in an important aspect: previous approaches do not explicitly incorporate this abstraction, and hence generalize poorly, whereas our learned neural programs incorporate recursion and achieve perfect generalization",0
"
base cases and reduction rules for topological sort",0
" to make an analogy to mdps, this procedure is analogous to how value iteration obtains the correct value for each state starting from any initialization",0
 ,1
" in practice, we can additionally use tail recursion optimization to avoid problems with the call stack growing too deep",0
" the general notion of recursion has been an important concept in many domains, including mathematics and computer science",1
"
under the assumption that there are no leading 0’s (except in the case of single digits) and the two numbers to be added have the same number of digits, the verification set for addition contains 20,181 input problems",0
" in contrast, each call to the recursive add always runs for a fixed number of steps, even on arbitrarily long problems in f , so we can test that it performs correctly on a small, fixed number of step input sequences",0
 recursion enables provably perfect generalization,1
"2 at any given time, the npi has access to values pointed to by four pointers in each of the four rows, represented by q(1, i1), q(2, i2), q(3, i3), and q(4, i4)",1
" plo and phi point to the lo and hi indices of the array, as in algorithm 3",0
"
note that v should also contain the necessary reductions, which corresponds to making the recursive calls at the correct time, as indicated by p ",1
" the learned recursive program is different from neural programs learned in all previous work in an important aspect: previous approaches do not explicitly incorporate this abstraction, and hence generalize poorly, whereas our learned neural programs incorporate recursion and achieve perfect generalization",0
" we can obtain this mapping by first considering the possible observations that can arise at the beginning of the entry function (add, bubblesort, toposort, and quicksort) for some valid program input, and iteratively applying the observation-to-observation mapping implied by the reference implementation’s step output at that point in the execution",0
 our result shows that the training procedure and the npi architecture is capable of generalizing from the step input-output pairs seen in the training data to the unseen ones present in the verification set,0
" 4: function toposort(dag) 5: while there is still a white vertex u: do 6: color[u] = grey 7: vactive = u 8: do 9: if vactive has a white child v then 10: color[v] = grey 11: push vactive onto s 12: vactive = v 13: else 14: color[vactive] = black 15: write vactive to result 16: if s is empty then pass 17: else pop the top vertex off s and set it to vactive 18: while s is not empty
topological sort",0
" however, to be sure that the generated v is complete (covers all the cases needed), we need to check all pairs of observations seen in adjacent step inputs (in particular, those before and after a primitive function call), in a similar way as if we were constructing v from scratch",1
5,0
"
the training data for the neural programmer-interpreter consists of full execution traces for the program of interest",1
", the trace corresponding to the problem “109 + 101”)",0
" that is to say, the network does not learn the true program semantics",0
training neural networks to synthesize robust programs from a small number of examples is a challenging task,0
" the learned recursive program is different from neural programs learned in all previous work in an important aspect: previous approaches do not explicitly incorporate this abstraction, and hence generalize poorly, whereas our learned neural programs incorporate recursion and achieve perfect generalization",0
" as this complexity interfered with determining the base cases and reductions, we changed the algorithm to its current form",0
 we adapt machinery from the original paper slightly to fit our needs,1
 we describe the recursive re-formulation of traces for two tasks from the original npi paper—grade-school addition and bubble sort,0
 this is due to the properties of the training set – we trained on 2 traces synthesized from arrays of length 7 and 1 trace synthesized from an array of length 6,0
" 2: initialize lo and hi to be 1 and n, where n is the length of a",0
"
the non-recursive trace loops on cycles of bubble and reset, which logically represents one bubble sweep through the array and reset of the two bubble pointers to the very beginning of the array, respectively",0
" provably perfect generalization implies the model will behave correctly, given any valid input",0
" for brevity, we refer the reader to the appendix for further details on the program set and non-recursive and recursive trace-generating functions used for topological sort",0
"
base cases and reduction rules for addition",0
" the only model, to our knowledge, that does not train on input-output pairs is the neural programmer-interpreter (reed & de freitas, 2016), which trains on synthetic execution traces",0
"
arg 2 (increment or decrement): up, down1 // top level topological sort call 2 toposort() { 3 while (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 } 0 11 traverse() { 12 check_child() 13 explore() 14 } 15 16 check_child() { 17 while (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 18 move(childlist[vactive], up) 19 } 20 21 explore() { 22 do 23 if (qcolor(dag[vactive][childlist[vactive]]) is white): 24 write(color_next, color_grey) 25 stack(push) 26 write(save) 27 write(active_neighb) 28 move(childlist[vsave], up) 29 else: 30 write(color_curr, color_black) 31 write(result) 32 move(presult, up) 33 if(pstack == 1): 34 break 35 else: 36 stack(pop) 37 check_child() 38 while (true) 39 } 40 41 stack(op) { 42 if (op == push): 43 write(stack_push) 44 move(pstack, up) 45 46 if (op == pop): 47 write(active_stack) 48 write(stack_pop) 49 move(pstack, down) 50 } 51 52 next_start() { 53 while(qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 54 move(pstart, up) 55 }altered recursive functions  // top level topological sort call 2 toposort() { 3 if (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 toposort() // recursive call 0 } 11 12 check_child() { 13 if (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 14 move(childlist[vactive], up) 15 check_child() // recursive call 16 } 17 18 explore() { 19 if (qcolor(dag[vactive][childlist[vactive]]) is white): 20 write(color_next, color_grey) 21 stack(push) 22 write(save) 23 write(active_neighb) 24 move(childlist[vsave], up) 25 else: 26 write(color_curr, color_black) 27 write(result) 28 move(presult, up) 29 if(pstack == 1): 30 return 31 else: 32 stack(pop) 33 check_child() 34 explore() // recursive call 35 } 36 37 next_start() { 38 if (qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 39 move(pstart, up) 40 next_start() // recursive call 41 }algorithm 4 iterative quicksort 1: initialize an array a to sort and two empty stacks slo and shi",0
"
as mentioned in section 1, all approaches to neural programming today fare poorly on this generalization issue",1
" a topological sort is a linear ordering of vertices such that for every directed edge (u, v) from u to v, u comes before v in the ordering",0
" also the dag can have potentially more than one connected component, meaning it is necessary to transition between these components appropriately",1
" the assumption of leading 0’s can be easily removed, at the cost of slightly increasing the size of the verification set",0
" empirically, we observe that the learned recursive programs solve all valid inputs with 100% accuracy after training on a very small number of examples, out-performing previous generalization results",0
"
note that the recursion for quicksort is not purely tail recursive and therefore represents a more complex kind of recursion that is harder to learn than in the previous tasks",0
 2: initialize an empty stack s and a directed acyclic graph dag to traverse,0
" for each length, we test each program on 30 randomly generated problems",0
" given verification sets that cover all the base cases and reduction rules, we can provide proofs that these learned programs generalize perfectly",0
" for example, for the addition task, the model is trained on short inputs and then tested on its ability to sum inputs with much longer numbers of digits",0
" we have three observations: the color of the start node, the color of the active node’s next child to be considered, and whether the stack is empty",1
 the sequence of primitive operations (move and write operations) for the non-recursive and recursive versions are exactly the same,1
", the trace corresponding to the problem “109 + 101”)",0
" the only model, to our knowledge, that does not train on input-output pairs is the neural programmer-interpreter (reed & de freitas, 2016), which trains on synthetic execution traces",0
 we instantiated recursion for the neural programmerinterpreter by changing the training traces,1
" the npi model has three learnable components: a task-agnostic core, a program-key embedding, and domain-specific encoders that allow the npi to operate in diverse environments",0
 the core controller acts as a dispatcher for the programs,0
" in order to compensate for this, ppivot and pj require special functions (move pivot lo and move j lo) to properly set them to lo in lines 11 and 12 of the partition function in algorithm 3",0
" recall that this verification procedure cannot be performed for the non-recursive versions, since the propagation of the hidden state in the core lstm module makes reasoning difficult and so we would need to check an unbounded number of examples",0
" also the dag can have potentially more than one connected component, meaning it is necessary to transition between these components appropriately",0
" note that recursion helps make this process tractable, because we only need to test a finite number of inputs to show that the model will work correctly on inputs of unbounded complexity",0
"6) on the verification set, and ensure they match the traces produced by the true program p ",0
" the core lstm module’s hidden state is preserved over all these add1 calls, and it is difficult to interpret with certainty what happens over longer timesteps without concretely evaluating the lstm with an input of that length",0
"
the training set for quicksort contains 4 traces, synthesized from arrays of length 5",0
 some functional programming languages go so far as not to define any looping constructs but rely solely on recursion to enable repeated execution of the same code,1
"
we note that for tasks with very large input domains, such as ones involving mnist digits or speech samples, the state space of base cases and reduction rules could be prohibitively large, possibly infinite",1
 stack push call2 pushes pivot + 1 and hi to qstacklo and qstackhi,1
" we make the trace recursive by adding a tail recursive call into the trace for the add program after calling add1 and lshift, the original paper uses k = 10, but we found it necessary to augment the range with an end token, in order to terminate properly",1
" both the non-recursive and recursive learned programs generalize on all graphs we tried, up to 120 vertices",1
"
bubble sort",0
"
the training set for topological sort contains 6 traces, with one synthesized from a graph of size 5 and the rest synthesized from graphs of size 7",0
" hence, if all subproblems in s are added correctly, we have proven that any member of f will be added correctly, thus eliminating an infinite family of inputs that need to be tested",0
" the npi model has three learnable components: a task-agnostic core, a program-key embedding, and domain-specific encoders that allow the npi to operate in diverse environments",1
 table 3 presents results on randomly generated arrays of varying length for the learned non-recursive and recursive programs,1
"
in this general neural programming architecture, we show it is easy to support recursion",1
" for many problems, the base cases and reduction rules usually consist of a finite (often small) number of cases",1
we now report on generalization for the varying tasks,0
 u varies with the number of vertices in the graph,1
" for grade-school addition, the domain-specific encoder is
fenc(q, i1, i2, i3, i4, at) =mlp ([q(1, i1), q(2, i2), q(3, i3), q(4, i4), at(1), at(2), at(3)]),
where the environment q ∈ r4×n×k is a scratch-pad that contains four rows (the first input number, the second input number, the carry bits, and the output) and n columns",0
g,1
" the pointers at index i1 and i2 are used to compare the pair of numbers considered during the bubble sweep, swapping them if the number at i1 is greater than that in i2",0
2,1
"
is critical for implementing recursion, since it permits us to restrict our attention to the currently relevant recursive call, ignoring irrelevant details about other contexts",0
"
arg 2 (object to copy): env stack lo peek, env stack hi peek, phi, plo, ppivot − 1, ppivot + 1, reset
env stack lo peek and env stack hi peek represent qstacklo(pstacklo − 1) and qstackhi(pstackhi − 1), respectively",1
"program descriptions calls arguments toposort perform topological
sort on graph traverse, next start, write, move
none
traverse traverse graph until stack is empty check child, explore none check child check if a white child exists; if so, set childlist[vactive] to point to it move none explore repeatedly traverse subgraphs until stack is empty stack, check child, write, move none stack interact with stack, either pushing or popping write, move push, pop next start move pstart until reaching a white vertex",1
" empirically, we observe that the learned recursive programs solve all valid inputs with 100% accuracy after training on a very small number of examples, out-performing previous generalization results",0
"
thus far, to evaluate the efficacy of neural models on programming tasks, the only metric that has been used is generalization of expected behavior to inputs of greater complexity (vinyals   (2015), kaiser & sutskever (2015), reed & de freitas (2016), graves   (2016), zaremba   (2016))",0
" training on the full recursive trace leads to perfect generalization, as shown in section 4",0
"
we construct input problems by splitting into two cases: one case in which the left column contains a null value and another in which the left column does not contain any null values",0
" each step during an execution of the program does one of three things: (1) another subprogram along with associated arguments is called, as in line 10, (2) the program writes to the environment if it is primitive, as in line 8, or (3) the loop is terminated if the return probability exceeds a threshold α, after which the stack frame is popped and control is returned to the caller",1
" in this version of curriculum learning, even though the inputs are gradually becoming more complex, the semantics of the program is succinct and does not change",0
"
algorithm 3 recursive quicksort 1: initialize an array a to sort",1
"
as a concrete instantiation, we show in this paper that we can enable recursive neural programs in the npi model, and thus enable perfectly generalizable neural programs for tasks such as sorting where the original, non-recursive npi program fails",1
3,0
" the architecture usually possesses some form of memory, which could be internal (such as the hidden state of a recurrent neural network) or external (such as a discrete “scratch pad” or a memory block with differentiable access)",0
" at any given time, the npi has access to the values referred to by two pointers, represented by q(1, i1) and q(1, i2),",1
"
recursion drastically reduces the number of configurations we need to consider during the verification phase and makes the proof tractable, because it introduces structure that eliminates infinitely long sequences of step inputs that would otherwise need to be considered",0
" a single element of an execution trace consists of a step input-step output pair, which can be synthesized from algorithm 1: this corresponds to, for a given time-step, the step input tuple (e, p, a) and step output tuple (r, p2, a2)",0
 the memory updates of these neural programs are so complex and interdependent that it is difficult to reason about the behaviors of the learned neural program under previously unseen situations (such as problems with longer inputs),0
" on the other hand, the non-recursive version has low accuracy, beginning from size 5, and fails completely for graphs of size 8 and beyond",0
" both the non-recursive and recursive learned programs generalize on all graphs we tried, up to 120 vertices",1
"
is critical for implementing recursion, since it permits us to restrict our attention to the currently relevant recursive call, ignoring irrelevant details about other contexts",0
" consequently, it is infeasible to construct a verification set that covers all cases, and the verification procedure we have described is inadequate",0
g,1
"
in this paper, we propose to resolve these issues by explicitly incorporating recursion into neural architectures",0
" as an application, we incorporate recursion into the neural programmer-interpreter architecture and consider four sample tasks: grade-school addition, bubble sort, topological sort, and quicksort",0
"
base cases and reduction rules for addition",1
"
directed acyclic graphs are structurally more diverse than inputs in the two tasks of grade-school addition and bubble sort",1
" childlist ∈ nu is a vector of pointers, where childlist[i] points to the next child under consideration for vertex i",0
" here as a concrete and general example, we consider a general neural programming architecture (npa), similar to neural programmer-interpreter (npi) in reed & de freitas (2016)",0
", to change the value of phi) none described belowstack",1
" for example, a step input-step output pair in lines 2 and 3 of the left-hand side of figure 1 is (add1, write out 1)",0
" each step during an execution of the program does one of three things: (1) another subprogram along with associated arguments is called, as in line 10, (2) the program writes to the environment if it is primitive, as in line 8, or (3) the loop is terminated if the return probability exceeds a threshold α, after which the stack frame is popped and control is returned to the caller",0
" for example, a step input-step output pair in lines 2 and 3 of the left-hand side of figure 1 is (add1, write out 1)",0
 we use the lomuto partition scheme; the logic for the recursive trace is shown in algorithm 3,0
"
algorithm 2 shows the topological sort task of interest",1
"
in what follows, we describe the way in which we constructed npi training traces so as to make them contain recursive elements and thus enable npi to learn recursive programs",1
 a1a0 + bnbn−1 ,1
" 9: else 10: function run(e, p2, a2)
a description of the inference procedure is given in algorithm 1",0
"
thus far, to evaluate the efficacy of neural models on programming tasks, the only metric that has been used is generalization of expected behavior to inputs of greater complexity (vinyals   (2015), kaiser & sutskever (2015), reed & de freitas (2016), graves   (2016), zaremba   (2016))",0
 the training set for addition contains 200 traces,0
"
in addition, none of the current approaches to neural programming provide a method or even aim to enable provable guarantees about generalization",0
" the write operation has the following arguments:
arg 1 (object to write): env stack lo, env stack hi, phi, plo
env stack lo and env stack hi represent qstacklo(pstacklo) and qstackhi(pstackhi), respectively",1
"
move",0
" as described in the verification procedure, it is possible to prove our learned recursive program generalizes perfectly by testing on an appropriate set of problem inputs, i",0
"
through careful reasoning about the possible set of environment observations created by all valid inputs, and how each of the operations in the execution trace affects the environment, we can construct v using the procedure described in section 3",1
"
arg 2 (increment or decrement): up, down1 // top level topological sort call 2 toposort() { 3 while (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 } 0 11 traverse() { 12 check_child() 13 explore() 14 } 15 16 check_child() { 17 while (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 18 move(childlist[vactive], up) 19 } 20 21 explore() { 22 do 23 if (qcolor(dag[vactive][childlist[vactive]]) is white): 24 write(color_next, color_grey) 25 stack(push) 26 write(save) 27 write(active_neighb) 28 move(childlist[vsave], up) 29 else: 30 write(color_curr, color_black) 31 write(result) 32 move(presult, up) 33 if(pstack == 1): 34 break 35 else: 36 stack(pop) 37 check_child() 38 while (true) 39 } 40 41 stack(op) { 42 if (op == push): 43 write(stack_push) 44 move(pstack, up) 45 46 if (op == pop): 47 write(active_stack) 48 write(stack_pop) 49 move(pstack, down) 50 } 51 52 next_start() { 53 while(qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 54 move(pstart, up) 55 }altered recursive functions  // top level topological sort call 2 toposort() { 3 if (qcolor(pstart) is a valid color): // color invalid when all vertices explored 4 write(active_start) 5 write(color_curr, color_grey) 6 traverse() 7 move(pstart, up) 8 next_start() 9 toposort() // recursive call 0 } 11 12 check_child() { 13 if (qcolor(dag[vactive][childlist[vactive]]) is not white and is not invalid): // color invalid when all children explored 14 move(childlist[vactive], up) 15 check_child() // recursive call 16 } 17 18 explore() { 19 if (qcolor(dag[vactive][childlist[vactive]]) is white): 20 write(color_next, color_grey) 21 stack(push) 22 write(save) 23 write(active_neighb) 24 move(childlist[vsave], up) 25 else: 26 write(color_curr, color_black) 27 write(result) 28 move(presult, up) 29 if(pstack == 1): 30 return 31 else: 32 stack(pop) 33 check_child() 34 explore() // recursive call 35 } 36 37 next_start() { 38 if (qcolor(pstart) is not white and is not invalid): // color invalid when all vertices explored 39 move(pstart, up) 40 next_start() // recursive call 41 }algorithm 4 iterative quicksort 1: initialize an array a to sort and two empty stacks slo and shi",1
"
bubble sort",1
" we show that by modifying the training procedure, we enable the npi model to learn recursive neural programs",0
" we prove every member of f is added properly, given that subproblems s = {anan−1 + bnbn−1, an−1an−2 + bn−1bn−2, ",1
g,1
" thus, recursion helps decompose a problem and makes it easier to reason about a program’s behavior for previously unseen situations such as longer inputs",0
6,0
" the core lstm module’s hidden state is preserved over all these add1 calls, and it is difficult to interpret with certainty what happens over longer timesteps without concretely evaluating the lstm with an input of that length",0
 the environment and return probability are omitted for readability,1
" the general notion of recursion has been an important concept in many domains, including mathematics and computer science",0
" by iterating with this procedure, and then running p on the input observation set that we obtain for the entry point function, we can obtain v precisely",0
 ,0
 we instantiated recursion for the neural programmerinterpreter by changing the training traces,0
"
an alternative way of representing the environment slice is to expose the values of the absolute vertices to the model; however, this makes it difficult to scale the model to larger graphs, since large vertex values are not seen during training time",0
 we describe the recursive re-formulation of traces for two tasks from the original npi paper—grade-school addition and bubble sort,0
 this makes it difficult to reason about what the model will do when given complex inputs,0
e,1
 we created a program set that reflects the semantics of algorithm 2,0
 this modification also enables us to sort arrays containing arbitrary comparable elements,0
"
move",0
" in all experiments, α is set to 0",0
"6) on the verification set, and ensure they match the traces produced by the true program p ",1
" thus, we demonstrate that recursion enables provably perfect generalization for different tasks, including addition, topological sort, quicksort, and a variant of bubble sort",1
"
recursion drastically reduces the number of configurations we need to consider during the verification phase and makes the proof tractable, because it introduces structure that eliminates infinitely long sequences of step inputs that would otherwise need to be considered",1
" it is straightforward to deal with all other operations, which do not induce partial environment states",0
"
the dag is represented as an adjacency list where dag[i][j] refers to the j-th child of vertex i",0
 the training set for addition contains 200 traces,1
"
we refer the reader to the appendix for the non-recursive trace generating functions",1
"
algorithm 2 depth first search topological sort 1: color all vertices white",0
" for brevity, we refer the reader to the appendix for information about the program set and non-recursive and recursive trace-generating functions for quicksort",0
 this makes it difficult to reason about what the model will do when given complex inputs,0
" that is to say, the network does not learn the true program semantics",0
"
arg 2 (auxiliary variable): color grey, color black
color grey and color black color the given vertex grey and black, respectively",0
" bstep, used in bubble, compares pairs of numbers, continuously moving the bubble pointers once to the right each time until reaching the end of the array",1
" for example, a step input-step output pair in lines 2 and 3 of the left-hand side of figure 1 is (add1, write out 1)",0
" this agrees with the generalization of non-recursive addition in reed & de freitas (2016), where they reported generalization up to 3000 digits",0
"
base cases and reduction rules for quicksort",0
", the trace corresponding to the array [3,2])",0
 u varies with the number of vertices in the graph,0
 the model described in the training setup of section 4 (trained on 6 traces) was verified to be correct via the matching procedure described in section 4,1
 add1 is a subprogram that adds the current column (writing the appropriate digit to the output row and carrying a bit to the next column if needed),0
" as with the original npi architecture, the experiments for this paper always used a 3-tuple of integers at = (at(1), at(2), at(3))",0
" numerous models have been proposed for learning programs; to name a few, this includes the differentiable neural computer (graves  , 2016), neural turing machine (graves  , 2014), neural gpu (kaiser & sutskever, 2015), neural programmer (neelakantan  , 2015), pointer network (vinyals  , 2015), hierarchical attentive memory (andrychowicz & kurach, 2016), and neural random access machine (kurach  , 2016)",1
 our result shows that the training procedure and the npi architecture is capable of generalizing from the step input-output pairs seen in the training data to the unseen ones present in the verification set,1
" the single-digit multiplication task in zaremba   (2016), the bubble sort task in reed & de freitas (2016), and the graph tasks in graves   (2016))",0
"
note that the recursion for quicksort is not purely tail recursive and therefore represents a more complex kind of recursion that is harder to learn than in the previous tasks",1
" full recursion, in addition to making the aforementioned tail recursive call, adds two additional recursive calls; bstep and lshift are made tail recursive",0
" if yes, then this indicates that the learned neural program achieves provably perfect generalization",0
 add1 is a subprogram that adds the current column (writing the appropriate digit to the output row and carrying a bit to the next column if needed),0
" the non-recursive and partially recursive versions are unable to sort long arrays, beyond length 8",0
" in this version of curriculum learning, even though the inputs are gradually becoming more complex, the semantics of the program is succinct and does not change",1
"
an alternative way of representing the environment slice is to expose the values of the absolute vertices to the model; however, this makes it difficult to scale the model to larger graphs, since large vertex values are not seen during training time",0
5,1
" in practice, we can additionally use tail recursion optimization to avoid problems with the call stack growing too deep",1
" lshift, used in reset, shifts the pointers left until reaching the start token",1
"
base cases and reduction rules for bubble sort",0
"
one common strategy to improve generalization is to use curriculum learning, where the model is trained on inputs of gradually increasing complexity",1
"
algorithm 3 recursive quicksort 1: initialize an array a to sort",0
"2 at any given time, the npi has access to values pointed to by four pointers in each of the four rows, represented by q(1, i1), q(2, i2), q(3, i3), and q(4, i4)",0
" to make an analogy to mdps, this procedure is analogous to how value iteration obtains the correct value for each state starting from any initialization",1
" this guarantees that the step input sequences considered during verification contain all step input sequences which arise during execution of an unseen problem in f , leading to generalization to any problem in f ",0
"
it is sufficient to construct problems where every transition between two adjacent columns is covered",1
 the first is the actual model architecture,0
g,1
 the sequence of primitive operations (move and write operations) for the non-recursive and recursive versions are exactly the same,0
" as described in the verification procedure, it is possible to prove our learned recursive program generalizes perfectly by testing on an appropriate set of problem inputs, i",0
"
recursion drastically reduces the number of configurations we need to consider during the verification phase and makes the proof tractable, because it introduces structure that eliminates infinitely long sequences of step inputs that would otherwise need to be considered",1
" bstep, used in bubble, compares pairs of numbers, continuously moving the bubble pointers once to the right each time until reaching the end of the array",0
" for tasks other than addition, it is difficult to analytically determine the verification set, so instead, we randomly generate input candidates until they completely cover the base cases and reduction rules",0
" we make the trace recursive by adding a tail recursive call into the trace for the add program after calling add1 and lshift, the original paper uses k = 10, but we found it necessary to augment the range with an end token, in order to terminate properly",1
" the verification set can be very small, as we found a 10-element array ([8,2,1,2,0,8,5,8,3,7]) is sufficient to cover all of v ",1
"
base cases and reduction rules for bubble sort",0
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,1
" the move operation has the following arguments:
arg 1 (pointer): pstacklo, pstackhi, pj , ppivot
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values",1
" observe that partially recursive does slightly better than non-recursive for the setting in which the length of the array is 3, and that the fully recursive version is able to sort every array given to it",0
" partial recursion only adds a tail recursive call to bubblesort after bubble and reset, similar to the tail recursive call described previously for addition",1
"6) on the verification set, and ensure they match the traces produced by the true program p ",0
"as discussed in section 2, the neural programmer-interpreter (npi) is an instance of a neural programmer architecture and hence it naturally supports recursion",0
" a single element of an execution trace consists of a step input-step output pair, which can be synthesized from algorithm 1: this corresponds to, for a given time-step, the step input tuple (e, p, a) and step output tuple (r, p2, a2)",0
 this modification also enables us to sort arrays containing arbitrary comparable elements,0
" for these models, table 2 presents results on randomly generated dags of varying graph sizes (varying in the number of vertices)",1
"we train using the adam optimizer and use a 2-layer lstm and task-specific state encoders for the external environments, as described in reed & de freitas (2016)",0
" recall that this verification procedure cannot be performed for the non-recursive versions, since the propagation of the hidden state in the core lstm module makes reasoning difficult and so we would need to check an unbounded number of examples",1
"
the non-recursive trace loops on cycles of add1 and lshift",1
 we use the lomuto partition scheme; the logic for the recursive trace is shown in algorithm 3,0
"1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 while (pstacklo 6= 1): 6 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 7 stack(stack_pop) 8 else: 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 stack(stack_pop) 12 partition() 13 stack(stack_push_call2) 14 stack(stack_push_call1) 15 } 16 17 partition() { 18 set_pivot_lo() 19 set_j_lo() 20 compswap_loop() 21 swap(ppivot, phi) 22 set_j_null() 23 } 24 25 compswap_loop() { 26 while (pj 6= phi): 27 compswap() 28 move(pj, up) 29 } 30 31 compswap() { 32 if (a[pj ] ≤ a[phi]): 33 swap(ppivot, pj) 34 move(ppivot, up) 35 } 36 37 stack(op) { 38 if (op == stack_push_call1): 39 write(env_stack_lo, plo) 40 write(env_stack_hi, ppivot − 1) 41 move(pstacklo, up) 42 move(pstackhi, up) 43 44 if (op == stack_push_call2): 45 write(env_stack_lo, ppivot + 1) 46 write(env_stack_hi, phi) 47 move(pstacklo, up) 48 move(pstackhi, up) 49 50 if (op == stack_pop): 51 write(env_stack_lo, reset) 52 write(env_stack_hi, reset) 53 move(pstacklo, down) 54 move(pstackhi, down) 55 }altered recursive functions 1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 6 partition() 7 stack(stack_push_call2) 8 stack(stack_push_call1) 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 quicksort() // recursive call 12 stack(stack_pop) 13 write(phi, env_stack_hi_peek) 14 write(plo, env_stack_lo_peek) 15 quicksort() // recursive call 16 stack(stack_pop) 17 } 18 19 compswap_loop() { 20 if (pj 6= phi): 21 compswap() 22 move(pj, up) 23 compswap_loop() // recursive call 24 }in this section, we describe the space of base cases and reduction rules that must be covered for each of the four sample tasks, in order to create the verification set",0
 we then construct a verification set of size 73 by ensuring that randomly generated graphs cover the analytically derived v ,1
" this makes it so that we only need to consider the vertices in the subgraph that are currently relevant for computing the sort, allowing simpler reasoning about behavior for large graphs",1
"1, we hypothesize the non-recursive programs do not generalize well because they have learned spurious dependencies specific to the training set, such as length of the input problems",0
 we choose to implement a topological sort task for graphs,1
"
as in line 13 of the right-hand side of figure 1",0
" this agrees with the generalization of non-recursive addition in reed & de freitas (2016), where they reported generalization up to 3000 digits",0
 the logic for the non-recursive trace is shown in algorithm 4 in the appendix,0
" at end, pointer ppivot is moved to the pivot compswap loop, move pivot lo, move j lo, swap
none
compswap loop runs the for loop inside the partition function compswap, move none compswap compares a[pivot] ≤ a[j]; if so, perform a swap and increment ppivot swap, move none set pivot lo sets ppivot to lo index none none set j lo sets pj to lo index none none set j null sets pj to −∞ none none stack pushes lo/hi states
onto stacks slo and shi according to argument (described below)
write, move described below
move moves pointer one unit up or down none described below swap swaps elements at given array indices none described below write write a value either to stack (e",0
 the training set for addition contains 200 traces,0
" however, to be sure that the generated v is complete (covers all the cases needed), we need to check all pairs of observations seen in adjacent step inputs (in particular, those before and after a primitive function call), in a similar way as if we were constructing v from scratch",1
" in particular, in the explore function, adding a tail recursive call resets and stores the hidden states associated with vertices in a stack-like fashion",0
" this is possible if and only if the graph has no directed cycles; that is to say, it must be a directed acyclic graph (dag)",0
 stack push call2 pushes pivot + 1 and hi to qstacklo and qstackhi,0
" as with the others, we apply the procedure described in section 3",0
" lshift, used in reset, shifts the pointers left until reaching the start token",0
" in this version, there is a dependence on length: bstep and lshift are called a number of times equivalent to one less than the length of the input array, in bubble and reset respectively",0
g,0
"
algorithm 2 shows the topological sort task of interest",0
" both the non-recursive and recursive learned programs generalize on all graphs we tried, up to 120 vertices",1
" given a precise definition of p , it may be possible to automate the generation of v from p in future work",1
" in this version of curriculum learning, even though the inputs are gradually becoming more complex, the semantics of the program is succinct and does not change",0
" although the model is exposed to more and more data, it might learn spurious and overly complex representations of the program, as suggested in zaremba   (2016)",1
 ,0
" the assumption of leading 0’s can be easily removed, at the cost of slightly increasing the size of the verification set",0
" to the best of our knowledge, this is the first time verification has been applied to a neural program, providing provable guarantees about its behavior",0
" we then construct problem configurations that span all possible valid environment states (for instance, in order to force the carry bit in a column to be 1, one can add the sum “1+9” in the column to the right)",0
" on the other hand, the non-recursive version has low accuracy, beginning from size 5, and fails completely for graphs of size 8 and beyond",0
 stack pop pushes −∞ values to qstacklo and qstackhi,0
 ,1
"
base cases and reduction rules for quicksort",0
" thus, we demonstrate that recursion enables provably perfect generalization for different tasks, including addition, topological sort, quicksort, and a variant of bubble sort",1
"
base cases and reduction rules for quicksort",0
" full recursion, in addition to making the aforementioned tail recursive call, adds two additional recursive calls; bstep and lshift are made tail recursive",0
 we instantiated recursion for the neural programmerinterpreter by changing the training traces,1
" a single element of an execution trace consists of a step input-step output pair, which can be synthesized from algorithm 1: this corresponds to, for a given time-step, the step input tuple (e, p, a) and step output tuple (r, p2, a2)",1
" formally, a function exhibits recursive behavior when it possesses two properties: (1) base cases— terminating scenarios that do not use recursion to produce answers; (2) a set of rules that reduces all other problems toward the base cases",1
"
is critical for implementing recursion, since it permits us to restrict our attention to the currently relevant recursive call, ignoring irrelevant details about other contexts",1
 we then construct a verification set of size 73 by ensuring that randomly generated graphs cover the analytically derived v ,0
"program descriptions calls arguments quicksort run the quicksort
routine in place for the array a, for indices from lo to hi non-recursive: partition, stack, write recursive: same as non-recursive version, along with quicksort implicitly: array a to sort, lo, hi
partition runs the partition function",1
"
base cases and reduction rules for quicksort",1
 the first is the actual model architecture,0
" the move operation has the following arguments:
arg 1 (pointer): pstacklo, pstackhi, pj , ppivot
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values",0
"1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 while (pstacklo 6= 1): 6 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 7 stack(stack_pop) 8 else: 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 stack(stack_pop) 12 partition() 13 stack(stack_push_call2) 14 stack(stack_push_call1) 15 } 16 17 partition() { 18 set_pivot_lo() 19 set_j_lo() 20 compswap_loop() 21 swap(ppivot, phi) 22 set_j_null() 23 } 24 25 compswap_loop() { 26 while (pj 6= phi): 27 compswap() 28 move(pj, up) 29 } 30 31 compswap() { 32 if (a[pj ] ≤ a[phi]): 33 swap(ppivot, pj) 34 move(ppivot, up) 35 } 36 37 stack(op) { 38 if (op == stack_push_call1): 39 write(env_stack_lo, plo) 40 write(env_stack_hi, ppivot − 1) 41 move(pstacklo, up) 42 move(pstackhi, up) 43 44 if (op == stack_push_call2): 45 write(env_stack_lo, ppivot + 1) 46 write(env_stack_hi, phi) 47 move(pstacklo, up) 48 move(pstackhi, up) 49 50 if (op == stack_pop): 51 write(env_stack_lo, reset) 52 write(env_stack_hi, reset) 53 move(pstacklo, down) 54 move(pstackhi, down) 55 }altered recursive functions 1 initialize plo to 1 and phi to n (length of array) 2 initialize pj to −∞ 3 4 quicksort() { 5 if (qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1)): 6 partition() 7 stack(stack_push_call2) 8 stack(stack_push_call1) 9 write(phi, env_stack_hi_peek) 0 write(plo, env_stack_lo_peek) 11 quicksort() // recursive call 12 stack(stack_pop) 13 write(phi, env_stack_hi_peek) 14 write(plo, env_stack_lo_peek) 15 quicksort() // recursive call 16 stack(stack_pop) 17 } 18 19 compswap_loop() { 20 if (pj 6= phi): 21 compswap() 22 move(pj, up) 23 compswap_loop() // recursive call 24 }in this section, we describe the space of base cases and reduction rules that must be covered for each of the four sample tasks, in order to create the verification set",1
"
we refer the reader to the appendix for the non-recursive trace generating functions",0
 the model described in the training setup of section 4 (trained on 6 traces) was verified to be correct via the matching procedure described in section 4,1
"
algorithm 2 depth first search topological sort 1: color all vertices white",0
"
note that the training set can often be considerably smaller than the verification set, and despite this, the learned model can still pass the entire verification set",1
 the training set for addition contains 200 traces,0
 recursion can be implemented differently for different neural programming models,1
" childlist ∈ nu is a vector of pointers, where childlist[i] points to the next child under consideration for vertex i",1
 this procedure verifies that all base cases and reductions are handled properly by the model via explicit tests,0
" the second is the training procedure, which consists of the form of the training data and the optimization process",0
" for each task, given the verification set, we check the traces and results of the learned, to-be-verified neural program (described in section 4",0
" this is highly undesirable, since being able to provide the correct answer in all possible settings is one of the most important aspects of any learned neural program",0
g,1
 errors arise due to undesirable and uninterpretable dependencies and associations the architecture learns to store in some high-dimensional hidden state,0
 reset represents a −∞ value,0
" in computer science, recursion (as opposed to iteration) involves solving a larger problem by combining solutions to smaller instances of the same problem",1
" for brevity, we refer the reader to the appendix for further details on the program set and non-recursive and recursive trace-generating functions used for topological sort",1
"
we construct input problems by splitting into two cases: one case in which the left column contains a null value and another in which the left column does not contain any null values",0
 the program terminates when seeing no numbers in the current column,0
g,1
" if it is too small, then execution of the program on some input might require evaluation of m(i) on some i /∈ v , and so the behavior ofm(i) might deviate from p (i)",0
" et+1 ∼ fenv(et, pt, at)",1
" finally, the boolean pstack == 1 is used to check whether the stack is empty in line 18 of algorithm 2",0
" the recursive version of topological sort solves all graph instances we tried, from graphs of size 5 through 70",0
" training on the full recursive trace leads to perfect generalization, as shown in section 4",0
 we outline how to construct this set,0
 this is due to the properties of the training set – we trained on 2 traces synthesized from arrays of length 7 and 1 trace synthesized from an array of length 6,0
" also, compared to the bubble pointers in bubble sort, the pointers that perform the comparison for quicksort (the compswap function) are usually not adjacent to each other, making quicksort less local than bubble sort",1
" on the other hand, the non-recursive version has low accuracy, beginning from size 5, and fails completely for graphs of size 8 and beyond",1
" 2: initialize lo and hi to be 1 and n, where n is the length of a",0
" our implementation uses two stacks qstacklo and
qstackhi, each in ru , that store the arguments to the recursive quicksort calls in algorithm 3; before each recursive call, the appropriate arguments are popped off the stack and written to plo and phi",1
"
we experiment with two levels of recursion—partial and full",0
"
base cases and reduction rules for topological sort",0
"
move",1
"
grade school addition",1
" for brevity, we refer the reader to the appendix for further details on the program set and non-recursive and recursive trace-generating functions used for topological sort",1
" in future work, we would like to decrease supervision and construct models that are capable of coming up with recursive abstractions themselves",0
" for each length, we test each program on 30 randomly generated problems",0
" given verification sets that cover all the base cases and reduction rules, we can provide proofs that these learned programs generalize perfectly",1
 this algorithm is a variant of depth first search,0
" nevertheless, we would like the model to quickly learn to represent the right semantics of the underlying program from a small number of training examples, not an exhaustive number of them",1
" an example of part of an addition task trace, written in shorthand, is given in figure 1",0
" the write operation has the following arguments:
arg 1 (object to write): env stack lo, env stack hi, phi, plo
env stack lo and env stack hi represent qstacklo(pstacklo) and qstackhi(pstackhi), respectively",1
"we show that if we incorporate recursion, the learned npi programs can achieve provably perfect generalization for different tasks",1
" the outputs rt, pt+1, at+1 are used to determine the next action, as described in algorithm 1",0
" we made the assumption of equivalent lengths in order to parametrize the input format with respect to length, but this assumption can be removed as well",0
" the outputs rt, pt+1, at+1 are used to determine the next action, as described in algorithm 1",1
"
given a verification set, we can then run the model on the verification set to check if the produced traces and results are correct",1
" the move operation has the following arguments:
arg 1 (pointer): pstacklo, pstackhi, pj , ppivot
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values",0
" 3: 4: function partition(a, lo, hi) 5: pivot = lo 6: for j ∈ [lo, hi− 1] : do 7: if a[j] ≤ a[hi] then 8: swap a[pivot] with a[j] 9: pivot = pivot+ 1 0: swap a[pivot] with a[hi] 11: return pivot 12: 13: function quicksort(a, lo, hi) 14: while slo and shi are not empty: do 15: pop states off slo and shi, writing them to lo and hi",1
"
grade-school addition",0
 the program terminates when seeing no numbers in the current column,0
" nevertheless, we would like the model to quickly learn to represent the right semantics of the underlying program from a small number of training examples, not an exhaustive number of them",0
" in fact, it is one of our future directions to explore new ways to train a recursive neural program without providing explicit training execution traces or with only partial or non-recursive traces",0
" consequently, it is infeasible to construct a verification set that covers all cases, and the verification procedure we have described is inadequate",1
 this algorithm is a variant of depth first search,0
" as aforementioned, the npi model naturally supports recursion",0
"
bubble sort",1
 we leave this as future work to devise a verification procedure more appropriate to this setting,0
 indentation indicates the stack is one level deeper than before,0
"
for addition, we analytically determine the verification set",0
 the training set for addition contains 200 traces,1
" lshift, used in reset, shifts the pointers left until reaching the start token",0
"2 at any given time, the npi has access to values pointed to by four pointers in each of the four rows, represented by q(1, i1), q(2, i2), q(3, i3), and q(4, i4)",1
" we also hope to decrease supervision, for example by training with only partial or non-recursive traces, and to develop novel neural programming architectures integrated directly with a notion of recursion",1
" a topological sort is a linear ordering of vertices such that for every directed edge (u, v) from u to v, u comes before v in the ordering",0
" however, note that there is no provable guarantee that the non-recursive learned program will generalize to all inputs, whereas we show later that the recursive learned program has a provable guarantee of perfect generalization",0
"
it is sufficient to construct problems where every transition between two adjacent columns is covered",1
"
in what follows, we describe the way in which we constructed npi training traces so as to make them contain recursive elements and thus enable npi to learn recursive programs",0
"
quicksort",0
" this agrees with the generalization of non-recursive addition in reed & de freitas (2016), where they reported generalization up to 3000 digits",1
" the write operation has the following arguments:
arg 1 (object to write): env stack lo, env stack hi, phi, plo
env stack lo and env stack hi represent qstacklo(pstacklo) and qstackhi(pstackhi), respectively",0
 3: begin traversing from vertex 1 in the dag,1
"
we refer the reader to the appendix for the non-recursive trace generating functions",0
 the maximum problem length in this training set is 3 (e,0
"as discussed in section 2, the neural programmer-interpreter (npi) is an instance of a neural programmer architecture and hence it naturally supports recursion",0
" that is to say, the network does not learn the true program semantics",1
"6) on the verification set, and ensure they match the traces produced by the true program p ",0
"
we would also like to point out that in this paper, we provide as an example one way to train a recursive neural program, by providing a certain training execution trace to the npi model",0
 some functional programming languages go so far as not to define any looping constructs but rely solely on recursion to enable repeated execution of the same code,0
 ,0
" as with the others, we apply the procedure described in section 3",1
"
in addition, none of the current approaches to neural programming provide a method or even aim to enable provable guarantees about generalization",1
" in fact, it is one of our future directions to explore new ways to train a recursive neural program without providing explicit training execution traces or with only partial or non-recursive traces",1
" also, compared to the bubble pointers in bubble sort, the pointers that perform the comparison for quicksort (the compswap function) are usually not adjacent to each other, making quicksort less local than bubble sort",1
" , a1a0 + b1b0} are added properly",0
" note that recursion helps make this process tractable, because we only need to test a finite number of inputs to show that the model will work correctly on inputs of unbounded complexity",1
 we find that recursion makes it easier for the network to learn the right program and generalize to unknown situations,0
 indentation indicates the stack is one level deeper than before,0
" however, models that make use of this strategy eventually fail after a certain level of complexity (e",1
" if no white vertex is found, the entire execution is terminated move none write write a value either to environment (e",0
 we then construct a verification set of size 73 by ensuring that randomly generated graphs cover the analytically derived v ,1
" for example, for the addition task, the model is trained on short inputs and then tested on its ability to sum inputs with much longer numbers of digits",0
 we adapt machinery from the original paper slightly to fit our needs,0
this material is in part based upon work supported by the national science foundation under grant no,0
" m is considered to have perfect generalization if m can give the right answer for any input, such as inputs of arbitrary length",0
" the move operation has the following arguments:
arg 1 (pointer): presult, pstack, pstart, childlist[vactive], childlist[vsave]
note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 5 values",0
", qstacklo or qstackhi) or to pointer (e",1
" for problems where the base cases may be extremely large or infinite, such as certain forms of motor control, recursion can still help reduce the problem of generalization to these two aspects and make the generalization problem significantly simpler to handle and reason about",1
" if it is too small, then execution of the program on some input might require evaluation of m(i) on some i /∈ v , and so the behavior ofm(i) might deviate from p (i)",1
g,0
" training on the full recursive trace leads to perfect generalization, as shown in section 4",0
 we outline how to construct this set,0
" in the non-recursive trace, there are four functions that can be made recursive—toposort, check child, explore, and next start, and we add a tail recursive call to each of these functions in order to make the recursive trace",1
" via the recursive call, we effectively forget that the column just added exists, since the recursive call to add starts with a new hidden state for the lstm controller",0
"
the training set for topological sort contains 6 traces, with one synthesized from a graph of size 5 and the rest synthesized from graphs of size 7",0
e,1
"
it is sufficient to construct problems where every transition between two adjacent columns is covered",1
 we leave this as future work to devise a verification procedure more appropriate to this setting,1
"when constructing a neural network for the purpose of learning a program, there are two orthogonal aspects to consider",0
"
inside bubble and reset, there are two operations that can be made recursive",0
"
base cases and reduction rules for bubble sort",0
" plo and phi point to the lo and hi indices of the array, as in algorithm 3",1
"
by reasoning about the possible set of environment observations created by all valid inputs, we construct v using the procedure described in section 3",1
" provably perfect generalization implies the model will behave correctly, given any valid input",1
" the architecture usually possesses some form of memory, which could be internal (such as the hidden state of a recurrent neural network) or external (such as a discrete “scratch pad” or a memory block with differentiable access)",0
"
to construct this set, by using the reference implementation of each subprogram, we construct a mapping between two sets of environment observations: the first set consists of all observations that can occur at the beginning of a particular subprogram’s invocation, and the second set contains the observations at the end of that subprogram",0
" we prove every member of f is added properly, given that subproblems s = {anan−1 + bnbn−1, an−1an−2 + bn−1bn−2, ",1
 we find that recursion makes it easier for the network to learn the right program and generalize to unknown situations,0
 recursion enables provable guarantees on neural programs’ behavior without needing to exhaustively enumerate all possible inputs to the programs,1
"
an alternative way of representing the environment slice is to expose the values of the absolute vertices to the model; however, this makes it difficult to scale the model to larger graphs, since large vertex values are not seen during training time",1
" in future work, we would like to decrease supervision and construct models that are capable of coming up with recursive abstractions themselves",1
"1, we hypothesize the non-recursive programs do not generalize well because they have learned spurious dependencies specific to the training set, such as length of the input problems",1
" to make an analogy to mdps, this procedure is analogous to how value iteration obtains the correct value for each state starting from any initialization",1
"1, we hypothesize the non-recursive programs do not generalize well because they have learned spurious dependencies specific to the training set, such as length of the input problems",0
" in practice, we can additionally use tail recursion optimization to avoid problems with the call stack growing too deep",0
" the only model, to our knowledge, that does not train on input-output pairs is the neural programmer-interpreter (reed & de freitas, 2016), which trains on synthetic execution traces",1
" at any given time, the npi has access to the values referred to by two pointers, represented by q(1, i1) and q(1, i2),",0
" furthermore, given a verification input set that covers all base cases and reduction rules, we can formally prove that the learned neural programs achieve perfect generalization after verifying its behavior on the verification input set",1
" however, note that there is no provable guarantee that the non-recursive learned program will generalize to all inputs, whereas we show later that the recursive learned program has a provable guarantee of perfect generalization",0
" this makes it so that we only need to consider the vertices in the subgraph that are currently relevant for computing the sort, allowing simpler reasoning about behavior for large graphs",0
" in computer science, recursion (as opposed to iteration) involves solving a larger problem by combining solutions to smaller instances of the same problem",1
3 to construct v and then empirically create a verification set which covers v ,0
 almost all architectures train on program input/output pairs,0
" if the program is primitive, the next environmental state et+1 will be affected by pt and at, i",0
"
we note that for tasks with very large input domains, such as ones involving mnist digits or speech samples, the state space of base cases and reduction rules could be prohibitively large, possibly infinite",0
as mentioned in section 2,1
" each time a subprogram is called, the stack depth increases",0
" partial recursion only adds a tail recursive call to bubblesort after bubble and reset, similar to the tail recursive call described previously for addition",0
 u varies with the number of vertices in the graph,0
" this is highly undesirable, since being able to provide the correct answer in all possible settings is one of the most important aspects of any learned neural program",0
 the original version of the bubblesort implementation exposes the values within the array,0
" qcolor(pstart) contains the color of the current start vertex, used in the evaluation of the condition in the while loop in line 5 of algorithm 2",1
" formally, a function exhibits recursive behavior when it possesses two properties: (1) base cases— terminating scenarios that do not use recursion to produce answers; (2) a set of rules that reduces all other problems toward the base cases",1
" for each graph size, we test the learned programs on 30 randomly generated dags",0
 the degree for any vertex in the dag is variable,0
 these pointers are referred to as bubble pointers,0
" we call this set of inputs the verification set, sv ",1
"
grade-school addition",1
" note that recursion helps make this process tractable, because we only need to test a finite number of inputs to show that the model will work correctly on inputs of unbounded complexity",1
 this is the first time one can provide provable guarantees of perfect generalization for neural programs,0
 our result shows that the training procedure and the npi architecture is capable of generalizing from the step input-output pairs seen in the training data to the unseen ones present in the verification set,1
 the program terminates when seeing no numbers in the current column,0
"
for topological sort, the domain-specific encoder is
fenc(dag,qcolor, pstack, pstart, vactive, childlist, at)
=mlp ([qcolor(pstart), qcolor(dag[vactive][childlist[vactive]]), pstack == 1, at(1), at(2), at(3)]),
where qcolor ∈ ru×4 is a scratch-pad that contains u rows, each containing one of four colors (white, gray, black, invalid) with one-hot encoding",0
" for each task, given the verification set, we check the traces and results of the learned, to-be-verified neural program (described in section 4",0
" this makes it so that we only need to consider the vertices in the subgraph that are currently relevant for computing the sort, allowing simpler reasoning about behavior for large graphs",0
"
bubble sort",0
" 3: 4: function quicksort(a, lo, hi) 5: if lo < hi: then 6: p = partition(a, lo, hi) 7: quicksort(a, lo, p− 1) 8: quicksort(a, p+ 1, hi) 9: 10: function partition(a, lo, hi) 11: pivot = lo 12: for j ∈ [lo, hi− 1] : do 13: if a[j] ≤ a[hi] then 14: swap a[pivot] with a[j] 15: pivot = pivot+ 1 16: swap a[pivot] with a[hi] 17: return pivot
for quicksort, the domain-specific encoder is
fenc(qarray, qstacklo, qstackhi, plo, phi, pstacklo, pstackhi, ppivot, pj , at) =
mlp ([qarray(pj) ≤ qarray(phi), pj == phi, qstacklo(pstacklo − 1) < qstackhi(pstackhi − 1), pstacklo == 1, at(1), at(2), at(3)]),
whereqarray ∈ ru×11 is a scratch-pad that contains u rows, each containing one of 11 values (one of the numbers 0 through 9 or an invalid state)",1
 18: push lo and p− 1 to slo and shi,0
" in particular, given that a recursion is defined by two properties as mentioned before, the base cases and the set of reduction rules, we can prove a recursive neural program generalizes perfectly if we can prove that (1) it performs correctly on the base cases; (2) it learns the reduction rules correctly",0
 table 4 demonstrates that the accuracy of the non-recursive program degrades sharply when moving from arrays of length 7 to arrays of length 8,0
 b1b0 where no carry operations occur,1
"program descriptions calls arguments toposort perform topological
sort on graph traverse, next start, write, move
none
traverse traverse graph until stack is empty check child, explore none check child check if a white child exists; if so, set childlist[vactive] to point to it move none explore repeatedly traverse subgraphs until stack is empty stack, check child, write, move none stack interact with stack, either pushing or popping write, move push, pop next start move pstart until reaching a white vertex",0
 the memory updates of these neural programs are so complex and interdependent that it is difficult to reason about the behaviors of the learned neural program under previously unseen situations (such as problems with longer inputs),1
" m , as described in algorithm 1, processes the sequence of step inputs by using an lstm",0
" as a consequence, our learned neural programs empirically achieve perfect generalization from a very small number of training examples",0
" lshift moves the four pointers to the left, to move to the next column",1
" however, the authors of npi did not consider explicitly the notion of recursion and as a consequence, did not learn recursive programs",0
" the architecture usually possesses some form of memory, which could be internal (such as the hidden state of a recurrent neural network) or external (such as a discrete “scratch pad” or a memory block with differentiable access)",0
" lshift, used in reset, shifts the pointers left until reaching the start token",0
"
we experiment with two levels of recursion—partial and full",0
" finally, the boolean pstack == 1 is used to check whether the stack is empty in line 18 of algorithm 2",0
 ,1
" for brevity, we refer the reader to the appendix for information about the program set and non-recursive and recursive trace-generating functions for quicksort",0
"
an alternative way of representing the environment slice is to expose the values of the absolute vertices to the model; however, this makes it difficult to scale the model to larger graphs, since large vertex values are not seen during training time",1
" 2: initialize lo and hi to be 1 and n, where n is the length of a",0
" the partial recursion is not enough for perfect generalization, as will be presented later in section 4",0
" at each time step, the core controller can decide to select one of the programs to call with certain arguments",1
" at end, pointer ppivot is moved to the pivot compswap loop, move pivot lo, move j lo, swap
none
compswap loop runs the for loop inside the partition function compswap, move none compswap compares a[pivot] ≤ a[j]; if so, perform a swap and increment ppivot swap, move none set pivot lo sets ppivot to lo index none none set j lo sets pj to lo index none none set j null sets pj to −∞ none none stack pushes lo/hi states
onto stacks slo and shi according to argument (described below)
write, move described below
move moves pointer one unit up or down none described below swap swaps elements at given array indices none described below write write a value either to stack (e",1
" in order to claim a proof, we must verify the model produces correct behavior over all base cases and reductions, as described in section 2",0
"
an alternative method is to run p on many different program inputs and then observe step input sequences which occur, to create v ",1
", to color a vertex) or variable (e",0
"
move",0
" the partial recursion is not enough for perfect generalization, as will be presented later in section 4",0
" given a precise definition of p , it may be possible to automate the generation of v from p in future work",1
"
note that the training set can often be considerably smaller than the verification set, and despite this, the learned model can still pass the entire verification set",1
 table 4 demonstrates that the accuracy of the non-recursive program degrades sharply when moving from arrays of length 7 to arrays of length 8,1
" for each task, given the verification set, we check the traces and results of the learned, to-be-verified neural program (described in section 4",0
 recursion enables provably perfect generalization,0
" as a consequence, our learned neural programs empirically achieve perfect generalization from a very small number of training examples",1
" here as a concrete and general example, we consider a general neural programming architecture (npa), similar to neural programmer-interpreter (npi) in reed & de freitas (2016)",0
" that is to say, the network does not learn the true program semantics",0
" for problems where the base cases may be extremely large or infinite, such as certain forms of motor control, recursion can still help reduce the problem of generalization to these two aspects and make the generalization problem significantly simpler to handle and reason about",1
" 2: initialize lo and hi to be 1 and n, where n is the length of a",1
" for each length, we test each program on 30 randomly generated problems",0
" consequently, there is no concept of length relevant to the problem, which has traditionally been an important focus of length-based curriculum learning",1
"
we experiment with two levels of recursion—partial and full",0
" however, the authors of npi did not consider explicitly the notion of recursion and as a consequence, did not learn recursive programs",0
" in fact, we argue that recursion is an essential element for neural programs to generalize, and makes it tractable to prove the generalization of neural programs",1
"
inside bubble and reset, there are two operations that can be made recursive",0
" if a white vertex is found, set pstart to point to it; this signifies the start of a traversal of a new connected component",1
 this paper is the first (to our knowledge) to investigate the important problem of provable generalization properties of neural programs,1
" this is highly undesirable, since being able to provide the correct answer in all possible settings is one of the most important aspects of any learned neural program",1
"
the three environment observations aid with control flow in algorithm 2",0
" in this section, we give a brief review of the npi architecture from reed & de freitas (2016) as background",0
"
quicksort",1
" to make an analogy to mdps, this procedure is analogous to how value iteration obtains the correct value for each state starting from any initialization",0
 we then construct a verification set of size 73 by ensuring that randomly generated graphs cover the analytically derived v ,0
" hence, if all subproblems in s are added correctly, we have proven that any member of f will be added correctly, thus eliminating an infinite family of inputs that need to be tested",0
 some functional programming languages go so far as not to define any looping constructs but rely solely on recursion to enable repeated execution of the same code,0
we now report on generalization for the varying tasks,1
"
note that the recursion for quicksort is not purely tail recursive and therefore represents a more complex kind of recursion that is harder to learn than in the previous tasks",1
" if the step output specifies a primitive function call, we need to reason about how it can affect the environment so as to change the observation in the next step input",1
" if a white vertex is found, set pstart to point to it; this signifies the start of a traversal of a new connected component",0
" in this pair, the step input runs a subprogram add1 that has no arguments, and the step output contains a program write that has arguments of out and 1",1
" the stack operation has the following arguments:
arg 1 (operation): stack push call1, stack push call2, stack pop
stack push call1 pushes lo and pivot−1 toqstacklo andqstackhi",0
"
the operations we need to be concerned most about are carry and lshift, which induce partial environment states spanning two columns",1
", the verification set",1
 the environment and return probability are omitted for readability,1
" for non-primitive subprograms, we can update the observation-to-observation mapping currently associated with the subprogram and then apply that mapping to the current set",1
" , a1a0 + b1b0} are added properly",1
" if no white vertex is found, the entire execution is terminated move none write write a value either to environment (e",0
"
as mentioned in section 1, all approaches to neural programming today fare poorly on this generalization issue",0
" to the best of our knowledge, this is the first time verification has been applied to a neural program, providing provable guarantees about its behavior",1
 figure 2 shows examples of traces for the different versions of bubble sort,0
" plo and phi point to the lo and hi indices of the array, as in algorithm 3",0
" as with the original npi architecture, the experiments for this paper always used a 3-tuple of integers at = (at(1), at(2), at(3))",0
"
for addition, we analytically determine the verification set",1
 recursion enables provable guarantees on neural programs’ behavior without needing to exhaustively enumerate all possible inputs to the programs,1
we emphasize the overall goal of this work is to enable the learning of a recursive program,0
" note that the argument is the identity of the pointer, not what the pointer points to; in other words, arg 1 can only take one of 4 values, and arg 2 can only take one of 7 values",1
"
we construct input problems by splitting into two cases: one case in which the left column contains a null value and another in which the left column does not contain any null values",0
 ,0
", to change the value of vactive) none described below move move a pointer (e",0
" if the step output specifies a primitive function call, we need to reason about how it can affect the environment so as to change the observation in the next step input",0
g,0
" by iterating with this procedure, and then running p on the input observation set that we obtain for the entry point function, we can obtain v precisely",1
g,0
" in this version, there is a dependence on length: bstep and lshift are called a number of times equivalent to one less than the length of the input array, in bubble and reset respectively",0
" as with the others, we apply the procedure described in section 3",1
" in fact, it is one of our future directions to explore new ways to train a recursive neural program without providing explicit training execution traces or with only partial or non-recursive traces",0
" numerous models have been proposed for learning programs; to name a few, this includes the differentiable neural computer (graves  , 2016), neural turing machine (graves  , 2014), neural gpu (kaiser & sutskever, 2015), neural programmer (neelakantan  , 2015), pointer network (vinyals  , 2015), hierarchical attentive memory (andrychowicz & kurach, 2016), and neural random access machine (kurach  , 2016)",0
 recursion can be implemented differently for different neural programming models,0
"
grade-school addition",1
" at each time step, the core controller can decide to select one of the programs to call with certain arguments",0
", the trace corresponding to the array [3,2])",0
