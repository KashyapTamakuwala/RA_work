{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel, utils\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification,Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "transformers.logging.set_verbosity_error()\n",
    "utils.logging.set_verbosity_error()  # Suppress standard warnings\n",
    "\n",
    "from bertviz import model_view, head_view\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "import eli5\n",
    "from eli5.lime import TextExplainer\n",
    "# from captum.attr import IntegratedGradients\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_train_data = pd.read_csv('../sentence_split/train_sentence_data.csv')\n",
    "sen_test_data = pd.read_csv('../sentence_split/test_sentence_data.csv')\n",
    "sen_val_data = pd.read_csv('../sentence_split/val_sentence_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_train_data = pd.read_csv(\"../complete_sentence/train_processed_data.csv\")\n",
    "doc_test_data = pd.read_csv(\"../complete_sentence/train_processed_data.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_train_data = sen_train_data.drop(['File_id'],axis=1)\n",
    "sen_test_data = sen_test_data.drop(['File_id'],axis=1)\n",
    "sen_val_data = sen_val_data.drop(['File_id'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_20(data):\n",
    "#     data_1 = data.loc[data['Status'] ==1].iloc[:20]\n",
    "#     data_2 = data.loc[data['Status'] ==0].iloc[:20]\n",
    "#     frames = [data_1, data_2]\n",
    "#     return pd.concat(frames)\n",
    "\n",
    "# sen_train_data = get_20(sen_train_data) \n",
    "# sen_test_data = get_20(sen_test_data)\n",
    "# sen_val_data = get_20(sen_val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_train_text = doc_train_data.Paper_text.values.tolist()\n",
    "doc_train_status = doc_train_data.Status.values.tolist()\n",
    "\n",
    "doc_test_text = doc_test_data.Paper_text.values.tolist()\n",
    "doc_test_status = doc_test_data.Status.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_train_data=sen_train_data.sample(frac = 1)\n",
    "sen_test_data=sen_test_data.sample(frac = 1)\n",
    "sen_val_data=sen_val_data.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_train_data.to_csv('Train_auto.csv',index=False)\n",
    "sen_test_data.to_csv('Test_auto.csv',index=False)\n",
    "sen_val_data.to_csv('val_auto.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    83160\n",
       "1    55044\n",
       "Name: Status, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen_train_data.Status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /home/015953353/.cache/huggingface/datasets/csv/default-893d49f64543a901/0.0.0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a499a96c9e5a49febb32c0d4c81958b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "424dbb52445840648cbeb7b294096c54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1e8203e64ac42a9986f29112c906244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/015953353/.cache/huggingface/datasets/csv/default-893d49f64543a901/0.0.0. Subsequent calls will reuse this data.\n",
      "Downloading and preparing dataset csv/default to /home/015953353/.cache/huggingface/datasets/csv/default-198c1f644b0bafd4/0.0.0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71af137872474fd786bf63e2e9e54f8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93961a45205143da80d5ccb90dd6feac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7628672b1c3a43128a8b6cface3d89a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/015953353/.cache/huggingface/datasets/csv/default-198c1f644b0bafd4/0.0.0. Subsequent calls will reuse this data.\n",
      "Downloading and preparing dataset csv/default to /home/015953353/.cache/huggingface/datasets/csv/default-ba837e8abe899d4a/0.0.0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cbc58e021694be0a951858efc1d5385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "343160f9201644a7bbe884c6c22e7d7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd0a205eec3c410f99198ed5805a92e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/015953353/.cache/huggingface/datasets/csv/default-ba837e8abe899d4a/0.0.0. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "# Load train and validation datasets from CSV files\n",
    "train_dataset = Dataset.from_csv('Train_auto.csv')\n",
    "val_dataset = Dataset.from_csv('val_auto.csv')\n",
    "test_dataset = Dataset.from_csv('Test_auto.csv')\n",
    "\n",
    "# Rename the columns to 'text' and 'label' to match the expected format for sequence classification\n",
    "train_dataset = train_dataset.rename_column('Sentence', 'text').rename_column('Status', 'label')\n",
    "val_dataset = val_dataset.rename_column('Sentence', 'text').rename_column('Status', 'label')\n",
    "test_dataset = test_dataset.rename_column('Sentence', 'text').rename_column('Status', 'label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1db06d737ce148fc80451aedbe1a3973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/138204 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d7be22a315640339b79f9e0feec959a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15444 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "433aab6714c047a182c9edb14ee10652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15048 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', cache_dir='../bert-base-uncased')\n",
    "\n",
    "# Define a function to tokenize the text and create input sequences\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding=True, truncation=True,max_length=512)\n",
    "\n",
    "# Apply the tokenization function to the train and validation datasets\n",
    "train_dataset = train_dataset.map(tokenize, batched=True, batch_size=len(train_dataset))\n",
    "val_dataset = val_dataset.map(tokenize, batched=True, batch_size=len(val_dataset))\n",
    "test_dataset = test_dataset.map(tokenize, batched=True, batch_size=len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', cache_dir='../bert-base-uncased', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/015953353/ra_env/lib/python3.7/site-packages/torch/cuda/__init__.py:88: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "/home/015953353/ra_env/lib/python3.7/site-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results/bert',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/015953353/ra_env/lib/python3.7/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                     # the instantiated Transformers model to be trained\n",
    "    args=training_args,              # training arguments, defined above\n",
    "    train_dataset=train_dataset,     # training dataset\n",
    "    eval_dataset=val_dataset,        # evaluation dataset\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate(test_dataset)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_adapter(texts: List[str]):\n",
    "    \n",
    "    all_scores = []\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), 64)):\n",
    "\n",
    "        batch = texts[i:i+64]\n",
    "        #print(batch)\n",
    "        \n",
    "        # use bert encoder to tokenize text \n",
    "        encoded_input = tokenizer(batch, \n",
    "          return_tensors='pt', \n",
    "          padding=True, \n",
    "          truncation=True, \n",
    "          max_length=model.config.max_position_embeddings-2)\n",
    "\n",
    "        # run the model\n",
    "        output = model(**encoded_input)\n",
    "        #print(output)\n",
    "        # by default this model gives raw logits rather \n",
    "        # than a nice smooth softmax so we apply it ourselves here\n",
    "        scores = output[0].softmax(1).detach().numpy()\n",
    "        #print(scores)\n",
    "\n",
    "        all_scores.extend(scores)\n",
    "\n",
    "    return np.array(all_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = sen_test_data.Sentence\n",
    "lab = sen_test_data.Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specter_exp = TextExplainer(n_samples=5000, random_state=42)\n",
    "# specter_exp.fit(sen[0], model_adapter)\n",
    "# specter_exp.explain_prediction(target_names=list(model.config.id2label.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = specter_exp.explain_weights()\n",
    "\n",
    "# sen_positivie_words= {}\n",
    "\n",
    "# for i in weights.targets[0].feature_weights.pos:\n",
    "#     #print(i.feature)\n",
    "#     g = sen_positivie_words.get(i.feature,-1)\n",
    "#     if g==-1:\n",
    "#         sen_positivie_words[i.feature]=1\n",
    "#     else:\n",
    "#         sen_positivie_words[i.feature]+=1\n",
    "        \n",
    "\n",
    "# sen_negative_words= {}\n",
    "\n",
    "# for i in weights.targets[0].feature_weights.neg:\n",
    "#     #print(i.feature)\n",
    "#     g = sen_negative_words.get(i.feature,-1)\n",
    "#     if g==-1:\n",
    "#         sen_negative_words[i.feature]=1\n",
    "#     else:\n",
    "#         sen_negative_words[i.feature]+=1\n",
    "\n",
    "        \n",
    "# wc = WordCloud(background_color=\"white\",width=1000,height=1000,relative_scaling=0.5,normalize_plurals=False).generate_from_frequencies(sen_positivie_words)\n",
    "# plt.imshow(wc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wc = WordCloud(background_color=\"white\",width=1000,height=1000,relative_scaling=0.5,normalize_plurals=False).generate_from_frequencies(sen_negative_words)\n",
    "# plt.imshow(wc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## very inportant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(weights.targets[0].feature_weights.pos[0].weight,weights.targets[0].feature_weights.pos[0].feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = specter_exp.explain_prediction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_train_data = pd.read_csv(\"./complete_sentence/train_processed_data.csv\")\n",
    "doc_test_data = pd.read_csv(\"./complete_sentence/train_processed_data.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_train_data = get_20(doc_train_data)\n",
    "doc_test_data = get_20(doc_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_train_text = doc_train_data.Paper_text.values.tolist()\n",
    "doc_train_status = doc_train_data.Status.values.tolist()\n",
    "\n",
    "doc_test_text = doc_test_data.Paper_text.values.tolist()\n",
    "doc_test_status = doc_test_data.Status.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Padding(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self,model,tokenizer):\n",
    "        self.model = model # Configure model to return attention values\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mxlenght = 400\n",
    "        print('\\n>>>>>>>init() called.\\n')\n",
    "\n",
    "    def fit(self, X, y = None):\n",
    "        print('\\n>>>>>>>fit() called.\\n')\n",
    "        return self\n",
    "    \n",
    "\n",
    "    def transform(self, X, y = None):\n",
    "        embeddings = []\n",
    "        for i in tqdm(X):\n",
    "            temp = [] \n",
    "            sentence_list = i.split(\".\")\n",
    "            for i in sentence_list:\n",
    "                if len(i)==0:\n",
    "                    continue\n",
    "                encoded_input = self.tokenizer(i,return_tensors='pt', padding=True, truncation=True,max_length=model.config.max_position_embeddings-2)\n",
    "                output = self.model(**encoded_input)\n",
    "                pred = np.argmax(output[0].softmax(1).detach().numpy())\n",
    "                temp.append(pred)\n",
    "            size = self.mxlenght - len(temp)\n",
    "            if size > 0:\n",
    "                temp.extend([-1]*size)\n",
    "            elif size < 0:\n",
    "                temp = temp[0:self.mxlenght]\n",
    "            else:\n",
    "                pass\n",
    "            embeddings.append(temp)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe1 = Pipeline(steps=[\n",
    "                       ('Documnet Embeddings', Padding(model,tokenizer)), # this will trigger a call to __init__\n",
    "                       ('Logistic Regression', LogisticRegression(solver='lbfgs')),\n",
    "\n",
    "])\n",
    "\n",
    "pipe1.fit(doc_train_text, doc_train_status)\n",
    "pipe1.score(doc_test_text,doc_test_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target =['Reject','Accept']\n",
    "doc = doc_test_text[0]\n",
    "pipe1_exp = TextExplainer(n_samples=10,random_state=42)\n",
    "pipe1_exp.fit(doc, pipe1.predict_proba)\n",
    "pipe1_exp.show_prediction(target_names= target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = pipe1_exp.explain_weights(top= None)\n",
    "positivie_words= {}\n",
    "\n",
    "for i in weights.targets[0].feature_weights.pos:\n",
    "    #print(i.feature)\n",
    "    g = positivie_words.get(i.feature,-1)\n",
    "    if g==-1:\n",
    "        positivie_words[i.feature]=1\n",
    "    else:\n",
    "        positivie_words[i.feature]+=1\n",
    "        \n",
    "\n",
    "negative_words= {}\n",
    "\n",
    "for i in weights.targets[0].feature_weights.neg:\n",
    "    #print(i.feature)\n",
    "    g = negative_words.get(i.feature,-1)\n",
    "    if g==-1:\n",
    "        negative_words[i.feature]=1\n",
    "    else:\n",
    "        negative_words[i.feature]+=1\n",
    "# print(weights.targets[0].feature_weights.pos[0].weight,weights.targets[0].feature_weights.pos[0].feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(background_color=\"white\",width=1000,height=1000,relative_scaling=0.5,normalize_plurals=False).generate_from_frequencies(positivie_words)\n",
    "plt.imshow(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(background_color=\"white\",width=1000,height=1000,relative_scaling=0.5,normalize_plurals=False).generate_from_frequencies(negative_words)\n",
    "plt.imshow(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_prediction(doc):\n",
    "    #print(doc)\n",
    "    y_pred = pipe1.predict_proba([doc])[0]\n",
    "    tar =['Reject','Accept']\n",
    "    for target, prob in zip(tar, y_pred):\n",
    "        print(\"{:.3f} {}\".format(prob, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_prediction(doc_test_text[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ra_env",
   "language": "python",
   "name": "ra_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
